[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2511.04804v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04804v2",
                "title": "Simplex-FEM Networks (SiFEN): Learning A Triangulated Function Approximator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplex-FEM Networks (SiFEN): Learning A Triangulated Function Approximator"
                },
                "updated": "2026-01-08T17:07:57Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    7,
                    57,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04804v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial predictor that represents f: R^d -> R^k as a globally C^r finite-element field on a learned simplicial mesh in an optionally warped input space. Each query activates exactly one simplex and at most d+1 basis functions via barycentric coordinates, yielding explicit locality, controllable smoothness, and cache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with a light invertible warp and trains end-to-end with shape regularization, semi-discrete OT coverage, and differentiable edge flips. Under standard shape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic FEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic approximation tasks, tabular regression/classification, and as a drop-in head on compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter budgets, improves calibration (lower ECE/Brier), and reduces inference latency due to geometric locality. These properties make SiFEN a compact, interpretable, and theoretically grounded alternative to dense MLPs and edge-spline networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial predictor that represents f: R^d -> R^k as a globally C^r finite-element field on a learned simplicial mesh in an optionally warped input space. Each query activates exactly one simplex and at most d+1 basis functions via barycentric coordinates, yielding explicit locality, controllable smoothness, and cache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with a light invertible warp and trains end-to-end with shape regularization, semi-discrete OT coverage, and differentiable edge flips. Under standard shape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic FEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic approximation tasks, tabular regression/classification, and as a drop-in head on compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter budgets, improves calibration (lower ECE/Brier), and reduces inference latency due to geometric locality. These properties make SiFEN a compact, interpretable, and theoretically grounded alternative to dense MLPs and edge-spline networks."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T20:49:13Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    20,
                    49,
                    13,
                    3,
                    310,
                    0
                ],
                "arxiv_comment": "We will improve our work soon",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Chaymae Yahyati"
                    },
                    {
                        "name": "Ismail Lamaakal"
                    },
                    {
                        "name": "Khalid El Makkaoui"
                    },
                    {
                        "name": "Ibrahim Ouahbi"
                    },
                    {
                        "name": "Yassine Maleh"
                    }
                ],
                "author_detail": {
                    "name": "Yassine Maleh"
                },
                "author": "Yassine Maleh"
            },
            {
                "id": "http://arxiv.org/abs/2512.07155v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07155v4",
                "title": "CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics"
                },
                "updated": "2026-01-08T10:29:58Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    10,
                    29,
                    58,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07155v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07155v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T04:39:12Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    4,
                    39,
                    12,
                    0,
                    342,
                    0
                ],
                "arxiv_comment": "Please visit our project page at https://cmlab-korea.github.io/CHIMERA/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Dahyeon Kye"
                    },
                    {
                        "name": "Jeahun Sung"
                    },
                    {
                        "name": "Minkyu Jeon"
                    },
                    {
                        "name": "Jihyong Oh"
                    }
                ],
                "author_detail": {
                    "name": "Jihyong Oh"
                },
                "author": "Jihyong Oh"
            },
            {
                "id": "http://arxiv.org/abs/2601.04786v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04786v1",
                "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentOCR: Reimagining Agent History via Optical Self-Compression"
                },
                "updated": "2026-01-08T10:10:20Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    10,
                    10,
                    20,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04786v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T10:10:20Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    10,
                    10,
                    20,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Work in progress",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Lang Feng"
                    },
                    {
                        "name": "Fuchao Yang"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Xin Cheng"
                    },
                    {
                        "name": "Haiyang Xu"
                    },
                    {
                        "name": "Zhenglin Wan"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An"
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.10367v4",
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "updated": "2026-01-08T09:19:47Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    9,
                    19,
                    47,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.10367v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.10367v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Client-side metadata caching has long been considered an effective method for accelerating metadata operations in distributed file systems (DFSs). However, we have found that client-side state (e.g., caching) is not only ineffective but also consumes valuable memory resources in the deep learning pipelines. We thus propose FalconFS, a DFS optimized for deep learning pipelines with the stateless-client architecture. Specifically, instead of performing client-side path resolution and caching, FalconFS efficiently resolves paths on the server side using hybrid metadata indexing and lazy namespace replication. FalconFS also boosts server concurrency with concurrent request merging and provides easy deployment with VFS shortcut. Evaluations against CephFS and Lustre show that FalconFS achieves up to 5.72$\\times$ throughput for small file read/write and up to 12.81$\\times$ throughput for deep learning model training. FalconFS has been running in Huawei autonomous driving system's production environment with 10,000 NPUs for one year and has been open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for accelerating metadata operations in distributed file systems (DFSs). However, we have found that client-side state (e.g., caching) is not only ineffective but also consumes valuable memory resources in the deep learning pipelines. We thus propose FalconFS, a DFS optimized for deep learning pipelines with the stateless-client architecture. Specifically, instead of performing client-side path resolution and caching, FalconFS efficiently resolves paths on the server side using hybrid metadata indexing and lazy namespace replication. FalconFS also boosts server concurrency with concurrent request merging and provides easy deployment with VFS shortcut. Evaluations against CephFS and Lustre show that FalconFS achieves up to 5.72$\\times$ throughput for small file read/write and up to 12.81$\\times$ throughput for deep learning model training. FalconFS has been running in Huawei autonomous driving system's production environment with 10,000 NPUs for one year and has been open-sourced."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "arxiv_comment": "Accepted by NSDI'26",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen"
            },
            {
                "id": "http://arxiv.org/abs/2601.04719v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04719v1",
                "title": "GPU-Accelerated INT8 Quantization for KV Cache Compression in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-Accelerated INT8 Quantization for KV Cache Compression in Large Language Models"
                },
                "updated": "2026-01-08T08:35:56Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    8,
                    35,
                    56,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04719v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The key-value (KV) cache in large language models presents a significant memory bottleneck during inference, growing linearly with sequence length and often exceeding the memory footprint of model weights themselves. We implement and evaluate GPU-accelerated INT8 quantization for KV cache compression, achieving 4$\\times$ memory reduction with minimal accuracy degradation. We develop four CUDA kernel variants -- naive, tiled, coarsened, and vectorized -- and benchmark them across realistic workload sizes up to 1 billion elements. Our vectorized kernel achieves up to 1,694$\\times$ speedup over CPU baselines while maintaining reconstruction error below 0.004 and attention score error below 0.1 even for 8K-dimensional heads. These results demonstrate that INT8 quantization provides a practical approach for reducing memory pressure in LLM inference with negligible computational overhead (6--58ms) and minimal impact on downstream model behavior",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in large language models presents a significant memory bottleneck during inference, growing linearly with sequence length and often exceeding the memory footprint of model weights themselves. We implement and evaluate GPU-accelerated INT8 quantization for KV cache compression, achieving 4$\\times$ memory reduction with minimal accuracy degradation. We develop four CUDA kernel variants -- naive, tiled, coarsened, and vectorized -- and benchmark them across realistic workload sizes up to 1 billion elements. Our vectorized kernel achieves up to 1,694$\\times$ speedup over CPU baselines while maintaining reconstruction error below 0.004 and attention score error below 0.1 even for 8K-dimensional heads. These results demonstrate that INT8 quantization provides a practical approach for reducing memory pressure in LLM inference with negligible computational overhead (6--58ms) and minimal impact on downstream model behavior"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T08:35:56Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    8,
                    35,
                    56,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Maanas Taneja"
                    },
                    {
                        "name": "Purab Shingvi"
                    }
                ],
                "author_detail": {
                    "name": "Purab Shingvi"
                },
                "author": "Purab Shingvi"
            },
            {
                "id": "http://arxiv.org/abs/2601.04707v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04707v1",
                "title": "MQ-GNN: A Multi-Queue Pipelined Architecture for Scalable and Efficient GNN Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MQ-GNN: A Multi-Queue Pipelined Architecture for Scalable and Efficient GNN Training"
                },
                "updated": "2026-01-08T08:19:47Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    8,
                    19,
                    47,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04707v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/ACCESS.2025.3539976",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Graph Neural Networks (GNNs) are powerful tools for learning graph-structured data, but their scalability is hindered by inefficient mini-batch generation, data transfer bottlenecks, and costly inter-GPU synchronization. Existing training frameworks fail to overlap these stages, leading to suboptimal resource utilization. This paper proposes MQ-GNN, a multi-queue pipelined framework that maximizes training efficiency by interleaving GNN training stages and optimizing resource utilization. MQ-GNN introduces Ready-to-Update Asynchronous Consistent Model (RaCoM), which enables asynchronous gradient sharing and model updates while ensuring global consistency through adaptive periodic synchronization. Additionally, it employs global neighbor sampling with caching to reduce data transfer overhead and an adaptive queue-sizing strategy to balance computation and memory efficiency. Experiments on four large-scale datasets and ten baseline models demonstrate that MQ-GNN achieves up to \\boldmath $\\bm{4.6\\,\\times}$ faster training time and 30% improved GPU utilization while maintaining competitive accuracy. These results establish MQ-GNN as a scalable and efficient solution for multi-GPU GNN training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are powerful tools for learning graph-structured data, but their scalability is hindered by inefficient mini-batch generation, data transfer bottlenecks, and costly inter-GPU synchronization. Existing training frameworks fail to overlap these stages, leading to suboptimal resource utilization. This paper proposes MQ-GNN, a multi-queue pipelined framework that maximizes training efficiency by interleaving GNN training stages and optimizing resource utilization. MQ-GNN introduces Ready-to-Update Asynchronous Consistent Model (RaCoM), which enables asynchronous gradient sharing and model updates while ensuring global consistency through adaptive periodic synchronization. Additionally, it employs global neighbor sampling with caching to reduce data transfer overhead and an adaptive queue-sizing strategy to balance computation and memory efficiency. Experiments on four large-scale datasets and ten baseline models demonstrate that MQ-GNN achieves up to \\boldmath $\\bm{4.6\\,\\times}$ faster training time and 30% improved GPU utilization while maintaining competitive accuracy. These results establish MQ-GNN as a scalable and efficient solution for multi-GPU GNN training."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T08:19:47Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    8,
                    19,
                    47,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "IEEE Access 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Irfan Ullah"
                    },
                    {
                        "name": "Young-Koo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Young-Koo Lee"
                },
                "author": "Young-Koo Lee",
                "arxiv_doi": "10.1109/ACCESS.2025.3539976"
            },
            {
                "id": "http://arxiv.org/abs/2506.08018v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.08018v2",
                "title": "KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache"
                },
                "updated": "2026-01-08T02:42:39Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    2,
                    42,
                    39,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.08018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.08018v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The high memory demands of the Key-Value (KV) Cache during the inference of Large Language Models (LLMs) severely restrict their deployment in resource-constrained platforms. Quantization can effectively alleviate the memory pressure caused by KV Cache. However, existing methods either rely on static one-size-fits-all precision allocation or fail to dynamically prioritize critical KV in long-context tasks, forcing memory-accuracy-throughput tradeoffs. In this work, we propose a novel mixed-precision quantization method for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to evaluate how individual Key and Value projection matrices affect the model loss, enabling layer-specific bit-width allocation for mix-precision quantization. It dynamically prioritizes higher precision for important layers while aggressively quantizing less influential ones, achieving a tunable balance between accuracy and efficiency. KVmix also introduces a dynamic long-context optimization strategy that adaptively keeps full-precision KV pairs for recent pivotal tokens and compresses older ones, achieving high-quality sequence generation with low memory usage. Additionally, KVmix provides efficient low-bit quantization and CUDA kernels to optimize computational overhead. On LLMs such as Llama and Mistral, KVmix achieves near-lossless inference performance with extremely low quantization configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x memory compression and a 5.3x speedup in inference throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high memory demands of the Key-Value (KV) Cache during the inference of Large Language Models (LLMs) severely restrict their deployment in resource-constrained platforms. Quantization can effectively alleviate the memory pressure caused by KV Cache. However, existing methods either rely on static one-size-fits-all precision allocation or fail to dynamically prioritize critical KV in long-context tasks, forcing memory-accuracy-throughput tradeoffs. In this work, we propose a novel mixed-precision quantization method for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to evaluate how individual Key and Value projection matrices affect the model loss, enabling layer-specific bit-width allocation for mix-precision quantization. It dynamically prioritizes higher precision for important layers while aggressively quantizing less influential ones, achieving a tunable balance between accuracy and efficiency. KVmix also introduces a dynamic long-context optimization strategy that adaptively keeps full-precision KV pairs for recent pivotal tokens and compresses older ones, achieving high-quality sequence generation with low memory usage. Additionally, KVmix provides efficient low-bit quantization and CUDA kernels to optimize computational overhead. On LLMs such as Llama and Mistral, KVmix achieves near-lossless inference performance with extremely low quantization configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x memory compression and a 5.3x speedup in inference throughput."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-18T07:04:53Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    7,
                    4,
                    53,
                    6,
                    138,
                    0
                ],
                "arxiv_comment": "AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Fei Li"
                    },
                    {
                        "name": "Song Liu"
                    },
                    {
                        "name": "Weiguo Wu"
                    },
                    {
                        "name": "Shiqiang Nie"
                    },
                    {
                        "name": "Jinyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinyu Wang"
                },
                "author": "Jinyu Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.20687v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20687v2",
                "title": "PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation"
                },
                "updated": "2026-01-08T01:32:26Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    1,
                    32,
                    26,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20687v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transformers operate as horizontal token-by-token scanners; at each generation step, attending to an ever-growing sequence of token-level states. This access pattern increases prefill latency and makes long-context decoding more memory-bound, as KV-cache reads and writes dominate inference time over arithmetic operations. We propose Parallel Hierarchical Operation for TOp-down Networks (PHOTON), a hierarchical autoregressive model that replaces horizontal scanning with vertical, multi-resolution context scanning. PHOTON maintains a hierarchy of latent streams: a bottom-up encoder compresses tokens into low-rate contextual states, while lightweight top-down decoders reconstruct fine-grained token representations in parallel. We further introduce recursive generation that updates only the coarsest latent stream and eliminates bottom-up re-encoding. Experimental results show that PHOTON is superior to competitive Transformer-based language models regarding the throughput-quality trade-off, providing advantages in long-context and multi-query tasks. In particular, this reduces decode-time KV-cache traffic, yielding up to $10^{3}\\times$ higher throughput per unit memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers operate as horizontal token-by-token scanners; at each generation step, attending to an ever-growing sequence of token-level states. This access pattern increases prefill latency and makes long-context decoding more memory-bound, as KV-cache reads and writes dominate inference time over arithmetic operations. We propose Parallel Hierarchical Operation for TOp-down Networks (PHOTON), a hierarchical autoregressive model that replaces horizontal scanning with vertical, multi-resolution context scanning. PHOTON maintains a hierarchy of latent streams: a bottom-up encoder compresses tokens into low-rate contextual states, while lightweight top-down decoders reconstruct fine-grained token representations in parallel. We further introduce recursive generation that updates only the coarsest latent stream and eliminates bottom-up re-encoding. Experimental results show that PHOTON is superior to competitive Transformer-based language models regarding the throughput-quality trade-off, providing advantages in long-context and multi-query tasks. In particular, this reduces decode-time KV-cache traffic, yielding up to $10^{3}\\times$ higher throughput per unit memory."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T19:26:59Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    19,
                    26,
                    59,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "17 pages, 10 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yuma Ichikawa"
                    },
                    {
                        "name": "Naoya Takagi"
                    },
                    {
                        "name": "Takumi Nakagawa"
                    },
                    {
                        "name": "Yuzi Kanazawa"
                    },
                    {
                        "name": "Akira Sakai"
                    }
                ],
                "author_detail": {
                    "name": "Akira Sakai"
                },
                "author": "Akira Sakai"
            },
            {
                "id": "http://arxiv.org/abs/2601.04426v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04426v1",
                "title": "XGrammar 2: Dynamic and Efficient Structured Generation Engine for Agentic LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XGrammar 2: Dynamic and Efficient Structured Generation Engine for Agentic LLMs"
                },
                "updated": "2026-01-07T22:18:51Z",
                "updated_parsed": [
                    2026,
                    1,
                    7,
                    22,
                    18,
                    51,
                    2,
                    7,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04426v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern LLM agents are required to handle increasingly complex structured generation tasks, such as tool calling and conditional structured generation. These tasks are significantly more dynamic than predefined structures, posing new challenges to the current structured generation engines. In this paper, we propose XGrammar 2, a highly optimized structured generation engine for agentic LLMs. XGrammar 2 accelerates the mask generation for these dynamic structured generation tasks through a new dynamic dispatching semantics: TagDispatch. We further introduce a just-in-time (JIT) compilation method to reduce compilation time and a cross-grammar caching mechanism to leverage the common sub-structures across different grammars. Additionally, we extend the previous PDA-based mask generation algorithm to the Earley-parser-based one and design a repetition compression algorithm to handle repetition structures in grammars. Evaluation results show that XGrammar 2 can achieve more than 6x speedup over the existing structured generation engines. Integrated with an LLM inference engine, XGrammar 2 can handle dynamic structured generation tasks with near-zero overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM agents are required to handle increasingly complex structured generation tasks, such as tool calling and conditional structured generation. These tasks are significantly more dynamic than predefined structures, posing new challenges to the current structured generation engines. In this paper, we propose XGrammar 2, a highly optimized structured generation engine for agentic LLMs. XGrammar 2 accelerates the mask generation for these dynamic structured generation tasks through a new dynamic dispatching semantics: TagDispatch. We further introduce a just-in-time (JIT) compilation method to reduce compilation time and a cross-grammar caching mechanism to leverage the common sub-structures across different grammars. Additionally, we extend the previous PDA-based mask generation algorithm to the Earley-parser-based one and design a repetition compression algorithm to handle repetition structures in grammars. Evaluation results show that XGrammar 2 can achieve more than 6x speedup over the existing structured generation engines. Integrated with an LLM inference engine, XGrammar 2 can handle dynamic structured generation tasks with near-zero overhead."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-07T22:18:51Z",
                "published_parsed": [
                    2026,
                    1,
                    7,
                    22,
                    18,
                    51,
                    2,
                    7,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Linzhang Li"
                    },
                    {
                        "name": "Yixin Dong"
                    },
                    {
                        "name": "Guanjie Wang"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Alexander Jiang"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen"
            },
            {
                "id": "http://arxiv.org/abs/2601.04359v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04359v1",
                "title": "PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache"
                },
                "updated": "2026-01-07T19:51:06Z",
                "updated_parsed": [
                    2026,
                    1,
                    7,
                    19,
                    51,
                    6,
                    2,
                    7,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04359v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A unified autoregressive model is a Transformer-based framework that addresses diverse multimodal tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the dominant bottleneck limiting inference efficiency and generative length. Unified autoregressive video generation inherits this limitation. Our analysis reveals that KV-cache tokens exhibit distinct spatiotemporal properties: (i) text and conditioning-image tokens act as persistent semantic anchors that consistently receive high attention, and (ii) attention to previous frames naturally decays with temporal distance. Leveraging these observations, we introduce PackCache, a training-free KV-cache management method that dynamically compacts the KV cache through three coordinated mechanisms: condition anchoring that preserves semantic references, cross-frame decay modeling that allocates cache budget according to temporal distance, and spatially preserving position embedding that maintains coherent 3D structure under cache removal. In terms of efficiency, PackCache accelerates end-to-end generation by 1.7-2.2x on 48-frame long sequences, showcasing its strong potential for enabling longer-sequence video generation. Notably, the final four frames - the portion most impacted by the progressively expanding KV-cache and thus the most expensive segment of the clip - PackCache delivers a 2.6x and 3.7x acceleration on A40 and H200, respectively, for 48-frame videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A unified autoregressive model is a Transformer-based framework that addresses diverse multimodal tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the dominant bottleneck limiting inference efficiency and generative length. Unified autoregressive video generation inherits this limitation. Our analysis reveals that KV-cache tokens exhibit distinct spatiotemporal properties: (i) text and conditioning-image tokens act as persistent semantic anchors that consistently receive high attention, and (ii) attention to previous frames naturally decays with temporal distance. Leveraging these observations, we introduce PackCache, a training-free KV-cache management method that dynamically compacts the KV cache through three coordinated mechanisms: condition anchoring that preserves semantic references, cross-frame decay modeling that allocates cache budget according to temporal distance, and spatially preserving position embedding that maintains coherent 3D structure under cache removal. In terms of efficiency, PackCache accelerates end-to-end generation by 1.7-2.2x on 48-frame long sequences, showcasing its strong potential for enabling longer-sequence video generation. Notably, the final four frames - the portion most impacted by the progressively expanding KV-cache and thus the most expensive segment of the clip - PackCache delivers a 2.6x and 3.7x acceleration on A40 and H200, respectively, for 48-frame videos."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-07T19:51:06Z",
                "published_parsed": [
                    2026,
                    1,
                    7,
                    19,
                    51,
                    6,
                    2,
                    7,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Kunyang Li"
                    },
                    {
                        "name": "Mubarak Shah"
                    },
                    {
                        "name": "Yuzhang Shang"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhang Shang"
                },
                "author": "Yuzhang Shang"
            },
            {
                "id": "http://arxiv.org/abs/2512.24449v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.24449v2",
                "title": "PackKV: Reducing KV Cache Memory Footprint through LLM-Aware Lossy Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PackKV: Reducing KV Cache Memory Footprint through LLM-Aware Lossy Compression"
                },
                "updated": "2026-01-07T19:29:44Z",
                "updated_parsed": [
                    2026,
                    1,
                    7,
                    19,
                    29,
                    44,
                    2,
                    7,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.24449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.24449v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transformer-based large language models (LLMs) have demonstrated remarkable potential across a wide range of practical applications. However, long-context inference remains a significant challenge due to the substantial memory requirements of the key-value (KV) cache, which can scale to several gigabytes as sequence length and batch size increase. In this paper, we present \\textbf{PackKV}, a generic and efficient KV cache management framework optimized for long-context generation. %, which synergistically supports both latency-critical and throughput-critical inference scenarios. PackKV introduces novel lossy compression techniques specifically tailored to the characteristics of KV cache data, featuring a careful co-design of compression algorithms and system architecture. Our approach is compatible with the dynamically growing nature of the KV cache while preserving high computational efficiency. Experimental results show that, under the same and minimum accuracy drop as state-of-the-art quantization methods, PackKV achieves, on average, \\textbf{153.2}\\% higher memory reduction rate for the K cache and \\textbf{179.6}\\% for the V cache. Furthermore, PackKV delivers extremely high execution throughput, effectively eliminating decompression overhead and accelerating the matrix-vector multiplication operation. Specifically, PackKV achieves an average throughput improvement of \\textbf{75.7}\\% for K and \\textbf{171.7}\\% for V across A100 and RTX Pro 6000 GPUs, compared to cuBLAS matrix-vector multiplication kernels, while demanding less GPU memory bandwidth. Code available on https://github.com/BoJiang03/PackKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have demonstrated remarkable potential across a wide range of practical applications. However, long-context inference remains a significant challenge due to the substantial memory requirements of the key-value (KV) cache, which can scale to several gigabytes as sequence length and batch size increase. In this paper, we present \\textbf{PackKV}, a generic and efficient KV cache management framework optimized for long-context generation. %, which synergistically supports both latency-critical and throughput-critical inference scenarios. PackKV introduces novel lossy compression techniques specifically tailored to the characteristics of KV cache data, featuring a careful co-design of compression algorithms and system architecture. Our approach is compatible with the dynamically growing nature of the KV cache while preserving high computational efficiency. Experimental results show that, under the same and minimum accuracy drop as state-of-the-art quantization methods, PackKV achieves, on average, \\textbf{153.2}\\% higher memory reduction rate for the K cache and \\textbf{179.6}\\% for the V cache. Furthermore, PackKV delivers extremely high execution throughput, effectively eliminating decompression overhead and accelerating the matrix-vector multiplication operation. Specifically, PackKV achieves an average throughput improvement of \\textbf{75.7}\\% for K and \\textbf{171.7}\\% for V across A100 and RTX Pro 6000 GPUs, compared to cuBLAS matrix-vector multiplication kernels, while demanding less GPU memory bandwidth. Code available on https://github.com/BoJiang03/PackKV"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-30T20:05:32Z",
                "published_parsed": [
                    2025,
                    12,
                    30,
                    20,
                    5,
                    32,
                    1,
                    364,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Taolue Yang"
                    },
                    {
                        "name": "Youyuan Liu"
                    },
                    {
                        "name": "Xubin He"
                    },
                    {
                        "name": "Sheng Di"
                    },
                    {
                        "name": "Sian Jin"
                    }
                ],
                "author_detail": {
                    "name": "Sian Jin"
                },
                "author": "Sian Jin"
            },
            {
                "id": "http://arxiv.org/abs/2601.03811v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.03811v1",
                "title": "EvalBlocks: A Modular Pipeline for Rapidly Evaluating Foundation Models in Medical Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvalBlocks: A Modular Pipeline for Rapidly Evaluating Foundation Models in Medical Imaging"
                },
                "updated": "2026-01-07T11:16:49Z",
                "updated_parsed": [
                    2026,
                    1,
                    7,
                    11,
                    16,
                    49,
                    2,
                    7,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.03811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.03811v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Developing foundation models in medical imaging requires continuous monitoring of downstream performance. Researchers are burdened with tracking numerous experiments, design choices, and their effects on performance, often relying on ad-hoc, manual workflows that are inherently slow and error-prone. We introduce EvalBlocks, a modular, plug-and-play framework for efficient evaluation of foundation models during development. Built on Snakemake, EvalBlocks supports seamless integration of new datasets, foundation models, aggregation methods, and evaluation strategies. All experiments and results are tracked centrally and are reproducible with a single command, while efficient caching and parallel execution enable scalable use on shared compute infrastructure. Demonstrated on five state-of-the-art foundation models and three medical imaging classification tasks, EvalBlocks streamlines model evaluation, enabling researchers to iterate faster and focus on model innovation rather than evaluation logistics. The framework is released as open source software at https://github.com/DIAGNijmegen/eval-blocks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing foundation models in medical imaging requires continuous monitoring of downstream performance. Researchers are burdened with tracking numerous experiments, design choices, and their effects on performance, often relying on ad-hoc, manual workflows that are inherently slow and error-prone. We introduce EvalBlocks, a modular, plug-and-play framework for efficient evaluation of foundation models during development. Built on Snakemake, EvalBlocks supports seamless integration of new datasets, foundation models, aggregation methods, and evaluation strategies. All experiments and results are tracked centrally and are reproducible with a single command, while efficient caching and parallel execution enable scalable use on shared compute infrastructure. Demonstrated on five state-of-the-art foundation models and three medical imaging classification tasks, EvalBlocks streamlines model evaluation, enabling researchers to iterate faster and focus on model innovation rather than evaluation logistics. The framework is released as open source software at https://github.com/DIAGNijmegen/eval-blocks."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-07T11:16:49Z",
                "published_parsed": [
                    2026,
                    1,
                    7,
                    11,
                    16,
                    49,
                    2,
                    7,
                    0
                ],
                "arxiv_comment": "Accepted at BVM 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jan Tagscherer"
                    },
                    {
                        "name": "Sarah de Boer"
                    },
                    {
                        "name": "Lena Philipp"
                    },
                    {
                        "name": "Fennie van der Graaf"
                    },
                    {
                        "name": "Dr Peeters"
                    },
                    {
                        "name": "Joeran Bosma"
                    },
                    {
                        "name": "Lars Leijten"
                    },
                    {
                        "name": "Bogdan Obreja"
                    },
                    {
                        "name": "Ewoud Smit"
                    },
                    {
                        "name": "Alessa Hering"
                    }
                ],
                "author_detail": {
                    "name": "Alessa Hering"
                },
                "author": "Alessa Hering"
            },
            {
                "id": "http://arxiv.org/abs/2505.22156v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.22156v3",
                "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing"
                },
                "updated": "2026-01-07T10:27:04Z",
                "updated_parsed": [
                    2026,
                    1,
                    7,
                    10,
                    27,
                    4,
                    2,
                    7,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.22156v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.22156v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Although existing model editing methods perform well in recalling exact edit facts, they often struggle in complex scenarios that require deeper semantic understanding rather than mere knowledge regurgitation. Leveraging the strong contextual reasoning abilities of large language models (LLMs), in-context learning (ICL) becomes a promising editing method by comprehending edit information through context encoding. However, this method is constrained by the limited context window of LLMs, leading to degraded performance and efficiency as the number of edits increases. To overcome this limitation, we propose InComeS, a flexible framework that enhances LLMs' ability to process editing contexts through explicit compression and selection mechanisms. Specifically, InComeS compresses each editing context into the key-value (KV) cache of a special gist token, enabling efficient handling of multiple edits without being restricted by the model's context window. Furthermore, specialized cross-attention modules are added to dynamically select the most relevant information from the gist pools, enabling adaptive and effective utilization of edit information. We conduct experiments on diverse model editing benchmarks with various editing formats, and the results demonstrate the effectiveness and efficiency of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing model editing methods perform well in recalling exact edit facts, they often struggle in complex scenarios that require deeper semantic understanding rather than mere knowledge regurgitation. Leveraging the strong contextual reasoning abilities of large language models (LLMs), in-context learning (ICL) becomes a promising editing method by comprehending edit information through context encoding. However, this method is constrained by the limited context window of LLMs, leading to degraded performance and efficiency as the number of edits increases. To overcome this limitation, we propose InComeS, a flexible framework that enhances LLMs' ability to process editing contexts through explicit compression and selection mechanisms. Specifically, InComeS compresses each editing context into the key-value (KV) cache of a special gist token, enabling efficient handling of multiple edits without being restricted by the model's context window. Furthermore, specialized cross-attention modules are added to dynamically select the most relevant information from the gist pools, enabling adaptive and effective utilization of edit information. We conduct experiments on diverse model editing benchmarks with various editing formats, and the results demonstrate the effectiveness and efficiency of our method."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-28T09:20:18Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "arxiv_comment": "18 pages,5 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam"
            },
            {
                "id": "http://arxiv.org/abs/2511.12631v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12631v2",
                "title": "Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation"
                },
                "updated": "2026-01-07T10:24:02Z",
                "updated_parsed": [
                    2026,
                    1,
                    7,
                    10,
                    24,
                    2,
                    2,
                    7,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12631v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-16T14:52:54Z",
                "published_parsed": [
                    2025,
                    11,
                    16,
                    14,
                    52,
                    54,
                    6,
                    320,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yushe Cao"
                    },
                    {
                        "name": "Dianxi Shi"
                    },
                    {
                        "name": "Xing Fu"
                    },
                    {
                        "name": "Xuechao Zou"
                    },
                    {
                        "name": "Haikuo Peng"
                    },
                    {
                        "name": "Xueqi Li"
                    },
                    {
                        "name": "Chun Yu"
                    },
                    {
                        "name": "Junliang Xing"
                    }
                ],
                "author_detail": {
                    "name": "Junliang Xing"
                },
                "author": "Junliang Xing"
            },
            {
                "id": "http://arxiv.org/abs/2601.03700v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.03700v1",
                "title": "ADEPT: Adaptive Dynamic Early-Exit Process for Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADEPT: Adaptive Dynamic Early-Exit Process for Transformers"
                },
                "updated": "2026-01-07T08:34:41Z",
                "updated_parsed": [
                    2026,
                    1,
                    7,
                    8,
                    34,
                    41,
                    2,
                    7,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.03700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.03700v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The inference of large language models imposes significant computational workloads, often requiring the processing of billions of parameters. Although early-exit strategies have proven effective in reducing computational demands by halting inference earlier, they apply either to only the first token in the generation phase or at the prompt level in the prefill phase. Thus, the Key-Value (KV) cache for skipped layers remains a bottleneck for subsequent token generation, limiting the benefits of early exit. We introduce ADEPT (Adaptive Dynamic Early-exit Process for Transformers), a novel approach designed to overcome this issue and enable dynamic early exit in both the prefill and generation phases. The proposed adaptive token-level early-exit mechanism adjusts computation dynamically based on token complexity, optimizing efficiency without compromising performance. ADEPT further enhances KV generation procedure by decoupling sequential dependencies in skipped layers, making token-level early exit more practical. Experimental results demonstrate that ADEPT improves efficiency by up to 25% in language generation tasks and achieves a 4x speed-up in downstream classification tasks, with up to a 45% improvement in performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference of large language models imposes significant computational workloads, often requiring the processing of billions of parameters. Although early-exit strategies have proven effective in reducing computational demands by halting inference earlier, they apply either to only the first token in the generation phase or at the prompt level in the prefill phase. Thus, the Key-Value (KV) cache for skipped layers remains a bottleneck for subsequent token generation, limiting the benefits of early exit. We introduce ADEPT (Adaptive Dynamic Early-exit Process for Transformers), a novel approach designed to overcome this issue and enable dynamic early exit in both the prefill and generation phases. The proposed adaptive token-level early-exit mechanism adjusts computation dynamically based on token complexity, optimizing efficiency without compromising performance. ADEPT further enhances KV generation procedure by decoupling sequential dependencies in skipped layers, making token-level early exit more practical. Experimental results demonstrate that ADEPT improves efficiency by up to 25% in language generation tasks and achieves a 4x speed-up in downstream classification tasks, with up to a 45% improvement in performance."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-07T08:34:41Z",
                "published_parsed": [
                    2026,
                    1,
                    7,
                    8,
                    34,
                    41,
                    2,
                    7,
                    0
                ],
                "arxiv_comment": "11 figures, 8 tables, 22 pages",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Sangmin Yoo"
                    },
                    {
                        "name": "Srikanth Malla"
                    },
                    {
                        "name": "Chiho Choi"
                    },
                    {
                        "name": "Wei D. Lu"
                    },
                    {
                        "name": "Joon Hee Choi"
                    }
                ],
                "author_detail": {
                    "name": "Joon Hee Choi"
                },
                "author": "Joon Hee Choi"
            },
            {
                "id": "http://arxiv.org/abs/2508.00412v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.00412v2",
                "title": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model"
                },
                "updated": "2026-01-07T03:25:48Z",
                "updated_parsed": [
                    2026,
                    1,
                    7,
                    3,
                    25,
                    48,
                    2,
                    7,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.00412v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.00412v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable generative capabilities, particularly benefiting from Transformer architectures that enhance visual and artistic fidelity. However, their inherently sequential denoising process results in high inference latency, limiting their deployment in real-time scenarios. Existing training-free acceleration approaches typically reuse intermediate features at fixed timesteps or layers, overlooking the evolving semantic focus across denoising stages and Transformer blocks.To address this, we propose Sortblock, a training-free inference acceleration framework that dynamically caches block-wise features based on their similarity across adjacent timesteps. By ranking the evolution of residuals, Sortblock adaptively determines a recomputation ratio, selectively skipping redundant computations while preserving generation quality. Furthermore, we incorporate a lightweight linear prediction mechanism to reduce accumulated errors in skipped blocks.Extensive experiments across various tasks and DiT architectures demonstrate that Sortblock achieves over 2$\\times$ inference speedup with minimal degradation in output quality, offering an effective and generalizable solution for accelerating diffusion-based generative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable generative capabilities, particularly benefiting from Transformer architectures that enhance visual and artistic fidelity. However, their inherently sequential denoising process results in high inference latency, limiting their deployment in real-time scenarios. Existing training-free acceleration approaches typically reuse intermediate features at fixed timesteps or layers, overlooking the evolving semantic focus across denoising stages and Transformer blocks.To address this, we propose Sortblock, a training-free inference acceleration framework that dynamically caches block-wise features based on their similarity across adjacent timesteps. By ranking the evolution of residuals, Sortblock adaptively determines a recomputation ratio, selectively skipping redundant computations while preserving generation quality. Furthermore, we incorporate a lightweight linear prediction mechanism to reduce accumulated errors in skipped blocks.Extensive experiments across various tasks and DiT architectures demonstrate that Sortblock achieves over 2$\\times$ inference speedup with minimal degradation in output quality, offering an effective and generalizable solution for accelerating diffusion-based generative models."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-01T08:10:54Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Zeyu Chen"
                    },
                    {
                        "name": "Yi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Liu"
                },
                "author": "Yi Liu"
            },
            {
                "id": "http://arxiv.org/abs/2506.02006v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.02006v2",
                "title": "MorphServe: Efficient and Workload-Aware LLM Serving via Runtime Quantized Layer Swapping and KV Cache Resizing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MorphServe: Efficient and Workload-Aware LLM Serving via Runtime Quantized Layer Swapping and KV Cache Resizing"
                },
                "updated": "2026-01-07T03:04:41Z",
                "updated_parsed": [
                    2026,
                    1,
                    7,
                    3,
                    4,
                    41,
                    2,
                    7,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.02006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.02006v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Efficiently serving large language models (LLMs) under dynamic and bursty workloads remains a key challenge for real-world deployment. Existing serving frameworks and static model compression techniques fail to adapt to workload fluctuations, leading to either service-level objective (SLO) violations under full-precision serving or persistent accuracy degradation with static quantization. We present MorphServe, a dynamic, workload-aware LLM serving framework based on morphological adaptation. MorphServe introduces two asynchronous, token-level runtime mechanisms: quantized layer swapping, which selectively replaces less impactful layers with quantized alternatives during high-load periods, and pressure-aware KV cache resizing, which dynamically adjusts KV cache capacity in response to memory pressure. These mechanisms enable state-preserving transitions with minimum runtime overhead and are fully compatible with modern scheduling and attention techniques. Extensive experiments on Vicuna and Llama family models with real-world workloads demonstrate that MorphServe reduces average SLO violations by 92.45 percent and improves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving, without compromising generation quality. These results establish MorphServe as a practical and elastic solution for LLM deployment in dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large language models (LLMs) under dynamic and bursty workloads remains a key challenge for real-world deployment. Existing serving frameworks and static model compression techniques fail to adapt to workload fluctuations, leading to either service-level objective (SLO) violations under full-precision serving or persistent accuracy degradation with static quantization. We present MorphServe, a dynamic, workload-aware LLM serving framework based on morphological adaptation. MorphServe introduces two asynchronous, token-level runtime mechanisms: quantized layer swapping, which selectively replaces less impactful layers with quantized alternatives during high-load periods, and pressure-aware KV cache resizing, which dynamically adjusts KV cache capacity in response to memory pressure. These mechanisms enable state-preserving transitions with minimum runtime overhead and are fully compatible with modern scheduling and attention techniques. Extensive experiments on Vicuna and Llama family models with real-world workloads demonstrate that MorphServe reduces average SLO violations by 92.45 percent and improves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving, without compromising generation quality. These results establish MorphServe as a practical and elastic solution for LLM deployment in dynamic environments."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-24T06:12:31Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    6,
                    12,
                    31,
                    5,
                    144,
                    0
                ],
                "arxiv_comment": "19 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Zhaoyuan Su"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Tingfeng Lan"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Yue Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yue Cheng"
                },
                "author": "Yue Cheng"
            },
            {
                "id": "http://arxiv.org/abs/2509.25454v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.25454v3",
                "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search"
                },
                "updated": "2026-01-07T02:08:44Z",
                "updated_parsed": [
                    2026,
                    1,
                    7,
                    2,
                    8,
                    44,
                    2,
                    7,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.25454v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.25454v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Although RLVR has become an essential component for developing advanced reasoning skills in language models, contemporary studies have documented training plateaus after thousands of optimization steps, i.e., notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search (MCTS) directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models, while using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although RLVR has become an essential component for developing advanced reasoning skills in language models, contemporary studies have documented training plateaus after thousands of optimization steps, i.e., notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search (MCTS) directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models, while using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-29T20:00:29Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    20,
                    0,
                    29,
                    0,
                    272,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Weihao Xuan"
                    },
                    {
                        "name": "Heli Qi"
                    },
                    {
                        "name": "Ximing Lu"
                    },
                    {
                        "name": "Aaron Tu"
                    },
                    {
                        "name": "Li Erran Li"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi"
            },
            {
                "id": "http://arxiv.org/abs/2601.03199v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.03199v1",
                "title": "DIP: Dynamic In-Context Planner For Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIP: Dynamic In-Context Planner For Diffusion Language Models"
                },
                "updated": "2026-01-06T17:24:16Z",
                "updated_parsed": [
                    2026,
                    1,
                    6,
                    17,
                    24,
                    16,
                    1,
                    6,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.03199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.03199v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion language models (DLMs) have shown strong potential for general natural language tasks with in-context examples. However, due to the bidirectional attention mechanism, DLMs incur substantial computational cost as context length increases. This work addresses this issue with a key discovery: unlike the sequential generation in autoregressive language models (ARLMs), the diffusion generation paradigm in DLMs allows \\textit{efficient dynamic adjustment of the context} during generation. Building on this insight, we propose \\textbf{D}ynamic \\textbf{I}n-Context \\textbf{P}lanner (DIP), a context-optimization method that dynamically selects and inserts in-context examples during generation, rather than providing all examples in the prompt upfront. Results show DIP maintains generation quality while achieving up to 12.9$\\times$ inference speedup over standard inference and 1.17$\\times$ over KV cache-enhanced inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have shown strong potential for general natural language tasks with in-context examples. However, due to the bidirectional attention mechanism, DLMs incur substantial computational cost as context length increases. This work addresses this issue with a key discovery: unlike the sequential generation in autoregressive language models (ARLMs), the diffusion generation paradigm in DLMs allows \\textit{efficient dynamic adjustment of the context} during generation. Building on this insight, we propose \\textbf{D}ynamic \\textbf{I}n-Context \\textbf{P}lanner (DIP), a context-optimization method that dynamically selects and inserts in-context examples during generation, rather than providing all examples in the prompt upfront. Results show DIP maintains generation quality while achieving up to 12.9$\\times$ inference speedup over standard inference and 1.17$\\times$ over KV cache-enhanced inference."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-06T17:24:16Z",
                "published_parsed": [
                    2026,
                    1,
                    6,
                    17,
                    24,
                    16,
                    1,
                    6,
                    0
                ],
                "arxiv_comment": "4 pages",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Han Meng"
                    },
                    {
                        "name": "Chenan Wang"
                    },
                    {
                        "name": "Haipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haipeng Chen"
                },
                "author": "Haipeng Chen"
            },
            {
                "id": "http://arxiv.org/abs/2601.03324v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.03324v1",
                "title": "Bare-Metal Tensor Virtualization: Overcoming the Memory Wall in Edge-AI Inference on ARM64",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bare-Metal Tensor Virtualization: Overcoming the Memory Wall in Edge-AI Inference on ARM64"
                },
                "updated": "2026-01-06T15:00:40Z",
                "updated_parsed": [
                    2026,
                    1,
                    6,
                    15,
                    0,
                    40,
                    1,
                    6,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.03324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.03324v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The deployment of Large Language Models (LLMs) on edge devices is fundamentally constrained by the \"Memory Wall\" the bottleneck where data movement latency outstrips arithmetic throughput. Standard inference runtimes often incur significant overhead through high-level abstractions, dynamic dispatch, and unaligned memory access patterns. In this work, we present a novel \"Virtual Tensor Core\" architecture implemented in software, optimized specifically for ARM64 microarchitectures (Apple Silicon). By bypassing standard library containers in favor of direct memory mapping (mmap) and implementing hand-tuned NEON SIMD kernels, we achieve a form of \"Software-Defined Direct Memory Access (DMA).\" Our proposed Tensor Virtualization Layout (TVL) guarantees 100% cache line utilization for weight matrices, while our zero-copy loader eliminates initialization latency. Experimental results on a 110M parameter model demonstrate a stable throughput of >60 tokens/second on M2 hardware. While proprietary hardware accelerators (e.g., Apple AMX) can achieve higher peak throughput, our architecture provides a fully open, portable, and deterministic reference implementation for studying the memory bottleneck on general-purpose ARM silicon, meeting the 200ms psycholinguistic latency threshold without opaque dependencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Large Language Models (LLMs) on edge devices is fundamentally constrained by the \"Memory Wall\" the bottleneck where data movement latency outstrips arithmetic throughput. Standard inference runtimes often incur significant overhead through high-level abstractions, dynamic dispatch, and unaligned memory access patterns. In this work, we present a novel \"Virtual Tensor Core\" architecture implemented in software, optimized specifically for ARM64 microarchitectures (Apple Silicon). By bypassing standard library containers in favor of direct memory mapping (mmap) and implementing hand-tuned NEON SIMD kernels, we achieve a form of \"Software-Defined Direct Memory Access (DMA).\" Our proposed Tensor Virtualization Layout (TVL) guarantees 100% cache line utilization for weight matrices, while our zero-copy loader eliminates initialization latency. Experimental results on a 110M parameter model demonstrate a stable throughput of >60 tokens/second on M2 hardware. While proprietary hardware accelerators (e.g., Apple AMX) can achieve higher peak throughput, our architecture provides a fully open, portable, and deterministic reference implementation for studying the memory bottleneck on general-purpose ARM silicon, meeting the 200ms psycholinguistic latency threshold without opaque dependencies."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-06T15:00:40Z",
                "published_parsed": [
                    2026,
                    1,
                    6,
                    15,
                    0,
                    40,
                    1,
                    6,
                    0
                ],
                "arxiv_comment": "14 pages, 2 figures. Code and data available at https://github.com/farukalpay/stories100m",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Bugra Kilictas"
                    },
                    {
                        "name": "Faruk Alpay"
                    }
                ],
                "author_detail": {
                    "name": "Faruk Alpay"
                },
                "author": "Faruk Alpay"
            },
            {
                "id": "http://arxiv.org/abs/2601.03067v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.03067v1",
                "title": "Joint Encoding of KV-Cache Blocks for Scalable LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Encoding of KV-Cache Blocks for Scalable LLM Serving"
                },
                "updated": "2026-01-06T14:50:58Z",
                "updated_parsed": [
                    2026,
                    1,
                    6,
                    14,
                    50,
                    58,
                    1,
                    6,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.03067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.03067v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern large language models (LLMs) drive interactive AI systems but are bottlenecked by the memory-heavy growth of key-value (KV) caches, which limits real-time throughput under concurrent loads. Existing KV-cache compression methods rely on rigid heuristics, disrupt tensor layouts, or require specialized compute, hindering scalability and deployment.\n  We propose joint encoding of KV-cache blocks, which fuses similar blocks across requests and input chunks into shared representations while preserving standard cache structure. This alleviates the KV-cache memory bottleneck, supporting high-concurrency serving without specialized hardware. Theoretically, we analyze the rate-distortion tradeoff of fused cache blocks under a Poisson process model. Empirically, our method achieves up to 4.38 $\\times$ KV-cache compression with negligible accuracy loss across diverse LLMs and benchmarks, outperforming recent structured and adaptive compression baselines. In real LLM serving, joint encoding improves the token throughput by $\\sim$40\\% on a single-machine vLLM benchmark, demonstrating substantial gains in inference throughput. Code is available at https://github.com/sef1/kv_fast_fusion  kv_joint_encoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) drive interactive AI systems but are bottlenecked by the memory-heavy growth of key-value (KV) caches, which limits real-time throughput under concurrent loads. Existing KV-cache compression methods rely on rigid heuristics, disrupt tensor layouts, or require specialized compute, hindering scalability and deployment.\n  We propose joint encoding of KV-cache blocks, which fuses similar blocks across requests and input chunks into shared representations while preserving standard cache structure. This alleviates the KV-cache memory bottleneck, supporting high-concurrency serving without specialized hardware. Theoretically, we analyze the rate-distortion tradeoff of fused cache blocks under a Poisson process model. Empirically, our method achieves up to 4.38 $\\times$ KV-cache compression with negligible accuracy loss across diverse LLMs and benchmarks, outperforming recent structured and adaptive compression baselines. In real LLM serving, joint encoding improves the token throughput by $\\sim$40\\% on a single-machine vLLM benchmark, demonstrating substantial gains in inference throughput. Code is available at https://github.com/sef1/kv_fast_fusion  kv_joint_encoding."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-06T14:50:58Z",
                "published_parsed": [
                    2026,
                    1,
                    6,
                    14,
                    50,
                    58,
                    1,
                    6,
                    0
                ],
                "arxiv_comment": "12 pages, 16 figures, 2 tables",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Joseph Kampeas"
                    },
                    {
                        "name": "Emir Haleva"
                    }
                ],
                "author_detail": {
                    "name": "Emir Haleva"
                },
                "author": "Emir Haleva"
            },
            {
                "id": "http://arxiv.org/abs/2502.16886v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.16886v3",
                "title": "Towards Threshold-Free KV Cache Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Threshold-Free KV Cache Pruning"
                },
                "updated": "2026-01-06T14:32:34Z",
                "updated_parsed": [
                    2026,
                    1,
                    6,
                    14,
                    32,
                    34,
                    1,
                    6,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.16886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.16886v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "To reduce memory consumption during LLM inference, prior works have proposed numerous methods that focus on KV cache pruning based on various criteria. While these techniques often accomplish lossless memory reduction on many datasets, they often rely on an under-emphasized condition: a dataset/domain-specific budget size threshold needs to be pre-determined to achieve the optimal performance. However, such input-specific tuning may be considerably limited in real-world scenarios, as open-domain inputs span diverse domains, lengths and difficulty levels, without clear boundaries for pre-tuning. Thus, the dependence of an input-sensitive threshold can be an inherent limitation that may cause large degradation on arbitrary inputs. In this work, we propose a new objective that lifts the threshold constraints for robust KV pruning, calling for \"threshold-free\" methods that automatically adjust budget sizes while ensuring full-cache performance. We then propose a novel method ReFreeKV as the first solution fulfilling this objective, validated by intensive experiments on 13 datasets of diverse context lengths, task types, and model sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To reduce memory consumption during LLM inference, prior works have proposed numerous methods that focus on KV cache pruning based on various criteria. While these techniques often accomplish lossless memory reduction on many datasets, they often rely on an under-emphasized condition: a dataset/domain-specific budget size threshold needs to be pre-determined to achieve the optimal performance. However, such input-specific tuning may be considerably limited in real-world scenarios, as open-domain inputs span diverse domains, lengths and difficulty levels, without clear boundaries for pre-tuning. Thus, the dependence of an input-sensitive threshold can be an inherent limitation that may cause large degradation on arbitrary inputs. In this work, we propose a new objective that lifts the threshold constraints for robust KV pruning, calling for \"threshold-free\" methods that automatically adjust budget sizes while ensuring full-cache performance. We then propose a novel method ReFreeKV as the first solution fulfilling this objective, validated by intensive experiments on 13 datasets of diverse context lengths, task types, and model sizes."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-24T06:33:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "arxiv_comment": "Substantial revision",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Xuanfan Ni"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li"
            },
            {
                "id": "http://arxiv.org/abs/2601.02790v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02790v1",
                "title": "RadioDiff-Flux: Efficient Radio Map Construction via Generative Denoise Diffusion Model Trajectory Midpoint Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RadioDiff-Flux: Efficient Radio Map Construction via Generative Denoise Diffusion Model Trajectory Midpoint Reuse"
                },
                "updated": "2026-01-06T07:57:27Z",
                "updated_parsed": [
                    2026,
                    1,
                    6,
                    7,
                    57,
                    27,
                    1,
                    6,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02790v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurate radio map (RM) construction is essential to enabling environment-aware and adaptive wireless communication. However, in future 6G scenarios characterized by high-speed network entities and fast-changing environments, it is very challenging to meet real-time requirements. Although generative diffusion models (DMs) can achieve state-of-the-art accuracy with second-level delay, their iterative nature leads to prohibitive inference latency in delay-sensitive scenarios. In this paper, by uncovering a key structural property of diffusion processes: the latent midpoints remain highly consistent across semantically similar scenes, we propose RadioDiff-Flux, a novel two-stage latent diffusion framework that decouples static environmental modeling from dynamic refinement, enabling the reuse of precomputed midpoints to bypass redundant denoising. In particular, the first stage generates a coarse latent representation using only static scene features, which can be cached and shared across similar scenarios. The second stage adapts this representation to dynamic conditions and transmitter locations using a pre-trained model, thereby avoiding repeated early-stage computation. The proposed RadioDiff-Flux significantly reduces inference time while preserving fidelity. Experiment results show that RadioDiff-Flux can achieve up to 50 acceleration with less than 0.15% accuracy loss, demonstrating its practical utility for fast, scalable RM generation in future 6G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate radio map (RM) construction is essential to enabling environment-aware and adaptive wireless communication. However, in future 6G scenarios characterized by high-speed network entities and fast-changing environments, it is very challenging to meet real-time requirements. Although generative diffusion models (DMs) can achieve state-of-the-art accuracy with second-level delay, their iterative nature leads to prohibitive inference latency in delay-sensitive scenarios. In this paper, by uncovering a key structural property of diffusion processes: the latent midpoints remain highly consistent across semantically similar scenes, we propose RadioDiff-Flux, a novel two-stage latent diffusion framework that decouples static environmental modeling from dynamic refinement, enabling the reuse of precomputed midpoints to bypass redundant denoising. In particular, the first stage generates a coarse latent representation using only static scene features, which can be cached and shared across similar scenarios. The second stage adapts this representation to dynamic conditions and transmitter locations using a pre-trained model, thereby avoiding repeated early-stage computation. The proposed RadioDiff-Flux significantly reduces inference time while preserving fidelity. Experiment results show that RadioDiff-Flux can achieve up to 50 acceleration with less than 0.15% accuracy loss, demonstrating its practical utility for fast, scalable RM generation in future 6G networks."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-06T07:57:27Z",
                "published_parsed": [
                    2026,
                    1,
                    6,
                    7,
                    57,
                    27,
                    1,
                    6,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xiucheng Wang"
                    },
                    {
                        "name": "Peilin Zheng"
                    },
                    {
                        "name": "Honggang Jia"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Ruijin Sun"
                    },
                    {
                        "name": "Conghao Zhou"
                    },
                    {
                        "name": "Xuemin Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xuemin Shen"
                },
                "author": "Xuemin Shen"
            },
            {
                "id": "http://arxiv.org/abs/2510.21361v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.21361v2",
                "title": "Compositional Monte Carlo Tree Diffusion for Extendable Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Monte Carlo Tree Diffusion for Extendable Planning"
                },
                "updated": "2026-01-06T04:54:34Z",
                "updated_parsed": [
                    2026,
                    1,
                    6,
                    4,
                    54,
                    34,
                    1,
                    6,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.21361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.21361v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured tree search to enable effective trajectory exploration through stepwise reasoning. However, MCTD remains fundamentally limited by training trajectory lengths. While periodic replanning allows plan concatenation for longer plan generation, the planning process remains locally confined, as MCTD searches within individual trajectories without access to global context. We propose Compositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates planning from individual trajectory optimization to reasoning over complete plan compositions. C-MCTD introduces three complementary components: (1) Online Composer, which performs globally-aware planning by searching across entire plan compositions; (2) Distributed Composer, which reduces search complexity through parallel exploration from multiple starting points; and (3) Preplan Composer, which accelerates inference by leveraging cached plan graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured tree search to enable effective trajectory exploration through stepwise reasoning. However, MCTD remains fundamentally limited by training trajectory lengths. While periodic replanning allows plan concatenation for longer plan generation, the planning process remains locally confined, as MCTD searches within individual trajectories without access to global context. We propose Compositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates planning from individual trajectory optimization to reasoning over complete plan compositions. C-MCTD introduces three complementary components: (1) Online Composer, which performs globally-aware planning by searching across entire plan compositions; (2) Distributed Composer, which reduces search complexity through parallel exploration from multiple starting points; and (3) Preplan Composer, which accelerates inference by leveraging cached plan graphs."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-24T11:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "arxiv_comment": "24 pages, 4 figures, NeurIPS 25 Spotlight",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jaesik Yoon"
                    },
                    {
                        "name": "Hyeonseo Cho"
                    },
                    {
                        "name": "Sungjin Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Ahn"
                },
                "author": "Sungjin Ahn"
            },
            {
                "id": "http://arxiv.org/abs/2601.01310v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01310v2",
                "title": "Making MoE-based LLM Inference Resilient with Tarragon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making MoE-based LLM Inference Resilient with Tarragon"
                },
                "updated": "2026-01-06T04:01:10Z",
                "updated_parsed": [
                    2026,
                    1,
                    6,
                    4,
                    1,
                    10,
                    1,
                    6,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01310v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01310v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mixture-of-Experts (MoE) models are increasingly used to serve LLMs at scale, but failures become common as deployment scale grows. Existing systems exhibit poor failure resilience: even a single worker failure triggers a coarse-grained, service-wide restart, discarding accumulated progress and halting the entire inference pipeline during recovery--an approach clearly ill-suited for latency-sensitive, LLM services.\n  We present Tarragon, a resilient MoE inference framework that confines the failures impact to individual workers while allowing the rest of the pipeline to continue making forward progress. Tarragon exploits the natural separation between the attention and expert computation in MoE-based transformers, treating attention workers (AWs) and expert workers (EWs) as distinct failure domains. Tarragon introduces a reconfigurable datapath to mask failures by rerouting requests to healthy workers. On top of this datapath, Tarragon implements a self-healing mechanism that relaxes the tightly synchronized execution of existing MoE frameworks. For stateful AWs, Tarragon performs asynchronous, incremental KV cache checkpointing with per-request restoration, and for stateless EWs, it leverages residual GPU memory to deploy shadow experts. These together keep recovery cost and recomputation overhead extremely low. Our evaluation shows that, compared to state-of-the-art MegaScale-Infer, Tarragon reduces failure-induced stalls by 160-213x (from ~64 s down to 0.3-0.4 s) while preserving performance when no failures occur.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are increasingly used to serve LLMs at scale, but failures become common as deployment scale grows. Existing systems exhibit poor failure resilience: even a single worker failure triggers a coarse-grained, service-wide restart, discarding accumulated progress and halting the entire inference pipeline during recovery--an approach clearly ill-suited for latency-sensitive, LLM services.\n  We present Tarragon, a resilient MoE inference framework that confines the failures impact to individual workers while allowing the rest of the pipeline to continue making forward progress. Tarragon exploits the natural separation between the attention and expert computation in MoE-based transformers, treating attention workers (AWs) and expert workers (EWs) as distinct failure domains. Tarragon introduces a reconfigurable datapath to mask failures by rerouting requests to healthy workers. On top of this datapath, Tarragon implements a self-healing mechanism that relaxes the tightly synchronized execution of existing MoE frameworks. For stateful AWs, Tarragon performs asynchronous, incremental KV cache checkpointing with per-request restoration, and for stateless EWs, it leverages residual GPU memory to deploy shadow experts. These together keep recovery cost and recomputation overhead extremely low. Our evaluation shows that, compared to state-of-the-art MegaScale-Infer, Tarragon reduces failure-induced stalls by 160-213x (from ~64 s down to 0.3-0.4 s) while preserving performance when no failures occur."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-04T00:13:31Z",
                "published_parsed": [
                    2026,
                    1,
                    4,
                    0,
                    13,
                    31,
                    6,
                    4,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Songyu Zhang"
                    },
                    {
                        "name": "Aaron Tam"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Shixiong Qi"
                    },
                    {
                        "name": "K. K. Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "K. K. Ramakrishnan"
                },
                "author": "K. K. Ramakrishnan"
            },
            {
                "id": "http://arxiv.org/abs/2512.24229v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.24229v2",
                "title": "High-Performance KV$_3$Sb$_5$/WSe$_2$ van der Waals Photodetectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Performance KV$_3$Sb$_5$/WSe$_2$ van der Waals Photodetectors"
                },
                "updated": "2026-01-06T02:54:45Z",
                "updated_parsed": [
                    2026,
                    1,
                    6,
                    2,
                    54,
                    45,
                    1,
                    6,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.24229v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.24229v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Kagome metals AV$_3$Sb$_5$ (A = K, Rb, Cs) have recently emerged as a promising platform for exploring correlated and topological quantum states, yet their potential for optoelectronic applications remains largely unexplored. Here, we report high-performance photodetectors based on van der Waals KV$_3$Sb$_5$/WSe$_2$ heterojunctions. A high-quality Schottky interface readily forms between KV$_3$Sb$_5$ and WSe$_2$, enabling efficient separation and transport of photoinduced carriers. Under 520 nm illumination, the device achieves an open-circuit voltage up to 0.6 V, a responsivity of 809 mA/W, and a fast response time of 18.3 us. This work demonstrates the promising optoelectronic applications of Kagome metals and highlights the potential of KV$_3$Sb$_5$-based van der Waals heterostructures for high-performance photodetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kagome metals AV$_3$Sb$_5$ (A = K, Rb, Cs) have recently emerged as a promising platform for exploring correlated and topological quantum states, yet their potential for optoelectronic applications remains largely unexplored. Here, we report high-performance photodetectors based on van der Waals KV$_3$Sb$_5$/WSe$_2$ heterojunctions. A high-quality Schottky interface readily forms between KV$_3$Sb$_5$ and WSe$_2$, enabling efficient separation and transport of photoinduced carriers. Under 520 nm illumination, the device achieves an open-circuit voltage up to 0.6 V, a responsivity of 809 mA/W, and a fast response time of 18.3 us. This work demonstrates the promising optoelectronic applications of Kagome metals and highlights the potential of KV$_3$Sb$_5$-based van der Waals heterostructures for high-performance photodetection."
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-30T13:40:49Z",
                "published_parsed": [
                    2025,
                    12,
                    30,
                    13,
                    40,
                    49,
                    1,
                    364,
                    0
                ],
                "arxiv_comment": "16 pages including Supporting Information",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci"
                },
                "authors": [
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Shaofeng Rao"
                    },
                    {
                        "name": "Yuxuan Hou"
                    },
                    {
                        "name": "Jiabo Liu"
                    },
                    {
                        "name": "Deng Hu"
                    },
                    {
                        "name": "Yufei Guo"
                    },
                    {
                        "name": "Jianzhou Zhao"
                    },
                    {
                        "name": "Hechen Ren"
                    },
                    {
                        "name": "Zhiwei Wang"
                    },
                    {
                        "name": "Fan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Fan Yang"
                },
                "author": "Fan Yang"
            },
            {
                "id": "http://arxiv.org/abs/2601.02569v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02569v1",
                "title": "LoRA-Drop: Temporal LoRA Decoding for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-Drop: Temporal LoRA Decoding for Efficient LLM Inference"
                },
                "updated": "2026-01-05T21:47:47Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    21,
                    47,
                    47,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02569v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autoregressive large language models (LLMs) are bottlenecked by sequential decoding, where each new token typically requires executing all transformer layers. Existing dynamic-depth and layer-skipping methods reduce this cost, but often rely on auxiliary routing mechanisms or incur accuracy degradation when bypassed layers are left uncompensated. We present \\textbf{LoRA-Drop}, a plug-and-play inference framework that accelerates decoding by applying a \\emph{temporal compute schedule} to a fixed subset of intermediate layers: on most decoding steps, selected layers reuse the previous-token hidden state and apply a low-rank LoRA correction, while periodic \\emph{refresh} steps execute the full model to prevent drift. LoRA-Drop requires no routing network, is compatible with standard KV caching, and can reduce KV-cache footprint by skipping KV updates in droppable layers during LoRA steps and refreshing periodically. Across \\textbf{LLaMA2-7B}, \\textbf{LLaMA3-8B}, \\textbf{Qwen2.5-7B}, and \\textbf{Qwen2.5-14B}, LoRA-Drop achieves up to \\textbf{2.6$\\times$ faster decoding} and \\textbf{45--55\\% KV-cache reduction} while staying within \\textbf{0.5 percentage points (pp)} of baseline accuracy. Evaluations on reasoning (GSM8K, MATH, BBH), code generation (HumanEval, MBPP), and long-context/multilingual benchmarks (LongBench, XNLI, XCOPA) identify a consistent \\emph{safe zone} of scheduling configurations that preserves quality while delivering substantial efficiency gains, providing a simple path toward adaptive-capacity inference in LLMs. Codes are available at https://github.com/hosseinbv/LoRA-Drop.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive large language models (LLMs) are bottlenecked by sequential decoding, where each new token typically requires executing all transformer layers. Existing dynamic-depth and layer-skipping methods reduce this cost, but often rely on auxiliary routing mechanisms or incur accuracy degradation when bypassed layers are left uncompensated. We present \\textbf{LoRA-Drop}, a plug-and-play inference framework that accelerates decoding by applying a \\emph{temporal compute schedule} to a fixed subset of intermediate layers: on most decoding steps, selected layers reuse the previous-token hidden state and apply a low-rank LoRA correction, while periodic \\emph{refresh} steps execute the full model to prevent drift. LoRA-Drop requires no routing network, is compatible with standard KV caching, and can reduce KV-cache footprint by skipping KV updates in droppable layers during LoRA steps and refreshing periodically. Across \\textbf{LLaMA2-7B}, \\textbf{LLaMA3-8B}, \\textbf{Qwen2.5-7B}, and \\textbf{Qwen2.5-14B}, LoRA-Drop achieves up to \\textbf{2.6$\\times$ faster decoding} and \\textbf{45--55\\% KV-cache reduction} while staying within \\textbf{0.5 percentage points (pp)} of baseline accuracy. Evaluations on reasoning (GSM8K, MATH, BBH), code generation (HumanEval, MBPP), and long-context/multilingual benchmarks (LongBench, XNLI, XCOPA) identify a consistent \\emph{safe zone} of scheduling configurations that preserves quality while delivering substantial efficiency gains, providing a simple path toward adaptive-capacity inference in LLMs. Codes are available at https://github.com/hosseinbv/LoRA-Drop.git."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T21:47:47Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    21,
                    47,
                    47,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hossein Rajabzadeh"
                    },
                    {
                        "name": "Maryam Dialameh"
                    },
                    {
                        "name": "Chul B. Park"
                    },
                    {
                        "name": "Il-Min Kim"
                    },
                    {
                        "name": "Hyock Ju Kwon"
                    }
                ],
                "author_detail": {
                    "name": "Hyock Ju Kwon"
                },
                "author": "Hyock Ju Kwon"
            },
            {
                "id": "http://arxiv.org/abs/2601.02333v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02333v1",
                "title": "High-repetition-rate terahertz and ultraviolet radiation for high-throughput ultrafast electron diffraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-repetition-rate terahertz and ultraviolet radiation for high-throughput ultrafast electron diffraction"
                },
                "updated": "2026-01-05T18:25:44Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    25,
                    44,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02333v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Scaling femtosecond terahertz (THz) and ultraviolet (UV) sources to high repetition rates is essential for high-throughput ultrafast spectroscopy and imaging applications. Yet, their efficient generation at high average power remains limited by thermal effects, phase-matching constraints, and material damage. Here, we demonstrate broadband THz and UV generation driven by a common Yb:KGW laser operating from at 40 - 600 kHz. THz radiation is produced by optical rectification in stoichiometric MgO:LiNbO$_3$ using a line-focus geometry, yielding single-cycle pulses of 55 - 92 nJ energy with peak electric fields of 37 - 90 kV/cm. Electro-optic sampling and beam-quality measurements reveal tunable control between central frequency, bandwidth and field amplitude by translating the generation region transversely within the crystal. Using shorter pump pulses preserves THz conversion efficiency, while longer pulses at 100 kHz reduce THz output by up to a factor of four due to cumulative thermal effects. Femtosecond 257.5 nm UV pulses are generated by cascaded fourth-harmonic generation in $$-barium borate with conversion efficiencies exceeding 10% at 40 kHz and stable operation up to 600 kHz. These results demonstrate a compact, thermally robust platform for high-average-power nonlinear conversion and are directly relevant to next-generation high-repetition-rate ultrafast electron diffraction and spectroscopy systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling femtosecond terahertz (THz) and ultraviolet (UV) sources to high repetition rates is essential for high-throughput ultrafast spectroscopy and imaging applications. Yet, their efficient generation at high average power remains limited by thermal effects, phase-matching constraints, and material damage. Here, we demonstrate broadband THz and UV generation driven by a common Yb:KGW laser operating from at 40 - 600 kHz. THz radiation is produced by optical rectification in stoichiometric MgO:LiNbO$_3$ using a line-focus geometry, yielding single-cycle pulses of 55 - 92 nJ energy with peak electric fields of 37 - 90 kV/cm. Electro-optic sampling and beam-quality measurements reveal tunable control between central frequency, bandwidth and field amplitude by translating the generation region transversely within the crystal. Using shorter pump pulses preserves THz conversion efficiency, while longer pulses at 100 kHz reduce THz output by up to a factor of four due to cumulative thermal effects. Femtosecond 257.5 nm UV pulses are generated by cascaded fourth-harmonic generation in $$-barium borate with conversion efficiencies exceeding 10% at 40 kHz and stable operation up to 600 kHz. These results demonstrate a compact, thermally robust platform for high-average-power nonlinear conversion and are directly relevant to next-generation high-repetition-rate ultrafast electron diffraction and spectroscopy systems."
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T18:25:44Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    25,
                    44,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics"
                },
                "authors": [
                    {
                        "name": "Andrey Ryabov"
                    },
                    {
                        "name": "Kasra Amini"
                    }
                ],
                "author_detail": {
                    "name": "Kasra Amini"
                },
                "author": "Kasra Amini"
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.18773v3",
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit KV Cache"
                },
                "updated": "2026-01-05T18:08:27Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    18,
                    8,
                    27,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.18773v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.18773v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The growth of long-context Large Language Models (LLMs) significantly increases memory and bandwidth pressure during autoregressive decoding due to the expanding Key-Value (KV) cache. While accuracy-preserving KV-cache quantization (e.g., 4-bit or 2-bit) reduces memory footprint, existing systems decode inefficiently by relying solely on CUDA cores, underutilizing Tensor Cores-the dominant compute resource on GPUs.\n  We present BitDecoding, the first inference system to efficiently decode low-bit KV caches by cooperatively leveraging CUDA cores and Tensor Cores. BitDecoding smartly induces Tensor-Core-friendly layouts, introduces warp-level dequantization parallelism, and provides unified system support through query transformation, high-performance tensor- and channel-wise quantization, and a software-pipelined dequantization kernel enabling mixed-precision execution. Architecture-aware optimizations further leverage Hopper's warpgroup tensor instructions and Blackwell's NVFP4 (MXFP4) tensor formats.\n  Evaluated on Blackwell, Hopper, and Ampere GPUs, BitDecoding achieves an average 7.5x decoding speedup over FP16 FlashDecoding-v2, up to 8.6x on Blackwell with NVFP4, and up to 4.3x over state-of-the-art approaches. On LLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding latency by 3x. BitDecoding is open-sourced at https://github.com/OpenBitSys/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of long-context Large Language Models (LLMs) significantly increases memory and bandwidth pressure during autoregressive decoding due to the expanding Key-Value (KV) cache. While accuracy-preserving KV-cache quantization (e.g., 4-bit or 2-bit) reduces memory footprint, existing systems decode inefficiently by relying solely on CUDA cores, underutilizing Tensor Cores-the dominant compute resource on GPUs.\n  We present BitDecoding, the first inference system to efficiently decode low-bit KV caches by cooperatively leveraging CUDA cores and Tensor Cores. BitDecoding smartly induces Tensor-Core-friendly layouts, introduces warp-level dequantization parallelism, and provides unified system support through query transformation, high-performance tensor- and channel-wise quantization, and a software-pipelined dequantization kernel enabling mixed-precision execution. Architecture-aware optimizations further leverage Hopper's warpgroup tensor instructions and Blackwell's NVFP4 (MXFP4) tensor formats.\n  Evaluated on Blackwell, Hopper, and Ampere GPUs, BitDecoding achieves an average 7.5x decoding speedup over FP16 FlashDecoding-v2, up to 8.6x on Blackwell with NVFP4, and up to 4.3x over state-of-the-art approaches. On LLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding latency by 3x. BitDecoding is open-sourced at https://github.com/OpenBitSys/BitDecoding."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang"
            },
            {
                "id": "http://arxiv.org/abs/2601.02281v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02281v1",
                "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams"
                },
                "updated": "2026-01-05T17:11:00Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    11,
                    0,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02281v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T17:11:00Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    17,
                    11,
                    0,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Shuai Yuan"
                    },
                    {
                        "name": "Yantai Yang"
                    },
                    {
                        "name": "Xiaotian Yang"
                    },
                    {
                        "name": "Xupeng Zhang"
                    },
                    {
                        "name": "Zhonghao Zhao"
                    },
                    {
                        "name": "Lingming Zhang"
                    },
                    {
                        "name": "Zhipeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhipeng Zhang"
                },
                "author": "Zhipeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2504.11320v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.11320v2",
                "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints"
                },
                "updated": "2026-01-05T14:10:45Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    14,
                    10,
                    45,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.11320v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.11320v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) power many modern applications, but their inference procedure poses unique scheduling challenges: the Key-Value (KV) cache grows dynamically during response generation, and memory overflow triggers eviction that can cascade into system-wide failures. Even when memory capacity exceeds the theoretical requirement, conventional scheduling algorithms fail because they do not account for this dynamic memory growth -- a system that should be stable can become unstable under poor scheduling.\n  This paper formulates LLM inference optimization as a multi-stage online scheduling problem. We develop a fluid dynamics approximation to establish a tractable benchmark and derive the Waiting for Accumulated Inference Threshold (WAIT) algorithm. WAIT uses threshold-based batching to prevent eviction by keeping the system near load balance, achieving near-optimal throughput when output lengths are known.\n  For practical settings where output lengths are unknown at arrival, we introduce Nested WAIT. Rather than predicting output lengths, Nested WAIT classifies prompts on-the-fly: short prompts complete early and exit, while longer prompts naturally advance to later segments. A safety buffer provides high-probability protection against memory overflow with only logarithmic overhead.\n  Theoretical analysis establishes near-optimal performance in the asymptotic regime. Experiments on Llama-7B with an A100 GPU demonstrate that our approach achieves superior throughput and reduced latency compared to vLLM and Sarathi. This work applies operations research principles to establish a theoretical framework for LLM deployment under memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) power many modern applications, but their inference procedure poses unique scheduling challenges: the Key-Value (KV) cache grows dynamically during response generation, and memory overflow triggers eviction that can cascade into system-wide failures. Even when memory capacity exceeds the theoretical requirement, conventional scheduling algorithms fail because they do not account for this dynamic memory growth -- a system that should be stable can become unstable under poor scheduling.\n  This paper formulates LLM inference optimization as a multi-stage online scheduling problem. We develop a fluid dynamics approximation to establish a tractable benchmark and derive the Waiting for Accumulated Inference Threshold (WAIT) algorithm. WAIT uses threshold-based batching to prevent eviction by keeping the system near load balance, achieving near-optimal throughput when output lengths are known.\n  For practical settings where output lengths are unknown at arrival, we introduce Nested WAIT. Rather than predicting output lengths, Nested WAIT classifies prompts on-the-fly: short prompts complete early and exit, while longer prompts naturally advance to later segments. A safety buffer provides high-probability protection against memory overflow with only logarithmic overhead.\n  Theoretical analysis establishes near-optimal performance in the asymptotic regime. Experiments on Llama-7B with an A100 GPU demonstrate that our approach achieves superior throughput and reduced latency compared to vLLM and Sarathi. This work applies operations research principles to establish a theoretical framework for LLM deployment under memory constraints."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-15T16:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "arxiv_comment": "49 pages, 18 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ruicheng Ao"
                    },
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.22673v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.22673v2",
                "title": "TravelBench: A Broader Real-World Benchmark for Multi-Turn and Tool-Using Travel Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TravelBench: A Broader Real-World Benchmark for Multi-Turn and Tool-Using Travel Planning"
                },
                "updated": "2026-01-05T13:19:13Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    13,
                    19,
                    13,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.22673v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.22673v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Travel planning is a natural real-world task to test large language models (LLMs) planning and tool-use abilities. Although prior work has studied LLM performance on travel planning, existing settings still differ from real-world needs, mainly due to limited domain coverage, insufficient modeling of users' implicit preferences in multi-turn conversations, and a lack of clear evaluation of agents' capability boundaries. To mitigate these gaps, we propose \\textbf{TravelBench}, a benchmark for fully real-world travel planning. We collect user queries, user profile and tools from real scenarios, and construct three subtasks-Single-Turn, Multi-Turn, and Unsolvable-to evaluate agent's three core capabilities in real settings: (1) solving problems autonomously, (2) interacting with users over multiple turns to refine requirements, and (3) recognizing the limits of own abilities. To enable stable tool invocation and reproducible evaluation, we cache real tool-call results and build a sandbox environment that integrates ten travel-related tools. Agents can combine these tools to solve most practical travel planning problems, and our systematic verification demonstrates the stability of the proposed benchmark. We further evaluate multiple LLMs on TravelBench and conduct an in-depth analysis of their behaviors and performance. TravelBench provides a practical and reproducible evaluation benchmark to advance research on LLM agents for travel planning.\\footnote{Our code and data will be available after internal review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Travel planning is a natural real-world task to test large language models (LLMs) planning and tool-use abilities. Although prior work has studied LLM performance on travel planning, existing settings still differ from real-world needs, mainly due to limited domain coverage, insufficient modeling of users' implicit preferences in multi-turn conversations, and a lack of clear evaluation of agents' capability boundaries. To mitigate these gaps, we propose \\textbf{TravelBench}, a benchmark for fully real-world travel planning. We collect user queries, user profile and tools from real scenarios, and construct three subtasks-Single-Turn, Multi-Turn, and Unsolvable-to evaluate agent's three core capabilities in real settings: (1) solving problems autonomously, (2) interacting with users over multiple turns to refine requirements, and (3) recognizing the limits of own abilities. To enable stable tool invocation and reproducible evaluation, we cache real tool-call results and build a sandbox environment that integrates ten travel-related tools. Agents can combine these tools to solve most practical travel planning problems, and our systematic verification demonstrates the stability of the proposed benchmark. We further evaluate multiple LLMs on TravelBench and conduct an in-depth analysis of their behaviors and performance. TravelBench provides a practical and reproducible evaluation benchmark to advance research on LLM agents for travel planning.\\footnote{Our code and data will be available after internal review."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-27T18:25:14Z",
                "published_parsed": [
                    2025,
                    12,
                    27,
                    18,
                    25,
                    14,
                    5,
                    361,
                    0
                ],
                "arxiv_comment": "In progress",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Xiang Cheng"
                    },
                    {
                        "name": "Yulan Hu"
                    },
                    {
                        "name": "Xiangwen Zhang"
                    },
                    {
                        "name": "Lu Xu"
                    },
                    {
                        "name": "Zheng Pan"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu"
            },
            {
                "id": "http://arxiv.org/abs/2601.02076v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02076v1",
                "title": "Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows"
                },
                "updated": "2026-01-05T12:57:33Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    57,
                    33,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02076v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T12:57:33Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    12,
                    57,
                    33,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yingte Shu"
                    },
                    {
                        "name": "Yuchuan Tian"
                    },
                    {
                        "name": "Chao Xu"
                    },
                    {
                        "name": "Yunhe Wang"
                    },
                    {
                        "name": "Hanting Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hanting Chen"
                },
                "author": "Hanting Chen"
            },
            {
                "id": "http://arxiv.org/abs/2601.02023v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.02023v1",
                "title": "Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs"
                },
                "updated": "2026-01-05T11:30:56Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    30,
                    56,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.02023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.02023v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T11:30:56Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    11,
                    30,
                    56,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "25 pages, 8 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Amirali Ebrahimzadeh"
                    },
                    {
                        "name": "Seyyed M. Salili"
                    }
                ],
                "author_detail": {
                    "name": "Seyyed M. Salili"
                },
                "author": "Seyyed M. Salili"
            },
            {
                "id": "http://arxiv.org/abs/2601.01925v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01925v1",
                "title": "AR-MOT: Autoregressive Multi-object Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AR-MOT: Autoregressive Multi-object Tracking"
                },
                "updated": "2026-01-05T09:17:28Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    9,
                    17,
                    28,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01925v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T09:17:28Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    9,
                    17,
                    28,
                    0,
                    5,
                    0
                ],
                "arxiv_comment": "12 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Lianjie Jia"
                    },
                    {
                        "name": "Yuhan Wu"
                    },
                    {
                        "name": "Binghao Ran"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Lijun Wang"
                    },
                    {
                        "name": "Huchuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Huchuan Lu"
                },
                "author": "Huchuan Lu"
            },
            {
                "id": "http://arxiv.org/abs/2305.07205v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2305.07205v3",
                "title": "Mem-Rec: Memory Efficient Recommendation System using Alternative Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem-Rec: Memory Efficient Recommendation System using Alternative Representation"
                },
                "updated": "2026-01-05T03:36:57Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    3,
                    36,
                    57,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2305.07205v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2305.07205v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deep learning-based recommendation systems (e.g., DLRMs) are widely used AI models to provide high-quality personalized recommendations. Training data used for modern recommendation systems commonly includes categorical features taking on tens-of-millions of possible distinct values. These categorical tokens are typically assigned learned vector representations, that are stored in large embedding tables, on the order of 100s of GB. Storing and accessing these tables represent a substantial burden in commercial deployments. Our work proposes MEM-REC, a novel alternative representation approach for embedding tables. MEM-REC leverages bloom filters and hashing methods to encode categorical features using two cache-friendly embedding tables. The first table (token embedding) contains raw embeddings (i.e. learned vector representation), and the second table (weight embedding), which is much smaller, contains weights to scale these raw embeddings to provide better discriminative capability to each data point. We provide a detailed architecture, design and analysis of MEM-REC addressing trade-offs in accuracy and computation requirements, in comparison with state-of-the-art techniques. We show that MEM-REC can not only maintain the recommendation quality and significantly reduce the memory footprint for commercial scale recommendation models but can also improve the embedding latency. In particular, based on our results, MEM-REC compresses the MLPerf CriteoTB benchmark DLRM model size by 2900x and performs up to 3.4x faster embeddings while achieving the same AUC as that of the full uncompressed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning-based recommendation systems (e.g., DLRMs) are widely used AI models to provide high-quality personalized recommendations. Training data used for modern recommendation systems commonly includes categorical features taking on tens-of-millions of possible distinct values. These categorical tokens are typically assigned learned vector representations, that are stored in large embedding tables, on the order of 100s of GB. Storing and accessing these tables represent a substantial burden in commercial deployments. Our work proposes MEM-REC, a novel alternative representation approach for embedding tables. MEM-REC leverages bloom filters and hashing methods to encode categorical features using two cache-friendly embedding tables. The first table (token embedding) contains raw embeddings (i.e. learned vector representation), and the second table (weight embedding), which is much smaller, contains weights to scale these raw embeddings to provide better discriminative capability to each data point. We provide a detailed architecture, design and analysis of MEM-REC addressing trade-offs in accuracy and computation requirements, in comparison with state-of-the-art techniques. We show that MEM-REC can not only maintain the recommendation quality and significantly reduce the memory footprint for commercial scale recommendation models but can also improve the embedding latency. In particular, based on our results, MEM-REC compresses the MLPerf CriteoTB benchmark DLRM model size by 2900x and performs up to 3.4x faster embeddings while achieving the same AUC as that of the full uncompressed model."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-05-12T02:36:07Z",
                "published_parsed": [
                    2023,
                    5,
                    12,
                    2,
                    36,
                    7,
                    4,
                    132,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "arxiv_journal_ref": "Proceedings of the 15th Asian Conference on Machine Learning (2023), PMLR 222:518-533",
                "authors": [
                    {
                        "name": "Gopi Krishna Jha"
                    },
                    {
                        "name": "Anthony Thomas"
                    },
                    {
                        "name": "Nilesh Jain"
                    },
                    {
                        "name": "Sameh Gobriel"
                    },
                    {
                        "name": "Tajana Rosing"
                    },
                    {
                        "name": "Ravi Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Iyer"
                },
                "author": "Ravi Iyer"
            },
            {
                "id": "http://arxiv.org/abs/2503.00022v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.00022v2",
                "title": "KVCrush: Key value cache size-reduction using similarity in head-behaviour",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCrush: Key value cache size-reduction using similarity in head-behaviour"
                },
                "updated": "2026-01-05T03:29:51Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    3,
                    29,
                    51,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.00022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.00022v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Key-value (KV) caching has emerged as a crucial optimization technique for accelerating inference in large language models (LLMs). By allowing the attention operation to scale linearly rather than quadratically with the total sequence length, KV caching significantly enhances generation throughput. However, due to large context lengths in the modern LLMs, the memory footprint of the KV is a huge bottleneck for model deployment directly impacting the model's batch size, hindering its ability to deliver high-throughput. Existing research addresses this challenge using several techniques, such as discarding low-attention tokens, quantization, and matrix approximation which typically lead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many KV compression technologies to improve the model accuracy at a much smaller memory. KVCrush provides an alternate representation scheme for key-value states, along with a low-overhead token pruning algorithm that accounts for the token distribution in the KV cache, which in turn allows for a a smaller footprint while maintaining the accuracy of the model. Based on our results, KVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop and achieves state-of-the-art average accuracy with minimal overhead, incurring less than 0.5% total inference latency. KVCrush not only outperforms the accuracy of state-of-the-art importance-based token retention schemes but is also compatible with typical practical LLM deployments using KV cache paging schemes such as vLLM and mixed precision quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has emerged as a crucial optimization technique for accelerating inference in large language models (LLMs). By allowing the attention operation to scale linearly rather than quadratically with the total sequence length, KV caching significantly enhances generation throughput. However, due to large context lengths in the modern LLMs, the memory footprint of the KV is a huge bottleneck for model deployment directly impacting the model's batch size, hindering its ability to deliver high-throughput. Existing research addresses this challenge using several techniques, such as discarding low-attention tokens, quantization, and matrix approximation which typically lead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many KV compression technologies to improve the model accuracy at a much smaller memory. KVCrush provides an alternate representation scheme for key-value states, along with a low-overhead token pruning algorithm that accounts for the token distribution in the KV cache, which in turn allows for a a smaller footprint while maintaining the accuracy of the model. Based on our results, KVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop and achieves state-of-the-art average accuracy with minimal overhead, incurring less than 0.5% total inference latency. KVCrush not only outperforms the accuracy of state-of-the-art importance-based token retention schemes but is also compatible with typical practical LLM deployments using KV cache paging schemes such as vLLM and mixed precision quantization."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-24T02:57:51Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    2,
                    57,
                    51,
                    0,
                    55,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "arxiv_journal_ref": "Proceedings of the 17th Asian Conference on Machine Learning (2025), PMLR",
                "authors": [
                    {
                        "name": "Gopi Krishna Jha"
                    },
                    {
                        "name": "Sameh Gobriel"
                    },
                    {
                        "name": "Liubov Talamanova"
                    },
                    {
                        "name": "Nilesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Nilesh Jain"
                },
                "author": "Nilesh Jain"
            },
            {
                "id": "http://arxiv.org/abs/2601.01712v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01712v1",
                "title": "RelayGR: Scaling Long-Sequence Generative Recommendation via Cross-Stage Relay-Race Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RelayGR: Scaling Long-Sequence Generative Recommendation via Cross-Stage Relay-Race Inference"
                },
                "updated": "2026-01-05T01:34:06Z",
                "updated_parsed": [
                    2026,
                    1,
                    5,
                    1,
                    34,
                    6,
                    0,
                    5,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01712v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Real-time recommender systems execute multi-stage cascades (retrieval, pre-processing, fine-grained ranking) under strict tail-latency SLOs, leaving only tens of milliseconds for ranking. Generative recommendation (GR) models can improve quality by consuming long user-behavior sequences, but in production their online sequence length is tightly capped by the ranking-stage P99 budget. We observe that the majority of GR tokens encode user behaviors that are independent of the item candidates, suggesting an opportunity to pre-infer a user-behavior prefix once and reuse it during ranking rather than recomputing it on the critical path. Realizing this idea at industrial scale is non-trivial: the prefix cache must survive across multiple pipeline stages before the final ranking instance is determined, the user population implies cache footprints far beyond a single device, and indiscriminate pre-inference would overload shared resources under high QPS. We present RelayGR, a production system that enables in-HBM relay-race inference for GR. RelayGR selectively pre-infers long-term user prefixes, keeps their KV caches resident in HBM over the request lifecycle, and ensures the subsequent ranking can consume them without remote fetches. RelayGR combines three techniques: 1) a sequence-aware trigger that admits only at-risk requests under a bounded cache footprint and pre-inference load, 2) an affinity-aware router that co-locates cache production and consumption by routing both the auxiliary pre-infer signal and the ranking request to the same instance, and 3) a memory-aware expander that uses server-local DRAM to capture short-term cross-request reuse while avoiding redundant reloads. We implement RelayGR on Huawei Ascend NPUs and evaluate it with real queries. Under a fixed P99 SLO, RelayGR supports up to 1.5$\\times$ longer sequences and improves SLO-compliant throughput by up to 3.6$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time recommender systems execute multi-stage cascades (retrieval, pre-processing, fine-grained ranking) under strict tail-latency SLOs, leaving only tens of milliseconds for ranking. Generative recommendation (GR) models can improve quality by consuming long user-behavior sequences, but in production their online sequence length is tightly capped by the ranking-stage P99 budget. We observe that the majority of GR tokens encode user behaviors that are independent of the item candidates, suggesting an opportunity to pre-infer a user-behavior prefix once and reuse it during ranking rather than recomputing it on the critical path. Realizing this idea at industrial scale is non-trivial: the prefix cache must survive across multiple pipeline stages before the final ranking instance is determined, the user population implies cache footprints far beyond a single device, and indiscriminate pre-inference would overload shared resources under high QPS. We present RelayGR, a production system that enables in-HBM relay-race inference for GR. RelayGR selectively pre-infers long-term user prefixes, keeps their KV caches resident in HBM over the request lifecycle, and ensures the subsequent ranking can consume them without remote fetches. RelayGR combines three techniques: 1) a sequence-aware trigger that admits only at-risk requests under a bounded cache footprint and pre-inference load, 2) an affinity-aware router that co-locates cache production and consumption by routing both the auxiliary pre-infer signal and the ranking request to the same instance, and 3) a memory-aware expander that uses server-local DRAM to capture short-term cross-request reuse while avoiding redundant reloads. We implement RelayGR on Huawei Ascend NPUs and evaluate it with real queries. Under a fixed P99 SLO, RelayGR supports up to 1.5$\\times$ longer sequences and improves SLO-compliant throughput by up to 3.6$\\times$."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-05T01:34:06Z",
                "published_parsed": [
                    2026,
                    1,
                    5,
                    1,
                    34,
                    6,
                    0,
                    5,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Jiarui Wang"
                    },
                    {
                        "name": "Huichao Chai"
                    },
                    {
                        "name": "Yuanhang Zhang"
                    },
                    {
                        "name": "Zongjin Zhou"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Xingkun Yang"
                    },
                    {
                        "name": "Qiang Tang"
                    },
                    {
                        "name": "Bo Pan"
                    },
                    {
                        "name": "Jiawei Zhu"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Yuting Yan"
                    },
                    {
                        "name": "Shulan Wang"
                    },
                    {
                        "name": "Yingjie Zhu"
                    },
                    {
                        "name": "Zhengfan Yuan"
                    },
                    {
                        "name": "Jiaqi Huang"
                    },
                    {
                        "name": "Yuhan Zhang"
                    },
                    {
                        "name": "Xiaosong Sun"
                    },
                    {
                        "name": "Zhinan Zhang"
                    },
                    {
                        "name": "Hong Zhu"
                    },
                    {
                        "name": "Yongsheng Zhang"
                    },
                    {
                        "name": "Tiantian Dong"
                    },
                    {
                        "name": "Zhong Xiao"
                    },
                    {
                        "name": "Deliang Liu"
                    },
                    {
                        "name": "Chengzhou Lu"
                    },
                    {
                        "name": "Yuan Sun"
                    },
                    {
                        "name": "Zhiyuan Chen"
                    },
                    {
                        "name": "Xinming Han"
                    },
                    {
                        "name": "Zaizhu Liu"
                    },
                    {
                        "name": "Yaoyuan Wang"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Jinxin Xu"
                    },
                    {
                        "name": "Yajing Sun"
                    },
                    {
                        "name": "Zhoujun Yu"
                    },
                    {
                        "name": "Wenting Zhou"
                    },
                    {
                        "name": "Qidong Zhang"
                    },
                    {
                        "name": "Zhengyong Zhang"
                    },
                    {
                        "name": "Zhonghai Gu"
                    },
                    {
                        "name": "Yibo Jin"
                    },
                    {
                        "name": "Yongxiang Feng"
                    },
                    {
                        "name": "Pengfei Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Zuo"
                },
                "author": "Pengfei Zuo"
            },
            {
                "id": "http://arxiv.org/abs/2512.23914v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23914v2",
                "title": "Hardware Acceleration for Neural Networks: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware Acceleration for Neural Networks: A Comprehensive Survey"
                },
                "updated": "2026-01-04T00:01:40Z",
                "updated_parsed": [
                    2026,
                    1,
                    4,
                    0,
                    1,
                    40,
                    6,
                    4,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23914v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Neural networks have become a dominant computational workload across cloud and edge platforms, but rapid growth in model size and deployment diversity has exposed hardware bottlenecks increasingly dominated by memory movement, communication, and irregular operators rather than peak arithmetic throughput. This survey reviews the technology landscape for hardware acceleration of deep learning, spanning GPUs and tensor-core architectures; domain-specific accelerators (e.g., TPUs/NPUs); FPGA-based designs; ASIC inference engines; and emerging LLM-serving accelerators such as LPUs (language processing units), alongside in-/near-memory computing and neuromorphic/analog approaches. We organize the space using a unified taxonomy across (i) workloads (CNNs, RNNs, GNNs, and Transformers/LLMs), (ii) execution settings (training vs.\\ inference; datacenter vs.\\ edge), and (iii) optimization levers (reduced precision, sparsity and pruning, operator fusion, compilation and scheduling, and memory-system/interconnect design). We synthesize key architectural ideas including systolic arrays, vector and SIMD engines, specialized attention and softmax kernels, quantization-aware datapaths, and high-bandwidth memory, and we discuss how software stacks and compilers bridge model semantics to hardware. Finally, we highlight open challenges -- including efficient long-context LLM inference (KV-cache management), robust support for dynamic and sparse workloads, energy- and security-aware deployment, and fair benchmarking -- and point to promising directions for the next generation of neural acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks have become a dominant computational workload across cloud and edge platforms, but rapid growth in model size and deployment diversity has exposed hardware bottlenecks increasingly dominated by memory movement, communication, and irregular operators rather than peak arithmetic throughput. This survey reviews the technology landscape for hardware acceleration of deep learning, spanning GPUs and tensor-core architectures; domain-specific accelerators (e.g., TPUs/NPUs); FPGA-based designs; ASIC inference engines; and emerging LLM-serving accelerators such as LPUs (language processing units), alongside in-/near-memory computing and neuromorphic/analog approaches. We organize the space using a unified taxonomy across (i) workloads (CNNs, RNNs, GNNs, and Transformers/LLMs), (ii) execution settings (training vs.\\ inference; datacenter vs.\\ edge), and (iii) optimization levers (reduced precision, sparsity and pruning, operator fusion, compilation and scheduling, and memory-system/interconnect design). We synthesize key architectural ideas including systolic arrays, vector and SIMD engines, specialized attention and softmax kernels, quantization-aware datapaths, and high-bandwidth memory, and we discuss how software stacks and compilers bridge model semantics to hardware. Finally, we highlight open challenges -- including efficient long-context LLM inference (KV-cache management), robust support for dynamic and sparse workloads, energy- and security-aware deployment, and fair benchmarking -- and point to promising directions for the next generation of neural acceleration."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-30T00:27:02Z",
                "published_parsed": [
                    2025,
                    12,
                    30,
                    0,
                    27,
                    2,
                    1,
                    364,
                    0
                ],
                "arxiv_comment": "error in section 5.1",
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Ayan Banerjee"
                    },
                    {
                        "name": "Sandeep Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Sandeep Gupta"
                },
                "author": "Sandeep Gupta"
            },
            {
                "id": "http://arxiv.org/abs/2601.01298v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01298v1",
                "title": "Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware"
                },
                "updated": "2026-01-03T23:11:21Z",
                "updated_parsed": [
                    2026,
                    1,
                    3,
                    23,
                    11,
                    21,
                    5,
                    3,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01298v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering \"System 2\" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering \"System 2\" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-03T23:11:21Z",
                "published_parsed": [
                    2026,
                    1,
                    3,
                    23,
                    11,
                    21,
                    5,
                    3,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jorge L. Ruiz Williams"
                    }
                ],
                "author_detail": {
                    "name": "Jorge L. Ruiz Williams"
                },
                "author": "Jorge L. Ruiz Williams"
            },
            {
                "id": "http://arxiv.org/abs/2601.01204v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01204v1",
                "title": "XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry Grounded Transformer with KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry Grounded Transformer with KV Cache Compression"
                },
                "updated": "2026-01-03T14:59:50Z",
                "updated_parsed": [
                    2026,
                    1,
                    3,
                    14,
                    59,
                    50,
                    5,
                    3,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01204v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Learning-based 3D visual geometry models have benefited substantially from large-scale transformers. Among these, StreamVGGT leverages frame-wise causal attention for strong streaming reconstruction, but suffers from unbounded KV cache growth, leading to escalating memory consumption and inference latency as input frames accumulate. We propose XStreamVGGT, a tuning-free approach that systematically compresses the KV cache through joint pruning and quantization, enabling extremely memory-efficient streaming inference. Specifically, redundant KVs originating from multi-view inputs are pruned through efficient token importance identification, enabling a fixed memory budget. Leveraging the unique distribution of KV tensors, we incorporate KV quantization to further reduce memory consumption. Extensive evaluations show that XStreamVGGT achieves mostly negligible performance degradation while substantially reducing memory usage by 4.42$\\times$ and accelerating inference by 5.48$\\times$, enabling scalable and practical streaming 3D applications. The code is available at https://github.com/ywh187/XStreamVGGT/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-based 3D visual geometry models have benefited substantially from large-scale transformers. Among these, StreamVGGT leverages frame-wise causal attention for strong streaming reconstruction, but suffers from unbounded KV cache growth, leading to escalating memory consumption and inference latency as input frames accumulate. We propose XStreamVGGT, a tuning-free approach that systematically compresses the KV cache through joint pruning and quantization, enabling extremely memory-efficient streaming inference. Specifically, redundant KVs originating from multi-view inputs are pruned through efficient token importance identification, enabling a fixed memory budget. Leveraging the unique distribution of KV tensors, we incorporate KV quantization to further reduce memory consumption. Extensive evaluations show that XStreamVGGT achieves mostly negligible performance degradation while substantially reducing memory usage by 4.42$\\times$ and accelerating inference by 5.48$\\times$, enabling scalable and practical streaming 3D applications. The code is available at https://github.com/ywh187/XStreamVGGT/."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-03T14:59:50Z",
                "published_parsed": [
                    2026,
                    1,
                    3,
                    14,
                    59,
                    50,
                    5,
                    3,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Weihao Ye"
                    },
                    {
                        "name": "Hansen Feng"
                    },
                    {
                        "name": "Keyu Fan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Dahai Yu"
                    },
                    {
                        "name": "Zhengwu Liu"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong"
            },
            {
                "id": "http://arxiv.org/abs/2601.01112v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01112v1",
                "title": "EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with Lexicon-Weak Supervision and KV-Off Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with Lexicon-Weak Supervision and KV-Off Evaluation"
                },
                "updated": "2026-01-03T08:25:58Z",
                "updated_parsed": [
                    2026,
                    1,
                    3,
                    8,
                    25,
                    58,
                    5,
                    3,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01112v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce EmoLoom-2B, a lightweight and reproducible pipeline that turns small language models under 2B parameters into fast screening candidates for joint emotion classification and Valence-Arousal-Dominance prediction. To ensure protocol-faithful and fair evaluation, we unify data loading, training, and inference under a single JSON input-output contract and remove avoidable variance by adopting KV-off decoding as the default setting. We incorporate two orthogonal semantic regularizers: a VAD-preserving constraint that aligns generated text with target VAD triples, and a lightweight external appraisal classifier that provides training-time guidance on goal attainment, controllability, certainty, and fairness without injecting long rationales. To improve polarity sensitivity, we introduce Valence Flip augmentation based on mirrored emotional pairs. During supervised fine-tuning, we apply A/B mixture sampling with entropy-aware temperature scheduling to balance coverage and convergence. Using Qwen-1.8B-Chat as the base model, EmoLoom-2B achieves strong performance on GoEmotions and EmpatheticDialogues, and demonstrates robust cross-corpus generalization on DailyDialog. The proposed recipe is budget-aware, auditable, and re-entrant, serving as a dependable screening pass before heavier training or multimodal fusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce EmoLoom-2B, a lightweight and reproducible pipeline that turns small language models under 2B parameters into fast screening candidates for joint emotion classification and Valence-Arousal-Dominance prediction. To ensure protocol-faithful and fair evaluation, we unify data loading, training, and inference under a single JSON input-output contract and remove avoidable variance by adopting KV-off decoding as the default setting. We incorporate two orthogonal semantic regularizers: a VAD-preserving constraint that aligns generated text with target VAD triples, and a lightweight external appraisal classifier that provides training-time guidance on goal attainment, controllability, certainty, and fairness without injecting long rationales. To improve polarity sensitivity, we introduce Valence Flip augmentation based on mirrored emotional pairs. During supervised fine-tuning, we apply A/B mixture sampling with entropy-aware temperature scheduling to balance coverage and convergence. Using Qwen-1.8B-Chat as the base model, EmoLoom-2B achieves strong performance on GoEmotions and EmpatheticDialogues, and demonstrates robust cross-corpus generalization on DailyDialog. The proposed recipe is budget-aware, auditable, and re-entrant, serving as a dependable screening pass before heavier training or multimodal fusion."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-03T08:25:58Z",
                "published_parsed": [
                    2026,
                    1,
                    3,
                    8,
                    25,
                    58,
                    5,
                    3,
                    0
                ],
                "arxiv_comment": "This paper presents an initial and self-contained study of a lightweight screening pipeline for emotion-aware language modeling, intended as a reproducible baseline and system-level design reference",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zilin Li"
                    },
                    {
                        "name": "Weiwei Xu"
                    },
                    {
                        "name": "Xuanbo Lu"
                    },
                    {
                        "name": "Zheda Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheda Liu"
                },
                "author": "Zheda Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.12595v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12595v2",
                "title": "Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation"
                },
                "updated": "2026-01-03T07:05:35Z",
                "updated_parsed": [
                    2026,
                    1,
                    3,
                    7,
                    5,
                    35,
                    5,
                    3,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12595v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12595v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-14T08:28:50Z",
                "published_parsed": [
                    2025,
                    12,
                    14,
                    8,
                    28,
                    50,
                    6,
                    348,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Karthikeya KV"
                    }
                ],
                "author_detail": {
                    "name": "Karthikeya KV"
                },
                "author": "Karthikeya KV"
            },
            {
                "id": "http://arxiv.org/abs/2601.01086v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01086v1",
                "title": "Decision-Aware Semantic State Synchronization in Compute-First Networking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Aware Semantic State Synchronization in Compute-First Networking"
                },
                "updated": "2026-01-03T06:22:48Z",
                "updated_parsed": [
                    2026,
                    1,
                    3,
                    6,
                    22,
                    48,
                    5,
                    3,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01086v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In Compute-First Networking (CFN), an Access Point (AP) makes task offloading decisions based on resource state information reported by a Service Node (SN). A fundamental challenge arises from the trade-off between update overhead and decision accuracy: Frequent state updates consume limited network resources, while infrequent updates lead to stale state views and degraded task performance, especially under high system load. Existing approaches based on periodic updates or Age of Information (AoI) mainly focus on temporal freshness and often overlook whether a state change is actually relevant to offloading decisions. This paper proposes SenseCFN, a decision-aware state synchronization framework for CFN. Instead of synchronizing raw resource states, SenseCFN focuses on identifying state changes that are likely to alter offloading decisions. To this end, we introduce a lightweight semantic state representation that captures decision-relevant system characteristics, along with a Semantic Deviation Index (SDI) to quantify the impact of state shifts on decision outcomes. Based on SDI, the SN triggers updates only when significant decision-impacting changes are detected. Meanwhile, the AP performs offloading decisions using cached semantic states with explicit awareness of potential staleness. The update and offloading policies are jointly optimized using a centralized training with distributed execution (CTDE) approach. Simulation results show that SenseCFN maintains a task success rate of up to 99.6% in saturation-prone scenarios, outperforming baseline methods by more than 25%, while reducing status update frequency by approximately 70% to 96%. These results indicate that decision-aware state synchronization provides an effective and practical alternative to purely time-based update strategies in CFN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Compute-First Networking (CFN), an Access Point (AP) makes task offloading decisions based on resource state information reported by a Service Node (SN). A fundamental challenge arises from the trade-off between update overhead and decision accuracy: Frequent state updates consume limited network resources, while infrequent updates lead to stale state views and degraded task performance, especially under high system load. Existing approaches based on periodic updates or Age of Information (AoI) mainly focus on temporal freshness and often overlook whether a state change is actually relevant to offloading decisions. This paper proposes SenseCFN, a decision-aware state synchronization framework for CFN. Instead of synchronizing raw resource states, SenseCFN focuses on identifying state changes that are likely to alter offloading decisions. To this end, we introduce a lightweight semantic state representation that captures decision-relevant system characteristics, along with a Semantic Deviation Index (SDI) to quantify the impact of state shifts on decision outcomes. Based on SDI, the SN triggers updates only when significant decision-impacting changes are detected. Meanwhile, the AP performs offloading decisions using cached semantic states with explicit awareness of potential staleness. The update and offloading policies are jointly optimized using a centralized training with distributed execution (CTDE) approach. Simulation results show that SenseCFN maintains a task success rate of up to 99.6% in saturation-prone scenarios, outperforming baseline methods by more than 25%, while reducing status update frequency by approximately 70% to 96%. These results indicate that decision-aware state synchronization provides an effective and practical alternative to purely time-based update strategies in CFN."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-03T06:22:48Z",
                "published_parsed": [
                    2026,
                    1,
                    3,
                    6,
                    22,
                    48,
                    5,
                    3,
                    0
                ],
                "arxiv_comment": "12 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Jianpeng Qi"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Chengrui Wang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Junyu Dong"
                    },
                    {
                        "name": "Yanwei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yanwei Yu"
                },
                "author": "Yanwei Yu"
            },
            {
                "id": "http://arxiv.org/abs/2601.01046v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01046v1",
                "title": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs"
                },
                "updated": "2026-01-03T02:55:43Z",
                "updated_parsed": [
                    2026,
                    1,
                    3,
                    2,
                    55,
                    43,
                    5,
                    3,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01046v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-03T02:55:43Z",
                "published_parsed": [
                    2026,
                    1,
                    3,
                    2,
                    55,
                    43,
                    5,
                    3,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yixuan Tang"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang"
            },
            {
                "id": "http://arxiv.org/abs/2512.09238v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09238v2",
                "title": "Training-free Context-adaptive Attention for Efficient Long Context Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free Context-adaptive Attention for Efficient Long Context Modeling"
                },
                "updated": "2026-01-02T12:55:29Z",
                "updated_parsed": [
                    2026,
                    1,
                    2,
                    12,
                    55,
                    29,
                    4,
                    2,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09238v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-10T01:54:57Z",
                "published_parsed": [
                    2025,
                    12,
                    10,
                    1,
                    54,
                    57,
                    2,
                    344,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zeng You"
                    },
                    {
                        "name": "Yaofo Chen"
                    },
                    {
                        "name": "Shuhai Zhang"
                    },
                    {
                        "name": "Zhijie Qiu"
                    },
                    {
                        "name": "Tingyu Wu"
                    },
                    {
                        "name": "Yingjian Li"
                    },
                    {
                        "name": "Yaowei Wang"
                    },
                    {
                        "name": "Mingkui Tan"
                    }
                ],
                "author_detail": {
                    "name": "Mingkui Tan"
                },
                "author": "Mingkui Tan"
            },
            {
                "id": "http://arxiv.org/abs/2601.00456v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.00456v1",
                "title": "ROBIN: Incremental Oblique Interleaved ECC for Reliability Improvement in STT-MRAM Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROBIN: Incremental Oblique Interleaved ECC for Reliability Improvement in STT-MRAM Caches"
                },
                "updated": "2026-01-01T19:45:12Z",
                "updated_parsed": [
                    2026,
                    1,
                    1,
                    19,
                    45,
                    12,
                    3,
                    1,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.00456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.00456v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Spin-Transfer Torque Magnetic RAM} (STT-MRAM) is a promising alternative for SRAMs in on-chip cache memories. Besides all its advantages, high error rate in STT-MRAM is a major limiting factor for on-chip cache memories. In this paper, we first present a comprehensive analysis that reveals that the conventional Error-Correcting Codes (ECCs) lose their efficiency due to data-dependent error patterns, and then propose an efficient ECC configuration, so-called ROBIN, to improve the correction capability. The evaluations show that the inefficiency of conventional ECC increases the cache error rate by an average of 151.7% while ROBIN reduces this value by more than 28.6x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin-Transfer Torque Magnetic RAM} (STT-MRAM) is a promising alternative for SRAMs in on-chip cache memories. Besides all its advantages, high error rate in STT-MRAM is a major limiting factor for on-chip cache memories. In this paper, we first present a comprehensive analysis that reveals that the conventional Error-Correcting Codes (ECCs) lose their efficiency due to data-dependent error patterns, and then propose an efficient ECC configuration, so-called ROBIN, to improve the correction capability. The evaluations show that the inefficiency of conventional ECC increases the cache error rate by an average of 151.7% while ROBIN reduces this value by more than 28.6x."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-01T19:45:12Z",
                "published_parsed": [
                    2026,
                    1,
                    1,
                    19,
                    45,
                    12,
                    3,
                    1,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Elham Cheshmikhani"
                    },
                    {
                        "name": "Hamed Farbeh"
                    },
                    {
                        "name": "Hossein Asadi"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Asadi"
                },
                "author": "Hossein Asadi"
            },
            {
                "id": "http://arxiv.org/abs/2601.00450v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.00450v1",
                "title": "Enhancing Reliability of STT-MRAM Caches by Eliminating Read Disturbance Accumulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Reliability of STT-MRAM Caches by Eliminating Read Disturbance Accumulation"
                },
                "updated": "2026-01-01T19:22:51Z",
                "updated_parsed": [
                    2026,
                    1,
                    1,
                    19,
                    22,
                    51,
                    3,
                    1,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.00450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.00450v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Spin-Transfer Torque Magnetic RAM (STT-MRAM) as one of the most promising replacements for SRAMs in on-chip cache memories benefits from higher density and scalability, near-zero leakage power, and non-volatility, but its reliability is threatened by high read disturbance error rate. Error-Correcting Codes (ECCs) are conventionally suggested to overcome the read disturbance errors in STT-MRAM caches. By employing aggressive ECCs and checking out a cache block on every read access, a high level of cache reliability is achieved. However, to minimize the cache access time in modern processors, all blocks in the target cache set are simultaneously read in parallel for tags comparison operation and only the requested block is sent out, if any, after checking its ECC. These extra cache block reads without checking their ECCs until requesting the blocks by the processor cause the accumulation of read disturbance error, which significantly degrade the cache reliability. In this paper, we first introduce and formulate the read disturbance accumulation phenomenon and reveal that this accumulation due to conventional parallel accesses of cache blocks significantly increases the cache error rate. Then, we propose a simple yet effective scheme, so-called Read Error Accumulation Preventer cache (REAP-cache), to completely eliminate the accumulation of read disturbances without compromising the cache performance. Our evaluations show that the proposed REAP-cache extends the cache Mean Time To Failure (MTTF) by 171x, while increases the cache area by less than 1% and energy consumption by only 2.7%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin-Transfer Torque Magnetic RAM (STT-MRAM) as one of the most promising replacements for SRAMs in on-chip cache memories benefits from higher density and scalability, near-zero leakage power, and non-volatility, but its reliability is threatened by high read disturbance error rate. Error-Correcting Codes (ECCs) are conventionally suggested to overcome the read disturbance errors in STT-MRAM caches. By employing aggressive ECCs and checking out a cache block on every read access, a high level of cache reliability is achieved. However, to minimize the cache access time in modern processors, all blocks in the target cache set are simultaneously read in parallel for tags comparison operation and only the requested block is sent out, if any, after checking its ECC. These extra cache block reads without checking their ECCs until requesting the blocks by the processor cause the accumulation of read disturbance error, which significantly degrade the cache reliability. In this paper, we first introduce and formulate the read disturbance accumulation phenomenon and reveal that this accumulation due to conventional parallel accesses of cache blocks significantly increases the cache error rate. Then, we propose a simple yet effective scheme, so-called Read Error Accumulation Preventer cache (REAP-cache), to completely eliminate the accumulation of read disturbances without compromising the cache performance. Our evaluations show that the proposed REAP-cache extends the cache Mean Time To Failure (MTTF) by 171x, while increases the cache area by less than 1% and energy consumption by only 2.7%."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-01T19:22:51Z",
                "published_parsed": [
                    2026,
                    1,
                    1,
                    19,
                    22,
                    51,
                    3,
                    1,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Elham Cheshmikhani"
                    },
                    {
                        "name": "Hamed Farbeh"
                    },
                    {
                        "name": "Hossein Asadi"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Asadi"
                },
                "author": "Hossein Asadi"
            },
            {
                "id": "http://arxiv.org/abs/2507.03445v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.03445v2",
                "title": "Quantum Algorithm for the Fixed-Radius Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Algorithm for the Fixed-Radius Neighbor Search"
                },
                "updated": "2026-01-01T15:41:16Z",
                "updated_parsed": [
                    2026,
                    1,
                    1,
                    15,
                    41,
                    16,
                    3,
                    1,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.03445v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.03445v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Neighbor search is a computationally demanding problem, usually both time- and memory-consuming. The main problem of this kind of algorithms is the long execution time due to cache misses. In this work, we propose a quantum algorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the fixed-point version of Grover's algorithm. We propose an efficient circuit for solving the FRANS with linear query complexity with the number of particles $N$. The quantum circuit returns the list of all the neighbors' pairs within the fixed radius, together with their distance, avoiding the slow down given by cache miss.\n  We analyzed the gate and the query complexity of the circuit. Our FRANS algorithm presents a query complexity of $\\mathcal{O}(N/\\sqrt{M})$, where $M$ is the number of solutions, reaching the optimal lower bound of the Grover's algorithm. We propose different implementations of the oracle, which must be chosen depending on the precise structure of the database. Among these, we present an implementation using the Chebyshev distance with depth $\\mathcal{O}(q_1)$, where $2^{q_1}$ is the number of grid points used to discretize a spatial dimension. State-of-the-art algorithms for state preparation allow for a trade-off between depth and width of the circuit, with a volume (depth$\\times$ width) of $\\mathcal{O}(N\\log(N))$. This unfavorable scaling can be brought down to $\\mathcal{O}(\\text{poly}(\\log N))$ in case of structured datasets. We proposed a stopping criterion based on Bayes interference and tested its validity on $1D$ simulations. Finally, we accounted for the readout complexity and assessed the resilience of the model to the readout error, suggesting an error correction-free strategy to check the accuracy of the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neighbor search is a computationally demanding problem, usually both time- and memory-consuming. The main problem of this kind of algorithms is the long execution time due to cache misses. In this work, we propose a quantum algorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the fixed-point version of Grover's algorithm. We propose an efficient circuit for solving the FRANS with linear query complexity with the number of particles $N$. The quantum circuit returns the list of all the neighbors' pairs within the fixed radius, together with their distance, avoiding the slow down given by cache miss.\n  We analyzed the gate and the query complexity of the circuit. Our FRANS algorithm presents a query complexity of $\\mathcal{O}(N/\\sqrt{M})$, where $M$ is the number of solutions, reaching the optimal lower bound of the Grover's algorithm. We propose different implementations of the oracle, which must be chosen depending on the precise structure of the database. Among these, we present an implementation using the Chebyshev distance with depth $\\mathcal{O}(q_1)$, where $2^{q_1}$ is the number of grid points used to discretize a spatial dimension. State-of-the-art algorithms for state preparation allow for a trade-off between depth and width of the circuit, with a volume (depth$\\times$ width) of $\\mathcal{O}(N\\log(N))$. This unfavorable scaling can be brought down to $\\mathcal{O}(\\text{poly}(\\log N))$ in case of structured datasets. We proposed a stopping criterion based on Bayes interference and tested its validity on $1D$ simulations. Finally, we accounted for the readout complexity and assessed the resilience of the model to the readout error, suggesting an error correction-free strategy to check the accuracy of the results."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-04T10:01:10Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "arxiv_comment": "19 pages, 10 figures, 1 table",
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Luca Cappelli"
                    },
                    {
                        "name": "Claudio Sanavio"
                    },
                    {
                        "name": "Alessandro Andrea Zecchi"
                    },
                    {
                        "name": "Giuseppe Murante"
                    },
                    {
                        "name": "Sauro Succi"
                    }
                ],
                "author_detail": {
                    "name": "Sauro Succi"
                },
                "author": "Sauro Succi"
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.15683v4",
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models"
                },
                "updated": "2026-01-01T05:45:17Z",
                "updated_parsed": [
                    2026,
                    1,
                    1,
                    5,
                    45,
                    17,
                    3,
                    1,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.15683v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.15683v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Private data holds promise for improving LLMs due to its high quality, but its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transformer-based federated split models are proposed, which offload most model parameters to the server (or distributed clients) while retaining only a small portion on the client to ensure data privacy. Despite this design, they still face three challenges: 1) Peer-to-peer key encryption struggles to secure transmitted vectors effectively; 2) The auto-regressive nature of LLMs means that federated split learning can only train and infer sequentially, causing high communication overhead; 3) Fixed partition points lack adaptability to downstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure, Efficient, and Adaptive Federated splitting framework based on LLaMA2. First, we inject Gaussian noise into forward-pass hidden states to enable secure end-to-end vector transmission. Second, we employ attention-mask compression and KV cache collaboration to reduce communication costs, accelerating training and inference. Third, we allow users to dynamically adjust the partition points for input/output blocks based on specific task requirements. Experiments on natural language understanding, summarization, and conversational QA tasks show that FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and achieves up to 8x speedups in training and inference. Further analysis of privacy attacks and different partition points also demonstrates the effectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transformer-based federated split models are proposed, which offload most model parameters to the server (or distributed clients) while retaining only a small portion on the client to ensure data privacy. Despite this design, they still face three challenges: 1) Peer-to-peer key encryption struggles to secure transmitted vectors effectively; 2) The auto-regressive nature of LLMs means that federated split learning can only train and infer sequentially, causing high communication overhead; 3) Fixed partition points lack adaptability to downstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure, Efficient, and Adaptive Federated splitting framework based on LLaMA2. First, we inject Gaussian noise into forward-pass hidden states to enable secure end-to-end vector transmission. Second, we employ attention-mask compression and KV cache collaboration to reduce communication costs, accelerating training and inference. Third, we allow users to dynamically adjust the partition points for input/output blocks based on specific task requirements. Experiments on natural language understanding, summarization, and conversational QA tasks show that FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and achieves up to 8x speedups in training and inference. Further analysis of privacy attacks and different partition points also demonstrates the effectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2506.01928v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.01928v2",
                "title": "Esoteric Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Esoteric Language Models"
                },
                "updated": "2025-12-31T19:08:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    31,
                    19,
                    8,
                    7,
                    2,
                    365,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.01928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.01928v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Within this family, Masked Diffusion Models (MDMs) currently perform best but still underperform AR models in perplexity and lack key inference-time efficiency features, most notably KV caching. We introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, smoothly interpolating between their perplexities while overcoming their respective limitations. Unlike prior work, which uses transformers with bidirectional attention as MDM denoisers, we exploit the connection between MDMs and Any-Order autoregressive models and adopt causal attention. This design lets us compute the exact likelihood of MDMs for the first time and, crucially, enables us \\to introduce KV caching for MDMs while preserving parallel generation for the first time, significantly improving inference efficiency. Combined with an optimized sampling schedule, Eso-LMs achieves a new state of the art on the speed-quality Pareto frontier for unconditional generation. On long contexts, it yields $\\mathbf{14 - 65{}\\times}$ faster inference than standard MDMs and $\\mathbf{3 - 4{}\\times}$ faster inference than prior semi-autoregressive approaches. We provide code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/Eso-LMs",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Within this family, Masked Diffusion Models (MDMs) currently perform best but still underperform AR models in perplexity and lack key inference-time efficiency features, most notably KV caching. We introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, smoothly interpolating between their perplexities while overcoming their respective limitations. Unlike prior work, which uses transformers with bidirectional attention as MDM denoisers, we exploit the connection between MDMs and Any-Order autoregressive models and adopt causal attention. This design lets us compute the exact likelihood of MDMs for the first time and, crucially, enables us \\to introduce KV caching for MDMs while preserving parallel generation for the first time, significantly improving inference efficiency. Combined with an optimized sampling schedule, Eso-LMs achieves a new state of the art on the speed-quality Pareto frontier for unconditional generation. On long contexts, it yields $\\mathbf{14 - 65{}\\times}$ faster inference than standard MDMs and $\\mathbf{3 - 4{}\\times}$ faster inference than prior semi-autoregressive approaches. We provide code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/Eso-LMs"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-02T17:47:27Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    47,
                    27,
                    0,
                    153,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Johnna Liu"
                    },
                    {
                        "name": "Deepansha Singh"
                    },
                    {
                        "name": "Zhoujun Cheng"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "John Thickstun"
                    },
                    {
                        "name": "Arash Vahdat"
                    }
                ],
                "author_detail": {
                    "name": "Arash Vahdat"
                },
                "author": "Arash Vahdat"
            },
            {
                "id": "http://arxiv.org/abs/2512.25065v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.25065v1",
                "title": "Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search"
                },
                "updated": "2025-12-31T18:58:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    31,
                    18,
                    58,
                    19,
                    2,
                    365,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.25065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.25065v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Resource-management tasks in modern operating and distributed systems continue to rely primarily on hand-designed heuristics for tasks such as scheduling, caching, or active queue management. Designing performant heuristics is an expensive, time-consuming process that we are forced to continuously go through due to the constant flux of hardware, workloads and environments.\n  We propose a new alternative: synthesizing instance-optimal heuristics -- specialized for the exact workloads and hardware where they will be deployed -- using code-generating large language models (LLMs). To make this synthesis tractable, Vulcan separates policy and mechanism through LLM-friendly, task-agnostic interfaces. With these interfaces, users specify the inputs and objectives of their desired policy, while Vulcan searches for performant policies via evolutionary search over LLM-generated code. This interface is expressive enough to capture a wide range of system policies, yet sufficiently constrained to allow even small, inexpensive LLMs to generate correct and executable code.\n  We use Vulcan to synthesize performant heuristics for cache eviction and memory tiering, and find that these heuristics outperform all human-designed state-of-the-art algorithms by upto 69% and 7.9% in performance for each of these tasks respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-management tasks in modern operating and distributed systems continue to rely primarily on hand-designed heuristics for tasks such as scheduling, caching, or active queue management. Designing performant heuristics is an expensive, time-consuming process that we are forced to continuously go through due to the constant flux of hardware, workloads and environments.\n  We propose a new alternative: synthesizing instance-optimal heuristics -- specialized for the exact workloads and hardware where they will be deployed -- using code-generating large language models (LLMs). To make this synthesis tractable, Vulcan separates policy and mechanism through LLM-friendly, task-agnostic interfaces. With these interfaces, users specify the inputs and objectives of their desired policy, while Vulcan searches for performant policies via evolutionary search over LLM-generated code. This interface is expressive enough to capture a wide range of system policies, yet sufficiently constrained to allow even small, inexpensive LLMs to generate correct and executable code.\n  We use Vulcan to synthesize performant heuristics for cache eviction and memory tiering, and find that these heuristics outperform all human-designed state-of-the-art algorithms by upto 69% and 7.9% in performance for each of these tasks respectively."
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-31T18:58:19Z",
                "published_parsed": [
                    2025,
                    12,
                    31,
                    18,
                    58,
                    19,
                    2,
                    365,
                    0
                ],
                "arxiv_comment": "27 pages, 11 figures, 7 tables",
                "arxiv_primary_category": {
                    "term": "cs.OS"
                },
                "authors": [
                    {
                        "name": "Rohit Dwivedula"
                    },
                    {
                        "name": "Divyanshu Saxena"
                    },
                    {
                        "name": "Sujay Yadalam"
                    },
                    {
                        "name": "Daehyeok Kim"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella"
            },
            {
                "id": "http://arxiv.org/abs/2410.00414v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2410.00414v4",
                "title": "Semantic Parsing with Candidate Expressions for Knowledge Base Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Parsing with Candidate Expressions for Knowledge Base Question Answering"
                },
                "updated": "2025-12-31T18:45:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    31,
                    18,
                    45,
                    49,
                    2,
                    365,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2410.00414v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2410.00414v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1016/j.eswa.2025.130564",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Semantic parsers convert natural language to logical forms, which can be evaluated on knowledge bases (KBs) to produce denotations. Recent semantic parsers have been developed with sequence-to-sequence (seq2seq) pre-trained language models (PLMs) or large language models, where the models treat logical forms as sequences of tokens. For syntactic and semantic validity, the semantic parsers use grammars that enable constrained decoding. However, the grammars lack the ability to utilize large information of KBs, although logical forms contain representations of KB elements, such as entities or relations. In this work, we propose a grammar augmented with candidate expressions for semantic parsing on a large KB with a seq2seq PLM. The grammar defines actions as production rules, and our semantic parser predicts actions during inference under the constraints by types and candidate expressions. We apply the grammar to knowledge base question answering, where the constraints by candidate expressions assist a semantic parser to generate valid KB elements. We also introduce two special rules, sub-type inference and union types, and a mask caching algorithm. In particular, sub-type inference and the mask caching algorithm greatly increase the decoding speed of our semantic parser. We experimented on two benchmarks, KQA Pro and Overnight, where the constraints by candidate expressions increased the accuracy of our semantic parser, whether it was trained with strong supervision or weak supervision. In addition, our semantic parser had a fast decoding speed in the experiments. Our source code is publicly available at https://github.com/daehwannam/candexpr-sp.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic parsers convert natural language to logical forms, which can be evaluated on knowledge bases (KBs) to produce denotations. Recent semantic parsers have been developed with sequence-to-sequence (seq2seq) pre-trained language models (PLMs) or large language models, where the models treat logical forms as sequences of tokens. For syntactic and semantic validity, the semantic parsers use grammars that enable constrained decoding. However, the grammars lack the ability to utilize large information of KBs, although logical forms contain representations of KB elements, such as entities or relations. In this work, we propose a grammar augmented with candidate expressions for semantic parsing on a large KB with a seq2seq PLM. The grammar defines actions as production rules, and our semantic parser predicts actions during inference under the constraints by types and candidate expressions. We apply the grammar to knowledge base question answering, where the constraints by candidate expressions assist a semantic parser to generate valid KB elements. We also introduce two special rules, sub-type inference and union types, and a mask caching algorithm. In particular, sub-type inference and the mask caching algorithm greatly increase the decoding speed of our semantic parser. We experimented on two benchmarks, KQA Pro and Overnight, where the constraints by candidate expressions increased the accuracy of our semantic parser, whether it was trained with strong supervision or weak supervision. In addition, our semantic parser had a fast decoding speed in the experiments. Our source code is publicly available at https://github.com/daehwannam/candexpr-sp.git."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-10-01T05:46:22Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    46,
                    22,
                    1,
                    275,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "arxiv_journal_ref": "Expert Syst. Appl. 306 (2026) 130564",
                "authors": [
                    {
                        "name": "Daehwan Nam"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "arxiv_doi": "10.1016/j.eswa.2025.130564"
            },
            {
                "id": "http://arxiv.org/abs/2512.24902v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.24902v1",
                "title": "Adaptive Resource Orchestration for Distributed Quantum Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Resource Orchestration for Distributed Quantum Computing Systems"
                },
                "updated": "2025-12-31T14:58:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    31,
                    14,
                    58,
                    5,
                    2,
                    365,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.24902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.24902v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Scaling quantum computing beyond a single device requires networking many quantum processing units (QPUs) into a coherent quantum-HPC system. We propose the Modular Entanglement Hub (ModEn-Hub) architecture: a hub-and-spoke photonic interconnect paired with a real-time quantum network orchestrator. ModEn-Hub centralizes entanglement sources and shared quantum memory to deliver on-demand, high-fidelity Bell pairs across heterogeneous QPUs, while the control plane schedules teleportation-based non-local gates, launches parallel entanglement attempts, and maintains a small ebit cache. To quantify benefits, we implement a lightweight, reproducible Monte Carlo study under realistic loss and tight round budgets, comparing a naive sequential baseline to an orchestrated policy with logarithmically scaled parallelism and opportunistic caching. Across 1-128 QPUs and 2,500 trials per point, ModEn-Hub-style orchestration sustains about 90% teleportation success while the baseline degrades toward about 30%, at the cost of higher average entanglement attempts (about 10-12 versus about 3). These results provide clear, high-level evidence that adaptive resource orchestration in the ModEn-Hub enables scalable and efficient quantum-HPC operation on near-term hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling quantum computing beyond a single device requires networking many quantum processing units (QPUs) into a coherent quantum-HPC system. We propose the Modular Entanglement Hub (ModEn-Hub) architecture: a hub-and-spoke photonic interconnect paired with a real-time quantum network orchestrator. ModEn-Hub centralizes entanglement sources and shared quantum memory to deliver on-demand, high-fidelity Bell pairs across heterogeneous QPUs, while the control plane schedules teleportation-based non-local gates, launches parallel entanglement attempts, and maintains a small ebit cache. To quantify benefits, we implement a lightweight, reproducible Monte Carlo study under realistic loss and tight round budgets, comparing a naive sequential baseline to an orchestrated policy with logarithmically scaled parallelism and opportunistic caching. Across 1-128 QPUs and 2,500 trials per point, ModEn-Hub-style orchestration sustains about 90% teleportation success while the baseline degrades toward about 30%, at the cost of higher average entanglement attempts (about 10-12 versus about 3). These results provide clear, high-level evidence that adaptive resource orchestration in the ModEn-Hub enables scalable and efficient quantum-HPC operation on near-term hardware."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-31T14:58:05Z",
                "published_parsed": [
                    2025,
                    12,
                    31,
                    14,
                    58,
                    5,
                    2,
                    365,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Kuan-Cheng Chen"
                    },
                    {
                        "name": "Felix Burt"
                    },
                    {
                        "name": "Nitish K. Panigrahy"
                    },
                    {
                        "name": "Kin K. Leung"
                    }
                ],
                "author_detail": {
                    "name": "Kin K. Leung"
                },
                "author": "Kin K. Leung"
            },
            {
                "id": "http://arxiv.org/abs/2512.24711v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.24711v1",
                "title": "MEIC-DT: Memory-Efficient Incremental Clustering for Long-Text Coreference Resolution with Dual-Threshold Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEIC-DT: Memory-Efficient Incremental Clustering for Long-Text Coreference Resolution with Dual-Threshold Constraints"
                },
                "updated": "2025-12-31T08:26:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    31,
                    8,
                    26,
                    34,
                    2,
                    365,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.24711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.24711v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In the era of large language models (LLMs), supervised neural methods remain the state-of-the-art (SOTA) for Coreference Resolution. Yet, their full potential is underexplored, particularly in incremental clustering, which faces the critical challenge of balancing efficiency with performance for long texts. To address the limitation, we propose \\textbf{MEIC-DT}, a novel dual-threshold, memory-efficient incremental clustering approach based on a lightweight Transformer. MEIC-DT features a dual-threshold constraint mechanism designed to precisely control the Transformer's input scale within a predefined memory budget. This mechanism incorporates a Statistics-Aware Eviction Strategy (\\textbf{SAES}), which utilizes distinct statistical profiles from the training and inference phases for intelligent cache management. Furthermore, we introduce an Internal Regularization Policy (\\textbf{IRP}) that strategically condenses clusters by selecting the most representative mentions, thereby preserving semantic integrity. Extensive experiments on common benchmarks demonstrate that MEIC-DT achieves highly competitive coreference performance under stringent memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of large language models (LLMs), supervised neural methods remain the state-of-the-art (SOTA) for Coreference Resolution. Yet, their full potential is underexplored, particularly in incremental clustering, which faces the critical challenge of balancing efficiency with performance for long texts. To address the limitation, we propose \\textbf{MEIC-DT}, a novel dual-threshold, memory-efficient incremental clustering approach based on a lightweight Transformer. MEIC-DT features a dual-threshold constraint mechanism designed to precisely control the Transformer's input scale within a predefined memory budget. This mechanism incorporates a Statistics-Aware Eviction Strategy (\\textbf{SAES}), which utilizes distinct statistical profiles from the training and inference phases for intelligent cache management. Furthermore, we introduce an Internal Regularization Policy (\\textbf{IRP}) that strategically condenses clusters by selecting the most representative mentions, thereby preserving semantic integrity. Extensive experiments on common benchmarks demonstrate that MEIC-DT achieves highly competitive coreference performance under stringent memory constraints."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-31T08:26:34Z",
                "published_parsed": [
                    2025,
                    12,
                    31,
                    8,
                    26,
                    34,
                    2,
                    365,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Kangyang Luo"
                    },
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Yuzhuo Bai"
                    },
                    {
                        "name": "Cheng Gao"
                    },
                    {
                        "name": "Zhitong Wang"
                    },
                    {
                        "name": "Cheng Huang"
                    },
                    {
                        "name": "Yingli Shen"
                    },
                    {
                        "name": "Yufeng Han"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Cunliang Kong"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun"
            },
            {
                "id": "http://arxiv.org/abs/2510.13940v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.13940v2",
                "title": "Less is More: Improving LLM Reasoning with Minimal Test-Time Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: Improving LLM Reasoning with Minimal Test-Time Intervention"
                },
                "updated": "2025-12-31T07:36:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    31,
                    7,
                    36,
                    24,
                    2,
                    365,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.13940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.13940v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent progress in large language models (LLMs) has focused on test-time scaling to improve reasoning via increased inference computation, but often at the cost of efficiency. We revisit test-time behavior and uncover a simple yet underexplored phenomenon: reasoning uncertainty is highly localized-only a small subset of high-entropy tokens dominantly affects output correctness. Motivated by this, we propose Minimal Test-Time Intervention (MTI), a training-free framework that enhances reasoning accuracy and stability with minimal overhead. MTI includes: (i) Selective CFG intervention, applying classifier-free guidance only at uncertain positions; and (ii) Lightweight negative-prompt guidance, reusing the main model's KV cache to approximate unconditional decoding efficiently. MTI yields consistent gains across general, coding, and STEM tasks-e.g., +9.28% average improvement on six benchmarks for DeepSeek-R1-7B and +11.25% on AIME2024 using Ling-mini-2.0-while remaining highly efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has focused on test-time scaling to improve reasoning via increased inference computation, but often at the cost of efficiency. We revisit test-time behavior and uncover a simple yet underexplored phenomenon: reasoning uncertainty is highly localized-only a small subset of high-entropy tokens dominantly affects output correctness. Motivated by this, we propose Minimal Test-Time Intervention (MTI), a training-free framework that enhances reasoning accuracy and stability with minimal overhead. MTI includes: (i) Selective CFG intervention, applying classifier-free guidance only at uncertain positions; and (ii) Lightweight negative-prompt guidance, reusing the main model's KV cache to approximate unconditional decoding efficiently. MTI yields consistent gains across general, coding, and STEM tasks-e.g., +9.28% average improvement on six benchmarks for DeepSeek-R1-7B and +11.25% on AIME2024 using Ling-mini-2.0-while remaining highly efficient."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-15T17:59:45Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    59,
                    45,
                    2,
                    288,
                    0
                ],
                "arxiv_comment": "Code: https://github.com/EnVision-Research/MTI",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Mingyang Zhang"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Ganggui Ding"
                    },
                    {
                        "name": "Liang Hou"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Ying-Cong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying-Cong Chen"
                },
                "author": "Ying-Cong Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.17298v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17298v2",
                "title": "ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration"
                },
                "updated": "2025-12-31T06:37:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    31,
                    6,
                    37,
                    0,
                    2,
                    365,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17298v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model's temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model's temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T07:27:19Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    7,
                    27,
                    19,
                    4,
                    353,
                    0
                ],
                "arxiv_comment": "Accepted for poster presentation at AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Fanpu Cao"
                    },
                    {
                        "name": "Yaofo Chen"
                    },
                    {
                        "name": "Zeng You"
                    },
                    {
                        "name": "Wei Luo"
                    },
                    {
                        "name": "Cen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cen Chen"
                },
                "author": "Cen Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.24255v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.24255v1",
                "title": "How Would Oblivious Memory Boost Graph Analytics on Trusted Processors?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Would Oblivious Memory Boost Graph Analytics on Trusted Processors?"
                },
                "updated": "2025-12-30T14:28:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    30,
                    14,
                    28,
                    29,
                    1,
                    364,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.24255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.24255v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Trusted processors provide a way to perform joint computations while preserving data privacy. To overcome the performance degradation caused by data-oblivious algorithms to prevent information leakage, we explore the benefits of oblivious memory (OM) integrated in processors, to which the accesses are unobservable by adversaries. We focus on graph analytics, an important application vulnerable to access-pattern attacks. With a co-design between storage structure and algorithms, our prototype system is 100x faster than baselines given an OM sized around the per-core cache which can be implemented on existing processors with negligible overhead. This gives insights into equipping trusted processors with OM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted processors provide a way to perform joint computations while preserving data privacy. To overcome the performance degradation caused by data-oblivious algorithms to prevent information leakage, we explore the benefits of oblivious memory (OM) integrated in processors, to which the accesses are unobservable by adversaries. We focus on graph analytics, an important application vulnerable to access-pattern attacks. With a co-design between storage structure and algorithms, our prototype system is 100x faster than baselines given an OM sized around the per-core cache which can be implemented on existing processors with negligible overhead. This gives insights into equipping trusted processors with OM."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-30T14:28:29Z",
                "published_parsed": [
                    2025,
                    12,
                    30,
                    14,
                    28,
                    29,
                    1,
                    364,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Jiping Yu"
                    },
                    {
                        "name": "Xiaowei Zhu"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Guanyu Feng"
                    },
                    {
                        "name": "Yunyi Chen"
                    },
                    {
                        "name": "Xiaoyu Fan"
                    },
                    {
                        "name": "Wenguang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenguang Chen"
                },
                "author": "Wenguang Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.24195v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.24195v1",
                "title": "CorGi: Contribution-Guided Block-Wise Interval Caching for Training-Free Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CorGi: Contribution-Guided Block-Wise Interval Caching for Training-Free Acceleration of Diffusion Transformers"
                },
                "updated": "2025-12-30T12:55:38Z",
                "updated_parsed": [
                    2025,
                    12,
                    30,
                    12,
                    55,
                    38,
                    1,
                    364,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.24195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.24195v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion transformer (DiT) achieves remarkable performance in visual generation, but its iterative denoising process combined with larger capacity leads to a high inference cost. Recent works have demonstrated that the iterative denoising process of DiT models involves substantial redundant computation across steps. To effectively reduce the redundant computation in DiT, we propose CorGi (Contribution-Guided Block-Wise Interval Caching), training-free DiT inference acceleration framework that selectively reuses the outputs of transformer blocks in DiT across denoising steps. CorGi caches low-contribution blocks and reuses them in later steps within each interval to reduce redundant computation while preserving generation quality. For text-to-image tasks, we further propose CorGi+, which leverages per-block cross-attention maps to identify salient tokens and applies partial attention updates to protect important object details. Evaluation on the state-of-the-art DiT models demonstrates that CorGi and CorGi+ achieve up to 2.0x speedup on average, while preserving high generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformer (DiT) achieves remarkable performance in visual generation, but its iterative denoising process combined with larger capacity leads to a high inference cost. Recent works have demonstrated that the iterative denoising process of DiT models involves substantial redundant computation across steps. To effectively reduce the redundant computation in DiT, we propose CorGi (Contribution-Guided Block-Wise Interval Caching), training-free DiT inference acceleration framework that selectively reuses the outputs of transformer blocks in DiT across denoising steps. CorGi caches low-contribution blocks and reuses them in later steps within each interval to reduce redundant computation while preserving generation quality. For text-to-image tasks, we further propose CorGi+, which leverages per-block cross-attention maps to identify salient tokens and applies partial attention updates to protect important object details. Evaluation on the state-of-the-art DiT models demonstrates that CorGi and CorGi+ achieve up to 2.0x speedup on average, while preserving high generation quality."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-30T12:55:38Z",
                "published_parsed": [
                    2025,
                    12,
                    30,
                    12,
                    55,
                    38,
                    1,
                    364,
                    0
                ],
                "arxiv_comment": "16 pages, 20 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yonglak Son"
                    },
                    {
                        "name": "Suhyeok Kim"
                    },
                    {
                        "name": "Seungryong Kim"
                    },
                    {
                        "name": "Young Geun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Young Geun Kim"
                },
                "author": "Young Geun Kim"
            },
            {
                "id": "http://arxiv.org/abs/2504.05646v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.05646v2",
                "title": "Lattice: Learning to Efficiently Compress the Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice: Learning to Efficiently Compress the Memory"
                },
                "updated": "2025-12-30T10:25:28Z",
                "updated_parsed": [
                    2025,
                    12,
                    30,
                    10,
                    25,
                    28,
                    1,
                    364,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.05646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.05646v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Attention mechanisms have revolutionized sequence learning but suffer from quadratic computational complexity. This paper introduces \\model, a novel recurrent neural network (RNN) mechanism that leverages the inherent low-rank structure of K-V matrices to efficiently compress the cache into a fixed number of memory slots, achieving sub-quadratic complexity. We formulate this compression as an online optimization problem and derive a dynamic memory update rule based on a single gradient descent step. The resulting recurrence features a state- and input-dependent gating mechanism, offering an interpretable memory update process. The core innovation is the orthogonal update: each memory slot is updated exclusively with information orthogonal to its current state, hence incorporating only novel, non-redundant data to minimize interference with previously stored information. We derive an efficient computation for this orthogonal update rule and further approximate it with chunk-wise parallelization to ensure training scalability. Empirically, Lattice outperforms strong baselines on language modeling and associative recall tasks across diverse context lengths and model sizes, achieving superior memory efficiency with significantly reduced memory sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have revolutionized sequence learning but suffer from quadratic computational complexity. This paper introduces \\model, a novel recurrent neural network (RNN) mechanism that leverages the inherent low-rank structure of K-V matrices to efficiently compress the cache into a fixed number of memory slots, achieving sub-quadratic complexity. We formulate this compression as an online optimization problem and derive a dynamic memory update rule based on a single gradient descent step. The resulting recurrence features a state- and input-dependent gating mechanism, offering an interpretable memory update process. The core innovation is the orthogonal update: each memory slot is updated exclusively with information orthogonal to its current state, hence incorporating only novel, non-redundant data to minimize interference with previously stored information. We derive an efficient computation for this orthogonal update rule and further approximate it with chunk-wise parallelization to ensure training scalability. Empirically, Lattice outperforms strong baselines on language modeling and associative recall tasks across diverse context lengths and model sizes, achieving superior memory efficiency with significantly reduced memory sizes."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-08T03:48:43Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Mahdi Karami"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni"
            },
            {
                "id": "http://arxiv.org/abs/2511.20649v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.20649v2",
                "title": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout"
                },
                "updated": "2025-12-30T10:14:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    30,
                    10,
                    14,
                    37,
                    1,
                    364,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.20649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.20649v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce $\\infty$-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish $\\infty$-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that $\\infty$-RoPE consistently surpasses previous autoregressive models in overall VBench scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce $\\infty$-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish $\\infty$-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that $\\infty$-RoPE consistently surpasses previous autoregressive models in overall VBench scores."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-25T18:59:46Z",
                "published_parsed": [
                    2025,
                    11,
                    25,
                    18,
                    59,
                    46,
                    1,
                    329,
                    0
                ],
                "arxiv_comment": "Project Page: https://infinity-rope.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hidir Yesiltepe"
                    },
                    {
                        "name": "Tuna Han Salih Meral"
                    },
                    {
                        "name": "Adil Kaan Akan"
                    },
                    {
                        "name": "Kaan Oktay"
                    },
                    {
                        "name": "Pinar Yanardag"
                    }
                ],
                "author_detail": {
                    "name": "Pinar Yanardag"
                },
                "author": "Pinar Yanardag"
            },
            {
                "id": "http://arxiv.org/abs/2509.01085v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.01085v4",
                "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Sparse Attention for Faster Video Diffusion Training"
                },
                "updated": "2025-12-30T08:58:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    30,
                    8,
                    58,
                    30,
                    1,
                    364,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.01085v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.01085v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit major computational bottlenecks when producing high-resolution, long-duration videos. The quadratic complexity of full attention leads to prohibitively high training and inference costs. Full attention inefficiency stems from two key challenges: excessive computation due to the inherent sparsity of Queries and Key-Value pairs, and redundant computation as fixed sparse patterns fail to leverage DiT's dynamic attention. To overcome this limitation, we propose a Bidirectional Sparse Attention (BSA) framework for faster video DiT training, the first to dynamically sparsify both Queries and Key-Value pairs within 3D full attention, thereby substantially improving training and inference efficiency. BSA addresses these issues through two key components. Query sparsity is optimized by selecting the most informative query tokens via semantic similarity and with a dynamic spatial-time training strategy, while KV sparsity is achieved by computing a statistical dynamic threshold to retain only the most salient KV blocks for computation. Extensive experiments demonstrate that BSA significantly accelerates DiT training across long sequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention training, while preserving or even surpassing the generative quality of full attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion Transformer (DiT) models excel in generative quality but hit major computational bottlenecks when producing high-resolution, long-duration videos. The quadratic complexity of full attention leads to prohibitively high training and inference costs. Full attention inefficiency stems from two key challenges: excessive computation due to the inherent sparsity of Queries and Key-Value pairs, and redundant computation as fixed sparse patterns fail to leverage DiT's dynamic attention. To overcome this limitation, we propose a Bidirectional Sparse Attention (BSA) framework for faster video DiT training, the first to dynamically sparsify both Queries and Key-Value pairs within 3D full attention, thereby substantially improving training and inference efficiency. BSA addresses these issues through two key components. Query sparsity is optimized by selecting the most informative query tokens via semantic similarity and with a dynamic spatial-time training strategy, while KV sparsity is achieved by computing a statistical dynamic threshold to retain only the most salient KV blocks for computation. Extensive experiments demonstrate that BSA significantly accelerates DiT training across long sequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention training, while preserving or even surpassing the generative quality of full attention."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-01T03:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Chuyu Shen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.24073v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.24073v1",
                "title": "CPePC: Cooperative and Predictive Popularity based Caching for Named Data Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CPePC: Cooperative and Predictive Popularity based Caching for Named Data Networks"
                },
                "updated": "2025-12-30T08:35:28Z",
                "updated_parsed": [
                    2025,
                    12,
                    30,
                    8,
                    35,
                    28,
                    1,
                    364,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.24073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.24073v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Caching content is an inherent feature of Named Data Networks. Limited cache capacity of routers warrants that the choice of content being cached is judiciously done. Existing techniques resort to caching popular content to maximize utilization. However, these methods experience significant overhead for coordinating and estimating the popularity of content. To address this issue, in this paper, we present CPePC, which is a cooperative caching technique designed to improve performance. It accomplishes this through a combination of two factors. First, CPePC enhances efficiency by minimizing the overhead of popularity estimation. Second, it forecasts a parameter that governs caching decisions. Efficiency in popularity estimation is achieved by dividing the network into several non-overlapping communities using a community estimation algorithm and selecting a leader node to coordinate this on behalf of all the nodes in the community. CPePC bases its caching decisions by predicting a parameter whose value is estimated using current cache occupancy and the popularity of the content into account. We present algorithms for community detection, leader selection, content popularity estimation, and caching decisions made by the CPePC method. We evaluate and compare it with six other state-of-the-art caching techniques, with simulations performed using a discrete event simulator to show that it outperforms others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching content is an inherent feature of Named Data Networks. Limited cache capacity of routers warrants that the choice of content being cached is judiciously done. Existing techniques resort to caching popular content to maximize utilization. However, these methods experience significant overhead for coordinating and estimating the popularity of content. To address this issue, in this paper, we present CPePC, which is a cooperative caching technique designed to improve performance. It accomplishes this through a combination of two factors. First, CPePC enhances efficiency by minimizing the overhead of popularity estimation. Second, it forecasts a parameter that governs caching decisions. Efficiency in popularity estimation is achieved by dividing the network into several non-overlapping communities using a community estimation algorithm and selecting a leader node to coordinate this on behalf of all the nodes in the community. CPePC bases its caching decisions by predicting a parameter whose value is estimated using current cache occupancy and the popularity of the content into account. We present algorithms for community detection, leader selection, content popularity estimation, and caching decisions made by the CPePC method. We evaluate and compare it with six other state-of-the-art caching techniques, with simulations performed using a discrete event simulator to show that it outperforms others."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-30T08:35:28Z",
                "published_parsed": [
                    2025,
                    12,
                    30,
                    8,
                    35,
                    28,
                    1,
                    364,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Pankaj Chaudhary"
                    },
                    {
                        "name": "Neminath Hubballi"
                    },
                    {
                        "name": "Sameer G. Kulkarni"
                    }
                ],
                "author_detail": {
                    "name": "Sameer G. Kulkarni"
                },
                "author": "Sameer G. Kulkarni"
            },
            {
                "id": "http://arxiv.org/abs/2512.23977v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23977v1",
                "title": "Causify DataFlow: A Framework For High-performance Machine Learning Stream Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causify DataFlow: A Framework For High-performance Machine Learning Stream Computing"
                },
                "updated": "2025-12-30T04:24:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    30,
                    4,
                    24,
                    4,
                    1,
                    364,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23977v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present DataFlow, a computational framework for building, testing, and deploying high-performance machine learning systems on unbounded time-series data. Traditional data science workflows assume finite datasets and require substantial reimplementation when moving from batch prototypes to streaming production systems. This gap introduces causality violations, batch boundary artifacts, and poor reproducibility of real-time failures.\n  DataFlow resolves these issues through a unified execution model based on directed acyclic graphs (DAGs) with point-in-time idempotency: outputs at any time t depend only on a fixed-length context window preceding t. This guarantee ensures that models developed in batch mode execute identically in streaming production without code changes. The framework enforces strict causality by automatically tracking knowledge time across all transformations, eliminating future-peeking bugs.\n  DataFlow supports flexible tiling across temporal and feature dimensions, allowing the same model to operate at different frequencies and memory profiles via configuration alone. It integrates natively with the Python data science stack and provides fit/predict semantics for online learning, caching and incremental computation, and automatic parallelization through DAG-based scheduling. We demonstrate its effectiveness across domains including financial trading, IoT, fraud detection, and real-time analytics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DataFlow, a computational framework for building, testing, and deploying high-performance machine learning systems on unbounded time-series data. Traditional data science workflows assume finite datasets and require substantial reimplementation when moving from batch prototypes to streaming production systems. This gap introduces causality violations, batch boundary artifacts, and poor reproducibility of real-time failures.\n  DataFlow resolves these issues through a unified execution model based on directed acyclic graphs (DAGs) with point-in-time idempotency: outputs at any time t depend only on a fixed-length context window preceding t. This guarantee ensures that models developed in batch mode execute identically in streaming production without code changes. The framework enforces strict causality by automatically tracking knowledge time across all transformations, eliminating future-peeking bugs.\n  DataFlow supports flexible tiling across temporal and feature dimensions, allowing the same model to operate at different frequencies and memory profiles via configuration alone. It integrates natively with the Python data science stack and provides fit/predict semantics for online learning, caching and incremental computation, and automatic parallelization through DAG-based scheduling. We demonstrate its effectiveness across domains including financial trading, IoT, fraud detection, and real-time analytics."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-30T04:24:04Z",
                "published_parsed": [
                    2025,
                    12,
                    30,
                    4,
                    24,
                    4,
                    1,
                    364,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Giacinto Paolo Saggese"
                    },
                    {
                        "name": "Paul Smith"
                    }
                ],
                "author_detail": {
                    "name": "Paul Smith"
                },
                "author": "Paul Smith"
            },
            {
                "id": "http://arxiv.org/abs/2512.23938v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23938v1",
                "title": "Learnable Query Aggregation with KV Routing for Cross-view Geo-localisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learnable Query Aggregation with KV Routing for Cross-view Geo-localisation"
                },
                "updated": "2025-12-30T01:51:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    30,
                    1,
                    51,
                    52,
                    1,
                    364,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23938v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cross-view geo-localisation (CVGL) aims to estimate the geographic location of a query image by matching it with images from a large-scale database. However, the significant view-point discrepancies present considerable challenges for effective feature aggregation and alignment. To address these challenges, we propose a novel CVGL system that incorporates three key improvements. Firstly, we leverage the DINOv2 backbone with a convolution adapter fine-tuning to enhance model adaptability to cross-view variations. Secondly, we propose a multi-scale channel reallocation module to strengthen the diversity and stability of spatial representations. Finally, we propose an improved aggregation module that integrates a Mixture-of-Experts (MoE) routing into the feature aggregation process. Specifically, the module dynamically selects expert subspaces for the keys and values in a cross-attention framework, enabling adaptive processing of heterogeneous input domains. Extensive experiments on the University-1652 and SUES-200 datasets demonstrate that our method achieves competitive performance with fewer trained parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-view geo-localisation (CVGL) aims to estimate the geographic location of a query image by matching it with images from a large-scale database. However, the significant view-point discrepancies present considerable challenges for effective feature aggregation and alignment. To address these challenges, we propose a novel CVGL system that incorporates three key improvements. Firstly, we leverage the DINOv2 backbone with a convolution adapter fine-tuning to enhance model adaptability to cross-view variations. Secondly, we propose a multi-scale channel reallocation module to strengthen the diversity and stability of spatial representations. Finally, we propose an improved aggregation module that integrates a Mixture-of-Experts (MoE) routing into the feature aggregation process. Specifically, the module dynamically selects expert subspaces for the keys and values in a cross-attention framework, enabling adaptive processing of heterogeneous input domains. Extensive experiments on the University-1652 and SUES-200 datasets demonstrate that our method achieves competitive performance with fewer trained parameters."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-30T01:51:52Z",
                "published_parsed": [
                    2025,
                    12,
                    30,
                    1,
                    51,
                    52,
                    1,
                    364,
                    0
                ],
                "arxiv_comment": "7 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hualin Ye"
                    },
                    {
                        "name": "Bingxi Liu"
                    },
                    {
                        "name": "Jixiang Du"
                    },
                    {
                        "name": "Yu Qin"
                    },
                    {
                        "name": "Ziyi Chen"
                    },
                    {
                        "name": "Hong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hong Zhang"
                },
                "author": "Hong Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.23852v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23852v1",
                "title": "Trellis: Learning to Compress Key-Value Memory in Attention Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trellis: Learning to Compress Key-Value Memory in Attention Models"
                },
                "updated": "2025-12-29T20:32:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    20,
                    32,
                    10,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23852v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transformers, while powerful, suffer from quadratic computational complexity and the ever-growing Key-Value (KV) cache of the attention mechanism. This paper introduces Trellis, a novel Transformer architecture with bounded memory that learns how to compress its key-value memory dynamically at test time. Trellis replaces the standard KV cache with a fixed-size memory and train a two-pass recurrent compression mechanism to store new keys and values into memory. To achieve this, it leverages an online gradient descent procedure with a forget gate, enabling the compressed memory to be updated recursively while learning to retain important contextual information from incoming tokens at test time. Extensive experiments on language modeling, common-sense reasoning, recall-intensive tasks, and time series show that the proposed architecture outperforms strong baselines. Notably, its performance gains increase as the sequence length grows, highlighting its potential for long-context applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, while powerful, suffer from quadratic computational complexity and the ever-growing Key-Value (KV) cache of the attention mechanism. This paper introduces Trellis, a novel Transformer architecture with bounded memory that learns how to compress its key-value memory dynamically at test time. Trellis replaces the standard KV cache with a fixed-size memory and train a two-pass recurrent compression mechanism to store new keys and values into memory. To achieve this, it leverages an online gradient descent procedure with a forget gate, enabling the compressed memory to be updated recursively while learning to retain important contextual information from incoming tokens at test time. Extensive experiments on language modeling, common-sense reasoning, recall-intensive tasks, and time series show that the proposed architecture outperforms strong baselines. Notably, its performance gains increase as the sequence length grows, highlighting its potential for long-context applications."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-29T20:32:10Z",
                "published_parsed": [
                    2025,
                    12,
                    29,
                    20,
                    32,
                    10,
                    0,
                    363,
                    0
                ],
                "arxiv_comment": "In Second Conference on Language Modeling (COLM) (2025)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Mahdi Karami"
                    },
                    {
                        "name": "Ali Behrouz"
                    },
                    {
                        "name": "Praneeth Kacham"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni"
            },
            {
                "id": "http://arxiv.org/abs/2512.23635v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23635v1",
                "title": "Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception"
                },
                "updated": "2025-12-29T17:48:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    17,
                    48,
                    56,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23635v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-29T17:48:56Z",
                "published_parsed": [
                    2025,
                    12,
                    29,
                    17,
                    48,
                    56,
                    0,
                    363,
                    0
                ],
                "arxiv_comment": "Accepted to AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Peidong Li"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Long Shi"
                    },
                    {
                        "name": "Dedong Liu"
                    },
                    {
                        "name": "Yitao Wu"
                    },
                    {
                        "name": "Jiajia Fu"
                    },
                    {
                        "name": "Dixiao Cui"
                    },
                    {
                        "name": "Lijun Zhao"
                    },
                    {
                        "name": "Lining Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lining Sun"
                },
                "author": "Lining Sun"
            },
            {
                "id": "http://arxiv.org/abs/2412.16060v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.16060v2",
                "title": "Adaptable TeaStore",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptable TeaStore"
                },
                "updated": "2025-12-29T14:34:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    14,
                    34,
                    18,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.16060v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.16060v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.4204/EPTCS.438.1",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Modern cloud-native systems require adapting dynamically to changing operational conditions, including service outages, traffic surges, and evolving user requirements. While existing benchmarks provide valuable testbeds for performance and scalability evaluation, they lack explicit support for studying adaptation mechanisms, reconfiguration strategies, and graceful degradation. These limitations hinder systematic research on self-adaptive architectures in realistic cloud environments.\n  To cover this gap, we introduce Adaptable TeaStore, an extension of the renowned TeaStore architecture that incorporates adaptability as a first-class design concern. Our extension distinguishes between mandatory and optional services, supports multiple component versions -- with varying resource requirements and functionality levels -- considers the outsourcing of functionalities to external providers, and provides local cache mechanisms for performance and resilience. These features enable the systematic exploration of reconfiguration policies across diverse operational scenarios.\n  We discuss a broad catalogue of reference adaptation scenarios centred around Adaptable TeaStore, useful to evaluate the ability of a given adaptation technology to address conditions such as component unavailability, cyberattacks, provider outages, benign/malicious traffic increases, and user-triggered reconfigurations. Moreover, we present an open-source implementation of the architecture with APIs for metrics collection and adaptation triggers, to enable reproducible experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern cloud-native systems require adapting dynamically to changing operational conditions, including service outages, traffic surges, and evolving user requirements. While existing benchmarks provide valuable testbeds for performance and scalability evaluation, they lack explicit support for studying adaptation mechanisms, reconfiguration strategies, and graceful degradation. These limitations hinder systematic research on self-adaptive architectures in realistic cloud environments.\n  To cover this gap, we introduce Adaptable TeaStore, an extension of the renowned TeaStore architecture that incorporates adaptability as a first-class design concern. Our extension distinguishes between mandatory and optional services, supports multiple component versions -- with varying resource requirements and functionality levels -- considers the outsourcing of functionalities to external providers, and provides local cache mechanisms for performance and resilience. These features enable the systematic exploration of reconfiguration policies across diverse operational scenarios.\n  We discuss a broad catalogue of reference adaptation scenarios centred around Adaptable TeaStore, useful to evaluate the ability of a given adaptation technology to address conditions such as component unavailability, cyberattacks, provider outages, benign/malicious traffic increases, and user-triggered reconfigurations. Moreover, we present an open-source implementation of the architecture with APIs for metrics collection and adaptation triggers, to enable reproducible experiments."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-20T17:06:11Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    6,
                    11,
                    4,
                    355,
                    0
                ],
                "arxiv_comment": "In Proceedings WACA 2025, arXiv:2512.22054",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "arxiv_journal_ref": "EPTCS 438, 2025, pp. 1-14",
                "authors": [
                    {
                        "name": "Simon Bliudze"
                    },
                    {
                        "name": "Giuseppe De Palma"
                    },
                    {
                        "name": "Saverio Giallorenzo"
                    },
                    {
                        "name": "Ivan Lanese"
                    },
                    {
                        "name": "Gianluigi Zavattaro"
                    },
                    {
                        "name": "Brice Arlon Zemtsop Ndadji"
                    }
                ],
                "author_detail": {
                    "name": "Brice Arlon Zemtsop Ndadji"
                },
                "arxiv_affiliation": "Univ. Lille, CNRS, Inria, Centrale Lille, CRIStAL, Lille, France",
                "author": "Brice Arlon Zemtsop Ndadji",
                "arxiv_doi": "10.4204/EPTCS.438.1"
            },
            {
                "id": "http://arxiv.org/abs/2510.13797v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.13797v3",
                "title": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons"
                },
                "updated": "2025-12-29T13:06:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    13,
                    6,
                    57,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.13797v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.13797v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The scalability of large language models for long-context reasoning is severely constrained by the linear growth of their Transformer key-value cache, which incurs significant memory and computational costs. We posit that as a model generates reasoning tokens, the informational value of past generated tokens diminishes, creating an opportunity for compression. In this work, we propose to periodically compress the generation KV cache with a learned, special-purpose token and evict compressed entries. We train the model to perform this compression via a modified joint distillation and reinforcement learning (RL) framework. Our training method minimizes overhead over the conventional RL process, as it leverages RL outputs for distillation. Empirically, our method achieves a superior memory-accuracy Pareto frontier compared to both the model without cache compression and training-free compression techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scalability of large language models for long-context reasoning is severely constrained by the linear growth of their Transformer key-value cache, which incurs significant memory and computational costs. We posit that as a model generates reasoning tokens, the informational value of past generated tokens diminishes, creating an opportunity for compression. In this work, we propose to periodically compress the generation KV cache with a learned, special-purpose token and evict compressed entries. We train the model to perform this compression via a modified joint distillation and reinforcement learning (RL) framework. Our training method minimizes overhead over the conventional RL process, as it leverages RL outputs for distillation. Empirically, our method achieves a superior memory-accuracy Pareto frontier compared to both the model without cache compression and training-free compression techniques."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-15T17:57:21Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    21,
                    2,
                    288,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Giovanni Monea"
                    },
                    {
                        "name": "Yair Feldman"
                    },
                    {
                        "name": "Shankar Padmanabhan"
                    },
                    {
                        "name": "Kiant Brantley"
                    },
                    {
                        "name": "Yoav Artzi"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Artzi"
                },
                "author": "Yoav Artzi"
            },
            {
                "id": "http://arxiv.org/abs/2512.23434v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23434v1",
                "title": "Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates"
                },
                "updated": "2025-12-29T12:52:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    12,
                    52,
                    57,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23434v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Consistent hashing is fundamental to distributed systems, but ring-based schemes can exhibit high peak-to-average load ratios unless they use many virtual nodes, while multi-probe methods improve balance at the cost of scattered memory accesses. This paper introduces Local Rendezvous Hashing (LRH), which preserves a token ring but restricts Highest Random Weight (HRW) selection to a cache-local window of C distinct neighboring physical nodes. LRH locates a key by one binary search, enumerates exactly C distinct candidates using precomputed next-distinct offsets, and chooses the HRW winner (optionally weighted). Lookup cost is O(log|R| + C). Under fixed-topology liveness changes, fixed-candidate filtering remaps only keys whose original winner is down, yielding zero excess churn. In a benchmark with N=5000, V=256 (|R|=1.28M), K=50M and C=8, LRH reduces Max/Avg load from 1.2785 to 1.0947 and achieves 60.05 Mkeys/s, about 6.8x faster than multi-probe consistent hashing with 8 probes (8.80 Mkeys/s) while approaching its balance (Max/Avg 1.0697). A microbenchmark indicates multi-probe assignment is dominated by repeated ring searches and memory traffic rather than probe-generation arithmetic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistent hashing is fundamental to distributed systems, but ring-based schemes can exhibit high peak-to-average load ratios unless they use many virtual nodes, while multi-probe methods improve balance at the cost of scattered memory accesses. This paper introduces Local Rendezvous Hashing (LRH), which preserves a token ring but restricts Highest Random Weight (HRW) selection to a cache-local window of C distinct neighboring physical nodes. LRH locates a key by one binary search, enumerates exactly C distinct candidates using precomputed next-distinct offsets, and chooses the HRW winner (optionally weighted). Lookup cost is O(log|R| + C). Under fixed-topology liveness changes, fixed-candidate filtering remaps only keys whose original winner is down, yielding zero excess churn. In a benchmark with N=5000, V=256 (|R|=1.28M), K=50M and C=8, LRH reduces Max/Avg load from 1.2785 to 1.0947 and achieves 60.05 Mkeys/s, about 6.8x faster than multi-probe consistent hashing with 8 probes (8.80 Mkeys/s) while approaching its balance (Max/Avg 1.0697). A microbenchmark indicates multi-probe assignment is dominated by repeated ring searches and memory traffic rather than probe-generation arithmetic."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-29T12:52:57Z",
                "published_parsed": [
                    2025,
                    12,
                    29,
                    12,
                    52,
                    57,
                    0,
                    363,
                    0
                ],
                "arxiv_comment": "14 pages, 10 figures. Includes appendices",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Yongjie Guan"
                    }
                ],
                "author_detail": {
                    "name": "Yongjie Guan"
                },
                "arxiv_affiliation": "Zhejiang University of Technology",
                "author": "Yongjie Guan"
            },
            {
                "id": "http://arxiv.org/abs/2512.23298v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23298v1",
                "title": "BRkNN-light: Batch Processing of Reverse k-Nearest Neighbor Queries for Moving Objects on Road Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRkNN-light: Batch Processing of Reverse k-Nearest Neighbor Queries for Moving Objects on Road Networks"
                },
                "updated": "2025-12-29T08:36:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    8,
                    36,
                    32,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23298v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3748777.3748791",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The Reverse $k$-Nearest Neighbor (R$k$NN) query over moving objects on road networks seeks to find all moving objects that consider the specified query point as one of their $k$ nearest neighbors. In location based services, many users probably submit R$k$NN queries simultaneously. However, existing methods largely overlook how to efficiently process multiple such queries together, missing opportunities to share redundant computations and thus reduce overall processing costs. To address this, this work is the first to explore batch processing of multiple R$k$NN queries, aiming to minimize total computation by sharing duplicate calculations across queries. To tackle this issue, we propose the BR$k$NN-Light algorithm, which uses rapid verification and pruning strategies based on geometric constraints, along with an optimized range search technique, to speed up the process of identifying the R$k$NNs for each query. Furthermore, it proposes a dynamic distance caching mechanism to enable computation reuse when handling multiple queries, thereby significantly reducing unnecessary computations. Experiments on multiple real-world road networks demonstrate the superiority of the BR$k$NN-Light algorithm on the processing of batch queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Reverse $k$-Nearest Neighbor (R$k$NN) query over moving objects on road networks seeks to find all moving objects that consider the specified query point as one of their $k$ nearest neighbors. In location based services, many users probably submit R$k$NN queries simultaneously. However, existing methods largely overlook how to efficiently process multiple such queries together, missing opportunities to share redundant computations and thus reduce overall processing costs. To address this, this work is the first to explore batch processing of multiple R$k$NN queries, aiming to minimize total computation by sharing duplicate calculations across queries. To tackle this issue, we propose the BR$k$NN-Light algorithm, which uses rapid verification and pruning strategies based on geometric constraints, along with an optimized range search technique, to speed up the process of identifying the R$k$NNs for each query. Furthermore, it proposes a dynamic distance caching mechanism to enable computation reuse when handling multiple queries, thereby significantly reducing unnecessary computations. Experiments on multiple real-world road networks demonstrate the superiority of the BR$k$NN-Light algorithm on the processing of batch queries."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-29T08:36:32Z",
                "published_parsed": [
                    2025,
                    12,
                    29,
                    8,
                    36,
                    32,
                    0,
                    363,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Anbang Song"
                    },
                    {
                        "name": "Ziqiang Yu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Yating Xu"
                    },
                    {
                        "name": "Mingjin Tao"
                    }
                ],
                "author_detail": {
                    "name": "Mingjin Tao"
                },
                "author": "Mingjin Tao",
                "arxiv_doi": "10.1145/3748777.3748791"
            },
            {
                "id": "http://arxiv.org/abs/2512.23290v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23290v1",
                "title": "Atomic-scale spin sensing of a 2D $d$-wave altermagnet via helical tunneling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atomic-scale spin sensing of a 2D $d$-wave altermagnet via helical tunneling"
                },
                "updated": "2025-12-29T08:22:06Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    8,
                    22,
                    6,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23290v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Altermagnetism simultaneously possesses nonrelativistic spin responses and zero net magnetization, thus combining advantages of ferromagnetism and antiferromagnetism. This superiority originates from its unique dual feature, i.e., opposite-magnetic sublattices in real space and alternating spin polarization in momentum space enforced by the same crystal symmetry. Therefore, the determination of an altermagnetic order and its unique spin response inherently necessitates atomic-scale spin-resolved measurements in real and momentum spaces, an experimental milestone yet to be achieved. Here, via utilizing the helical edge (hinge) modes of a higher order topological insulator as the spin sensor, we realize spin-resolved scanning tunneling microscopy which enables us to pin down the dual-space feature of a layered $d$-wave altermagnet, KV$_2$Se$_2$O. In real space, atomic-registered mapping demonstrates the checkerboard antiferromagnetic order together with density-wave lattice modulation, and in momentum space, spin-resolved spectroscopic imaging provides a direct visualization of d-wave spin splitting of the band structure. Critically, using this new topology-guaranteed spin filter we directly reveal the unidirectional, spin-polarized quasiparticle excitations originating from the crystal symmetry-paired X and Y valleys around opposite magnetic sublattices simultaneously --the unique spin response for $d$-wave altermagnetism. Our experiments establish a solid basis for the exploration and utilization of altermagnetism in layered materials and further facilitate access to atomic-scale spin sensing and manipulating of 2D quantum materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnetism simultaneously possesses nonrelativistic spin responses and zero net magnetization, thus combining advantages of ferromagnetism and antiferromagnetism. This superiority originates from its unique dual feature, i.e., opposite-magnetic sublattices in real space and alternating spin polarization in momentum space enforced by the same crystal symmetry. Therefore, the determination of an altermagnetic order and its unique spin response inherently necessitates atomic-scale spin-resolved measurements in real and momentum spaces, an experimental milestone yet to be achieved. Here, via utilizing the helical edge (hinge) modes of a higher order topological insulator as the spin sensor, we realize spin-resolved scanning tunneling microscopy which enables us to pin down the dual-space feature of a layered $d$-wave altermagnet, KV$_2$Se$_2$O. In real space, atomic-registered mapping demonstrates the checkerboard antiferromagnetic order together with density-wave lattice modulation, and in momentum space, spin-resolved spectroscopic imaging provides a direct visualization of d-wave spin splitting of the band structure. Critically, using this new topology-guaranteed spin filter we directly reveal the unidirectional, spin-polarized quasiparticle excitations originating from the crystal symmetry-paired X and Y valleys around opposite magnetic sublattices simultaneously --the unique spin response for $d$-wave altermagnetism. Our experiments establish a solid basis for the exploration and utilization of altermagnetism in layered materials and further facilitate access to atomic-scale spin sensing and manipulating of 2D quantum materials."
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-29T08:22:06Z",
                "published_parsed": [
                    2025,
                    12,
                    29,
                    8,
                    22,
                    6,
                    0,
                    363,
                    0
                ],
                "arxiv_comment": "21 pages and 5 figures. Extended data figures and Supplementary notes are available from the corresponding author upon request. Comments are welcome",
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall"
                },
                "authors": [
                    {
                        "name": "Zhuying Wang"
                    },
                    {
                        "name": "Shuikang Yu"
                    },
                    {
                        "name": "Xingkai Cheng"
                    },
                    {
                        "name": "Xiaoyu Xiao"
                    },
                    {
                        "name": "Wanru Ma"
                    },
                    {
                        "name": "Feixiong Quan"
                    },
                    {
                        "name": "Hongxi Song"
                    },
                    {
                        "name": "Kunming Zhang"
                    },
                    {
                        "name": "Yunmei Zhang"
                    },
                    {
                        "name": "Yitian Ma"
                    },
                    {
                        "name": "Wenhao Liu"
                    },
                    {
                        "name": "Priti Yadav"
                    },
                    {
                        "name": "Xiangbiao Shi"
                    },
                    {
                        "name": "Zhijun Wang"
                    },
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Bin Xiang"
                    },
                    {
                        "name": "Junwei Liu"
                    },
                    {
                        "name": "Zhenyu Wang"
                    },
                    {
                        "name": "Xianhui Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xianhui Chen"
                },
                "author": "Xianhui Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.09427v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09427v2",
                "title": "ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators"
                },
                "updated": "2025-12-29T07:47:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    7,
                    47,
                    50,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09427v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Device-memory management is a key bottleneck for serving large language models (LLMs) on accelerators whose memory has poor small-granularity random-access bandwidth (e.g., LPDDR5-class). Existing approaches either statically pre-allocate worst-case KV-cache per request, wasting substantial device memory, or rely on fine-grained paging that assumes high random-access tolerance and is therefore ill-suited to LPDDR-style systems. We present ODMA, an on-demand memory allocation framework for LLM serving on random-access-constrained device memory (RACM) platforms such as LPDDR5-based Cambricon MLUs. ODMA builds on generation-length prediction while addressing distribution drift and heavy-tailed request lengths via dynamic bucket partitioning and a large-bucket safeguard: bucket boundaries are periodically re-learned from online histograms, and high-uncertainty or overflowed requests fall back to a reserved large bucket for robustness. On Alpaca and Google-NQ, ODMA improves S3's predictor accuracy from 98.60% to 99.55% and from 82.68% to 93.36%, respectively. Serving DeepSeek-R1-Distill-Qwen-7B on four Cambricon MLU370-X4 accelerators, ODMA increases device-memory utilization from 55.05% to 72.45% on Alpaca and from 42.54% to 61.79% on Google-NQ, and boosts throughput by 23% and 27% over a static pre-allocation baseline. These results show that predictor-driven, hardware-aware allocation can unlock efficient LLM serving on RACM accelerators without hardware changes, complementing paging-centric designs tailored to HBM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device-memory management is a key bottleneck for serving large language models (LLMs) on accelerators whose memory has poor small-granularity random-access bandwidth (e.g., LPDDR5-class). Existing approaches either statically pre-allocate worst-case KV-cache per request, wasting substantial device memory, or rely on fine-grained paging that assumes high random-access tolerance and is therefore ill-suited to LPDDR-style systems. We present ODMA, an on-demand memory allocation framework for LLM serving on random-access-constrained device memory (RACM) platforms such as LPDDR5-based Cambricon MLUs. ODMA builds on generation-length prediction while addressing distribution drift and heavy-tailed request lengths via dynamic bucket partitioning and a large-bucket safeguard: bucket boundaries are periodically re-learned from online histograms, and high-uncertainty or overflowed requests fall back to a reserved large bucket for robustness. On Alpaca and Google-NQ, ODMA improves S3's predictor accuracy from 98.60% to 99.55% and from 82.68% to 93.36%, respectively. Serving DeepSeek-R1-Distill-Qwen-7B on four Cambricon MLU370-X4 accelerators, ODMA increases device-memory utilization from 55.05% to 72.45% on Alpaca and from 42.54% to 61.79% on Google-NQ, and boosts throughput by 23% and 27% over a static pre-allocation baseline. These results show that predictor-driven, hardware-aware allocation can unlock efficient LLM serving on RACM accelerators without hardware changes, complementing paging-centric designs tailored to HBM systems."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-10T08:52:20Z",
                "published_parsed": [
                    2025,
                    12,
                    10,
                    8,
                    52,
                    20,
                    2,
                    344,
                    0
                ],
                "arxiv_comment": "10 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Guoqiang Zou"
                    },
                    {
                        "name": "Wanyu Wang"
                    },
                    {
                        "name": "Hao Zheng"
                    },
                    {
                        "name": "Longxiang Yin"
                    },
                    {
                        "name": "Yinhe Han"
                    }
                ],
                "author_detail": {
                    "name": "Yinhe Han"
                },
                "author": "Yinhe Han"
            },
            {
                "id": "http://arxiv.org/abs/2512.23258v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23258v1",
                "title": "Plug-and-Play Fidelity Optimization for Diffusion Transformer Acceleration via Cumulative Error Minimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play Fidelity Optimization for Diffusion Transformer Acceleration via Cumulative Error Minimization"
                },
                "updated": "2025-12-29T07:36:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    7,
                    36,
                    36,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23258v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Although Diffusion Transformer (DiT) has emerged as a predominant architecture for image and video generation, its iterative denoising process results in slow inference, which hinders broader applicability and development. Caching-based methods achieve training-free acceleration, while suffering from considerable computational error. Existing methods typically incorporate error correction strategies such as pruning or prediction to mitigate it. However, their fixed caching strategy fails to adapt to the complex error variations during denoising, which limits the full potential of error correction. To tackle this challenge, we propose a novel fidelity-optimization plugin for existing error correction methods via cumulative error minimization, named CEM. CEM predefines the error to characterize the sensitivity of model to acceleration jointly influenced by timesteps and cache intervals. Guided by this prior, we formulate a dynamic programming algorithm with cumulative error approximation for strategy optimization, which achieves the caching error minimization, resulting in a substantial improvement in generation fidelity. CEM is model-agnostic and exhibits strong generalization, which is adaptable to arbitrary acceleration budgets. It can be seamlessly integrated into existing error correction frameworks and quantized models without introducing any additional computational overhead. Extensive experiments conducted on nine generation models and quantized methods across three tasks demonstrate that CEM significantly improves generation fidelity of existing acceleration models, and outperforms the original generation performance on FLUX.1-dev, PixArt-$$, StableDiffusion1.5 and Hunyuan. The code will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Diffusion Transformer (DiT) has emerged as a predominant architecture for image and video generation, its iterative denoising process results in slow inference, which hinders broader applicability and development. Caching-based methods achieve training-free acceleration, while suffering from considerable computational error. Existing methods typically incorporate error correction strategies such as pruning or prediction to mitigate it. However, their fixed caching strategy fails to adapt to the complex error variations during denoising, which limits the full potential of error correction. To tackle this challenge, we propose a novel fidelity-optimization plugin for existing error correction methods via cumulative error minimization, named CEM. CEM predefines the error to characterize the sensitivity of model to acceleration jointly influenced by timesteps and cache intervals. Guided by this prior, we formulate a dynamic programming algorithm with cumulative error approximation for strategy optimization, which achieves the caching error minimization, resulting in a substantial improvement in generation fidelity. CEM is model-agnostic and exhibits strong generalization, which is adaptable to arbitrary acceleration budgets. It can be seamlessly integrated into existing error correction frameworks and quantized models without introducing any additional computational overhead. Extensive experiments conducted on nine generation models and quantized methods across three tasks demonstrate that CEM significantly improves generation fidelity of existing acceleration models, and outperforms the original generation performance on FLUX.1-dev, PixArt-$$, StableDiffusion1.5 and Hunyuan. The code will be made publicly available."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-29T07:36:36Z",
                "published_parsed": [
                    2025,
                    12,
                    29,
                    7,
                    36,
                    36,
                    0,
                    363,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tong Shao"
                    },
                    {
                        "name": "Yusen Fu"
                    },
                    {
                        "name": "Guoying Sun"
                    },
                    {
                        "name": "Jingde Kong"
                    },
                    {
                        "name": "Zhuotao Tian"
                    },
                    {
                        "name": "Jingyong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jingyong Su"
                },
                "author": "Jingyong Su"
            },
            {
                "id": "http://arxiv.org/abs/2512.21734v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21734v2",
                "title": "Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation"
                },
                "updated": "2025-12-29T03:22:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    29,
                    3,
                    22,
                    1,
                    0,
                    363,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21734v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21734v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Real-time portrait animation is essential for interactive applications such as virtual assistants and live avatars, requiring high visual fidelity, temporal coherence, ultra-low latency, and responsive control from dynamic inputs like reference images and driving signals. While diffusion-based models achieve strong quality, their non-causal nature hinders streaming deployment. Causal autoregressive video generation approaches enable efficient frame-by-frame generation but suffer from error accumulation, motion discontinuities at chunk boundaries, and degraded long-term consistency. In this work, we present a novel streaming framework named Knot Forcing for real-time portrait animation that addresses these challenges through three key designs: (1) a chunk-wise generation strategy with global identity preservation via cached KV states of the reference image and local temporal modeling using sliding window attention; (2) a temporal knot module that overlaps adjacent chunks and propagates spatio-temporal cues via image-to-video conditioning to smooth inter-chunk motion transitions; and (3) A \"running ahead\" mechanism that dynamically updates the reference frame's temporal coordinate during inference, keeping its semantic context ahead of the current rollout frame to support long-term coherence. Knot Forcing enables high-fidelity, temporally consistent, and interactive portrait animation over infinite sequences, achieving real-time performance with strong visual stability on consumer-grade GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time portrait animation is essential for interactive applications such as virtual assistants and live avatars, requiring high visual fidelity, temporal coherence, ultra-low latency, and responsive control from dynamic inputs like reference images and driving signals. While diffusion-based models achieve strong quality, their non-causal nature hinders streaming deployment. Causal autoregressive video generation approaches enable efficient frame-by-frame generation but suffer from error accumulation, motion discontinuities at chunk boundaries, and degraded long-term consistency. In this work, we present a novel streaming framework named Knot Forcing for real-time portrait animation that addresses these challenges through three key designs: (1) a chunk-wise generation strategy with global identity preservation via cached KV states of the reference image and local temporal modeling using sliding window attention; (2) a temporal knot module that overlaps adjacent chunks and propagates spatio-temporal cues via image-to-video conditioning to smooth inter-chunk motion transitions; and (3) A \"running ahead\" mechanism that dynamically updates the reference frame's temporal coordinate during inference, keeping its semantic context ahead of the current rollout frame to support long-term coherence. Knot Forcing enables high-fidelity, temporally consistent, and interactive portrait animation over infinite sequences, achieving real-time performance with strong visual stability on consumer-grade GPUs."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-25T16:34:56Z",
                "published_parsed": [
                    2025,
                    12,
                    25,
                    16,
                    34,
                    56,
                    3,
                    359,
                    0
                ],
                "arxiv_comment": "Project Page: https://humanaigc.github.io/knot_forcing_demo_page/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Steven Xiao"
                    },
                    {
                        "name": "Xindi Zhang"
                    },
                    {
                        "name": "Dechao Meng"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Bang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bang Zhang"
                },
                "author": "Bang Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.23049v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.23049v1",
                "title": "Accelerating Language Model Workflows with Prompt Choreography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Language Model Workflows with Prompt Choreography"
                },
                "updated": "2025-12-28T19:21:11Z",
                "updated_parsed": [
                    2025,
                    12,
                    28,
                    19,
                    21,
                    11,
                    6,
                    362,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.23049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.23049v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models are increasingly deployed in multi-agent workflows. We introduce Prompt Choreography, a framework that efficiently executes LLM workflows by maintaining a dynamic, global KV cache. Each LLM call can attend to an arbitrary, reordered subset of previously encoded messages. Parallel calls are supported. Though caching messages' encodings sometimes gives different results from re-encoding them in a new context, we show in diverse settings that fine-tuning the LLM to work with the cache can help it mimic the original results. Prompt Choreography significantly reduces per-message latency (2.0--6.2$\\times$ faster time-to-first-token) and achieves substantial end-to-end speedups ($>$2.2$\\times$) in some workflows dominated by redundant computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are increasingly deployed in multi-agent workflows. We introduce Prompt Choreography, a framework that efficiently executes LLM workflows by maintaining a dynamic, global KV cache. Each LLM call can attend to an arbitrary, reordered subset of previously encoded messages. Parallel calls are supported. Though caching messages' encodings sometimes gives different results from re-encoding them in a new context, we show in diverse settings that fine-tuning the LLM to work with the cache can help it mimic the original results. Prompt Choreography significantly reduces per-message latency (2.0--6.2$\\times$ faster time-to-first-token) and achieves substantial end-to-end speedups ($>$2.2$\\times$) in some workflows dominated by redundant computation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-28T19:21:11Z",
                "published_parsed": [
                    2025,
                    12,
                    28,
                    19,
                    21,
                    11,
                    6,
                    362,
                    0
                ],
                "arxiv_comment": "to appear in TACL (final preprint of 2025-10-12); 10 pages + appendices",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "TJ Bai"
                    },
                    {
                        "name": "Jason Eisner"
                    }
                ],
                "author_detail": {
                    "name": "Jason Eisner"
                },
                "author": "Jason Eisner"
            },
            {
                "id": "http://arxiv.org/abs/2510.14973v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.14973v2",
                "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Is All You Need for KV Cache in Diffusion LLMs"
                },
                "updated": "2025-12-28T17:27:09Z",
                "updated_parsed": [
                    2025,
                    12,
                    28,
                    17,
                    27,
                    9,
                    6,
                    362,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.14973v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.14973v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods' decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens primarily act as a length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing a conservative lower bound on cache change for other tokens. Building on these, we propose ${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that jointly decides ${when}$ to refresh (via an attention-aware drift test on the most-attended token) and ${where}$ to refresh (via a depth-aware schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: $8.7\\times$ on GSM8K (256 tokens), and $45.1\\times$ on longer sequences, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput ($6.8\\times$ on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods' decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens primarily act as a length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing a conservative lower bound on cache change for other tokens. Building on these, we propose ${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that jointly decides ${when}$ to refresh (via an attention-aware drift test on the most-attended token) and ${where}$ to refresh (via a depth-aware schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: $8.7\\times$ on GSM8K (256 tokens), and $45.1\\times$ on longer sequences, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput ($6.8\\times$ on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-16T17:59:48Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "arxiv_comment": "Code at: https://github.com/VILA-Lab/Elastic-Cache",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Quan Nguyen-Tri"
                    },
                    {
                        "name": "Mukul Ranjan"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen"
            },
            {
                "id": "http://arxiv.org/abs/2512.22854v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.22854v1",
                "title": "ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning"
                },
                "updated": "2025-12-28T09:38:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    28,
                    9,
                    38,
                    36,
                    6,
                    362,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.22854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.22854v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Human-object interaction (HOI) video generation has garnered increasing attention due to its promising applications in digital humans, e-commerce, advertising, and robotics imitation learning. However, existing methods face two critical limitations: (1) a lack of effective mechanisms to inject multi-view information of the object into the model, leading to poor cross-view consistency, and (2) heavy reliance on fine-grained hand mesh annotations for modeling interaction occlusions. To address these challenges, we introduce ByteLoom, a Diffusion Transformer (DiT)-based framework that generates realistic HOI videos with geometrically consistent object illustration, using simplified human conditioning and 3D object inputs. We first propose an RCM-cache mechanism that leverages Relative Coordinate Maps (RCM) as a universal representation to maintain object's geometry consistency and precisely control 6-DoF object transformations in the meantime. To compensate HOI dataset scarcity and leverage existing datasets, we further design a training curriculum that enhances model capabilities in a progressive style and relaxes the demand of hand mesh. Extensive experiments demonstrate that our method faithfully preserves human identity and the object's multi-view geometry, while maintaining smooth motion and object manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-object interaction (HOI) video generation has garnered increasing attention due to its promising applications in digital humans, e-commerce, advertising, and robotics imitation learning. However, existing methods face two critical limitations: (1) a lack of effective mechanisms to inject multi-view information of the object into the model, leading to poor cross-view consistency, and (2) heavy reliance on fine-grained hand mesh annotations for modeling interaction occlusions. To address these challenges, we introduce ByteLoom, a Diffusion Transformer (DiT)-based framework that generates realistic HOI videos with geometrically consistent object illustration, using simplified human conditioning and 3D object inputs. We first propose an RCM-cache mechanism that leverages Relative Coordinate Maps (RCM) as a universal representation to maintain object's geometry consistency and precisely control 6-DoF object transformations in the meantime. To compensate HOI dataset scarcity and leverage existing datasets, we further design a training curriculum that enhances model capabilities in a progressive style and relaxes the demand of hand mesh. Extensive experiments demonstrate that our method faithfully preserves human identity and the object's multi-view geometry, while maintaining smooth motion and object manipulation."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-28T09:38:36Z",
                "published_parsed": [
                    2025,
                    12,
                    28,
                    9,
                    38,
                    36,
                    6,
                    362,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Bangya Liu"
                    },
                    {
                        "name": "Xinyu Gong"
                    },
                    {
                        "name": "Zelin Zhao"
                    },
                    {
                        "name": "Ziyang Song"
                    },
                    {
                        "name": "Yulei Lu"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suman Banerjee"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.22737v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.22737v1",
                "title": "WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference"
                },
                "updated": "2025-12-28T01:25:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    28,
                    1,
                    25,
                    48,
                    6,
                    362,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.22737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.22737v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autoregressive (AR) generation is the standard decoding paradigm for Large Language Models (LLMs), but its token-by-token nature limits parallelism at inference time. Diffusion Language Models (DLLMs) offer parallel decoding by recovering multiple masked tokens per step; however, in practice they often fail to translate this parallelism into deployment speed gains over optimized AR engines (e.g., vLLM). A key reason is that many DLLMs rely on bidirectional attention, which breaks standard prefix KV caching and forces repeated contextualization, undermining efficiency. We propose WeDLM, a diffusion decoding framework built entirely on standard causal attention to make parallel generation prefix-cache friendly. The core idea is to let each masked position condition on all currently observed tokens while keeping a strict causal mask, achieved by Topological Reordering that moves observed tokens to the physical prefix while preserving their logical positions. Building on this property, we introduce a streaming decoding procedure that continuously commits confident tokens into a growing left-to-right prefix and maintains a fixed parallel workload, avoiding the stop-and-wait behavior common in block diffusion methods. Experiments show that WeDLM preserves the quality of strong AR backbones while delivering substantial speedups, approaching 3x on challenging reasoning benchmarks and up to 10x in low-entropy generation regimes; critically, our comparisons are against AR baselines served by vLLM under matched deployment settings, demonstrating that diffusion-style decoding can outperform an optimized AR engine in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) generation is the standard decoding paradigm for Large Language Models (LLMs), but its token-by-token nature limits parallelism at inference time. Diffusion Language Models (DLLMs) offer parallel decoding by recovering multiple masked tokens per step; however, in practice they often fail to translate this parallelism into deployment speed gains over optimized AR engines (e.g., vLLM). A key reason is that many DLLMs rely on bidirectional attention, which breaks standard prefix KV caching and forces repeated contextualization, undermining efficiency. We propose WeDLM, a diffusion decoding framework built entirely on standard causal attention to make parallel generation prefix-cache friendly. The core idea is to let each masked position condition on all currently observed tokens while keeping a strict causal mask, achieved by Topological Reordering that moves observed tokens to the physical prefix while preserving their logical positions. Building on this property, we introduce a streaming decoding procedure that continuously commits confident tokens into a growing left-to-right prefix and maintains a fixed parallel workload, avoiding the stop-and-wait behavior common in block diffusion methods. Experiments show that WeDLM preserves the quality of strong AR backbones while delivering substantial speedups, approaching 3x on challenging reasoning benchmarks and up to 10x in low-entropy generation regimes; critically, our comparisons are against AR baselines served by vLLM under matched deployment settings, demonstrating that diffusion-style decoding can outperform an optimized AR engine in practice."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-28T01:25:48Z",
                "published_parsed": [
                    2025,
                    12,
                    28,
                    1,
                    25,
                    48,
                    6,
                    362,
                    0
                ],
                "arxiv_comment": "23 pages, 8 figures, project page: https://wedlm.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Minghua He"
                    },
                    {
                        "name": "Shaoxun Zeng"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Yuan Liu"
                    },
                    {
                        "name": "Xiao Zhou"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2510.15904v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.15904v2",
                "title": "NVM-in-Cache: Repurposing Commodity 6T SRAM Cache into NVM Analog Processing-in-Memory Engine using a Novel Compute-on-Powerline Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVM-in-Cache: Repurposing Commodity 6T SRAM Cache into NVM Analog Processing-in-Memory Engine using a Novel Compute-on-Powerline Scheme"
                },
                "updated": "2025-12-27T20:21:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    27,
                    20,
                    21,
                    3,
                    5,
                    361,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.15904v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.15904v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid growth of deep neural network (DNN) workloads has significantly increased the demand for large-capacity on-chip SRAM in machine learning (ML) applications, with SRAM arrays now occupying a substantial fraction of the total die area. To address the dual challenges of storage density and computation efficiency, this paper proposes an NVM-in-Cache architecture that integrates resistive RAM (RRAM) devices into a conventional 6T-SRAM cell, forming a compact 6T-2R bit-cell. This hybrid cell enables Processing-in-Memory (PIM) mode, which performs massively parallel multiply-and-accumulate (MAC) operations directly on cache power lines while preserving stored cache data. By exploiting the intrinsic properties of the 6T-2R structure, the architecture achieves additional storage capability, high computational throughput without any bit-cell area overhead. Circuit- and array-level simulations in GlobalFoundries 22nm FDSOI technology demonstrate that the proposed design achieves a throughput of 0.4 TOPS and 452.34 TOPS/W. For 128 row-parallel operations, the CIFAR-10 classification is demonstrated by mapping a Resnet-18 neural network, achieving an accuracy of 91.76%. These results highlight the potential of the NVM-in-Cache approach to serve as a scalable, energy-efficient computing method by re-purposing existing 6T SRAM cache architecture for next-generation AI accelerators and general purpose processors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of deep neural network (DNN) workloads has significantly increased the demand for large-capacity on-chip SRAM in machine learning (ML) applications, with SRAM arrays now occupying a substantial fraction of the total die area. To address the dual challenges of storage density and computation efficiency, this paper proposes an NVM-in-Cache architecture that integrates resistive RAM (RRAM) devices into a conventional 6T-SRAM cell, forming a compact 6T-2R bit-cell. This hybrid cell enables Processing-in-Memory (PIM) mode, which performs massively parallel multiply-and-accumulate (MAC) operations directly on cache power lines while preserving stored cache data. By exploiting the intrinsic properties of the 6T-2R structure, the architecture achieves additional storage capability, high computational throughput without any bit-cell area overhead. Circuit- and array-level simulations in GlobalFoundries 22nm FDSOI technology demonstrate that the proposed design achieves a throughput of 0.4 TOPS and 452.34 TOPS/W. For 128 row-parallel operations, the CIFAR-10 classification is demonstrated by mapping a Resnet-18 neural network, achieving an accuracy of 91.76%. These results highlight the potential of the NVM-in-Cache approach to serve as a scalable, energy-efficient computing method by re-purposing existing 6T SRAM cache architecture for next-generation AI accelerators and general purpose processors."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-15T01:09:18Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    1,
                    9,
                    18,
                    0,
                    258,
                    0
                ],
                "arxiv_comment": "11 pages",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Subhradip Chakraborty"
                    },
                    {
                        "name": "Ankur Singh"
                    },
                    {
                        "name": "Xuming Chen"
                    },
                    {
                        "name": "Gourav Datta"
                    },
                    {
                        "name": "Akhilesh R. Jaiswal"
                    }
                ],
                "author_detail": {
                    "name": "Akhilesh R. Jaiswal"
                },
                "author": "Akhilesh R. Jaiswal"
            },
            {
                "id": "http://arxiv.org/abs/2512.22581v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.22581v1",
                "title": "KV-Tracker: Real-Time Pose Tracking with Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Tracker: Real-Time Pose Tracking with Transformers"
                },
                "updated": "2025-12-27T13:02:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    27,
                    13,
                    2,
                    30,
                    5,
                    361,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.22581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.22581v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-view 3D geometry networks offer a powerful prior but are prohibitively slow for real-time applications. We propose a novel way to adapt them for online use, enabling real-time 6-DoF pose tracking and online reconstruction of objects and scenes from monocular RGB videos. Our method rapidly selects and manages a set of images as keyframes to map a scene or object via $^3$ with full bidirectional attention. We then cache the global self-attention block's key-value (KV) pairs and use them as the sole scene representation for online tracking. This allows for up to $15\\times$ speedup during inference without the fear of drift or catastrophic forgetting. Our caching strategy is model-agnostic and can be applied to other off-the-shelf multi-view networks without retraining. We demonstrate KV-Tracker on both scene-level tracking and the more challenging task of on-the-fly object tracking and reconstruction without depth measurements or object priors. Experiments on the TUM RGB-D, 7-Scenes, Arctic and OnePose datasets show the strong performance of our system while maintaining high frame-rates up to ${\\sim}27$ FPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view 3D geometry networks offer a powerful prior but are prohibitively slow for real-time applications. We propose a novel way to adapt them for online use, enabling real-time 6-DoF pose tracking and online reconstruction of objects and scenes from monocular RGB videos. Our method rapidly selects and manages a set of images as keyframes to map a scene or object via $^3$ with full bidirectional attention. We then cache the global self-attention block's key-value (KV) pairs and use them as the sole scene representation for online tracking. This allows for up to $15\\times$ speedup during inference without the fear of drift or catastrophic forgetting. Our caching strategy is model-agnostic and can be applied to other off-the-shelf multi-view networks without retraining. We demonstrate KV-Tracker on both scene-level tracking and the more challenging task of on-the-fly object tracking and reconstruction without depth measurements or object priors. Experiments on the TUM RGB-D, 7-Scenes, Arctic and OnePose datasets show the strong performance of our system while maintaining high frame-rates up to ${\\sim}27$ FPS."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-27T13:02:30Z",
                "published_parsed": [
                    2025,
                    12,
                    27,
                    13,
                    2,
                    30,
                    5,
                    361,
                    0
                ],
                "arxiv_comment": "Project Page: https://marwan99.github.io/kv_tracker",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Marwan Taher"
                    },
                    {
                        "name": "Ignacio Alzugaray"
                    },
                    {
                        "name": "Kirill Mazur"
                    },
                    {
                        "name": "Xin Kong"
                    },
                    {
                        "name": "Andrew J. Davison"
                    }
                ],
                "author_detail": {
                    "name": "Andrew J. Davison"
                },
                "author": "Andrew J. Davison"
            },
            {
                "id": "http://arxiv.org/abs/2601.00024v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.00024v1",
                "title": "Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach"
                },
                "updated": "2025-12-26T21:03:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    26,
                    21,
                    3,
                    47,
                    4,
                    360,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.00024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.00024v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-26T21:03:47Z",
                "published_parsed": [
                    2025,
                    12,
                    26,
                    21,
                    3,
                    47,
                    4,
                    360,
                    0
                ],
                "arxiv_comment": "9 pages, 6 figures, 2 algorithms",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Purushottam Saha"
                    },
                    {
                        "name": "Avirup Chakraborty"
                    },
                    {
                        "name": "Sourish Sarkar"
                    },
                    {
                        "name": "Subhamoy Maitra"
                    },
                    {
                        "name": "Diganta Mukherjee"
                    },
                    {
                        "name": "Tridib Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Tridib Mukherjee"
                },
                "author": "Tridib Mukherjee"
            },
            {
                "id": "http://arxiv.org/abs/2512.22118v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.22118v1",
                "title": "ProEdit: Inversion-based Editing From Prompts Done Right",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProEdit: Inversion-based Editing From Prompts Done Right"
                },
                "updated": "2025-12-26T18:59:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    26,
                    18,
                    59,
                    14,
                    4,
                    360,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.22118v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject's atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject's atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-26T18:59:14Z",
                "published_parsed": [
                    2025,
                    12,
                    26,
                    18,
                    59,
                    14,
                    4,
                    360,
                    0
                ],
                "arxiv_comment": "Equal contributions from first two authors. Project page: https://isee-laboratory.github.io/ProEdit/ Code: https://github.com/iSEE-Laboratory/ProEdit",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhi Ouyang"
                    },
                    {
                        "name": "Dian Zheng"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    },
                    {
                        "name": "Jian-Jian Jiang"
                    },
                    {
                        "name": "Kun-Yu Lin"
                    },
                    {
                        "name": "Jingke Meng"
                    },
                    {
                        "name": "Wei-Shi Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Shi Zheng"
                },
                "author": "Wei-Shi Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2512.21967v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21967v1",
                "title": "BLEST: Blazingly Efficient BFS using Tensor Cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLEST: Blazingly Efficient BFS using Tensor Cores"
                },
                "updated": "2025-12-26T10:30:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    26,
                    10,
                    30,
                    33,
                    4,
                    360,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21967v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Breadth-First Search (BFS) is a fundamental graph kernel that underpins a wide range of applications. While modern GPUs provide specialised Matrix-Multiply-Accumulate (MMA) units, e.g., Tensor Cores (TC), with extremely high throughput, they target dense operations, making it non-trivial to exploit them for irregular, unstructured graph computations. In particular, fully utilising them for a BFS requires an efficient mapping of the edge operations onto TCs while avoiding redundancy, load imbalance, and synchronisation. We present BLEST, a TC-accelerated framework that reformulates the pull-based BFS pipeline around a bitmap-oriented structure and a carefully engineered execution layout. BLEST introduces Binarised Virtual Slice Sets (BVSS) to enforce warp-level load balancing and to eliminate frontier-oblivious work assignment. To improve both memory efficiency and update locality across diverse graphs, we apply two complementary graph reordering strategies: a compression-oriented ordering for social-like graphs and a bandwidth-reducing ordering for non-social graphs. At the compute level, we develop a batched SpMSpV multiplication pattern that uses the bitwise TC tiles to handle dot products without wasting output entries, thereby reducing the number of required MMA calls. Finally, BLEST combines kernel fusion with a lazy vertex update scheme to reduce host-side synchronisation, mitigate atomic overheads, and improve cache locality. Experiments show that BLEST delivers, on average, $3.58\\times$, $4.64\\times$ and $4.9\\times$ speedup over BerryBees, Gunrock, and GSWITCH, respectively, across a broad set of real-world graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breadth-First Search (BFS) is a fundamental graph kernel that underpins a wide range of applications. While modern GPUs provide specialised Matrix-Multiply-Accumulate (MMA) units, e.g., Tensor Cores (TC), with extremely high throughput, they target dense operations, making it non-trivial to exploit them for irregular, unstructured graph computations. In particular, fully utilising them for a BFS requires an efficient mapping of the edge operations onto TCs while avoiding redundancy, load imbalance, and synchronisation. We present BLEST, a TC-accelerated framework that reformulates the pull-based BFS pipeline around a bitmap-oriented structure and a carefully engineered execution layout. BLEST introduces Binarised Virtual Slice Sets (BVSS) to enforce warp-level load balancing and to eliminate frontier-oblivious work assignment. To improve both memory efficiency and update locality across diverse graphs, we apply two complementary graph reordering strategies: a compression-oriented ordering for social-like graphs and a bandwidth-reducing ordering for non-social graphs. At the compute level, we develop a batched SpMSpV multiplication pattern that uses the bitwise TC tiles to handle dot products without wasting output entries, thereby reducing the number of required MMA calls. Finally, BLEST combines kernel fusion with a lazy vertex update scheme to reduce host-side synchronisation, mitigate atomic overheads, and improve cache locality. Experiments show that BLEST delivers, on average, $3.58\\times$, $4.64\\times$ and $4.9\\times$ speedup over BerryBees, Gunrock, and GSWITCH, respectively, across a broad set of real-world graphs."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-26T10:30:33Z",
                "published_parsed": [
                    2025,
                    12,
                    26,
                    10,
                    30,
                    33,
                    4,
                    360,
                    0
                ],
                "arxiv_comment": "13 pages, 3 figures, 4 tables, 3 algorithms, 46 references",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Deniz Elbek"
                    },
                    {
                        "name": "Kamer Kaya"
                    }
                ],
                "author_detail": {
                    "name": "Kamer Kaya"
                },
                "author": "Kamer Kaya"
            },
            {
                "id": "http://arxiv.org/abs/2512.21954v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21954v1",
                "title": "Latency-Optimal Cache-aided Multicast Streaming via Forward-Backward Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency-Optimal Cache-aided Multicast Streaming via Forward-Backward Reinforcement Learning"
                },
                "updated": "2025-12-26T10:00:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    26,
                    10,
                    0,
                    39,
                    4,
                    360,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21954v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We consider a cellular network equipped with cache-enabled base-stations (BSs) leveraging an orthogonal multipoint multicast (OMPMC) streaming scheme. The network operates in a time-slotted fashion to serve content-requesting users by streaming cached files. The users being unsatisfied by the multicat streaming face a delivery outage, implying that they will remain interested in their preference at the next time-slot, which leads to a forward dynamics on the user preference. To design a latency-optimal streaming policy, the dynamics of latency is properly modeled and included in the learning procedure. We show that this dynamics surprisingly represents a backward dynamics. The combination of problem's forward and backward dynamics then develops a forward-backward Markov decision process (FB-MDP) that fully captures the network evolution across time. This FB-MDP necessitates usage of a forward-backward multi-objective reinforcement learning (FB-MORL) algorithm to optimize the expected latency as well as other performance metrics of interest including the overall outage probability and total resource consumption. Simulation results show the merit of proposed FB-MORL algorithm in finding a promising dynamic cache policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a cellular network equipped with cache-enabled base-stations (BSs) leveraging an orthogonal multipoint multicast (OMPMC) streaming scheme. The network operates in a time-slotted fashion to serve content-requesting users by streaming cached files. The users being unsatisfied by the multicat streaming face a delivery outage, implying that they will remain interested in their preference at the next time-slot, which leads to a forward dynamics on the user preference. To design a latency-optimal streaming policy, the dynamics of latency is properly modeled and included in the learning procedure. We show that this dynamics surprisingly represents a backward dynamics. The combination of problem's forward and backward dynamics then develops a forward-backward Markov decision process (FB-MDP) that fully captures the network evolution across time. This FB-MDP necessitates usage of a forward-backward multi-objective reinforcement learning (FB-MORL) algorithm to optimize the expected latency as well as other performance metrics of interest including the overall outage probability and total resource consumption. Simulation results show the merit of proposed FB-MORL algorithm in finding a promising dynamic cache policy."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-26T10:00:39Z",
                "published_parsed": [
                    2025,
                    12,
                    26,
                    10,
                    0,
                    39,
                    4,
                    360,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Mohsen Amidzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Amidzadeh"
                },
                "author": "Mohsen Amidzadeh"
            },
            {
                "id": "http://arxiv.org/abs/2512.21859v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21859v1",
                "title": "TimeBill: Time-Budgeted Inference for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeBill: Time-Budgeted Inference for Large Language Models"
                },
                "updated": "2025-12-26T04:49:35Z",
                "updated_parsed": [
                    2025,
                    12,
                    26,
                    4,
                    49,
                    35,
                    4,
                    360,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21859v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-26T04:49:35Z",
                "published_parsed": [
                    2025,
                    12,
                    26,
                    4,
                    49,
                    35,
                    4,
                    360,
                    0
                ],
                "arxiv_comment": "Accepted to AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Qi Fan"
                    },
                    {
                        "name": "An Zou"
                    },
                    {
                        "name": "Yehan Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yehan Ma"
                },
                "author": "Yehan Ma"
            },
            {
                "id": "http://arxiv.org/abs/2512.22301v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.22301v1",
                "title": "A Statistical Side-Channel Risk Model for Timing Variability in Lattice-Based Post-Quantum Cryptography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Statistical Side-Channel Risk Model for Timing Variability in Lattice-Based Post-Quantum Cryptography"
                },
                "updated": "2025-12-26T03:12:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    26,
                    3,
                    12,
                    33,
                    4,
                    360,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.22301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.22301v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Timing side-channels are an important threat to cryptography that still needs to be addressed in implementations, and the advent of post-quantum cryptography raises this issue because the lattice-based schemes may produce secret-dependent timing variability with the help of complex arithmetic and control flow. Since also real timing measurements are affected by environmental noise (e.g. scheduling effects, contention, heavy tailed delays), in this work a scenario-based statistical risk model is proposed for timing leakage as a problem of distributional distinguishability under controlled execution conditions. We synthesize traces for two secret classes in idle, jitter and loaded scenarios and for multiple leakage models and quantify leakage with Welch's t-test, KS distance, Cliff's delta, mutual information, and distribution overlap to combine in a TLRI like manner to obtain a consistent score for ranking scenarios. Across representative lattice-based KEM families (Kyber, Saber, Frodo), idle conditions generally have the best distinguishability, jitter and loaded conditions erode distinguishability through an increase in variance and increase in overlap; cache-index and branch-style leakage tends to give the highest risk signals, and faster schemes can have a higher peak risk given similar leakage assumptions, allowing reproducible comparisons at an early design stage, prior to platform-specific validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timing side-channels are an important threat to cryptography that still needs to be addressed in implementations, and the advent of post-quantum cryptography raises this issue because the lattice-based schemes may produce secret-dependent timing variability with the help of complex arithmetic and control flow. Since also real timing measurements are affected by environmental noise (e.g. scheduling effects, contention, heavy tailed delays), in this work a scenario-based statistical risk model is proposed for timing leakage as a problem of distributional distinguishability under controlled execution conditions. We synthesize traces for two secret classes in idle, jitter and loaded scenarios and for multiple leakage models and quantify leakage with Welch's t-test, KS distance, Cliff's delta, mutual information, and distribution overlap to combine in a TLRI like manner to obtain a consistent score for ranking scenarios. Across representative lattice-based KEM families (Kyber, Saber, Frodo), idle conditions generally have the best distinguishability, jitter and loaded conditions erode distinguishability through an increase in variance and increase in overlap; cache-index and branch-style leakage tends to give the highest risk signals, and faster schemes can have a higher peak risk given similar leakage assumptions, allowing reproducible comparisons at an early design stage, prior to platform-specific validation."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-26T03:12:33Z",
                "published_parsed": [
                    2025,
                    12,
                    26,
                    3,
                    12,
                    33,
                    4,
                    360,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Aayush Mainali"
                    },
                    {
                        "name": "Sirjan Ghimire"
                    }
                ],
                "author_detail": {
                    "name": "Sirjan Ghimire"
                },
                "author": "Sirjan Ghimire"
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.20166v3",
                "title": "PIMphony: Overcoming Bandwidth and Capacity Inefficiency in PIM-based Long-Context LLM Inference System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIMphony: Overcoming Bandwidth and Capacity Inefficiency in PIM-based Long-Context LLM Inference System"
                },
                "updated": "2025-12-25T14:44:28Z",
                "updated_parsed": [
                    2025,
                    12,
                    25,
                    14,
                    44,
                    28,
                    3,
                    359,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.20166v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.20166v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The expansion of long-context Large Language Models (LLMs) creates significant memory system challenges. While Processing-in-Memory (PIM) is a promising accelerator, we identify that it suffers from critical inefficiencies when scaled to long contexts: severe channel underutilization, performance-limiting I/O bottlenecks, and massive memory waste from static KV cache management. In this work, we propose PIMphony, a PIM orchestrator that systematically resolves these issues with three co-designed techniques. First, Token-Centric PIM Partitioning (TCP) ensures high channel utilization regardless of batch size. Second, Dynamic PIM Command Scheduling (DCS) mitigates the I/O bottleneck by overlapping data movement and computation. Finally, a Dynamic PIM Access (DPA) controller enables dynamic memory management to eliminate static memory waste. Implemented via an MLIR-based compiler and evaluated on a cycle-accurate simulator, PIMphony significantly improves throughput for long-context LLM inference (up to 72B parameters and 1M context length). Our evaluations show performance boosts of up to 11.3x on PIM-only systems and 8.4x on xPU+PIM systems, enabling more efficient deployment of LLMs in real-world long-context applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of long-context Large Language Models (LLMs) creates significant memory system challenges. While Processing-in-Memory (PIM) is a promising accelerator, we identify that it suffers from critical inefficiencies when scaled to long contexts: severe channel underutilization, performance-limiting I/O bottlenecks, and massive memory waste from static KV cache management. In this work, we propose PIMphony, a PIM orchestrator that systematically resolves these issues with three co-designed techniques. First, Token-Centric PIM Partitioning (TCP) ensures high channel utilization regardless of batch size. Second, Dynamic PIM Command Scheduling (DCS) mitigates the I/O bottleneck by overlapping data movement and computation. Finally, a Dynamic PIM Access (DPA) controller enables dynamic memory management to eliminate static memory waste. Implemented via an MLIR-based compiler and evaluated on a cycle-accurate simulator, PIMphony significantly improves throughput for long-context LLM inference (up to 72B parameters and 1M context length). Our evaluations show performance boosts of up to 11.3x on PIM-only systems and 8.4x on xPU+PIM systems, enabling more efficient deployment of LLMs in real-world long-context applications."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "arxiv_comment": "21 pages, 20 figures, Accepted to 2026 IEEE International Symposium on High-Performance Computer Architecture",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Gyeonggeun Jung"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi"
            },
            {
                "id": "http://arxiv.org/abs/2512.21338v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21338v2",
                "title": "HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming"
                },
                "updated": "2025-12-25T14:18:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    25,
                    14,
                    18,
                    18,
                    3,
                    359,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21338v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-24T18:59:58Z",
                "published_parsed": [
                    2025,
                    12,
                    24,
                    18,
                    59,
                    58,
                    2,
                    358,
                    0
                ],
                "arxiv_comment": "Project Page: http://haonanqiu.com/projects/HiStream.html",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Shikun Liu"
                    },
                    {
                        "name": "Zijian Zhou"
                    },
                    {
                        "name": "Zhaochong An"
                    },
                    {
                        "name": "Weiming Ren"
                    },
                    {
                        "name": "Zhiheng Liu"
                    },
                    {
                        "name": "Jonas Schult"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Shoufa Chen"
                    },
                    {
                        "name": "Yuren Cong"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Juan-Manuel Perez-Rua"
                    }
                ],
                "author_detail": {
                    "name": "Juan-Manuel Perez-Rua"
                },
                "author": "Juan-Manuel Perez-Rua"
            },
            {
                "id": "http://arxiv.org/abs/2512.21615v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21615v1",
                "title": "Embedding Samples Dispatching for Recommendation Model Training in Edge Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding Samples Dispatching for Recommendation Model Training in Edge Environments"
                },
                "updated": "2025-12-25T10:23:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    25,
                    10,
                    23,
                    14,
                    3,
                    359,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21615v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Training deep learning recommendation models (DLRMs) on edge workers brings several benefits, particularly in terms of data privacy protection, low latency and personalization. However, due to the huge size of embedding tables, typical DLRM training frameworks adopt one or more parameter servers to maintain global embedding tables, while leveraging the edge workers cache part of them. This incurs significant transmission cost for embedding transmissions between workers and parameter servers, which can dominate the training cycle. In this paper, we investigate how to dispatch input embedding samples to appropriate edge workers to minimize the total embedding transmission cost when facing edge-specific challenges such as heterogeneous networks and limited resources. We develop ESD, a novel mechanism that optimizes the dispatch of input embedding samples to edge workers based on expected embedding transmission cost. We propose HybridDis as the dispatch decision method within ESD, which combines a resource-intensive optimal algorithm and a heuristic algorithm to balance decision quality and resource consumption. We implement a prototype of ESD and compare it with state-of-the-art mechanisms on real-world workloads. Extensive experimental results show that ESD reduces the embedding transmission cost by up to 36.76% and achieves up to 1.74 times speedup in end-to-end DLRM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training deep learning recommendation models (DLRMs) on edge workers brings several benefits, particularly in terms of data privacy protection, low latency and personalization. However, due to the huge size of embedding tables, typical DLRM training frameworks adopt one or more parameter servers to maintain global embedding tables, while leveraging the edge workers cache part of them. This incurs significant transmission cost for embedding transmissions between workers and parameter servers, which can dominate the training cycle. In this paper, we investigate how to dispatch input embedding samples to appropriate edge workers to minimize the total embedding transmission cost when facing edge-specific challenges such as heterogeneous networks and limited resources. We develop ESD, a novel mechanism that optimizes the dispatch of input embedding samples to edge workers based on expected embedding transmission cost. We propose HybridDis as the dispatch decision method within ESD, which combines a resource-intensive optimal algorithm and a heuristic algorithm to balance decision quality and resource consumption. We implement a prototype of ESD and compare it with state-of-the-art mechanisms on real-world workloads. Extensive experimental results show that ESD reduces the embedding transmission cost by up to 36.76% and achieves up to 1.74 times speedup in end-to-end DLRM training."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-25T10:23:14Z",
                "published_parsed": [
                    2025,
                    12,
                    25,
                    10,
                    23,
                    14,
                    3,
                    359,
                    0
                ],
                "arxiv_comment": "This paper is an English version of Samples Dispatching Mechanism for Accelerating Recommendation Model Training in Edge Intelligent Computing System published in 2025 in the Journal of Computer Research and Development",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Guopeng Li"
                    },
                    {
                        "name": "Haisheng Tan"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Hongqiu Ni"
                    },
                    {
                        "name": "Zilong Wang"
                    },
                    {
                        "name": "Xinyue Zhang"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Han Tian"
                    }
                ],
                "author_detail": {
                    "name": "Han Tian"
                },
                "author": "Han Tian"
            },
            {
                "id": "http://arxiv.org/abs/2512.21571v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21571v1",
                "title": "nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures"
                },
                "updated": "2025-12-25T08:27:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    25,
                    8,
                    27,
                    53,
                    3,
                    359,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21571v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The efficient deployment of large language models (LLMs) is hindered by memory architecture heterogeneity, where traditional compilers suffer from fragmented workflows and high adaptation costs. We present nncase, an open-source, end-to-end compilation framework designed to unify optimization across diverse targets. Central to nncase is an e-graph-based term rewriting engine that mitigates the phase ordering problem, enabling global exploration of computation and data movement strategies. The framework integrates three key modules: Auto Vectorize for adapting to heterogeneous computing units, Auto Distribution for searching parallel strategies with cost-aware communication optimization, and Auto Schedule for maximizing on-chip cache locality. Furthermore, a buffer-aware Codegen phase ensures efficient kernel instantiation. Evaluations show that nncase outperforms mainstream frameworks like MLC LLM and Intel IPEX on Qwen3 series models and achieves performance comparable to the hand-optimized llama.cpp on CPUs, demonstrating the viability of automated compilation for high-performance LLM deployment. The source code is available at https://github.com/kendryte/nncase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient deployment of large language models (LLMs) is hindered by memory architecture heterogeneity, where traditional compilers suffer from fragmented workflows and high adaptation costs. We present nncase, an open-source, end-to-end compilation framework designed to unify optimization across diverse targets. Central to nncase is an e-graph-based term rewriting engine that mitigates the phase ordering problem, enabling global exploration of computation and data movement strategies. The framework integrates three key modules: Auto Vectorize for adapting to heterogeneous computing units, Auto Distribution for searching parallel strategies with cost-aware communication optimization, and Auto Schedule for maximizing on-chip cache locality. Furthermore, a buffer-aware Codegen phase ensures efficient kernel instantiation. Evaluations show that nncase outperforms mainstream frameworks like MLC LLM and Intel IPEX on Qwen3 series models and achieves performance comparable to the hand-optimized llama.cpp on CPUs, demonstrating the viability of automated compilation for high-performance LLM deployment. The source code is available at https://github.com/kendryte/nncase."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-25T08:27:53Z",
                "published_parsed": [
                    2025,
                    12,
                    25,
                    8,
                    27,
                    53,
                    3,
                    359,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Hui Guo"
                    },
                    {
                        "name": "Qihang Zheng"
                    },
                    {
                        "name": "Chenghai Huo"
                    },
                    {
                        "name": "Dongliang Guo"
                    },
                    {
                        "name": "Haoqi Yang"
                    },
                    {
                        "name": "Yang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhang"
                },
                "author": "Yang Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2507.02227v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.02227v2",
                "title": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE Simulations"
                },
                "updated": "2025-12-25T05:02:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    25,
                    5,
                    2,
                    40,
                    3,
                    359,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.02227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.02227v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Neural networks have emerged as powerful surrogates for solving partial differential equations (PDEs), offering significant computational speedups over traditional methods. However, these models suffer from a critical limitation: error accumulation during long-term rollouts, where small inaccuracies compound exponentially, eventually causing complete divergence from physically valid solutions. We present PhysicsCorrect, a training-free correction framework that enforces PDE consistency at each prediction step by formulating correction as a linearized inverse problem based on PDE residuals. Our key innovation is an efficient caching strategy that precomputes the Jacobian and its pseudoinverse during an offline warm-up phase, reducing computational overhead by two orders of magnitude compared to standard correction approaches. Across three representative PDE systems, including Navier-Stokes fluid dynamics, wave equations, and the chaotic Kuramoto-Sivashinsky equation, PhysicsCorrect reduces prediction errors by up to 100x while adding negligible inference time (under 5%). The framework integrates seamlessly with diverse architectures, including Fourier Neural Operators, UNets, and Vision Transformers, effectively transforming unstable neural surrogates into reliable simulation tools that bridge the gap between deep learning's computational efficiency and the physical fidelity demanded by practical scientific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks have emerged as powerful surrogates for solving partial differential equations (PDEs), offering significant computational speedups over traditional methods. However, these models suffer from a critical limitation: error accumulation during long-term rollouts, where small inaccuracies compound exponentially, eventually causing complete divergence from physically valid solutions. We present PhysicsCorrect, a training-free correction framework that enforces PDE consistency at each prediction step by formulating correction as a linearized inverse problem based on PDE residuals. Our key innovation is an efficient caching strategy that precomputes the Jacobian and its pseudoinverse during an offline warm-up phase, reducing computational overhead by two orders of magnitude compared to standard correction approaches. Across three representative PDE systems, including Navier-Stokes fluid dynamics, wave equations, and the chaotic Kuramoto-Sivashinsky equation, PhysicsCorrect reduces prediction errors by up to 100x while adding negligible inference time (under 5%). The framework integrates seamlessly with diverse architectures, including Fourier Neural Operators, UNets, and Vision Transformers, effectively transforming unstable neural surrogates into reliable simulation tools that bridge the gap between deep learning's computational efficiency and the physical fidelity demanded by practical scientific applications."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-03T01:22:57Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "arxiv_comment": "AAAI 2026 Oral",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "arxiv_journal_ref": "Proceedings of the AAAI Conference on Artificial Intelligence, 2026",
                "authors": [
                    {
                        "name": "Xinquan Huang"
                    },
                    {
                        "name": "Paris Perdikaris"
                    }
                ],
                "author_detail": {
                    "name": "Paris Perdikaris"
                },
                "author": "Paris Perdikaris"
            },
            {
                "id": "http://arxiv.org/abs/2512.21487v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21487v1",
                "title": "Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism"
                },
                "updated": "2025-12-25T03:22:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    25,
                    3,
                    22,
                    3,
                    3,
                    359,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21487v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The mixture-of-experts (MoE) architecture scales model size with sublinear computational increase but suffers from memory-intensive inference due to KV caches and sparse expert activation. Recent disaggregated expert parallelism (DEP) distributes attention and experts to dedicated GPU groups but lacks support for shared experts and efficient task scheduling, limiting performance.\n  We propose FinDEP, a fine-grained task scheduling algorithm for DEP that maximizes task overlap to improve MoE inference throughput. FinDEP introduces three innovations: 1) partitioning computation/communication into smaller tasks for fine-grained pipelining, 2) formulating a scheduling optimization supporting variable granularity and ordering, and 3) developing an efficient solver for this large search space.\n  Experiments on four GPU systems with DeepSeek-V2 and Qwen3-MoE show FinDEP improves throughput by up to 1.61x over prior methods, achieving up to 1.24x speedup on a 32-GPU system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mixture-of-experts (MoE) architecture scales model size with sublinear computational increase but suffers from memory-intensive inference due to KV caches and sparse expert activation. Recent disaggregated expert parallelism (DEP) distributes attention and experts to dedicated GPU groups but lacks support for shared experts and efficient task scheduling, limiting performance.\n  We propose FinDEP, a fine-grained task scheduling algorithm for DEP that maximizes task overlap to improve MoE inference throughput. FinDEP introduces three innovations: 1) partitioning computation/communication into smaller tasks for fine-grained pipelining, 2) formulating a scheduling optimization supporting variable granularity and ordering, and 3) developing an efficient solver for this large search space.\n  Experiments on four GPU systems with DeepSeek-V2 and Qwen3-MoE show FinDEP improves throughput by up to 1.61x over prior methods, achieving up to 1.24x speedup on a 32-GPU system."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-25T03:22:03Z",
                "published_parsed": [
                    2025,
                    12,
                    25,
                    3,
                    22,
                    3,
                    3,
                    359,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Xinglin Pan"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Wenxiang Lin"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu"
            },
            {
                "id": "http://arxiv.org/abs/2512.21473v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21473v1",
                "title": "Demystifying ARM SME to Optimize General Matrix Multiplications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying ARM SME to Optimize General Matrix Multiplications"
                },
                "updated": "2025-12-25T02:25:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    25,
                    2,
                    25,
                    59,
                    3,
                    359,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21473v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "General Matrix Multiplication (GEMM) is a critical kernel in high-performance computing and deep learning. While modern architectures like ARM's Scalable Matrix Extension (SME) introduce dedicated hardware for matrix operations, existing linear algebra libraries fail to fully exploit its potential, particularly for large matrices. This paper presents MpGEMM, an open-source library that leverages key architectural features of SME to optimize GEMM across multiple precisions. Through a systematic characterization of SME, we derive optimization guidelines that inform our design. MpGEMM employs cache-aware partitioning, efficient data packing with on-the-fly transposition, and specialized micro-kernels that utilize multi-vector loads and all available tile registers. Evaluated on an Apple M4 Pro with real-world workloads from DeepSeek and LLaMA, MpGEMM achieves an average speedup of 1.23x over the vendor-optimized Apple Accelerate library and significantly outperforms other open-source alternatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General Matrix Multiplication (GEMM) is a critical kernel in high-performance computing and deep learning. While modern architectures like ARM's Scalable Matrix Extension (SME) introduce dedicated hardware for matrix operations, existing linear algebra libraries fail to fully exploit its potential, particularly for large matrices. This paper presents MpGEMM, an open-source library that leverages key architectural features of SME to optimize GEMM across multiple precisions. Through a systematic characterization of SME, we derive optimization guidelines that inform our design. MpGEMM employs cache-aware partitioning, efficient data packing with on-the-fly transposition, and specialized micro-kernels that utilize multi-vector loads and all available tile registers. Evaluated on an Apple M4 Pro with real-world workloads from DeepSeek and LLaMA, MpGEMM achieves an average speedup of 1.23x over the vendor-optimized Apple Accelerate library and significantly outperforms other open-source alternatives."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-25T02:25:59Z",
                "published_parsed": [
                    2025,
                    12,
                    25,
                    2,
                    25,
                    59,
                    3,
                    359,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Chencheng Deng"
                    },
                    {
                        "name": "Weiling Yang"
                    },
                    {
                        "name": "Jianbin Fang"
                    },
                    {
                        "name": "Dezun Dong"
                    }
                ],
                "author_detail": {
                    "name": "Dezun Dong"
                },
                "author": "Dezun Dong"
            },
            {
                "id": "http://arxiv.org/abs/2512.21295v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.21295v1",
                "title": "Enhancing Grid Resilience for Giga-Watt Scale Data Centers Using High Voltage Circuit Breaker Operated Braking Resistors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Grid Resilience for Giga-Watt Scale Data Centers Using High Voltage Circuit Breaker Operated Braking Resistors"
                },
                "updated": "2025-12-24T17:23:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    24,
                    17,
                    23,
                    48,
                    2,
                    358,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.21295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.21295v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As hyperscale and co-located data centers scale, the electric grid sees an increase in large, voltage-sensitive IT loads with these data center plant size ranging between 500 MW to 2 GW. A sudden loss of these loads as they switch to onsite UPS during grid voltage excursion events causes a grid frequency rise from generation and load imbalance, and a voltage rise because less power is flowing through the network. This paper proposes and theoretically demonstrates the use of high voltage circuit breaker operated braking resistors at data center transmission substations as an effective strategy in enhancing grid resilience under such large load loss scenarios. We developed a test bed to illustrate the dynamic behavior of the system with resistive braking on a gigawatt scale data center load cluster connected to a 345 kV network. The braking resistor(s), which in the case of inverter rich system comes in a multi-stage configuration, are connected or disconnected via high-speed circuit breaker(s). Results show that insertion for 0.25 to 0.85 seconds sufficiently reduce rate of change of frequency and provides time for primary governor response and capacitor switching to restore steady state. Sensitivity across different synchronous machines and inverter-based resource mix are tested and confirms robustness. We conclude circuit breaker controlled resistive braking is a practical means to enhance Bulk Electric System (BES) resilience for gigawatt scale data centers. The approach integrates with protection, needs no generator changes, and can be scaled with cluster size or growth of the data center facility load.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As hyperscale and co-located data centers scale, the electric grid sees an increase in large, voltage-sensitive IT loads with these data center plant size ranging between 500 MW to 2 GW. A sudden loss of these loads as they switch to onsite UPS during grid voltage excursion events causes a grid frequency rise from generation and load imbalance, and a voltage rise because less power is flowing through the network. This paper proposes and theoretically demonstrates the use of high voltage circuit breaker operated braking resistors at data center transmission substations as an effective strategy in enhancing grid resilience under such large load loss scenarios. We developed a test bed to illustrate the dynamic behavior of the system with resistive braking on a gigawatt scale data center load cluster connected to a 345 kV network. The braking resistor(s), which in the case of inverter rich system comes in a multi-stage configuration, are connected or disconnected via high-speed circuit breaker(s). Results show that insertion for 0.25 to 0.85 seconds sufficiently reduce rate of change of frequency and provides time for primary governor response and capacitor switching to restore steady state. Sensitivity across different synchronous machines and inverter-based resource mix are tested and confirms robustness. We conclude circuit breaker controlled resistive braking is a practical means to enhance Bulk Electric System (BES) resilience for gigawatt scale data centers. The approach integrates with protection, needs no generator changes, and can be scaled with cluster size or growth of the data center facility load."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-24T17:23:48Z",
                "published_parsed": [
                    2025,
                    12,
                    24,
                    17,
                    23,
                    48,
                    2,
                    358,
                    0
                ],
                "arxiv_comment": "Provincially accepted for publication in 2025 IEEE International Conference on Energy Technologies for Future Grids (ETFG) conference proceedings",
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Soham Ghosh"
                    },
                    {
                        "name": "Mohammad Ashraf Hossain Sadi"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Ashraf Hossain Sadi"
                },
                "author": "Mohammad Ashraf Hossain Sadi"
            },
            {
                "id": "http://arxiv.org/abs/2503.12491v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.12491v2",
                "title": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences"
                },
                "updated": "2025-12-24T13:21:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    24,
                    13,
                    21,
                    40,
                    2,
                    358,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.12491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.12491v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) excel at processing long sequences, boosting demand for key-value (KV) caching. While recent efforts to evict KV cache have alleviated the inference burden, they often fail to allocate resources rationally across layers with different attention patterns. In this paper, we introduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach that frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses layer-specific preferences by considering attention dynamics in both spatial and temporal dimensions, allocates rational cache size for layers accordingly, and manages memory constraints in a cascading manner. This approach enables a global view of cache allocation, adaptively distributing resources across diverse attention mechanisms while maintaining memory budgets. CAKE also employs a new eviction indicator that considers the shifting importance of tokens over time, addressing limitations in existing methods that overlook temporal dynamics. Comprehensive experiments on LongBench and NeedleBench show that CAKE maintains model performance with only 3.2% of the KV cache and consistently outperforms current baselines across various models and memory constraints, particularly in low-memory settings. Additionally, CAKE achieves over 10x speedup in decoding latency compared to full cache when processing contexts of 128K tokens with FlashAttention-2. Our code is available at https://github.com/antgroup/cakekv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at processing long sequences, boosting demand for key-value (KV) caching. While recent efforts to evict KV cache have alleviated the inference burden, they often fail to allocate resources rationally across layers with different attention patterns. In this paper, we introduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach that frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses layer-specific preferences by considering attention dynamics in both spatial and temporal dimensions, allocates rational cache size for layers accordingly, and manages memory constraints in a cascading manner. This approach enables a global view of cache allocation, adaptively distributing resources across diverse attention mechanisms while maintaining memory budgets. CAKE also employs a new eviction indicator that considers the shifting importance of tokens over time, addressing limitations in existing methods that overlook temporal dynamics. Comprehensive experiments on LongBench and NeedleBench show that CAKE maintains model performance with only 3.2% of the KV cache and consistently outperforms current baselines across various models and memory constraints, particularly in low-memory settings. Additionally, CAKE achieves over 10x speedup in decoding latency compared to full cache when processing contexts of 128K tokens with FlashAttention-2. Our code is available at https://github.com/antgroup/cakekv."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-16T12:49:44Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "arxiv_comment": "Accepted by ICLR 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Yuchen Cao"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Wen Hu"
                    },
                    {
                        "name": "Shixuan Fan"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Weiyao Lin"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.12284v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12284v3",
                "title": "V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval"
                },
                "updated": "2025-12-24T07:46:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    24,
                    7,
                    46,
                    59,
                    2,
                    358,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12284v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.\n  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.\n  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices."
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-13T11:02:04Z",
                "published_parsed": [
                    2025,
                    12,
                    13,
                    11,
                    2,
                    4,
                    5,
                    347,
                    0
                ],
                "arxiv_comment": "14 pages, 20 figures, conference, accepted by HPCA 2026",
                "arxiv_primary_category": {
                    "term": "eess.IV"
                },
                "authors": [
                    {
                        "name": "Donghyuk Kim"
                    },
                    {
                        "name": "Sejeong Yang"
                    },
                    {
                        "name": "Wonjin Shin"
                    },
                    {
                        "name": "Joo-Young Kim"
                    }
                ],
                "author_detail": {
                    "name": "Joo-Young Kim"
                },
                "author": "Joo-Young Kim"
            },
            {
                "id": "http://arxiv.org/abs/2512.20920v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20920v1",
                "title": "RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks"
                },
                "updated": "2025-12-24T03:56:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    24,
                    3,
                    56,
                    58,
                    2,
                    358,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20920v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Full parameter fine tuning is a key technique for adapting large language models (LLMs) to downstream tasks, but it incurs substantial memory overhead due to the need to cache extensive intermediate activations for backpropagation. This bottleneck makes full fine tuning of contemporary large scale LLMs challenging in practice. Existing distributed training frameworks such as DeepSpeed alleviate this issue using techniques like ZeRO and FSDP, which rely on multi GPU memory or CPU offloading, but often require additional hardware resources and reduce training speed. We introduce RevFFN, a memory efficient fine tuning paradigm for mixture of experts (MoE) LLMs. RevFFN employs carefully designed reversible Transformer blocks that allow reconstruction of layer input activations from outputs during backpropagation, eliminating the need to store most intermediate activations in memory. While preserving the expressive capacity of MoE architectures, this approach significantly reduces peak memory consumption for full parameter fine tuning. As a result, RevFFN enables efficient full fine tuning on a single consumer grade or server grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full parameter fine tuning is a key technique for adapting large language models (LLMs) to downstream tasks, but it incurs substantial memory overhead due to the need to cache extensive intermediate activations for backpropagation. This bottleneck makes full fine tuning of contemporary large scale LLMs challenging in practice. Existing distributed training frameworks such as DeepSpeed alleviate this issue using techniques like ZeRO and FSDP, which rely on multi GPU memory or CPU offloading, but often require additional hardware resources and reduce training speed. We introduce RevFFN, a memory efficient fine tuning paradigm for mixture of experts (MoE) LLMs. RevFFN employs carefully designed reversible Transformer blocks that allow reconstruction of layer input activations from outputs during backpropagation, eliminating the need to store most intermediate activations in memory. While preserving the expressive capacity of MoE architectures, this approach significantly reduces peak memory consumption for full parameter fine tuning. As a result, RevFFN enables efficient full fine tuning on a single consumer grade or server grade GPU."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-24T03:56:58Z",
                "published_parsed": [
                    2025,
                    12,
                    24,
                    3,
                    56,
                    58,
                    2,
                    358,
                    0
                ],
                "arxiv_comment": "Under submission",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ningyuan Liu"
                    },
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Kaitong Cai"
                    },
                    {
                        "name": "Keze Wang"
                    }
                ],
                "author_detail": {
                    "name": "Keze Wang"
                },
                "author": "Keze Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.15713v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15713v2",
                "title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models"
                },
                "updated": "2025-12-24T03:37:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    24,
                    3,
                    37,
                    34,
                    2,
                    358,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15713v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T18:59:55Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    18,
                    59,
                    55,
                    2,
                    351,
                    0
                ],
                "arxiv_comment": "11 pages, 5 figures, conference or other essential info",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Lunbin Zeng"
                    },
                    {
                        "name": "Jingfeng Yao"
                    },
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.20884v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20884v1",
                "title": "The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents"
                },
                "updated": "2025-12-24T02:02:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    24,
                    2,
                    2,
                    25,
                    2,
                    358,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20884v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $$. An optimal learning strategy: Targeting points of maximum ambiguity ($\\mathbb{E}[]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $$. An optimal learning strategy: Targeting points of maximum ambiguity ($\\mathbb{E}[]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-24T02:02:25Z",
                "published_parsed": [
                    2025,
                    12,
                    24,
                    2,
                    2,
                    25,
                    2,
                    358,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zan-Kai Chong"
                    },
                    {
                        "name": "Hiroyuki Ohsaki"
                    },
                    {
                        "name": "Bryan Ng"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Ng"
                },
                "author": "Bryan Ng"
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2601.05251v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05251v1",
                "title": "Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video"
                },
                "updated": "2026-01-08T18:59:56Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    59,
                    56,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05251v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object's complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object's overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object's complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object's overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:59:56Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    59,
                    56,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "15 pages, 8 figures, project page: https://mesh-4d.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zeren Jiang"
                    },
                    {
                        "name": "Chuanxia Zheng"
                    },
                    {
                        "name": "Iro Laina"
                    },
                    {
                        "name": "Diane Larlus"
                    },
                    {
                        "name": "Andrea Vedaldi"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Vedaldi"
                },
                "author": "Andrea Vedaldi"
            },
            {
                "id": "http://arxiv.org/abs/2601.05248v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05248v1",
                "title": "LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model"
                },
                "updated": "2026-01-08T18:59:53Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    59,
                    53,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05248v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. However, explicit reasoning typically incurs non-negligible inference latency, which constrains the temporal resolution required for robotic manipulation. Moreover, such reasoning is confined to the linguistic space, imposing a representational bottleneck that struggles to faithfully capture ineffable physical attributes. To mitigate these limitations, we propose LaST$_0$, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize. Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories. Furthermore, LaST$_0$ adopts a dual-system architecture implemented via a Mixture-of-Transformers design, where a reasoning expert conducts low-frequency latent inference and an acting expert generates high-frequency actions conditioned on robotics-oriented latent representations. To facilitate coordination, LaST$_0$ is trained with heterogeneous operation frequencies, enabling adaptive switching between reasoning and action inference rates during deployment. Across ten simulated and six real-world manipulation tasks, LaST$_0$ improves mean success rates by 8% and 13% over prior VLA methods, respectively, while achieving substantially faster inference. Project website: https://sites.google.com/view/last0",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. However, explicit reasoning typically incurs non-negligible inference latency, which constrains the temporal resolution required for robotic manipulation. Moreover, such reasoning is confined to the linguistic space, imposing a representational bottleneck that struggles to faithfully capture ineffable physical attributes. To mitigate these limitations, we propose LaST$_0$, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize. Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories. Furthermore, LaST$_0$ adopts a dual-system architecture implemented via a Mixture-of-Transformers design, where a reasoning expert conducts low-frequency latent inference and an acting expert generates high-frequency actions conditioned on robotics-oriented latent representations. To facilitate coordination, LaST$_0$ is trained with heterogeneous operation frequencies, enabling adaptive switching between reasoning and action inference rates during deployment. Across ten simulated and six real-world manipulation tasks, LaST$_0$ improves mean success rates by 8% and 13% over prior VLA methods, respectively, while achieving substantially faster inference. Project website: https://sites.google.com/view/last0"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:59:53Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    59,
                    53,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Zhuoyang Liu"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Chengkai Hou"
                    },
                    {
                        "name": "Chenyang Gu"
                    },
                    {
                        "name": "Jiale Yu"
                    },
                    {
                        "name": "Xiangju Mi"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Zhengping Che"
                    },
                    {
                        "name": "Jian Tang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2601.05240v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05240v1",
                "title": "Robust Reasoning as a Symmetry-Protected Topological Phase",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Reasoning as a Symmetry-Protected Topological Phase"
                },
                "updated": "2026-01-08T18:58:34Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    58,
                    34,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05240v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models suffer from \"hallucinations\"-logical inconsistencies induced by semantic noise. We propose that current architectures operate in a \"Metric Phase,\" where causal order is vulnerable to spontaneous symmetry breaking. Here, we identify robust inference as an effective Symmetry-Protected Topological phase, where logical operations are formally isomorphic to non-Abelian anyon braiding, replacing fragile geometric interpolation with robust topological invariants. Empirically, we demonstrate a sharp topological phase transition: while Transformers and RNNs exhibit gapless decay, our Holonomic Network reveals a macroscopic \"mass gap,\" maintaining invariant fidelity below a critical noise threshold. Furthermore, in a variable-binding task on $S_{10}$ ($3.6 \\times 10^6$ states) representing symbolic manipulation, we demonstrate holonomic generalization: the topological model maintains perfect fidelity extrapolating $100\\times$ beyond training ($L=50 \\to 5000$), consistent with a theoretically indefinite causal horizon, whereas Transformers lose logical coherence. Ablation studies indicate this protection emerges strictly from non-Abelian gauge symmetry. This provides strong evidence for a new universality class for logical reasoning, linking causal stability to the topology of the semantic manifold.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models suffer from \"hallucinations\"-logical inconsistencies induced by semantic noise. We propose that current architectures operate in a \"Metric Phase,\" where causal order is vulnerable to spontaneous symmetry breaking. Here, we identify robust inference as an effective Symmetry-Protected Topological phase, where logical operations are formally isomorphic to non-Abelian anyon braiding, replacing fragile geometric interpolation with robust topological invariants. Empirically, we demonstrate a sharp topological phase transition: while Transformers and RNNs exhibit gapless decay, our Holonomic Network reveals a macroscopic \"mass gap,\" maintaining invariant fidelity below a critical noise threshold. Furthermore, in a variable-binding task on $S_{10}$ ($3.6 \\times 10^6$ states) representing symbolic manipulation, we demonstrate holonomic generalization: the topological model maintains perfect fidelity extrapolating $100\\times$ beyond training ($L=50 \\to 5000$), consistent with a theoretically indefinite causal horizon, whereas Transformers lose logical coherence. Ablation studies indicate this protection emerges strictly from non-Abelian gauge symmetry. This provides strong evidence for a new universality class for logical reasoning, linking causal stability to the topology of the semantic manifold."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:58:34Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    58,
                    34,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ilmo Sung"
                    }
                ],
                "author_detail": {
                    "name": "Ilmo Sung"
                },
                "author": "Ilmo Sung"
            },
            {
                "id": "http://arxiv.org/abs/2601.05233v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05233v1",
                "title": "Three-dimensional scene reconstruction using Roman slitless spectra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Three-dimensional scene reconstruction using Roman slitless spectra"
                },
                "updated": "2026-01-08T18:57:07Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    57,
                    7,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05233v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Nancy Grace Roman Space Telescope will carry out a wide-field imaging and slitless spectroscopic survey of Type Ia Supernovae to improve our understanding of dark energy. Crucial to this endeavor is obtaining supernova spectra uncontaminated by light from their host galaxies. However, obtaining such spectra is made more difficult by the inherent problem in wide-field slitless spectroscopic surveys: the blending of spectra of close objects. The spectrum of a supernova will blend with the host galaxy, even from regions distant from the supernova on the sky. If not properly removed, this contamination will introduce systematic bias when the supernova spectra are later used to determine intrinsic supernova parameters and to infer the parameters of dark energy. To address this problem we developed an algorithm that makes use of the spectroscopic observations of the host galaxy at all available observatory roll angles to reconstruct a three-dimensional (3d; 2d spatial, 1d spectral) representation of the underlying host galaxy that accurately matches the 2d slitless spectrum of the host galaxy when projected to an arbitrary rotation angle. We call this ``scene reconstruction''. The projection of the reconstructed scene can be subtracted from an observation of a supernova to remove the contamination from the underlying host. Using simulated Roman data, we show that our method has extremely small systematic errors and significantly less random noise than if we subtracted a single perfectly aligned spectrum of the host obtained before or after the supernova was visible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nancy Grace Roman Space Telescope will carry out a wide-field imaging and slitless spectroscopic survey of Type Ia Supernovae to improve our understanding of dark energy. Crucial to this endeavor is obtaining supernova spectra uncontaminated by light from their host galaxies. However, obtaining such spectra is made more difficult by the inherent problem in wide-field slitless spectroscopic surveys: the blending of spectra of close objects. The spectrum of a supernova will blend with the host galaxy, even from regions distant from the supernova on the sky. If not properly removed, this contamination will introduce systematic bias when the supernova spectra are later used to determine intrinsic supernova parameters and to infer the parameters of dark energy. To address this problem we developed an algorithm that makes use of the spectroscopic observations of the host galaxy at all available observatory roll angles to reconstruct a three-dimensional (3d; 2d spatial, 1d spectral) representation of the underlying host galaxy that accurately matches the 2d slitless spectrum of the host galaxy when projected to an arbitrary rotation angle. We call this ``scene reconstruction''. The projection of the reconstructed scene can be subtracted from an observation of a supernova to remove the contamination from the underlying host. Using simulated Roman data, we show that our method has extremely small systematic errors and significantly less random noise than if we subtracted a single perfectly aligned spectrum of the host obtained before or after the supernova was visible."
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:57:07Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    57,
                    7,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "57 pages, 52 figures. Accepted for publication in The Astrophysical Journal (ApJ). Reconstruction and simulation source code available at https://gitlab.com/astraatmadja/Ilia",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM"
                },
                "authors": [
                    {
                        "name": "Tri L. Astraatmadja"
                    },
                    {
                        "name": "Andrew S. Fruchter"
                    },
                    {
                        "name": "Susana E. Deustua"
                    },
                    {
                        "name": "Helen Qu"
                    },
                    {
                        "name": "Masao Sako"
                    },
                    {
                        "name": "Russell E. Ryan"
                    },
                    {
                        "name": "Yannick Copin"
                    },
                    {
                        "name": "Greg Aldering"
                    },
                    {
                        "name": "Rebekah A. Hounsell"
                    },
                    {
                        "name": "David Rubin"
                    },
                    {
                        "name": "Llus Galbany"
                    },
                    {
                        "name": "Saul Perlmutter"
                    },
                    {
                        "name": "Benjamin M. Rose"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin M. Rose"
                },
                "author": "Benjamin M. Rose"
            },
            {
                "id": "http://arxiv.org/abs/2601.05229v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05229v1",
                "title": "Mitigating Simulator Dependence in AI Parameter Inference for the Epoch of Reionization: The Importance of Simulation Diversity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Simulator Dependence in AI Parameter Inference for the Epoch of Reionization: The Importance of Simulation Diversity"
                },
                "updated": "2026-01-08T18:55:01Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    55,
                    1,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05229v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The 21cm signal of neutral hydrogen contains a wealth of information about the poorly constrained era of cosmological history, the Epoch of Reionization (EoR). Recently, AI models trained on EoR simulations have gained significant attention as a powerful and flexible option for inferring parameters from 21cm observations. However, previous works show that AI models trained on data from one simulator fail to generalize to data from another, raising doubts about AI models' ability to accurately infer parameters from observation. We develop a new strategy for training AI models on cosmological simulations based on the principle that increasing the diversity of the training dataset improves model robustness by averaging out spurious and contradictory information. We train AI models on data from different combinations of four simulators, then compare the models' performance when predicting on data from held-out simulators acting as proxies for the real universe. We find that models trained on data from multiple simulators perform better on data from a held-out simulator than models trained on data from a single simulator, indicating that increasing the diversity of the training dataset improves a model's ability to generalize. This result suggests that future EoR parameter inference methods can mitigate simulator-specific bias by incorporating multiple simulation approaches into their analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 21cm signal of neutral hydrogen contains a wealth of information about the poorly constrained era of cosmological history, the Epoch of Reionization (EoR). Recently, AI models trained on EoR simulations have gained significant attention as a powerful and flexible option for inferring parameters from 21cm observations. However, previous works show that AI models trained on data from one simulator fail to generalize to data from another, raising doubts about AI models' ability to accurately infer parameters from observation. We develop a new strategy for training AI models on cosmological simulations based on the principle that increasing the diversity of the training dataset improves model robustness by averaging out spurious and contradictory information. We train AI models on data from different combinations of four simulators, then compare the models' performance when predicting on data from held-out simulators acting as proxies for the real universe. We find that models trained on data from multiple simulators perform better on data from a held-out simulator than models trained on data from a single simulator, indicating that increasing the diversity of the training dataset improves a model's ability to generalize. This result suggests that future EoR parameter inference methods can mitigate simulator-specific bias by incorporating multiple simulation approaches into their analyses."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:55:01Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    55,
                    1,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Submitted for review to the Monthly Notices of the Royal Astronomical Society",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "authors": [
                    {
                        "name": "Jasper Solt"
                    },
                    {
                        "name": "Jonathan C. Pober"
                    },
                    {
                        "name": "Stephen H. Bach"
                    }
                ],
                "author_detail": {
                    "name": "Stephen H. Bach"
                },
                "author": "Stephen H. Bach"
            },
            {
                "id": "http://arxiv.org/abs/2601.05227v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05227v1",
                "title": "Stochastic Deep Learning: A Probabilistic Framework for Modeling Uncertainty in Structured Temporal Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Deep Learning: A Probabilistic Framework for Modeling Uncertainty in Structured Temporal Data"
                },
                "updated": "2026-01-08T18:53:59Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    53,
                    59,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05227v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "I propose a novel framework that integrates stochastic differential equations (SDEs) with deep generative models to improve uncertainty quantification in machine learning applications involving structured and temporal data. This approach, termed Stochastic Latent Differential Inference (SLDI), embeds an It SDE in the latent space of a variational autoencoder, allowing for flexible, continuous-time modeling of uncertainty while preserving a principled mathematical foundation. The drift and diffusion terms of the SDE are parameterized by neural networks, enabling data-driven inference and generalizing classical time series models to handle irregular sampling and complex dynamic structure.\n  A central theoretical contribution is the co-parameterization of the adjoint state with a dedicated neural network, forming a coupled forward-backward system that captures not only latent evolution but also gradient dynamics. I introduce a pathwise-regularized adjoint loss and analyze variance-reduced gradient flows through the lens of stochastic calculus, offering new tools for improving training stability in deep latent SDEs. My paper unifies and extends variational inference, continuous-time generative modeling, and control-theoretic optimization, providing a rigorous foundation for future developments in stochastic probabilistic machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I propose a novel framework that integrates stochastic differential equations (SDEs) with deep generative models to improve uncertainty quantification in machine learning applications involving structured and temporal data. This approach, termed Stochastic Latent Differential Inference (SLDI), embeds an It SDE in the latent space of a variational autoencoder, allowing for flexible, continuous-time modeling of uncertainty while preserving a principled mathematical foundation. The drift and diffusion terms of the SDE are parameterized by neural networks, enabling data-driven inference and generalizing classical time series models to handle irregular sampling and complex dynamic structure.\n  A central theoretical contribution is the co-parameterization of the adjoint state with a dedicated neural network, forming a coupled forward-backward system that captures not only latent evolution but also gradient dynamics. I introduce a pathwise-regularized adjoint loss and analyze variance-reduced gradient flows through the lens of stochastic calculus, offering new tools for improving training stability in deep latent SDEs. My paper unifies and extends variational inference, continuous-time generative modeling, and control-theoretic optimization, providing a rigorous foundation for future developments in stochastic probabilistic machine learning."
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:53:59Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    53,
                    59,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "20 pages, 6330 words",
                "arxiv_primary_category": {
                    "term": "stat.ML"
                },
                "authors": [
                    {
                        "name": "James Rice"
                    }
                ],
                "author_detail": {
                    "name": "James Rice"
                },
                "author": "James Rice"
            },
            {
                "id": "http://arxiv.org/abs/2501.09135v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2501.09135v3",
                "title": "HAFix: History-Augmented Large Language Models for Bug Fixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAFix: History-Augmented Large Language Models for Bug Fixing"
                },
                "updated": "2026-01-08T18:43:43Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    43,
                    43,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2501.09135v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2501.09135v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent studies have explored the performance of Large Language Models (LLMs) on various Software Engineering (SE) tasks, such as code generation and bug fixing. However, these approaches typically rely on the context data from the current snapshot of the project, overlooking the potential of rich historical data residing in real-world software repositories. Additionally, the impact of prompt styles on LLM performance for SE tasks within a historical context remains underexplored. To address these gaps, we propose HAFix, which stands for History-Augmented LLMs on Bug Fixing, a novel approach that leverages seven individual historical heuristics associated with bugs and aggregates the results of these heuristics (HAFix-Agg) to enhance LLMs' bug-fixing capabilities. To empirically evaluate HAFix, we employ three Code LLMs (i.e., Code Llama, DeepSeek-Coder and DeepSeek-Coder-V2-Lite models) on 51 single-line Python bugs from BugsInPy and 116 single-line Java bugs from Defects4J. Our evaluation demonstrates that multiple HAFix heuristics achieve statistically significant improvements compared to a non-historical baseline inspired by GitHub Copilot. Furthermore, the aggregated HAFix variant HAFix-Agg achieves substantial improvements by combining the complementary strengths of individual heuristics, increasing bug-fixing rates by an average of 45.05% on BugsInPy and 49.92% on Defects4J relative to the corresponding baseline. Moreover, within the context of historical heuristics, we identify the Instruction prompt style as the most effective template compared to the InstructionLabel and InstructionMask for LLMs in bug fixing. Finally, we evaluate the cost of HAFix in terms of inference time and token usage, and provide a pragmatic trade-off analysis of the cost and bug-fixing performance, offering valuable insights for the practical deployment of our approach in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have explored the performance of Large Language Models (LLMs) on various Software Engineering (SE) tasks, such as code generation and bug fixing. However, these approaches typically rely on the context data from the current snapshot of the project, overlooking the potential of rich historical data residing in real-world software repositories. Additionally, the impact of prompt styles on LLM performance for SE tasks within a historical context remains underexplored. To address these gaps, we propose HAFix, which stands for History-Augmented LLMs on Bug Fixing, a novel approach that leverages seven individual historical heuristics associated with bugs and aggregates the results of these heuristics (HAFix-Agg) to enhance LLMs' bug-fixing capabilities. To empirically evaluate HAFix, we employ three Code LLMs (i.e., Code Llama, DeepSeek-Coder and DeepSeek-Coder-V2-Lite models) on 51 single-line Python bugs from BugsInPy and 116 single-line Java bugs from Defects4J. Our evaluation demonstrates that multiple HAFix heuristics achieve statistically significant improvements compared to a non-historical baseline inspired by GitHub Copilot. Furthermore, the aggregated HAFix variant HAFix-Agg achieves substantial improvements by combining the complementary strengths of individual heuristics, increasing bug-fixing rates by an average of 45.05% on BugsInPy and 49.92% on Defects4J relative to the corresponding baseline. Moreover, within the context of historical heuristics, we identify the Instruction prompt style as the most effective template compared to the InstructionLabel and InstructionMask for LLMs in bug fixing. Finally, we evaluate the cost of HAFix in terms of inference time and token usage, and provide a pragmatic trade-off analysis of the cost and bug-fixing performance, offering valuable insights for the practical deployment of our approach in real-world scenarios."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-01-15T20:39:32Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    39,
                    32,
                    2,
                    15,
                    0
                ],
                "arxiv_comment": "Evaluated HAFix on two datasets (BugsInPy, Defects4J) and three LLMs (CodeLlama, DeepSeek-Coder, DeepSeek-Coder-V2) and optimized the figures and tables for better readability",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Yu Shi"
                    },
                    {
                        "name": "Abdul Ali Bangash"
                    },
                    {
                        "name": "Emad Fallahzadeh"
                    },
                    {
                        "name": "Bram Adams"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan"
            },
            {
                "id": "http://arxiv.org/abs/2601.05215v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05215v1",
                "title": "MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents"
                },
                "updated": "2026-01-08T18:39:52Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    39,
                    52,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05215v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present \\textsc{MineNPC-Task}, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world \\emph{Minecraft}. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan/act/memory events-including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts and reports outcomes relative to the total number of attempted subtasks, derived from in-world evidence.\n  As an initial snapshot, we instantiate the framework with GPT-4o and evaluate \\textbf{216} subtasks across \\textbf{8} experienced players. We observe recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, alongside recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively, while highlighting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and harness to support transparent, reproducible evaluation of future memory-aware embodied agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \\textsc{MineNPC-Task}, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world \\emph{Minecraft}. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan/act/memory events-including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts and reports outcomes relative to the total number of attempted subtasks, derived from in-world evidence.\n  As an initial snapshot, we instantiate the framework with GPT-4o and evaluate \\textbf{216} subtasks across \\textbf{8} experienced players. We observe recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, alongside recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively, while highlighting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and harness to support transparent, reproducible evaluation of future memory-aware embodied agents."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:39:52Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    39,
                    52,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Tamil Sudaravan Mohan Doss"
                    },
                    {
                        "name": "Michael Xu"
                    },
                    {
                        "name": "Sudha Rao"
                    },
                    {
                        "name": "Andrew D. Wilson"
                    },
                    {
                        "name": "Balasaravanan Thoravi Kumaravel"
                    }
                ],
                "author_detail": {
                    "name": "Balasaravanan Thoravi Kumaravel"
                },
                "author": "Balasaravanan Thoravi Kumaravel"
            },
            {
                "id": "http://arxiv.org/abs/2601.05214v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05214v1",
                "title": "Internal Representations as Indicators of Hallucinations in Agent Tool Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal Representations as Indicators of Hallucinations in Agent Tool Selection"
                },
                "updated": "2026-01-08T18:38:45Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    38,
                    45,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05214v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:38:45Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    38,
                    45,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Kait Healy"
                    },
                    {
                        "name": "Bharathi Srinivasan"
                    },
                    {
                        "name": "Visakh Madathil"
                    },
                    {
                        "name": "Jing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Wu"
                },
                "author": "Jing Wu"
            },
            {
                "id": "http://arxiv.org/abs/2601.05212v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05212v1",
                "title": "FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching"
                },
                "updated": "2026-01-08T18:36:29Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    36,
                    29,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05212v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Brain Magnetic Resonance Imaging (MRI) plays a central role in studying neurological development, aging, and diseases. One key application is Brain Age Prediction (BAP), which estimates an individual's biological brain age from MRI data. Effective BAP models require large, diverse, and age-balanced datasets, whereas existing 3D MRI datasets are demographically skewed, limiting fairness and generalizability. Acquiring new data is costly and ethically constrained, motivating generative data augmentation. Current generative methods are often based on latent diffusion models, which operate in learned low dimensional latent spaces to address the memory demands of volumetric MRI data. However, these methods are typically slow at inference, may introduce artifacts due to latent compression, and are rarely conditioned on age, thereby affecting the BAP performance. In this work, we propose FlowLet, a conditional generative framework that synthesizes age-conditioned 3D MRIs by leveraging flow matching within an invertible 3D wavelet domain, helping to avoid reconstruction artifacts and reducing computational demands. Experiments show that FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with data generated by FlowLet improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain Magnetic Resonance Imaging (MRI) plays a central role in studying neurological development, aging, and diseases. One key application is Brain Age Prediction (BAP), which estimates an individual's biological brain age from MRI data. Effective BAP models require large, diverse, and age-balanced datasets, whereas existing 3D MRI datasets are demographically skewed, limiting fairness and generalizability. Acquiring new data is costly and ethically constrained, motivating generative data augmentation. Current generative methods are often based on latent diffusion models, which operate in learned low dimensional latent spaces to address the memory demands of volumetric MRI data. However, these methods are typically slow at inference, may introduce artifacts due to latent compression, and are rarely conditioned on age, thereby affecting the BAP performance. In this work, we propose FlowLet, a conditional generative framework that synthesizes age-conditioned 3D MRIs by leveraging flow matching within an invertible 3D wavelet domain, helping to avoid reconstruction artifacts and reducing computational demands. Experiments show that FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with data generated by FlowLet improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:36:29Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    36,
                    29,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Danilo Danese"
                    },
                    {
                        "name": "Angela Lombardi"
                    },
                    {
                        "name": "Matteo Attimonelli"
                    },
                    {
                        "name": "Giuseppe Fasano"
                    },
                    {
                        "name": "Tommaso Di Noia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Di Noia"
                },
                "author": "Tommaso Di Noia"
            },
            {
                "id": "http://arxiv.org/abs/2601.01266v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01266v2",
                "title": "From Policy to Logic for Efficient and Interpretable Coverage Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Policy to Logic for Efficient and Interpretable Coverage Assessment"
                },
                "updated": "2026-01-08T18:28:40Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    28,
                    40,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01266v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01266v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-03T19:24:51Z",
                "published_parsed": [
                    2026,
                    1,
                    3,
                    19,
                    24,
                    51,
                    5,
                    3,
                    0
                ],
                "arxiv_comment": "Accepted at AIMedHealth @ AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Rhitabrat Pokharel"
                    },
                    {
                        "name": "Hamid Reza Hassanzadeh"
                    },
                    {
                        "name": "Ameeta Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Ameeta Agrawal"
                },
                "author": "Ameeta Agrawal"
            },
            {
                "id": "http://arxiv.org/abs/2601.05200v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05200v1",
                "title": "Multivector Reranking in the Era of Strong First-Stage Retrievers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivector Reranking in the Era of Strong First-Stage Retrievers"
                },
                "updated": "2026-01-08T18:22:18Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    22,
                    18,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05200v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Learned multivector representations power modern search systems with strong retrieval effectiveness, but their real-world use is limited by the high cost of exhaustive token-level retrieval. Therefore, most systems adopt a \\emph{gather-and-refine} strategy, where a lightweight gather phase selects candidates for full scoring. However, this approach requires expensive searches over large token-level indexes and often misses the documents that would rank highest under full similarity. In this paper, we reproduce several state-of-the-art multivector retrieval methods on two publicly available datasets, providing a clear picture of the current multivector retrieval field and observing the inefficiency of token-level gathering. Building on top of that, we show that replacing the token-level gather phase with a single-vector document retriever -- specifically, a learned sparse retriever (LSR) -- produces a smaller and more semantically coherent candidate set. This recasts the gather-and-refine pipeline into the well-established two-stage retrieval architecture. As retrieval latency decreases, query encoding with two neural encoders becomes the dominant computational bottleneck. To mitigate this, we integrate recent inference-free LSR methods, demonstrating that they preserve the retrieval effectiveness of the dual-encoder pipeline while substantially reducing query encoding time. Finally, we investigate multiple reranking configurations that balance efficiency, memory, and effectiveness, and we introduce two optimization techniques that prune low-quality candidates early. Empirical results show that these techniques improve retrieval efficiency by up to 1.8$\\times$ with no loss in quality. Overall, our two-stage approach achieves over $24\\times$ speedup over the state-of-the-art multivector retrieval systems, while maintaining comparable or superior retrieval quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned multivector representations power modern search systems with strong retrieval effectiveness, but their real-world use is limited by the high cost of exhaustive token-level retrieval. Therefore, most systems adopt a \\emph{gather-and-refine} strategy, where a lightweight gather phase selects candidates for full scoring. However, this approach requires expensive searches over large token-level indexes and often misses the documents that would rank highest under full similarity. In this paper, we reproduce several state-of-the-art multivector retrieval methods on two publicly available datasets, providing a clear picture of the current multivector retrieval field and observing the inefficiency of token-level gathering. Building on top of that, we show that replacing the token-level gather phase with a single-vector document retriever -- specifically, a learned sparse retriever (LSR) -- produces a smaller and more semantically coherent candidate set. This recasts the gather-and-refine pipeline into the well-established two-stage retrieval architecture. As retrieval latency decreases, query encoding with two neural encoders becomes the dominant computational bottleneck. To mitigate this, we integrate recent inference-free LSR methods, demonstrating that they preserve the retrieval effectiveness of the dual-encoder pipeline while substantially reducing query encoding time. Finally, we investigate multiple reranking configurations that balance efficiency, memory, and effectiveness, and we introduce two optimization techniques that prune low-quality candidates early. Empirical results show that these techniques improve retrieval efficiency by up to 1.8$\\times$ with no loss in quality. Overall, our two-stage approach achieves over $24\\times$ speedup over the state-of-the-art multivector retrieval systems, while maintaining comparable or superior retrieval quality."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:22:18Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    22,
                    18,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "17 pages, 2 figures, ECIR 2026",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Silvio Martinico"
                    },
                    {
                        "name": "Franco Maria Nardini"
                    },
                    {
                        "name": "Cosimo Rulli"
                    },
                    {
                        "name": "Rossano Venturini"
                    }
                ],
                "author_detail": {
                    "name": "Rossano Venturini"
                },
                "author": "Rossano Venturini"
            },
            {
                "id": "http://arxiv.org/abs/2601.05193v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05193v1",
                "title": "Cell size control in bacteria is modulated through extrinsic noise, single-cell- and population-growth",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell size control in bacteria is modulated through extrinsic noise, single-cell- and population-growth"
                },
                "updated": "2026-01-08T18:17:17Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    17,
                    17,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05193v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Living cells maintain size homeostasis by actively compensating for size fluctuations. Here, we present two stochastic maps that unify phenomenological models by integrating fluctuating single-cell growth rates and size-dependent noise mechanisms with cell size control. One map is applicable to mother machine lineages and the other to lineage trees of exponentially-growing cell populations, which reveals that population dynamics alter size control measured in mother machine experiments. For example, an adder can become more sizer-like or more timer-like at the population level depending on the noise statistics. Our analysis of bacterial data identifies extrinsic noise as the dominant mechanism of size variability, characterized by a quadratic conditional variance-mean relationship for division size across growth conditions. This finding contradicts the reported independence of added size relative to birth size but is consistent with the adder property in terms of the independence of the mean added size. Finally, we derive a trade-off between population-growth-rate gain and division-size noise. Correlations between size control quantifiers and single-cell growth rates inferred from data indicate that bacteria prioritize a narrow division-size distribution over growth rate maximisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Living cells maintain size homeostasis by actively compensating for size fluctuations. Here, we present two stochastic maps that unify phenomenological models by integrating fluctuating single-cell growth rates and size-dependent noise mechanisms with cell size control. One map is applicable to mother machine lineages and the other to lineage trees of exponentially-growing cell populations, which reveals that population dynamics alter size control measured in mother machine experiments. For example, an adder can become more sizer-like or more timer-like at the population level depending on the noise statistics. Our analysis of bacterial data identifies extrinsic noise as the dominant mechanism of size variability, characterized by a quadratic conditional variance-mean relationship for division size across growth conditions. This finding contradicts the reported independence of added size relative to birth size but is consistent with the adder property in terms of the independence of the mean added size. Finally, we derive a trade-off between population-growth-rate gain and division-size noise. Correlations between size control quantifiers and single-cell growth rates inferred from data indicate that bacteria prioritize a narrow division-size distribution over growth rate maximisation."
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.CB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:17:17Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    17,
                    17,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE"
                },
                "authors": [
                    {
                        "name": "Arthur Genthon"
                    },
                    {
                        "name": "Philipp Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Thomas"
                },
                "author": "Philipp Thomas"
            },
            {
                "id": "http://arxiv.org/abs/2412.10233v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.10233v4",
                "title": "Nonequilibrium fluctuation-response relations for state observables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonequilibrium fluctuation-response relations for state observables"
                },
                "updated": "2026-01-08T18:17:02Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    17,
                    2,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.10233v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.10233v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Time-integrated state observables, which quantify the fraction of time spent by the system in a specific pool of states, are important in many fields, such as chemical sensing or the theory of fluorescence spectroscopy. We derive exact identities, called Fluctuation-Response Relations (FRRs), that connect the fluctuations of such observables to their response to external perturbations in nonequilibrium steady state of Markov jump processes. Using these results, we derive a first known upper bound on fluctuations of state observables, as well as some new lower bounds. We further demonstrate how our identities provide a deeper understanding of the mechanistic origin of fluctuations and reveal their properties dependent only on system topology, which may be relevant for model inference using measured data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-integrated state observables, which quantify the fraction of time spent by the system in a specific pool of states, are important in many fields, such as chemical sensing or the theory of fluorescence spectroscopy. We derive exact identities, called Fluctuation-Response Relations (FRRs), that connect the fluctuations of such observables to their response to external perturbations in nonequilibrium steady state of Markov jump processes. Using these results, we derive a first known upper bound on fluctuations of state observables, as well as some new lower bounds. We further demonstrate how our identities provide a deeper understanding of the mechanistic origin of fluctuations and reveal their properties dependent only on system topology, which may be relevant for model inference using measured data."
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-13T16:02:38Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    2,
                    38,
                    4,
                    348,
                    0
                ],
                "arxiv_comment": "9 pages, 1 figure. Sign issue fixed in Eq. (2)",
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech"
                },
                "authors": [
                    {
                        "name": "Krzysztof Ptaszynski"
                    },
                    {
                        "name": "Timur Aslyamov"
                    },
                    {
                        "name": "Massimiliano Esposito"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Esposito"
                },
                "author": "Massimiliano Esposito"
            },
            {
                "id": "http://arxiv.org/abs/2601.05192v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05192v1",
                "title": "LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation"
                },
                "updated": "2026-01-08T18:15:34Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    15,
                    34,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05192v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:15:34Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    15,
                    34,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Samy Haffoudhi"
                    },
                    {
                        "name": "Fabian M. Suchanek"
                    },
                    {
                        "name": "Nils Holzenberger"
                    }
                ],
                "author_detail": {
                    "name": "Nils Holzenberger"
                },
                "author": "Nils Holzenberger"
            },
            {
                "id": "http://arxiv.org/abs/2601.05187v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05187v1",
                "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning"
                },
                "updated": "2026-01-08T18:10:35Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    10,
                    35,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05187v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:10:35Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    10,
                    35,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yanchang Liang"
                    },
                    {
                        "name": "Xiaowei Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Zhao"
                },
                "author": "Xiaowei Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2601.05184v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05184v1",
                "title": "Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop"
                },
                "updated": "2026-01-08T18:08:15Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    8,
                    15,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05184v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid advancement of large language models (LLMs) has led to growing interest in using synthetic data to train future models. However, this creates a self-consuming retraining loop, where models are trained on their own outputs and may cause performance drops and induce emerging biases. In real-world applications, previously deployed LLMs may influence the data they generate, leading to a dynamic system driven by user feedback. For example, if a model continues to underserve users from a group, less query data will be collected from this particular demographic of users. In this study, we introduce the concept of \\textbf{S}elf-\\textbf{C}onsuming \\textbf{P}erformative \\textbf{L}oop (\\textbf{SCPL}) and investigate the role of synthetic data in shaping bias during these dynamic iterative training processes under controlled performative feedback. This controlled setting is motivated by the inaccessibility of real-world user preference data from dynamic production systems, and enables us to isolate and analyze feedback-driven bias evolution in a principled manner. We focus on two types of loops, including the typical retraining setting and the incremental fine-tuning setting, which is largely underexplored. Through experiments on three real-world tasks, we find that the performative loop increases preference bias and decreases disparate bias. We design a reward-based rejection sampling strategy to mitigate the bias, moving towards more trustworthy self-improving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has led to growing interest in using synthetic data to train future models. However, this creates a self-consuming retraining loop, where models are trained on their own outputs and may cause performance drops and induce emerging biases. In real-world applications, previously deployed LLMs may influence the data they generate, leading to a dynamic system driven by user feedback. For example, if a model continues to underserve users from a group, less query data will be collected from this particular demographic of users. In this study, we introduce the concept of \\textbf{S}elf-\\textbf{C}onsuming \\textbf{P}erformative \\textbf{L}oop (\\textbf{SCPL}) and investigate the role of synthetic data in shaping bias during these dynamic iterative training processes under controlled performative feedback. This controlled setting is motivated by the inaccessibility of real-world user preference data from dynamic production systems, and enables us to isolate and analyze feedback-driven bias evolution in a principled manner. We focus on two types of loops, including the typical retraining setting and the incremental fine-tuning setting, which is largely underexplored. Through experiments on three real-world tasks, we find that the performative loop increases preference bias and decreases disparate bias. We design a reward-based rejection sampling strategy to mitigate the bias, moving towards more trustworthy self-improving systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:08:15Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    8,
                    15,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yaxuan Wang"
                    },
                    {
                        "name": "Zhongteng Cai"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Xueru Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu"
            },
            {
                "id": "http://arxiv.org/abs/2404.10899v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2404.10899v2",
                "title": "Demonstrating the power and flexibility of variational assumptions for amortized neural posterior estimation in environmental applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demonstrating the power and flexibility of variational assumptions for amortized neural posterior estimation in environmental applications"
                },
                "updated": "2026-01-08T18:07:47Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    7,
                    47,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2404.10899v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2404.10899v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Classic Bayesian methods with complex models are frequently infeasible due to an intractable likelihood. Simulation-based inference methods, such as Approximate Bayesian Computing (ABC), calculate posteriors without accessing a likelihood function by leveraging the fact that data can be quickly simulated from the model, but converge slowly and/or poorly in high-dimensional settings. In this paper, we propose a framework for Bayesian posterior estimation by mapping data to posteriors of parameters using a neural network trained on data simulated from the complex model. Posterior distributions of model parameters are efficiently obtained by feeding observed data into the trained neural network. We show theoretically that our posteriors converge to the true posteriors in Kullback-Leibler divergence. Our approach yields computationally efficient and theoretically justified uncertainty quantification, which is lacking in existing simulation-based neural network approaches. Comprehensive simulation studies highlight our method's robustness and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classic Bayesian methods with complex models are frequently infeasible due to an intractable likelihood. Simulation-based inference methods, such as Approximate Bayesian Computing (ABC), calculate posteriors without accessing a likelihood function by leveraging the fact that data can be quickly simulated from the model, but converge slowly and/or poorly in high-dimensional settings. In this paper, we propose a framework for Bayesian posterior estimation by mapping data to posteriors of parameters using a neural network trained on data simulated from the complex model. Posterior distributions of model parameters are efficiently obtained by feeding observed data into the trained neural network. We show theoretically that our posteriors converge to the true posteriors in Kullback-Leibler divergence. Our approach yields computationally efficient and theoretically justified uncertainty quantification, which is lacking in existing simulation-based neural network approaches. Comprehensive simulation studies highlight our method's robustness and accuracy."
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-04-16T20:40:15Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    20,
                    40,
                    15,
                    1,
                    107,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO"
                },
                "authors": [
                    {
                        "name": "Elliot Maceda"
                    },
                    {
                        "name": "Emily C. Hector"
                    },
                    {
                        "name": "Amanda Lenzi"
                    },
                    {
                        "name": "Brian J. Reich"
                    }
                ],
                "author_detail": {
                    "name": "Brian J. Reich"
                },
                "author": "Brian J. Reich"
            },
            {
                "id": "http://arxiv.org/abs/2505.21072v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.21072v3",
                "title": "Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation"
                },
                "updated": "2026-01-08T18:06:58Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    6,
                    58,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.21072v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.21072v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) enhanced with retrieval, an approach known as Retrieval-Augmented Generation (RAG), have achieved strong performance in open-domain question answering. However, RAG remains prone to hallucinations: factually incorrect outputs may arise from inaccuracies in the model's internal knowledge and the retrieved context. Existing approaches to mitigating hallucinations often conflate factuality with faithfulness to the retrieved evidence, incorrectly labeling factually correct statements as hallucinations if they are not explicitly supported by the retrieval. In this paper, we introduce FRANQ, a new method for hallucination detection in RAG outputs. FRANQ applies distinct uncertainty quantification (UQ) techniques to estimate factuality, conditioning on whether a statement is faithful to the retrieved context. To evaluate FRANQ and competing UQ methods, we construct a new long-form question answering dataset annotated for both factuality and faithfulness, combining automated labeling with manual validation of challenging cases. Extensive experiments across multiple datasets, tasks, and LLMs show that FRANQ achieves more accurate detection of factual errors in RAG-generated responses compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) enhanced with retrieval, an approach known as Retrieval-Augmented Generation (RAG), have achieved strong performance in open-domain question answering. However, RAG remains prone to hallucinations: factually incorrect outputs may arise from inaccuracies in the model's internal knowledge and the retrieved context. Existing approaches to mitigating hallucinations often conflate factuality with faithfulness to the retrieved evidence, incorrectly labeling factually correct statements as hallucinations if they are not explicitly supported by the retrieval. In this paper, we introduce FRANQ, a new method for hallucination detection in RAG outputs. FRANQ applies distinct uncertainty quantification (UQ) techniques to estimate factuality, conditioning on whether a statement is faithful to the retrieved context. To evaluate FRANQ and competing UQ methods, we construct a new long-form question answering dataset annotated for both factuality and faithfulness, combining automated labeling with manual validation of challenging cases. Extensive experiments across multiple datasets, tasks, and LLMs show that FRANQ achieves more accurate detection of factual errors in RAG-generated responses compared to existing approaches."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-27T11:56:59Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    11,
                    56,
                    59,
                    1,
                    147,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ekaterina Fadeeva"
                    },
                    {
                        "name": "Aleksandr Rubashevskii"
                    },
                    {
                        "name": "Dzianis Piatrashyn"
                    },
                    {
                        "name": "Roman Vashurin"
                    },
                    {
                        "name": "Shehzaad Dhuliawala"
                    },
                    {
                        "name": "Artem Shelmanov"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Maxim Panov"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Panov"
                },
                "author": "Maxim Panov"
            },
            {
                "id": "http://arxiv.org/abs/2601.05175v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05175v1",
                "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice"
                },
                "updated": "2026-01-08T18:00:59Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    0,
                    59,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05175v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:00:59Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    0,
                    59,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Project page: https://ivul-kaust.github.io/projects/videoauto-r1/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Shuming Liu"
                    },
                    {
                        "name": "Mingchen Zhuge"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Lemeng Wu"
                    },
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Chenchen Zhu"
                    },
                    {
                        "name": "Zhipeng Cai"
                    },
                    {
                        "name": "Chong Zhou"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Ernie Chang"
                    },
                    {
                        "name": "Saksham Suri"
                    },
                    {
                        "name": "Hongyu Xu"
                    },
                    {
                        "name": "Qi Qian"
                    },
                    {
                        "name": "Wei Wen"
                    },
                    {
                        "name": "Balakrishnan Varadarajan"
                    },
                    {
                        "name": "Zhuang Liu"
                    },
                    {
                        "name": "Hu Xu"
                    },
                    {
                        "name": "Florian Bordes"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yunyang Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yunyang Xiong"
                },
                "author": "Yunyang Xiong"
            },
            {
                "id": "http://arxiv.org/abs/2601.05172v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05172v1",
                "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoV: Chain-of-View Prompting for Spatial Reasoning"
                },
                "updated": "2026-01-08T17:59:42Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    59,
                    42,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05172v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T17:59:42Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    59,
                    42,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Haoyu Zhao"
                    },
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Weijie Wang"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Ruihan Zhu"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang"
            },
            {
                "id": "http://arxiv.org/abs/2601.05170v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05170v1",
                "title": "Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference"
                },
                "updated": "2026-01-08T17:58:52Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    58,
                    52,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05170v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of the meta-inferential properties they entail. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of the meta-inferential properties they entail. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T17:58:52Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    58,
                    52,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Rasmus Blanck"
                    },
                    {
                        "name": "Bill Noble"
                    },
                    {
                        "name": "Stergios Chatzikyriakidis"
                    }
                ],
                "author_detail": {
                    "name": "Stergios Chatzikyriakidis"
                },
                "author": "Stergios Chatzikyriakidis"
            },
            {
                "id": "http://arxiv.org/abs/2601.05167v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05167v1",
                "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RelayLLM: Efficient Reasoning via Collaborative Decoding"
                },
                "updated": "2026-01-08T17:56:16Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    56,
                    16,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05167v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T17:56:16Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    56,
                    16,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Chengsong Huang"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Langlin Huang"
                    },
                    {
                        "name": "Jinyuan Li"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Jiaxin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxin Huang"
                },
                "author": "Jiaxin Huang"
            },
            {
                "id": "http://arxiv.org/abs/2509.18693v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.18693v3",
                "title": "MVT: Mask-Grounded Vision-Language Models for Taxonomy-Aligned Land-Cover Tagging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVT: Mask-Grounded Vision-Language Models for Taxonomy-Aligned Land-Cover Tagging"
                },
                "updated": "2026-01-08T17:56:05Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    56,
                    5,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.18693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.18693v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Land-cover understanding in remote sensing increasingly demands class-agnostic systems that generalize across datasets while remaining spatially precise and interpretable. We study a geometry-first discovery-and-interpretation setting under domain shift, where candidate regions are delineated class-agnostically and supervision avoids lexical class names via anonymized identifiers. Complementary to open-set recognition and open-world learning, we focus on coupling class-agnostic mask evidence with taxonomy-grounded scene interpretation, rather than unknown rejection or continual class expansion. We propose MVT, a three-stage framework that (i) extracts boundary-faithful region masks using SAM2 with domain adaptation, (ii) performs mask-grounded semantic tagging and scene description generation via dual-step LoRA fine-tuning of multimodal LLMs, and (iii) evaluates outputs with LLM-as-judge scoring calibrated by stratified expert ratings. On cross-dataset segmentation transfer (train on OpenEarthMap, evaluate on LoveDA), domain-adapted SAM2 improves mask quality; meanwhile, dual-step MLLM fine-tuning yields more accurate taxonomy-aligned tags and more informative mask-grounded scene descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Land-cover understanding in remote sensing increasingly demands class-agnostic systems that generalize across datasets while remaining spatially precise and interpretable. We study a geometry-first discovery-and-interpretation setting under domain shift, where candidate regions are delineated class-agnostically and supervision avoids lexical class names via anonymized identifiers. Complementary to open-set recognition and open-world learning, we focus on coupling class-agnostic mask evidence with taxonomy-grounded scene interpretation, rather than unknown rejection or continual class expansion. We propose MVT, a three-stage framework that (i) extracts boundary-faithful region masks using SAM2 with domain adaptation, (ii) performs mask-grounded semantic tagging and scene description generation via dual-step LoRA fine-tuning of multimodal LLMs, and (iii) evaluates outputs with LLM-as-judge scoring calibrated by stratified expert ratings. On cross-dataset segmentation transfer (train on OpenEarthMap, evaluate on LoveDA), domain-adapted SAM2 improves mask quality; meanwhile, dual-step MLLM fine-tuning yields more accurate taxonomy-aligned tags and more informative mask-grounded scene descriptions."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-23T06:23:56Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    6,
                    23,
                    56,
                    1,
                    266,
                    0
                ],
                "arxiv_comment": "The project is available at https://charlescsyyy.github.io/MVT",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Siyi Chen"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Weicong Pang"
                    },
                    {
                        "name": "Ruiming Yang"
                    },
                    {
                        "name": "Ziru Chen"
                    },
                    {
                        "name": "Renjun Gao"
                    },
                    {
                        "name": "Alexis Kai Hon Lau"
                    },
                    {
                        "name": "Dasa Gu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Cheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Li"
                },
                "author": "Cheng Li"
            },
            {
                "id": "http://arxiv.org/abs/2601.05162v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05162v1",
                "title": "GenAI-DrawIO-Creator: A Framework for Automated Diagram Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenAI-DrawIO-Creator: A Framework for Automated Diagram Generation"
                },
                "updated": "2026-01-08T17:51:35Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    51,
                    35,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05162v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diagrams are crucial for communicating complex information, yet creating and modifying them remains a labor-intensive task. We present GenAI-DrawIO-Creator, a novel framework that leverages Large Language Models (LLMs) to automate diagram generation and manipulation in the structured XML format used by draw.io. Our system integrates Claude 3.7 to reason about structured visual data and produce valid diagram representations. Key contributions include a high-level system design enabling real-time diagram updates, specialized prompt engineering and error-checking to ensure well-formed XML outputs. We demonstrate a working prototype capable of generating accurate diagrams (such as network architectures and flowcharts) from natural language or code, and even replicating diagrams from images. Simulated evaluations show that our approach significantly reduces diagram creation time and produces outputs with high structural fidelity. Our results highlight the promise of Claude 3.7 in handling structured visual reasoning tasks and lay the groundwork for future research in AI-assisted diagramming applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagrams are crucial for communicating complex information, yet creating and modifying them remains a labor-intensive task. We present GenAI-DrawIO-Creator, a novel framework that leverages Large Language Models (LLMs) to automate diagram generation and manipulation in the structured XML format used by draw.io. Our system integrates Claude 3.7 to reason about structured visual data and produce valid diagram representations. Key contributions include a high-level system design enabling real-time diagram updates, specialized prompt engineering and error-checking to ensure well-formed XML outputs. We demonstrate a working prototype capable of generating accurate diagrams (such as network architectures and flowcharts) from natural language or code, and even replicating diagrams from images. Simulated evaluations show that our approach significantly reduces diagram creation time and produces outputs with high structural fidelity. Our results highlight the promise of Claude 3.7 in handling structured visual reasoning tasks and lay the groundwork for future research in AI-assisted diagramming applications."
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T17:51:35Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    51,
                    35,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Jinze Yu"
                    },
                    {
                        "name": "Dayuan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Dayuan Jiang"
                },
                "author": "Dayuan Jiang"
            },
            {
                "id": "http://arxiv.org/abs/2601.05159v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05159v1",
                "title": "Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering"
                },
                "updated": "2026-01-08T17:49:13Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    49,
                    13,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05159v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T17:49:13Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    49,
                    13,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Shuliang Liu"
                    },
                    {
                        "name": "Songbo Yang"
                    },
                    {
                        "name": "Dong Fang"
                    },
                    {
                        "name": "Sihang Jia"
                    },
                    {
                        "name": "Yuqi Tang"
                    },
                    {
                        "name": "Lingfeng Su"
                    },
                    {
                        "name": "Ruoshui Peng"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu"
            },
            {
                "id": "http://arxiv.org/abs/2510.04487v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.04487v4",
                "title": "Forking-Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forking-Sequences"
                },
                "updated": "2026-01-08T17:43:12Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    43,
                    12,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.04487v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.04487v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While accuracy is a critical requirement for time series forecasting, an equally important desideratum is forecast stability across forecast creation dates (FCDs). Even highly accurate models can produce erratic revisions between FCDs, disrupting downstream decision-making. To improve forecast stability of such revisions, several state-of-the-art models including MQCNN, MQT, and SPADE employ a powerful yet underexplored neural network architectural design known as forking-sequences. This architectural design jointly encodes and decodes the entire time series across all FCDs, producing an entire multi-horizon forecast grid in a single forward pass. This approach contrasts with conventional neural forecasting methods that process FCDs independently, generating only a single multi-horizon forecast per forward pass. In this work, we formalize the forking-sequences design and motivate its broader adoption by introducing a metric for quantifying excess volatility in forecast revisions and by providing theoretical and empirical analysis. We theoretically motivate three key benefits of forking-sequences: (i) increased forecast stability through ensembling; (ii) gradient variance reduction, leading to more stable and consistent training steps; and (iii) improved computational efficiency during inference. We validate the benefits of forking-sequences compared to baseline window-sampling on the M-series benchmark, using 16 datasets from the M1, M3, M4, and Tourism competitions. We observe median accuracy improvements across datasets of 29.7%, 46.2%, 49.3%, 28.6%, 24.7%, and 6.4% for MLP, RNN, LSTM, CNN, Transformer, and StateSpace-based architectures, respectively. We then show that forecast ensembling during inference can improve median forecast stability by 10.8%, 13.2%, 13.0%, 10.9%, 10.2%, and 11.2% for these respective models trained with forking-sequences, while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While accuracy is a critical requirement for time series forecasting, an equally important desideratum is forecast stability across forecast creation dates (FCDs). Even highly accurate models can produce erratic revisions between FCDs, disrupting downstream decision-making. To improve forecast stability of such revisions, several state-of-the-art models including MQCNN, MQT, and SPADE employ a powerful yet underexplored neural network architectural design known as forking-sequences. This architectural design jointly encodes and decodes the entire time series across all FCDs, producing an entire multi-horizon forecast grid in a single forward pass. This approach contrasts with conventional neural forecasting methods that process FCDs independently, generating only a single multi-horizon forecast per forward pass. In this work, we formalize the forking-sequences design and motivate its broader adoption by introducing a metric for quantifying excess volatility in forecast revisions and by providing theoretical and empirical analysis. We theoretically motivate three key benefits of forking-sequences: (i) increased forecast stability through ensembling; (ii) gradient variance reduction, leading to more stable and consistent training steps; and (iii) improved computational efficiency during inference. We validate the benefits of forking-sequences compared to baseline window-sampling on the M-series benchmark, using 16 datasets from the M1, M3, M4, and Tourism competitions. We observe median accuracy improvements across datasets of 29.7%, 46.2%, 49.3%, 28.6%, 24.7%, and 6.4% for MLP, RNN, LSTM, CNN, Transformer, and StateSpace-based architectures, respectively. We then show that forecast ensembling during inference can improve median forecast stability by 10.8%, 13.2%, 13.0%, 10.9%, 10.2%, and 11.2% for these respective models trained with forking-sequences, while maintaining accuracy."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-06T04:51:06Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    4,
                    51,
                    6,
                    0,
                    279,
                    0
                ],
                "arxiv_comment": "Presented at the GPU-Accelerated and Scalable Optimization (ScaleOpt) Workshop, NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Willa Potosnak"
                    },
                    {
                        "name": "Malcolm Wolff"
                    },
                    {
                        "name": "Mengfei Cao"
                    },
                    {
                        "name": "Ruijun Ma"
                    },
                    {
                        "name": "Tatiana Konstantinova"
                    },
                    {
                        "name": "Dmitry Efimov"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Boris Oreshkin"
                    },
                    {
                        "name": "Kin G. Olivares"
                    }
                ],
                "author_detail": {
                    "name": "Kin G. Olivares"
                },
                "author": "Kin G. Olivares"
            },
            {
                "id": "http://arxiv.org/abs/2601.04168v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04168v2",
                "title": "A framework for LISA population inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A framework for LISA population inference"
                },
                "updated": "2026-01-08T17:36:44Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    36,
                    44,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04168v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Laser Interferometer Space Antenna (LISA) is expected to have a source rich data stream containing signals from large numbers of many different types of source. This will include both individually resolvable signals and overlapping stochastic backgrounds, a regime intermediate between current ground-based detectors and pulsar timing arrays. The resolved sources and backgrounds will be fitted together in a high dimensional Global Fit. To extract information about the astrophysical populations to which the sources belong, we need to decode the information in the Global Fit, which requires new methodology that has not been required for the analysis of current gravitational wave detectors. Here, we %start that development, presenting present a hierarchical Bayesian framework to infer the properties of astrophysical populations directly from the output of a LISA Global Fit, consistently accounting for information encoded in both the resolved sources and the unresolved background. Using a simplified model of the Global Fit, we illustrate how the interplay between resolved and unresolved components affects population inference and highlight the impact of data analysis choices, such as the signal-to-noise threshold for resolved sources, on the results. Our approach provides a practical foundation for population inference using LISA data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Laser Interferometer Space Antenna (LISA) is expected to have a source rich data stream containing signals from large numbers of many different types of source. This will include both individually resolvable signals and overlapping stochastic backgrounds, a regime intermediate between current ground-based detectors and pulsar timing arrays. The resolved sources and backgrounds will be fitted together in a high dimensional Global Fit. To extract information about the astrophysical populations to which the sources belong, we need to decode the information in the Global Fit, which requires new methodology that has not been required for the analysis of current gravitational wave detectors. Here, we %start that development, presenting present a hierarchical Bayesian framework to infer the properties of astrophysical populations directly from the output of a LISA Global Fit, consistently accounting for information encoded in both the resolved sources and the unresolved background. Using a simplified model of the Global Fit, we illustrate how the interplay between resolved and unresolved components affects population inference and highlight the impact of data analysis choices, such as the signal-to-noise threshold for resolved sources, on the results. Our approach provides a practical foundation for population inference using LISA data."
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-07T18:35:53Z",
                "published_parsed": [
                    2026,
                    1,
                    7,
                    18,
                    35,
                    53,
                    2,
                    7,
                    0
                ],
                "arxiv_comment": "Extend appendix to treat LVK limit, minor changes to main text",
                "arxiv_primary_category": {
                    "term": "gr-qc"
                },
                "authors": [
                    {
                        "name": "Alexandre Toubiana"
                    },
                    {
                        "name": "Jonathan Gair"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Gair"
                },
                "author": "Jonathan Gair"
            },
            {
                "id": "http://arxiv.org/abs/2601.05145v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05145v1",
                "title": "How deep can a cosmic void be? Voids-informed theoretical bounds in Galileon gravity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How deep can a cosmic void be? Voids-informed theoretical bounds in Galileon gravity"
                },
                "updated": "2026-01-08T17:32:49Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    32,
                    49,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05145v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We establish a general, void-based consistency test for Galileon scalar-tensor theories. We show that the previously reported unphysical breakdown of the predicted Newtonian force in certain Galileon models is controlled by a single condition linking non-linear void dynamics to the cosmic expansion history. This connection yields a redshift-dependent upper bound on the allowed depth of voids and promotes this requirement to a new viability condition, complementary to standard stability criteria. As an example, we apply this void-based criterion to a linear parameterization in the scale factor constrained by theoretical and observational bounds; we find that $\\sim 60\\%$ of the parameter space is excluded, with most problematic models failing by $z\\lesssim 10$. These results position cosmic voids as sharp, broadly applicable, theory-informed filters for viable modified gravity, enabling more informed priors and parameter-space choices in future cosmological inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We establish a general, void-based consistency test for Galileon scalar-tensor theories. We show that the previously reported unphysical breakdown of the predicted Newtonian force in certain Galileon models is controlled by a single condition linking non-linear void dynamics to the cosmic expansion history. This connection yields a redshift-dependent upper bound on the allowed depth of voids and promotes this requirement to a new viability condition, complementary to standard stability criteria. As an example, we apply this void-based criterion to a linear parameterization in the scale factor constrained by theoretical and observational bounds; we find that $\\sim 60\\%$ of the parameter space is excluded, with most problematic models failing by $z\\lesssim 10$. These results position cosmic voids as sharp, broadly applicable, theory-informed filters for viable modified gravity, enabling more informed priors and parameter-space choices in future cosmological inference."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T17:32:49Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    32,
                    49,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "5 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "authors": [
                    {
                        "name": "Tommaso Moretti"
                    },
                    {
                        "name": "Noemi Frusciante"
                    },
                    {
                        "name": "Giovanni Verza"
                    },
                    {
                        "name": "Francesco Pace"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pace"
                },
                "author": "Francesco Pace"
            },
            {
                "id": "http://arxiv.org/abs/2601.05144v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05144v1",
                "title": "Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models"
                },
                "updated": "2026-01-08T17:32:22Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    32,
                    22,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05144v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reasoning Large Language Models (RLLMs) excelling in complex tasks present unique challenges for digital watermarking, as existing methods often disrupt logical coherence or incur high computational costs. Token-based watermarking techniques can corrupt the reasoning flow by applying pseudo-random biases, while semantic-aware approaches improve quality but introduce significant latency or require auxiliary models. This paper introduces ReasonMark, a novel watermarking framework specifically designed for reasoning-intensive LLMs. Our approach decouples generation into an undisturbed Thinking Phase and a watermarked Answering Phase. We propose a Criticality Score to identify semantically pivotal tokens from the reasoning trace, which are distilled into a Principal Semantic Vector (PSV). The PSV then guides a semantically-adaptive mechanism that modulates watermark strength based on token-PSV alignment, ensuring robustness without compromising logical integrity. Extensive experiments show ReasonMark surpasses state-of-the-art methods by reducing text Perplexity by 0.35, increasing translation BLEU score by 0.164, and raising mathematical accuracy by 0.67 points. These advancements are achieved alongside a 0.34% higher watermark detection AUC and stronger robustness to attacks, all with a negligible increase in latency. This work enables the traceable and trustworthy deployment of reasoning LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Large Language Models (RLLMs) excelling in complex tasks present unique challenges for digital watermarking, as existing methods often disrupt logical coherence or incur high computational costs. Token-based watermarking techniques can corrupt the reasoning flow by applying pseudo-random biases, while semantic-aware approaches improve quality but introduce significant latency or require auxiliary models. This paper introduces ReasonMark, a novel watermarking framework specifically designed for reasoning-intensive LLMs. Our approach decouples generation into an undisturbed Thinking Phase and a watermarked Answering Phase. We propose a Criticality Score to identify semantically pivotal tokens from the reasoning trace, which are distilled into a Principal Semantic Vector (PSV). The PSV then guides a semantically-adaptive mechanism that modulates watermark strength based on token-PSV alignment, ensuring robustness without compromising logical integrity. Extensive experiments show ReasonMark surpasses state-of-the-art methods by reducing text Perplexity by 0.35, increasing translation BLEU score by 0.164, and raising mathematical accuracy by 0.67 points. These advancements are achieved alongside a 0.34% higher watermark detection AUC and stronger robustness to attacks, all with a negligible increase in latency. This work enables the traceable and trustworthy deployment of reasoning LLMs in real-world applications."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T17:32:22Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    32,
                    22,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Shuliang Liu"
                    },
                    {
                        "name": "Xingyu Li"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Bingchen Duan"
                    },
                    {
                        "name": "Qi Zheng"
                    },
                    {
                        "name": "Dong Fang"
                    },
                    {
                        "name": "Lingfeng Su"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu"
            },
            {
                "id": "http://arxiv.org/abs/2506.22543v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.22543v2",
                "title": "Simulation-based population inference of LISA's Galactic binaries: Bypassing the global fit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based population inference of LISA's Galactic binaries: Bypassing the global fit"
                },
                "updated": "2026-01-08T17:23:16Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    23,
                    16,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.22543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.22543v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1103/shym-w46f",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The Laser Interferometer Space Antenna (LISA) is expected to detect thousands of individually resolved gravitational wave sources, overlapping in time and frequency, on top of unresolved astrophysical and/or primordial backgrounds. Disentangling resolved sources from backgrounds and extracting their parameters in a computationally intensive \"global fit\" is normally regarded as a necessary step toward reconstructing the properties of the underlying astrophysical populations. Here, we show that it is in principle feasible to infer the population properties of the most numerous of LISA sources -- Galactic double white dwarfs -- directly from the frequency (or, equivalently, time) strain series by adopting a simulation-based approach, without extracting and estimating the parameters of each single source. By training a normalizing flow on a custom-designed compression of simulated LISA frequency series from the Galactic double white dwarf population, we demonstrate how to infer the posterior distribution of population parameters (e.g., mass function, frequency, and spatial distributions). This allows for extracting information on the population parameters from both resolved and unresolved sources simultaneously and in a computationally efficient manner. This approach can be extended to other source classes (e.g., massive and stellar-mass black holes, extreme mass ratio inspirals) and to scenarios involving non-Gaussian or non-stationary noise (e.g., data gaps), provided that fast and accurate simulations are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Laser Interferometer Space Antenna (LISA) is expected to detect thousands of individually resolved gravitational wave sources, overlapping in time and frequency, on top of unresolved astrophysical and/or primordial backgrounds. Disentangling resolved sources from backgrounds and extracting their parameters in a computationally intensive \"global fit\" is normally regarded as a necessary step toward reconstructing the properties of the underlying astrophysical populations. Here, we show that it is in principle feasible to infer the population properties of the most numerous of LISA sources -- Galactic double white dwarfs -- directly from the frequency (or, equivalently, time) strain series by adopting a simulation-based approach, without extracting and estimating the parameters of each single source. By training a normalizing flow on a custom-designed compression of simulated LISA frequency series from the Galactic double white dwarf population, we demonstrate how to infer the posterior distribution of population parameters (e.g., mass function, frequency, and spatial distributions). This allows for extracting information on the population parameters from both resolved and unresolved sources simultaneously and in a computationally efficient manner. This approach can be extended to other source classes (e.g., massive and stellar-mass black holes, extreme mass ratio inspirals) and to scenarios involving non-Gaussian or non-stationary noise (e.g., data gaps), provided that fast and accurate simulations are available."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-27T18:00:03Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    18,
                    0,
                    3,
                    4,
                    178,
                    0
                ],
                "arxiv_comment": "20 pages, 12 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "arxiv_journal_ref": "Phys. Rev. D 112, 103043 (2025)",
                "authors": [
                    {
                        "name": "Rahul Srinivasan"
                    },
                    {
                        "name": "Enrico Barausse"
                    },
                    {
                        "name": "Natalia Korsakova"
                    },
                    {
                        "name": "Roberto Trotta"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Trotta"
                },
                "author": "Roberto Trotta",
                "arxiv_doi": "10.1103/shym-w46f"
            },
            {
                "id": "http://arxiv.org/abs/2601.05134v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05134v1",
                "title": "Sequential Subspace Noise Injection Prevents Accuracy Collapse in Certified Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Subspace Noise Injection Prevents Accuracy Collapse in Certified Unlearning"
                },
                "updated": "2026-01-08T17:23:13Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    23,
                    13,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05134v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Certified unlearning based on differential privacy offers strong guarantees but remains largely impractical: the noisy fine-tuning approaches proposed so far achieve these guarantees but severely reduce model accuracy. We propose sequential noise scheduling, which distributes the noise budget across orthogonal subspaces of the parameter space, rather than injecting it all at once. This simple modification mitigates the destructive effect of noise while preserving the original certification guarantees. We extend the analysis of noisy fine-tuning to the subspace setting, proving that the same $(\\varepsilon,)$ privacy budget is retained. Empirical results on image classification benchmarks show that our approach substantially improves accuracy after unlearning while remaining robust to membership inference attacks. These results show that certified unlearning can achieve both rigorous guarantees and practical utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Certified unlearning based on differential privacy offers strong guarantees but remains largely impractical: the noisy fine-tuning approaches proposed so far achieve these guarantees but severely reduce model accuracy. We propose sequential noise scheduling, which distributes the noise budget across orthogonal subspaces of the parameter space, rather than injecting it all at once. This simple modification mitigates the destructive effect of noise while preserving the original certification guarantees. We extend the analysis of noisy fine-tuning to the subspace setting, proving that the same $(\\varepsilon,)$ privacy budget is retained. Empirical results on image classification benchmarks show that our approach substantially improves accuracy after unlearning while remaining robust to membership inference attacks. These results show that certified unlearning can achieve both rigorous guarantees and practical utility."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T17:23:13Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    23,
                    13,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Polina Dolgova"
                    },
                    {
                        "name": "Sebastian U. Stich"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian U. Stich"
                },
                "author": "Sebastian U. Stich"
            },
            {
                "id": "http://arxiv.org/abs/2601.05128v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05128v1",
                "title": "Revealing the Truth: Calculating True Values in Causal Inference Simulation Studies via Gaussian Quadrature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing the Truth: Calculating True Values in Causal Inference Simulation Studies via Gaussian Quadrature"
                },
                "updated": "2026-01-08T17:17:56Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    17,
                    56,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05128v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Simulation studies are used to understand the properties of statistical methods. A key luxury in many simulation studies is knowledge of the true value (i.e. the estimand) being targeted. With this oracle knowledge in-hand, the researcher conducting the simulation study can assess across repeated realizations of the data how well a given method recovers the truth. In causal inference simulation studies, the truth is rarely a simple parameter of the statistical model chosen to generate the data. Instead, the estimand is often an average treatment effect, marginalized over the distribution of confounders and/or mediators. Luckily, these variables are often generated from common distributions such as the normal, uniform, exponential, or gamma. For all these distributions, Gaussian quadratures provide efficient and accurate calculation for integrands with integral kernels that stem from known probability density functions. We demonstrate through four applications how to use Gaussian quadrature to accurately and efficiently compute the true causal estimand. We also compare the pros and cons of Gauss-Hermite quadrature to Monte Carlo integration approaches, which we use as benchmarks. Overall, we demonstrate that the Gaussian quadrature is an accurate tool with negligible computation time, yet is underused for calculating the true causal estimands in simulation studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation studies are used to understand the properties of statistical methods. A key luxury in many simulation studies is knowledge of the true value (i.e. the estimand) being targeted. With this oracle knowledge in-hand, the researcher conducting the simulation study can assess across repeated realizations of the data how well a given method recovers the truth. In causal inference simulation studies, the truth is rarely a simple parameter of the statistical model chosen to generate the data. Instead, the estimand is often an average treatment effect, marginalized over the distribution of confounders and/or mediators. Luckily, these variables are often generated from common distributions such as the normal, uniform, exponential, or gamma. For all these distributions, Gaussian quadratures provide efficient and accurate calculation for integrands with integral kernels that stem from known probability density functions. We demonstrate through four applications how to use Gaussian quadrature to accurately and efficiently compute the true causal estimand. We also compare the pros and cons of Gauss-Hermite quadrature to Monte Carlo integration approaches, which we use as benchmarks. Overall, we demonstrate that the Gaussian quadrature is an accurate tool with negligible computation time, yet is underused for calculating the true causal estimands in simulation studies."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T17:17:56Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    17,
                    56,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Alex Ocampo"
                    },
                    {
                        "name": "Enrico Giudice"
                    },
                    {
                        "name": "Zachary R. McCaw"
                    },
                    {
                        "name": "Tim P. Morris"
                    }
                ],
                "author_detail": {
                    "name": "Tim P. Morris"
                },
                "author": "Tim P. Morris"
            },
            {
                "id": "http://arxiv.org/abs/2503.19850v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.19850v3",
                "title": "FALCONEye: Finding Answers and Localizing Content in ONE-hour-long videos with multi-modal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FALCONEye: Finding Answers and Localizing Content in ONE-hour-long videos with multi-modal LLMs"
                },
                "updated": "2026-01-08T17:17:54Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    17,
                    54,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.19850v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.19850v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Finding information in hour-long videos is a challenging task even for top-performing Vision Language Models (VLMs), as encoding visual content quickly exceeds available context windows. To tackle this challenge, we present FALCONEye, a novel video agent based on a training-free, model-agnostic meta-architecture composed of a VLM and a Large Language Model (LLM). FALCONEye answers open-ended questions using an exploration-based search algorithm guided by calibrated confidence from the VLM's answers. We also introduce the FALCON-Bench benchmark, extending Question Answering problem to Video Answer Search-requiring models to return both the answer and its supporting temporal window for open-ended questions in hour-long videos. With just a 7B VLM and a lightweight LLM, FALCONEye outscores all open-source 7B VLMs and comparable agents in FALCON-Bench. It further demonstrates its generalization capability in MLVU benchmark with shorter videos and different tasks, surpassing GPT-4o on single-detail tasks while slashing inference cost by roughly an order of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding information in hour-long videos is a challenging task even for top-performing Vision Language Models (VLMs), as encoding visual content quickly exceeds available context windows. To tackle this challenge, we present FALCONEye, a novel video agent based on a training-free, model-agnostic meta-architecture composed of a VLM and a Large Language Model (LLM). FALCONEye answers open-ended questions using an exploration-based search algorithm guided by calibrated confidence from the VLM's answers. We also introduce the FALCON-Bench benchmark, extending Question Answering problem to Video Answer Search-requiring models to return both the answer and its supporting temporal window for open-ended questions in hour-long videos. With just a 7B VLM and a lightweight LLM, FALCONEye outscores all open-source 7B VLMs and comparable agents in FALCON-Bench. It further demonstrates its generalization capability in MLVU benchmark with shorter videos and different tasks, surpassing GPT-4o on single-detail tasks while slashing inference cost by roughly an order of magnitude."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-25T17:17:19Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    17,
                    19,
                    1,
                    84,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Carlos Plou"
                    },
                    {
                        "name": "Cesar Borja"
                    },
                    {
                        "name": "Ruben Martinez-Cantin"
                    },
                    {
                        "name": "Ana C. Murillo"
                    }
                ],
                "author_detail": {
                    "name": "Ana C. Murillo"
                },
                "author": "Ana C. Murillo"
            },
            {
                "id": "http://arxiv.org/abs/2411.05729v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.05729v2",
                "title": "Graph-Dictionary Signal Model for Sparse Representations of Multivariate Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-Dictionary Signal Model for Sparse Representations of Multivariate Data"
                },
                "updated": "2026-01-08T17:09:51Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    9,
                    51,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.05729v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.05729v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Representing and exploiting multivariate signals requires capturing relations between variables, which we can represent by graphs. Graph dictionaries allow to describe complex relational information as a sparse sum of simpler structures, but no prior model exists to infer such underlying structure elements from data. We define a novel Graph-Dictionary signal model, where a finite set of graphs characterizes relationships in data distribution as filters on the weighted sum of their Laplacians. We propose a framework to infer the graph dictionary representation from observed node signals, which allows to include a priori knowledge about signal properties, and about underlying graphs and their coefficients. We introduce a bilinear generalization of the primal-dual splitting algorithm to solve the learning problem. We show the capability of our method to reconstruct graphs from signals in multiple synthetic settings, where our model outperforms popular baselines. Then, we exploit graph-dictionary representations in an illustrative motor imagery decoding task on brain activity data, where we classify imagined motion better than standard methods relying on many more features. Our graph-dictionary model bridges a gap between sparse representations of multivariate data and a structured decomposition of sample-varying relationships into a sparse combination of elementary graph atoms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representing and exploiting multivariate signals requires capturing relations between variables, which we can represent by graphs. Graph dictionaries allow to describe complex relational information as a sparse sum of simpler structures, but no prior model exists to infer such underlying structure elements from data. We define a novel Graph-Dictionary signal model, where a finite set of graphs characterizes relationships in data distribution as filters on the weighted sum of their Laplacians. We propose a framework to infer the graph dictionary representation from observed node signals, which allows to include a priori knowledge about signal properties, and about underlying graphs and their coefficients. We introduce a bilinear generalization of the primal-dual splitting algorithm to solve the learning problem. We show the capability of our method to reconstruct graphs from signals in multiple synthetic settings, where our model outperforms popular baselines. Then, we exploit graph-dictionary representations in an illustrative motor imagery decoding task on brain activity data, where we classify imagined motion better than standard methods relying on many more features. Our graph-dictionary model bridges a gap between sparse representations of multivariate data and a structured decomposition of sample-varying relationships into a sparse combination of elementary graph atoms."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-08T17:40:43Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    17,
                    40,
                    43,
                    4,
                    313,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "William Cappelletti"
                    },
                    {
                        "name": "Pascal Frossard"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Frossard"
                },
                "author": "Pascal Frossard"
            },
            {
                "id": "http://arxiv.org/abs/2511.04804v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04804v2",
                "title": "Simplex-FEM Networks (SiFEN): Learning A Triangulated Function Approximator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplex-FEM Networks (SiFEN): Learning A Triangulated Function Approximator"
                },
                "updated": "2026-01-08T17:07:57Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    7,
                    57,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04804v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial predictor that represents f: R^d -> R^k as a globally C^r finite-element field on a learned simplicial mesh in an optionally warped input space. Each query activates exactly one simplex and at most d+1 basis functions via barycentric coordinates, yielding explicit locality, controllable smoothness, and cache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with a light invertible warp and trains end-to-end with shape regularization, semi-discrete OT coverage, and differentiable edge flips. Under standard shape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic FEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic approximation tasks, tabular regression/classification, and as a drop-in head on compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter budgets, improves calibration (lower ECE/Brier), and reduces inference latency due to geometric locality. These properties make SiFEN a compact, interpretable, and theoretically grounded alternative to dense MLPs and edge-spline networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial predictor that represents f: R^d -> R^k as a globally C^r finite-element field on a learned simplicial mesh in an optionally warped input space. Each query activates exactly one simplex and at most d+1 basis functions via barycentric coordinates, yielding explicit locality, controllable smoothness, and cache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with a light invertible warp and trains end-to-end with shape regularization, semi-discrete OT coverage, and differentiable edge flips. Under standard shape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic FEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic approximation tasks, tabular regression/classification, and as a drop-in head on compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter budgets, improves calibration (lower ECE/Brier), and reduces inference latency due to geometric locality. These properties make SiFEN a compact, interpretable, and theoretically grounded alternative to dense MLPs and edge-spline networks."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T20:49:13Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    20,
                    49,
                    13,
                    3,
                    310,
                    0
                ],
                "arxiv_comment": "We will improve our work soon",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Chaymae Yahyati"
                    },
                    {
                        "name": "Ismail Lamaakal"
                    },
                    {
                        "name": "Khalid El Makkaoui"
                    },
                    {
                        "name": "Ibrahim Ouahbi"
                    },
                    {
                        "name": "Yassine Maleh"
                    }
                ],
                "author_detail": {
                    "name": "Yassine Maleh"
                },
                "author": "Yassine Maleh"
            },
            {
                "id": "http://arxiv.org/abs/2601.05114v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05114v1",
                "title": "Evaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior"
                },
                "updated": "2026-01-08T17:02:22Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    2,
                    22,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05114v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-as-judge systems promise scalable, consistent evaluation. We find the opposite: judges are consistent, but not with each other; they are consistent with themselves. Across 3,240 evaluations (9 judges x 120 unique video x pack items x 3 independent runs), inter-judge agreement is near-zero (Krippendorff's  = 0.042). On two dimensions, judges disagree more than random noise would predict ( < 0). Yet this disagreement isn't chaos; it's structured. A classifier identifies which judge produced an evaluation with 77.1% accuracy from rubric scores alone, rising to 89.9% with disposition features. Within model families, the signal is even stronger: GPT-4.1 and GPT-5.2 are distinguishable with 99.6% accuracy. We call this the reliability paradox: judges cannot agree on what constitutes quality, yet their disagreement patterns are so stable they function as fingerprints. Each judge implements a distinct, stable theory of quality: an \"evaluative disposition\" that shapes how it interprets any rubric. We characterize these dispositions along multiple axes: harshness/leniency, dimension emphasis, within-judge stability (ICC), and evidence behavior (receipt validity, semantic linkage via NLI, and shotgun index). The implication is stark: LLM judges are not interchangeable instruments measuring a shared construct. They are distinct measurement devices, each encoding its own implicit theory of quality. Averaging their scores produces a synthetic verdict that corresponds to no judge's actual values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-judge systems promise scalable, consistent evaluation. We find the opposite: judges are consistent, but not with each other; they are consistent with themselves. Across 3,240 evaluations (9 judges x 120 unique video x pack items x 3 independent runs), inter-judge agreement is near-zero (Krippendorff's  = 0.042). On two dimensions, judges disagree more than random noise would predict ( < 0). Yet this disagreement isn't chaos; it's structured. A classifier identifies which judge produced an evaluation with 77.1% accuracy from rubric scores alone, rising to 89.9% with disposition features. Within model families, the signal is even stronger: GPT-4.1 and GPT-5.2 are distinguishable with 99.6% accuracy. We call this the reliability paradox: judges cannot agree on what constitutes quality, yet their disagreement patterns are so stable they function as fingerprints. Each judge implements a distinct, stable theory of quality: an \"evaluative disposition\" that shapes how it interprets any rubric. We characterize these dispositions along multiple axes: harshness/leniency, dimension emphasis, within-judge stability (ICC), and evidence behavior (receipt validity, semantic linkage via NLI, and shotgun index). The implication is stark: LLM judges are not interchangeable instruments measuring a shared construct. They are distinct measurement devices, each encoding its own implicit theory of quality. Averaging their scores produces a synthetic verdict that corresponds to no judge's actual values."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T17:02:22Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    2,
                    22,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "23 pages, 6 figures, code and artifacts at : https://github.com/wajid-nasser/evaluative-fingerprints",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Wajid Nasser"
                    }
                ],
                "author_detail": {
                    "name": "Wajid Nasser"
                },
                "author": "Wajid Nasser"
            },
            {
                "id": "http://arxiv.org/abs/2507.11939v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.11939v2",
                "title": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering"
                },
                "updated": "2026-01-08T17:00:25Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    0,
                    25,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.11939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.11939v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Charts are a universally adopted medium for data communication, yet existing chart understanding benchmarks are overwhelmingly English-centric, limiting their accessibility and relevance to global audiences. To address this limitation, we introduce PolyChartQA, the first large-scale multilingual benchmark for chart question answering, comprising 22,606 charts and 26,151 QA pairs across 10 diverse languages. PolyChartQA is constructed through a scalable pipeline that enables efficient multilingual chart generation via data translation and code reuse, supported by LLM-based translation and rigorous quality control. We systematically evaluate multilingual chart understanding with PolyChartQA on state-of-the-art LVLMs and reveal a significant performance gap between English and other languages, particularly low-resource ones. Additionally, we introduce a companion multilingual chart question answering training set, PolyChartQA-Train, on which fine-tuning LVLMs yields substantial gains in multilingual chart understanding across diverse model sizes and architectures. Together, our benchmark provides a foundation for developing globally inclusive vision-language models capable of understanding charts across diverse linguistic contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charts are a universally adopted medium for data communication, yet existing chart understanding benchmarks are overwhelmingly English-centric, limiting their accessibility and relevance to global audiences. To address this limitation, we introduce PolyChartQA, the first large-scale multilingual benchmark for chart question answering, comprising 22,606 charts and 26,151 QA pairs across 10 diverse languages. PolyChartQA is constructed through a scalable pipeline that enables efficient multilingual chart generation via data translation and code reuse, supported by LLM-based translation and rigorous quality control. We systematically evaluate multilingual chart understanding with PolyChartQA on state-of-the-art LVLMs and reveal a significant performance gap between English and other languages, particularly low-resource ones. Additionally, we introduce a companion multilingual chart question answering training set, PolyChartQA-Train, on which fine-tuning LVLMs yields substantial gains in multilingual chart understanding across diverse model sizes and architectures. Together, our benchmark provides a foundation for developing globally inclusive vision-language models capable of understanding charts across diverse linguistic contexts."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-16T06:09:02Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    9,
                    2,
                    2,
                    197,
                    0
                ],
                "arxiv_comment": "Work in Progress",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yichen Xu"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Jianzhe Ma"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Qin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Qin Jin"
                },
                "author": "Qin Jin"
            },
            {
                "id": "http://arxiv.org/abs/2601.05111v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05111v1",
                "title": "Agent-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-as-a-Judge"
                },
                "updated": "2026-01-08T16:58:10Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    58,
                    10,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05111v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:58:10Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    58,
                    10,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Runyang You"
                    },
                    {
                        "name": "Hongru Cai"
                    },
                    {
                        "name": "Caiqi Zhang"
                    },
                    {
                        "name": "Qiancheng Xu"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li"
            },
            {
                "id": "http://arxiv.org/abs/2601.05110v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05110v1",
                "title": "GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts"
                },
                "updated": "2026-01-08T16:58:07Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    58,
                    7,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05110v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Reasoning Models (LRMs) achieve remarkable performance by explicitly generating multi-step chains of thought, but this capability incurs substantial inference latency and computational cost. Collaborative inference offers a promising solution by selectively allocating work between lightweight and large models, yet a fundamental challenge remains: determining when a reasoning step requires the capacity of a large model or the efficiency of a small model. Existing routing strategies either rely on local token probabilities or post-hoc verification, introducing significant inference overhead. In this work, we propose a novel perspective on step-wise collaboration: the difficulty of a reasoning step can be inferred from its very first token. Inspired by the \"Aha Moment\" phenomenon in LRMs, we show that the entropy of the initial token serves as a strong predictor of step difficulty. Building on this insight, we introduce GlimpRouter, a training-free step-wise collaboration framework. GlimpRouter employs a lightweight model to generate only the first token of each reasoning step and routes the step to a larger model only when the initial token entropy exceeds a threshold. Experiments on multiple benchmarks demonstrate that our approach significantly reduces inference latency while preserving accuracy. For instance, GlimpRouter attains a substantial 10.7% improvement in accuracy while reducing inference latency by 25.9% compared to a standalone large model on AIME25. These results suggest a simple yet effective mechanism for reasoning: allocating computation based on a glimpse of thought rather than full-step evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) achieve remarkable performance by explicitly generating multi-step chains of thought, but this capability incurs substantial inference latency and computational cost. Collaborative inference offers a promising solution by selectively allocating work between lightweight and large models, yet a fundamental challenge remains: determining when a reasoning step requires the capacity of a large model or the efficiency of a small model. Existing routing strategies either rely on local token probabilities or post-hoc verification, introducing significant inference overhead. In this work, we propose a novel perspective on step-wise collaboration: the difficulty of a reasoning step can be inferred from its very first token. Inspired by the \"Aha Moment\" phenomenon in LRMs, we show that the entropy of the initial token serves as a strong predictor of step difficulty. Building on this insight, we introduce GlimpRouter, a training-free step-wise collaboration framework. GlimpRouter employs a lightweight model to generate only the first token of each reasoning step and routes the step to a larger model only when the initial token entropy exceeds a threshold. Experiments on multiple benchmarks demonstrate that our approach significantly reduces inference latency while preserving accuracy. For instance, GlimpRouter attains a substantial 10.7% improvement in accuracy while reducing inference latency by 25.9% compared to a standalone large model on AIME25. These results suggest a simple yet effective mechanism for reasoning: allocating computation based on a glimpse of thought rather than full-step evaluation."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:58:07Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    58,
                    7,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Code available at https://github.com/Zengwh02/GlimpRouter",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Wenhao Zeng"
                    },
                    {
                        "name": "Xuteng Zhang"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Chao Hu"
                    },
                    {
                        "name": "Yuting Chen"
                    },
                    {
                        "name": "Beijun Shen"
                    },
                    {
                        "name": "Xiaodong Gu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong Gu"
                },
                "author": "Xiaodong Gu"
            },
            {
                "id": "http://arxiv.org/abs/2601.05109v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05109v1",
                "title": "Nalar: An agent serving framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nalar: An agent serving framework"
                },
                "updated": "2026-01-08T16:56:40Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    56,
                    40,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05109v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-driven agentic applications increasingly automate complex, multi-step tasks, but serving them efficiently remains challenging due to heterogeneous components, dynamic and model-driven control flow, long-running state, and unpredictable latencies. Nalar is a ground-up agent-serving framework that cleanly separates workflow specification from execution while providing the runtime visibility and control needed for robust performance. Nalar preserves full Python expressiveness, using lightweight auto-generated stubs that turn agent and tool invocations into futures carrying dependency and context metadata. A managed state layer decouples logical state from physical placement, enabling safe reuse, migration, and consistent retry behavior. A two-level control architecture combines global policy computation with local event-driven enforcement to support adaptive routing, scheduling, and resource management across evolving workflows. Together, these mechanisms allow Nalar to deliver scalable, efficient, and policy-driven serving of heterogeneous agentic applications without burdening developers with orchestration logic. Across three agentic workloads, Nalar cuts tail latency by 34--74\\%, achieves up to $2.9\\times$ speedups, sustains 80 RPS where baselines fail, and scales to 130K futures with sub-500 ms control overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-driven agentic applications increasingly automate complex, multi-step tasks, but serving them efficiently remains challenging due to heterogeneous components, dynamic and model-driven control flow, long-running state, and unpredictable latencies. Nalar is a ground-up agent-serving framework that cleanly separates workflow specification from execution while providing the runtime visibility and control needed for robust performance. Nalar preserves full Python expressiveness, using lightweight auto-generated stubs that turn agent and tool invocations into futures carrying dependency and context metadata. A managed state layer decouples logical state from physical placement, enabling safe reuse, migration, and consistent retry behavior. A two-level control architecture combines global policy computation with local event-driven enforcement to support adaptive routing, scheduling, and resource management across evolving workflows. Together, these mechanisms allow Nalar to deliver scalable, efficient, and policy-driven serving of heterogeneous agentic applications without burdening developers with orchestration logic. Across three agentic workloads, Nalar cuts tail latency by 34--74\\%, achieves up to $2.9\\times$ speedups, sustains 80 RPS where baselines fail, and scales to 130K futures with sub-500 ms control overhead."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:56:40Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    56,
                    40,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Marco Laju"
                    },
                    {
                        "name": "Donghyun Son"
                    },
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Nitin Kedia"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Jayanth Srinivasa"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella"
            },
            {
                "id": "http://arxiv.org/abs/2601.05107v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05107v1",
                "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction"
                },
                "updated": "2026-01-08T16:54:30Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    54,
                    30,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05107v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \\textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \\textbf{Stee}rable \\textbf{M}emory Agent, \\texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \\textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \\textbf{Stee}rable \\textbf{M}emory Agent, \\texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:54:30Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    54,
                    30,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Muzhao Tian"
                    },
                    {
                        "name": "Zisu Huang"
                    },
                    {
                        "name": "Xiaohua Wang"
                    },
                    {
                        "name": "Jingwen Xu"
                    },
                    {
                        "name": "Zhengkang Guo"
                    },
                    {
                        "name": "Qi Qian"
                    },
                    {
                        "name": "Yuanzhe Shen"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Jiakang Yuan"
                    },
                    {
                        "name": "Changze Lv"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqing Zheng"
                },
                "author": "Xiaoqing Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2601.05106v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05106v1",
                "title": "Token-Level LLM Collaboration via FusionRoute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Level LLM Collaboration via FusionRoute"
                },
                "updated": "2026-01-08T16:53:16Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    53,
                    16,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05106v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:53:16Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    53,
                    16,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "25 pages",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Nuoya Xiong"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Hanqing Zeng"
                    },
                    {
                        "name": "Zhaorun Chen"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Shuchao Bi"
                    },
                    {
                        "name": "Lizhu Zhang"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhuokai Zhao"
                },
                "author": "Zhuokai Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.16282v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16282v2",
                "title": "CALM: A CKA-Guided Adaptive Layer-Wise Modularization Framework for LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALM: A CKA-Guided Adaptive Layer-Wise Modularization Framework for LLM Quantization"
                },
                "updated": "2026-01-08T16:51:18Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    51,
                    18,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16282v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16282v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CALM (A CKA-guided Adaptive Layer-wise Modularization)a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. CALM independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMsincluding LLaMA and Qwenin terms of perplexity (PPL) and downstream task performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CALM (A CKA-guided Adaptive Layer-wise Modularization)a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. CALM independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMsincluding LLaMA and Qwenin terms of perplexity (PPL) and downstream task performance."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T08:01:19Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    8,
                    1,
                    19,
                    3,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jinhao Zhang"
                    },
                    {
                        "name": "Yunquan Zhang"
                    },
                    {
                        "name": "Daning Chen"
                    },
                    {
                        "name": "JunSun"
                    },
                    {
                        "name": "Zicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Zicheng Yan"
                },
                "author": "Zicheng Yan"
            },
            {
                "id": "http://arxiv.org/abs/2601.05103v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05103v1",
                "title": "Semantically Orthogonal Framework for Citation Classification: Disentangling Intent and Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantically Orthogonal Framework for Citation Classification: Disentangling Intent and Content"
                },
                "updated": "2026-01-08T16:48:36Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    48,
                    36,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05103v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1007/978-3-032-05409-8_12",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Understanding the role of citations is essential for research assessment and citation-aware digital libraries. However, existing citation classification frameworks often conflate citation intent (why a work is cited) with cited content type (what part is cited), limiting their effectiveness in auto classification due to a dilemma between fine-grained type distinctions and practical classification reliability. We introduce SOFT, a Semantically Orthogonal Framework with Two dimensions that explicitly separates citation intent from cited content type, drawing inspiration from semantic role theory. We systematically re-annotate the ACL-ARC dataset using SOFT and release a cross-disciplinary test set sampled from ACT2. Evaluation with both zero-shot and fine-tuned Large Language Models demonstrates that SOFT enables higher agreement between human annotators and LLMs, and supports stronger classification performance and robust cross-domain generalization compared to ACL-ARC and SciCite annotation frameworks. These results confirm SOFT's value as a clear, reusable annotation standard, improving clarity, consistency, and generalizability for digital libraries and scholarly communication infrastructures. All code and data are publicly available on GitHub https://github.com/zhiyintan/SOFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the role of citations is essential for research assessment and citation-aware digital libraries. However, existing citation classification frameworks often conflate citation intent (why a work is cited) with cited content type (what part is cited), limiting their effectiveness in auto classification due to a dilemma between fine-grained type distinctions and practical classification reliability. We introduce SOFT, a Semantically Orthogonal Framework with Two dimensions that explicitly separates citation intent from cited content type, drawing inspiration from semantic role theory. We systematically re-annotate the ACL-ARC dataset using SOFT and release a cross-disciplinary test set sampled from ACT2. Evaluation with both zero-shot and fine-tuned Large Language Models demonstrates that SOFT enables higher agreement between human annotators and LLMs, and supports stronger classification performance and robust cross-domain generalization compared to ACL-ARC and SciCite annotation frameworks. These results confirm SOFT's value as a clear, reusable annotation standard, improving clarity, consistency, and generalizability for digital libraries and scholarly communication infrastructures. All code and data are publicly available on GitHub https://github.com/zhiyintan/SOFT."
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:48:36Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    48,
                    36,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Accepted at the 29th International Conference on Theory and Practice of Digital Libraries (TPDL 2025)",
                "arxiv_primary_category": {
                    "term": "cs.DL"
                },
                "authors": [
                    {
                        "name": "Changxu Duan"
                    },
                    {
                        "name": "Zhiyin Tan"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyin Tan"
                },
                "author": "Zhiyin Tan",
                "arxiv_doi": "10.1007/978-3-032-05409-8_12"
            },
            {
                "id": "http://arxiv.org/abs/2601.05101v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05101v1",
                "title": "Arabic Prompts with English Tools: A Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabic Prompts with English Tools: A Benchmark"
                },
                "updated": "2026-01-08T16:47:09Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    47,
                    9,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05101v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are now integral to numerous industries, increasingly serving as the core reasoning engine for autonomous agents that perform complex tasks through tool-use. While the development of Arabic-native LLMs is accelerating, the benchmarks for evaluating their capabilities lag behind, with most existing frameworks focusing on English. A critical and overlooked area is tool-calling, where the performance of models prompted in non-English languages like Arabic is poorly understood, especially since these models are often pretrained on predominantly English data. This paper addresses this critical gap by introducing the first dedicated benchmark for evaluating the tool-calling and agentic capabilities of LLMs in the Arabic language. Our work provides a standardized framework to measure the functional accuracy and robustness of models in Arabic agentic workflows. Our findings reveal a huge performance gap: when users interact in Arabic, tool-calling accuracy drops by an average of 5-10\\%, regardless of whether the tool descriptions themselves are in Arabic or English. By shedding light on these critical challenges, this benchmark aims to foster the development of more reliable and linguistically equitable AI agents for Arabic-speaking users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are now integral to numerous industries, increasingly serving as the core reasoning engine for autonomous agents that perform complex tasks through tool-use. While the development of Arabic-native LLMs is accelerating, the benchmarks for evaluating their capabilities lag behind, with most existing frameworks focusing on English. A critical and overlooked area is tool-calling, where the performance of models prompted in non-English languages like Arabic is poorly understood, especially since these models are often pretrained on predominantly English data. This paper addresses this critical gap by introducing the first dedicated benchmark for evaluating the tool-calling and agentic capabilities of LLMs in the Arabic language. Our work provides a standardized framework to measure the functional accuracy and robustness of models in Arabic agentic workflows. Our findings reveal a huge performance gap: when users interact in Arabic, tool-calling accuracy drops by an average of 5-10\\%, regardless of whether the tool descriptions themselves are in Arabic or English. By shedding light on these critical challenges, this benchmark aims to foster the development of more reliable and linguistically equitable AI agents for Arabic-speaking users."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:47:09Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    47,
                    9,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "10 pages, 10 figures, LLMs, Big Data, and Multilinguality for All (LLMs4All) Workshop at IEEE BigData 2025 Conference, Macau, December 10, 2025",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Konstantin Kubrak"
                    },
                    {
                        "name": "Ahmed El-Moselhy"
                    },
                    {
                        "name": "Ammar Alsulami"
                    },
                    {
                        "name": "Remaz Altuwaim"
                    },
                    {
                        "name": "Hassan Ismail Fawaz"
                    },
                    {
                        "name": "Faisal Alsaby"
                    }
                ],
                "author_detail": {
                    "name": "Faisal Alsaby"
                },
                "author": "Faisal Alsaby"
            },
            {
                "id": "http://arxiv.org/abs/2507.21928v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.21928v4",
                "title": "Vibe Coding as a Reconfiguration of Intent Mediation in Software Development: Definition, Implications, and Research Agenda",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vibe Coding as a Reconfiguration of Intent Mediation in Software Development: Definition, Implications, and Research Agenda"
                },
                "updated": "2026-01-08T16:44:45Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    44,
                    45,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.21928v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.21928v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/ACCESS.2025.3645466",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Software development is undergoing a fundamental transformation as vibe coding becomes widespread, with large portions of contemporary codebases now being generated by Artificial Intelligence (AI). The disconnect between rapid adoption and limited conceptual understanding highlights the need for an inquiry into this emerging paradigm. Drawing on an intent perspective and historical analysis, we define vibe coding as a software development paradigm where humans and Generative AI (GenAI) engage in collaborative flow to co-create software artifacts through natural language dialogue, shifting the mediation of developer intent from deterministic instruction to probabilistic inference. By intent mediation, we refer to the fundamental process through which developers translate their conceptual goals into representations that computational systems can execute. Our results show that vibe coding redistributes epistemic labor between humans and machines, shifting expertise from technical implementation toward collaborative orchestration. We identify key opportunities, including democratization, acceleration, and systemic leverage, alongside risks such as black-box codebases, responsibility gaps, and ecosystem bias. We conclude with a research agenda spanning human-, technology-, and organization-centered directions to guide future investigations of this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software development is undergoing a fundamental transformation as vibe coding becomes widespread, with large portions of contemporary codebases now being generated by Artificial Intelligence (AI). The disconnect between rapid adoption and limited conceptual understanding highlights the need for an inquiry into this emerging paradigm. Drawing on an intent perspective and historical analysis, we define vibe coding as a software development paradigm where humans and Generative AI (GenAI) engage in collaborative flow to co-create software artifacts through natural language dialogue, shifting the mediation of developer intent from deterministic instruction to probabilistic inference. By intent mediation, we refer to the fundamental process through which developers translate their conceptual goals into representations that computational systems can execute. Our results show that vibe coding redistributes epistemic labor between humans and machines, shifting expertise from technical implementation toward collaborative orchestration. We identify key opportunities, including democratization, acceleration, and systemic leverage, alongside risks such as black-box codebases, responsibility gaps, and ecosystem bias. We conclude with a research agenda spanning human-, technology-, and organization-centered directions to guide future investigations of this paradigm."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-29T15:44:55Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    15,
                    44,
                    55,
                    1,
                    210,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "arxiv_journal_ref": "IEEE Access, 13, pp. 213242-213259 (2025)",
                "authors": [
                    {
                        "name": "Christian Meske"
                    },
                    {
                        "name": "Tobias Hermanns"
                    },
                    {
                        "name": "Esther von der Weiden"
                    },
                    {
                        "name": "Kai-Uwe Loser"
                    },
                    {
                        "name": "Thorsten Berger"
                    }
                ],
                "author_detail": {
                    "name": "Thorsten Berger"
                },
                "author": "Thorsten Berger",
                "arxiv_doi": "10.1109/ACCESS.2025.3645466"
            },
            {
                "id": "http://arxiv.org/abs/2510.19044v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.19044v2",
                "title": "The slope and scatter of the star forming main sequence at z~5 : reconciling observations with simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The slope and scatter of the star forming main sequence at z~5 : reconciling observations with simulations"
                },
                "updated": "2026-01-08T16:37:35Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    37,
                    35,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.19044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.19044v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Galaxies exhibit a tight correlation between their star-formation rate and stellar mass over a wide redshift range known as the star-forming main sequence (SFMS). With JWST, we can now investigate the SFMS at high redshifts down to masses of $\\sim10^6$ M$_{\\odot}$, using sensitive star-formation rate tracers such as H$$ emission -- which allow us to probe the variability in star formation histories. We present inferences of the SFMS based on 316 H$$-selected galaxies at $z\\sim4$-$5$ with $\\log(\\rm M_\\star/M_\\odot) = 6.4$ -$10.6$. These galaxies were identified behind the Abell 2744 lensing cluster with NIRCam grism spectroscopy from the ``All the Little Things'' (ALT) survey. At face value, our data suggest a shallow slope of the SFMS (SFR $\\propto \\mathrm{M}_\\star^$, with $=0.45$). After correcting for the H$$-flux limited nature of our survey using a Bayesian framework, the slope steepens to $= 0.59^{+0.10}_{-0.09}$, whereas current data on their own are inconclusive on the mass dependence of the scatter. These slopes differ significantly from the slope of $\\approx1$ expected from the observed evolution of the galaxy stellar mass function and from simulations. When fixing the slope to $=1$, we find evidence for a decreasing intrinsic scatter with stellar mass (from $\\approx 0.5$ dex at M$_\\star=10^8$ M$_\\odot$ to $0.4$ dex at M$_\\star=10^{10}$ M$_\\odot$). This tension might be explained by a (combination of) luminosity-dependent SFR(H$$) calibration, a population of (mini)-quenched low-mass galaxies, or underestimated dust attenuation in high-mass galaxies. Future deep observations across facilities can quantify these processes, enabling better insights into the variability of star formation histories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxies exhibit a tight correlation between their star-formation rate and stellar mass over a wide redshift range known as the star-forming main sequence (SFMS). With JWST, we can now investigate the SFMS at high redshifts down to masses of $\\sim10^6$ M$_{\\odot}$, using sensitive star-formation rate tracers such as H$$ emission -- which allow us to probe the variability in star formation histories. We present inferences of the SFMS based on 316 H$$-selected galaxies at $z\\sim4$-$5$ with $\\log(\\rm M_\\star/M_\\odot) = 6.4$ -$10.6$. These galaxies were identified behind the Abell 2744 lensing cluster with NIRCam grism spectroscopy from the ``All the Little Things'' (ALT) survey. At face value, our data suggest a shallow slope of the SFMS (SFR $\\propto \\mathrm{M}_\\star^$, with $=0.45$). After correcting for the H$$-flux limited nature of our survey using a Bayesian framework, the slope steepens to $= 0.59^{+0.10}_{-0.09}$, whereas current data on their own are inconclusive on the mass dependence of the scatter. These slopes differ significantly from the slope of $\\approx1$ expected from the observed evolution of the galaxy stellar mass function and from simulations. When fixing the slope to $=1$, we find evidence for a decreasing intrinsic scatter with stellar mass (from $\\approx 0.5$ dex at M$_\\star=10^8$ M$_\\odot$ to $0.4$ dex at M$_\\star=10^{10}$ M$_\\odot$). This tension might be explained by a (combination of) luminosity-dependent SFR(H$$) calibration, a population of (mini)-quenched low-mass galaxies, or underestimated dust attenuation in high-mass galaxies. Future deep observations across facilities can quantify these processes, enabling better insights into the variability of star formation histories."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-21T19:57:17Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    19,
                    57,
                    17,
                    1,
                    294,
                    0
                ],
                "arxiv_comment": "Main text 15 pages with 11 figures. Figure 7 shows the key measurements and Fig 8 model inference. Accepted for publication in A&A",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "Claudia Di Cesare"
                    },
                    {
                        "name": "Jorryt Matthee"
                    },
                    {
                        "name": "Rohan P. Naidu"
                    },
                    {
                        "name": "Alberto Torralba"
                    },
                    {
                        "name": "Gauri Kotiwale"
                    },
                    {
                        "name": "Ivan G. Kramarenko"
                    },
                    {
                        "name": "Jeremy Blaizot"
                    },
                    {
                        "name": "Joakim Rosdahl"
                    },
                    {
                        "name": "Joel Leja"
                    },
                    {
                        "name": "Edoardo Iani"
                    },
                    {
                        "name": "Angela Adamo"
                    },
                    {
                        "name": "Alba Covelo-Paz"
                    },
                    {
                        "name": "Lukas J. Furtak"
                    },
                    {
                        "name": "Kasper E. Heintz"
                    },
                    {
                        "name": "Sara Mascia"
                    },
                    {
                        "name": "Benjamn Navarrete"
                    },
                    {
                        "name": "Pascal A. Oesch"
                    },
                    {
                        "name": "Michael Romano"
                    },
                    {
                        "name": "Irene Shivaei"
                    },
                    {
                        "name": "Sandro Tacchella"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Tacchella"
                },
                "author": "Sandro Tacchella"
            },
            {
                "id": "http://arxiv.org/abs/2601.05087v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05087v1",
                "title": "Online Bayesian Learning of Agent Behavior in Differential Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Bayesian Learning of Agent Behavior in Differential Games"
                },
                "updated": "2026-01-08T16:35:43Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    35,
                    43,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05087v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This work introduces an online Bayesian game-theoretic method for behavior identification in multi-agent dynamical systems. By casting Hamilton-Jacobi-Bellman optimality conditions as linear-in-parameter residuals, the method enables fast sequential Bayesian updates, uncertainty-aware inference, and robust prediction from limited, noisy data-without history stacks. The approach accommodates nonlinear dynamics and nonquadratic value functions through basis expansions, providing flexible models. Experiments, including linear-quadratic and nonlinear shared-control scenarios, demonstrate accurate prediction with quantified uncertainty, highlighting the method's relevance for adaptive interaction and real-time decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces an online Bayesian game-theoretic method for behavior identification in multi-agent dynamical systems. By casting Hamilton-Jacobi-Bellman optimality conditions as linear-in-parameter residuals, the method enables fast sequential Bayesian updates, uncertainty-aware inference, and robust prediction from limited, noisy data-without history stacks. The approach accommodates nonlinear dynamics and nonquadratic value functions through basis expansions, providing flexible models. Experiments, including linear-quadratic and nonlinear shared-control scenarios, demonstrate accurate prediction with quantified uncertainty, highlighting the method's relevance for adaptive interaction and real-time decision making."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:35:43Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    35,
                    43,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Francesco Bianchin"
                    },
                    {
                        "name": "Robert Lefringhausen"
                    },
                    {
                        "name": "Sandra Hirche"
                    }
                ],
                "author_detail": {
                    "name": "Sandra Hirche"
                },
                "author": "Sandra Hirche"
            },
            {
                "id": "http://arxiv.org/abs/2506.15480v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.15480v2",
                "title": "Instruction Tuning with and without Context: Behavioral Shifts and Downstream Impact",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Tuning with and without Context: Behavioral Shifts and Downstream Impact"
                },
                "updated": "2026-01-08T16:32:25Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    32,
                    25,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.15480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.15480v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Instruction tuning is a widely used approach to improve the instruction-following ability of large language models (LLMs). Instruction-tuning datasets typically include a mixture of context-augmented and context-free examples, yet prior work has largely combined these data types without examining their distinct effects. In this paper, we investigate how training LLMs with or without context affects model behavior and downstream performance. First, in the text domain, we show that LLMs trained with context attend more strongly to the provided knowledge, achieving better grounding. We also observe that context-augmented training shifts how LLMs use knowledge: models store and leverage less on parametric knowledge and instead depend more on the provided context. Second, we observe that using LLM trained with context-augmented data as the backbone for vision-language models reduces hallucination and improves grounding in the visual domain. Finally, we explore practical strategies for real-world deployments where context availability varies. We show that maintaining separate context-augmented and context-free models and routing inputs between them yields more robust overall performance than training a single mixed model, as it better preserves their complementary strengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning is a widely used approach to improve the instruction-following ability of large language models (LLMs). Instruction-tuning datasets typically include a mixture of context-augmented and context-free examples, yet prior work has largely combined these data types without examining their distinct effects. In this paper, we investigate how training LLMs with or without context affects model behavior and downstream performance. First, in the text domain, we show that LLMs trained with context attend more strongly to the provided knowledge, achieving better grounding. We also observe that context-augmented training shifts how LLMs use knowledge: models store and leverage less on parametric knowledge and instead depend more on the provided context. Second, we observe that using LLM trained with context-augmented data as the backbone for vision-language models reduces hallucination and improves grounding in the visual domain. Finally, we explore practical strategies for real-world deployments where context availability varies. We show that maintaining separate context-augmented and context-free models and routing inputs between them yields more robust overall performance than training a single mixed model, as it better preserves their complementary strengths."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-18T14:13:56Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    14,
                    13,
                    56,
                    2,
                    169,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hyunji Lee"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Yunjae Won"
                    },
                    {
                        "name": "Hanseok Oh"
                    },
                    {
                        "name": "Geewook Kim"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Minjoon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Minjoon Seo"
                },
                "author": "Minjoon Seo"
            },
            {
                "id": "http://arxiv.org/abs/2601.05083v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05083v1",
                "title": "Driving on Registers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driving on Registers"
                },
                "updated": "2026-01-08T16:28:24Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    28,
                    24,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05083v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present DrivoR, a simple and efficient transformer-based architecture for end-to-end autonomous driving. Our approach builds on pretrained Vision Transformers (ViTs) and introduces camera-aware register tokens that compress multi-camera features into a compact scene representation, significantly reducing downstream computation without sacrificing accuracy. These tokens drive two lightweight transformer decoders that generate and then score candidate trajectories. The scoring decoder learns to mimic an oracle and predicts interpretable sub-scores representing aspects such as safety, comfort, and efficiency, enabling behavior-conditioned driving at inference. Despite its minimal design, DrivoR outperforms or matches strong contemporary baselines across NAVSIM-v1, NAVSIM-v2, and the photorealistic closed-loop HUGSIM benchmark. Our results show that a pure-transformer architecture, combined with targeted token compression, is sufficient for accurate, efficient, and adaptive end-to-end driving. Code and checkpoints will be made available via the project page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DrivoR, a simple and efficient transformer-based architecture for end-to-end autonomous driving. Our approach builds on pretrained Vision Transformers (ViTs) and introduces camera-aware register tokens that compress multi-camera features into a compact scene representation, significantly reducing downstream computation without sacrificing accuracy. These tokens drive two lightweight transformer decoders that generate and then score candidate trajectories. The scoring decoder learns to mimic an oracle and predicts interpretable sub-scores representing aspects such as safety, comfort, and efficiency, enabling behavior-conditioned driving at inference. Despite its minimal design, DrivoR outperforms or matches strong contemporary baselines across NAVSIM-v1, NAVSIM-v2, and the photorealistic closed-loop HUGSIM benchmark. Our results show that a pure-transformer architecture, combined with targeted token compression, is sufficient for accurate, efficient, and adaptive end-to-end driving. Code and checkpoints will be made available via the project page."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:28:24Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    28,
                    24,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ellington Kirby"
                    },
                    {
                        "name": "Alexandre Boulch"
                    },
                    {
                        "name": "Yihong Xu"
                    },
                    {
                        "name": "Yuan Yin"
                    },
                    {
                        "name": "Gilles Puy"
                    },
                    {
                        "name": "loi Zablocki"
                    },
                    {
                        "name": "Andrei Bursuc"
                    },
                    {
                        "name": "Spyros Gidaris"
                    },
                    {
                        "name": "Renaud Marlet"
                    },
                    {
                        "name": "Florent Bartoccioni"
                    },
                    {
                        "name": "Anh-Quan Cao"
                    },
                    {
                        "name": "Nermin Samet"
                    },
                    {
                        "name": "Tuan-Hung VU"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord"
            },
            {
                "id": "http://arxiv.org/abs/2506.03259v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.03259v2",
                "title": "Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems"
                },
                "updated": "2026-01-08T16:22:24Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    22,
                    24,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.03259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.03259v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Purpose: This study aims to evaluate the effectiveness of large language models (LLMs) in automating disease annotation of CT radiology reports. We compare a rule-based algorithm (RBA), RadBERT, and three lightweight open-weight LLMs for multi-disease labeling of chest, abdomen, and pelvis (CAP) CT reports.\n  Materials and Methods: This retrospective study analyzed 40,833 chest-abdomen-pelvis (CAP) CT reports from 29,540 patients, with 1,789 reports manually annotated across three organ systems. External validation was conducted using the CT RATE dataset. Three open-weight LLMs were tested with zero-shot prompting. Performance was evaluated using Cohen's Kappa ($$) and micro/macro-averaged F1 scores.\n  Results: In the internal test set of 12,197 CAP reports from 8,854 patients, Llama-3.1 8B and Gemma-3 27B showed the highest agreement ($$ median: 0.87). On the manually annotated set, Gemma-3 27B achieved the top macro-F1 (0.82), followed by Llama-3.1 8B (0.79), while the RBA scored lowest (0.64). On the CT RATE dataset (lungs/pleura labels only), Llama-3.1 8B performed best (0.91), with Gemma-3 27B close behind (0.89). Performance differences were mainly due to differing labeling practices, especially for labels with high subjectivity such as atelectasis.\n  Conclusion: Lightweight LLMs outperform rule-based methods for CT report annotation and generalize across organ systems with zero-shot prompting. However, binary labels alone cannot capture the full nuance of report language. LLMs can provide a flexible, efficient solution aligned with clinical judgment and user needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: This study aims to evaluate the effectiveness of large language models (LLMs) in automating disease annotation of CT radiology reports. We compare a rule-based algorithm (RBA), RadBERT, and three lightweight open-weight LLMs for multi-disease labeling of chest, abdomen, and pelvis (CAP) CT reports.\n  Materials and Methods: This retrospective study analyzed 40,833 chest-abdomen-pelvis (CAP) CT reports from 29,540 patients, with 1,789 reports manually annotated across three organ systems. External validation was conducted using the CT RATE dataset. Three open-weight LLMs were tested with zero-shot prompting. Performance was evaluated using Cohen's Kappa ($$) and micro/macro-averaged F1 scores.\n  Results: In the internal test set of 12,197 CAP reports from 8,854 patients, Llama-3.1 8B and Gemma-3 27B showed the highest agreement ($$ median: 0.87). On the manually annotated set, Gemma-3 27B achieved the top macro-F1 (0.82), followed by Llama-3.1 8B (0.79), while the RBA scored lowest (0.64). On the CT RATE dataset (lungs/pleura labels only), Llama-3.1 8B performed best (0.91), with Gemma-3 27B close behind (0.89). Performance differences were mainly due to differing labeling practices, especially for labels with high subjectivity such as atelectasis.\n  Conclusion: Lightweight LLMs outperform rule-based methods for CT report annotation and generalize across organ systems with zero-shot prompting. However, binary labels alone cannot capture the full nuance of report language. LLMs can provide a flexible, efficient solution aligned with clinical judgment and user needs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-03T18:00:08Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    0,
                    8,
                    1,
                    154,
                    0
                ],
                "arxiv_comment": "18 pages, 9 figures, to be submitted in Radiology: Artificial Intelligence",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Michael E. Garcia-Alcoser"
                    },
                    {
                        "name": "Mobina GhojoghNejad"
                    },
                    {
                        "name": "Fakrul Islam Tushar"
                    },
                    {
                        "name": "David Kim"
                    },
                    {
                        "name": "Kyle J. Lafata"
                    },
                    {
                        "name": "Geoffrey D. Rubin"
                    },
                    {
                        "name": "Joseph Y. Lo"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Y. Lo"
                },
                "author": "Joseph Y. Lo"
            },
            {
                "id": "http://arxiv.org/abs/2601.05075v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05075v1",
                "title": "SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment"
                },
                "updated": "2026-01-08T16:19:24Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    19,
                    24,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05075v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:19:24Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    19,
                    24,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ziyang Chen"
                    },
                    {
                        "name": "Zhenxuan Huang"
                    },
                    {
                        "name": "Yile Wang"
                    },
                    {
                        "name": "Weiqin Wang"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Hui Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hui Huang"
                },
                "author": "Hui Huang"
            },
            {
                "id": "http://arxiv.org/abs/2510.17722v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.17722v2",
                "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues"
                },
                "updated": "2026-01-08T16:16:20Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    16,
                    20,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.17722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.17722v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. However, existing evaluation benchmarks remain limited to single-turn question answering, overlooking the complexity of multi-turn dialogues in real-world scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video understanding benchmark for evaluating MLLMs in multi-turn dialogues. Specifically, our MT-Video-Bench mainly assesses 6 core competencies that focus on perceptivity and interactivity, encompassing 1,000 meticulously curated multi-turn dialogues from diverse domains. These capabilities are rigorously aligned with real-world applications, such as interactive sports analysis and multi-turn video-based intelligent tutoring. With MT-Video-Bench, we extensively evaluate various state-of-the-art open-source and closed-source MLLMs, revealing their significant performance discrepancies and limitations in handling multi-turn video dialogues. The benchmark will be publicly available to foster future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. However, existing evaluation benchmarks remain limited to single-turn question answering, overlooking the complexity of multi-turn dialogues in real-world scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video understanding benchmark for evaluating MLLMs in multi-turn dialogues. Specifically, our MT-Video-Bench mainly assesses 6 core competencies that focus on perceptivity and interactivity, encompassing 1,000 meticulously curated multi-turn dialogues from diverse domains. These capabilities are rigorously aligned with real-world applications, such as interactive sports analysis and multi-turn video-based intelligent tutoring. With MT-Video-Bench, we extensively evaluate various state-of-the-art open-source and closed-source MLLMs, revealing their significant performance discrepancies and limitations in handling multi-turn video dialogues. The benchmark will be publicly available to foster future research."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-20T16:38:40Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    38,
                    40,
                    0,
                    293,
                    0
                ],
                "arxiv_comment": "Project Website: https://github.com/NJU-LINK/MT-Video-Bench",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yaning Pan"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Guohui Zhang"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Yongqian Wen"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Haoxuan Hu"
                    },
                    {
                        "name": "Zhiyu Pan"
                    },
                    {
                        "name": "Yibing Huang"
                    },
                    {
                        "name": "Zhidong Gan"
                    },
                    {
                        "name": "Yonghong Lin"
                    },
                    {
                        "name": "An Ping"
                    },
                    {
                        "name": "Shihao Li"
                    },
                    {
                        "name": "Yanghai Wang"
                    },
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Jiaheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Liu"
                },
                "author": "Jiaheng Liu"
            },
            {
                "id": "http://arxiv.org/abs/2601.05063v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05063v1",
                "title": "Quantitative mapping from conventional MRI using self-supervised physics-guided deep learning: applications to a large-scale, clinically heterogeneous dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantitative mapping from conventional MRI using self-supervised physics-guided deep learning: applications to a large-scale, clinically heterogeneous dataset"
                },
                "updated": "2026-01-08T16:08:58Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    8,
                    58,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05063v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Magnetic resonance imaging (MRI) is a cornerstone of clinical neuroimaging, yet conventional MRIs provide qualitative information heavily dependent on scanner hardware and acquisition settings. While quantitative MRI (qMRI) offers intrinsic tissue parameters, the requirement for specialized acquisition protocols and reconstruction algorithms restricts its availability and impedes large-scale biomarker research. This study presents a self-supervised physics-guided deep learning framework to infer quantitative T1, T2, and proton-density (PD) maps directly from widely available clinical conventional T1-weighted, T2-weighted, and FLAIR MRIs. The framework was trained and evaluated on a large-scale, clinically heterogeneous dataset comprising 4,121 scan sessions acquired at our institution over six years on four different 3 T MRI scanner systems, capturing real-world clinical variability. The framework integrates Bloch-based signal models directly into the training objective. Across more than 600 test sessions, the generated maps exhibited white matter and gray matter values consistent with literature ranges. Additionally, the generated maps showed invariance to scanner hardware and acquisition protocol groups, with inter-group coefficients of variation $\\leq$ 1.1%. Subject-specific analyses demonstrated excellent voxel-wise reproducibility across scanner systems and sequence parameters, with Pearson $r$ and concordance correlation coefficients exceeding 0.82 for T1 and T2. Mean relative voxel-wise differences were low across all quantitative parameters, especially for T2 ($<$ 6%). These results indicate that the proposed framework can robustly transform diverse clinical conventional MRI data into quantitative maps, potentially paving the way for large-scale quantitative biomarker research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic resonance imaging (MRI) is a cornerstone of clinical neuroimaging, yet conventional MRIs provide qualitative information heavily dependent on scanner hardware and acquisition settings. While quantitative MRI (qMRI) offers intrinsic tissue parameters, the requirement for specialized acquisition protocols and reconstruction algorithms restricts its availability and impedes large-scale biomarker research. This study presents a self-supervised physics-guided deep learning framework to infer quantitative T1, T2, and proton-density (PD) maps directly from widely available clinical conventional T1-weighted, T2-weighted, and FLAIR MRIs. The framework was trained and evaluated on a large-scale, clinically heterogeneous dataset comprising 4,121 scan sessions acquired at our institution over six years on four different 3 T MRI scanner systems, capturing real-world clinical variability. The framework integrates Bloch-based signal models directly into the training objective. Across more than 600 test sessions, the generated maps exhibited white matter and gray matter values consistent with literature ranges. Additionally, the generated maps showed invariance to scanner hardware and acquisition protocol groups, with inter-group coefficients of variation $\\leq$ 1.1%. Subject-specific analyses demonstrated excellent voxel-wise reproducibility across scanner systems and sequence parameters, with Pearson $r$ and concordance correlation coefficients exceeding 0.82 for T1 and T2. Mean relative voxel-wise differences were low across all quantitative parameters, especially for T2 ($<$ 6%). These results indicate that the proposed framework can robustly transform diverse clinical conventional MRI data into quantitative maps, potentially paving the way for large-scale quantitative biomarker research."
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:08:58Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    8,
                    58,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "30 pages, 13 figures, full paper",
                "arxiv_primary_category": {
                    "term": "physics.med-ph"
                },
                "authors": [
                    {
                        "name": "Jelmer van Lune"
                    },
                    {
                        "name": "Stefano Mandija"
                    },
                    {
                        "name": "Oscar van der Heide"
                    },
                    {
                        "name": "Matteo Maspero"
                    },
                    {
                        "name": "Martin B. Schilder"
                    },
                    {
                        "name": "Jan Willem Dankbaar"
                    },
                    {
                        "name": "Cornelis A. T. van den Berg"
                    },
                    {
                        "name": "Alessandro Sbrizzi"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Sbrizzi"
                },
                "author": "Alessandro Sbrizzi"
            },
            {
                "id": "http://arxiv.org/abs/2601.05062v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05062v1",
                "title": "Compositional Steering of Large Language Models with Steering Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Steering of Large Language Models with Steering Tokens"
                },
                "updated": "2026-01-08T16:08:44Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    8,
                    44,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05062v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \\textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \\emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \\textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \\textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \\textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \\textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \\emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \\textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \\textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \\textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:08:44Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    8,
                    44,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Gorjan Radevski"
                    },
                    {
                        "name": "Kiril Gashteovski"
                    },
                    {
                        "name": "Giwon Hong"
                    },
                    {
                        "name": "Carolin Lawrence"
                    },
                    {
                        "name": "Goran Glava"
                    }
                ],
                "author_detail": {
                    "name": "Goran Glava"
                },
                "author": "Goran Glava"
            },
            {
                "id": "http://arxiv.org/abs/2510.21007v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.21007v3",
                "title": "Can Confidence Estimates Decide When Chain-of-Thought Is Necessary for LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Confidence Estimates Decide When Chain-of-Thought Is Necessary for LLMs?"
                },
                "updated": "2026-01-08T16:05:16Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    5,
                    16,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.21007v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.21007v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Chain-of-thought (CoT) prompting is a common technique for improving the reasoning abilities of large language models (LLMs). However, extended reasoning is often unnecessary and substantially increases token usage. As such, a key question becomes how to optimally allocate compute to when reasoning is actually needed. We study this through confidence-gated CoT, where a model produces a direct answer and a confidence estimate to decide whether to invoke CoT. We present an evaluation framework together with the first systematic study of confidence signals for this decision. We evaluate four representative confidence measures and compare them with random gating and an oracle upper bound. Experiments across two model families and diverse reasoning tasks show that existing training-free confidence measures can reduce redundant reasoning. However, we also find that the utility of individual confidence measures is inconsistent across settings. Through our evaluation framework and analysis, our study provides practical guidance toward developing and evaluating models that selectively use CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) prompting is a common technique for improving the reasoning abilities of large language models (LLMs). However, extended reasoning is often unnecessary and substantially increases token usage. As such, a key question becomes how to optimally allocate compute to when reasoning is actually needed. We study this through confidence-gated CoT, where a model produces a direct answer and a confidence estimate to decide whether to invoke CoT. We present an evaluation framework together with the first systematic study of confidence signals for this decision. We evaluate four representative confidence measures and compare them with random gating and an oracle upper bound. Experiments across two model families and diverse reasoning tasks show that existing training-free confidence measures can reduce redundant reasoning. However, we also find that the utility of individual confidence measures is inconsistent across settings. Through our evaluation framework and analysis, our study provides practical guidance toward developing and evaluating models that selectively use CoT."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-23T21:33:28Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    21,
                    33,
                    28,
                    3,
                    296,
                    0
                ],
                "arxiv_comment": "Under Review",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Samuel Lewis-Lim"
                    },
                    {
                        "name": "Xingwei Tan"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Nikolaos Aletras"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Aletras"
                },
                "author": "Nikolaos Aletras"
            },
            {
                "id": "http://arxiv.org/abs/2601.05053v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05053v1",
                "title": "Reinforced Efficient Reasoning via Semantically Diverse Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Efficient Reasoning via Semantically Diverse Exploration"
                },
                "updated": "2026-01-08T15:56:44Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    56,
                    44,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05053v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:56:44Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    56,
                    44,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Ziqi Zhao"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Jiahong Zou"
                    },
                    {
                        "name": "Liu Yang"
                    },
                    {
                        "name": "Zhiwei Xu"
                    },
                    {
                        "name": "Xuri Ge"
                    },
                    {
                        "name": "Zhumin Chen"
                    },
                    {
                        "name": "Xinyu Ma"
                    },
                    {
                        "name": "Daiting Shi"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xin Xin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xin"
                },
                "author": "Xin Xin"
            },
            {
                "id": "http://arxiv.org/abs/2601.05051v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05051v1",
                "title": "Publishing FAIR and Machine-actionable Reviews in Materials Science: The Case for Symbolic Knowledge in Neuro-symbolic Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Publishing FAIR and Machine-actionable Reviews in Materials Science: The Case for Symbolic Knowledge in Neuro-symbolic Artificial Intelligence"
                },
                "updated": "2026-01-08T15:56:17Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    56,
                    17,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05051v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Scientific reviews are central to knowledge integration in materials science, yet their key insights remain locked in narrative text and static PDF tables, limiting reuse by humans and machines alike. This article presents a case study in atomic layer deposition and etching (ALD/E) where we publish review tables as FAIR, machine-actionable comparisons in the Open Research Knowledge Graph (ORKG), turning them into structured, queryable knowledge. Building on this, we contrast symbolic querying over ORKG with large language model-based querying, and argue that a curated symbolic layer should remain the backbone of reliable neurosymbolic AI in materials science, with LLMs serving as complementary, symbolically grounded interfaces rather than standalone sources of truth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific reviews are central to knowledge integration in materials science, yet their key insights remain locked in narrative text and static PDF tables, limiting reuse by humans and machines alike. This article presents a case study in atomic layer deposition and etching (ALD/E) where we publish review tables as FAIR, machine-actionable comparisons in the Open Research Knowledge Graph (ORKG), turning them into structured, queryable knowledge. Building on this, we contrast symbolic querying over ORKG with large language model-based querying, and argue that a curated symbolic layer should remain the backbone of reliable neurosymbolic AI in materials science, with LLMs serving as complementary, symbolically grounded interfaces rather than standalone sources of truth."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:56:17Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    56,
                    17,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "35 pages, 11 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Soren Auer"
                    },
                    {
                        "name": "Eleni Poupaki"
                    },
                    {
                        "name": "Alex Watkins"
                    },
                    {
                        "name": "Anjana Devi"
                    },
                    {
                        "name": "Riikka L. Puurunen"
                    },
                    {
                        "name": "Bora Karasulu"
                    },
                    {
                        "name": "Adrie Mackus"
                    },
                    {
                        "name": "Erwin Kessels"
                    }
                ],
                "author_detail": {
                    "name": "Erwin Kessels"
                },
                "author": "Erwin Kessels"
            },
            {
                "id": "http://arxiv.org/abs/2601.05050v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05050v1",
                "title": "Large language models can effectively convince people to believe conspiracies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can effectively convince people to believe conspiracies"
                },
                "updated": "2026-01-08T15:56:05Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    56,
                    5,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05050v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have been shown to be persuasive across a variety of context. But it remains unclear whether this persuasive power advantages truth over falsehood, or if LLMs can promote misbeliefs just as easily as refuting them. Here, we investigate this question across three pre-registered experiments in which participants (N = 2,724 Americans) discussed a conspiracy theory they were uncertain about with GPT-4o, and the model was instructed to either argue against (\"debunking\") or for (\"bunking\") that conspiracy. When using a \"jailbroken\" GPT-4o variant with guardrails removed, the AI was as effective at increasing conspiracy belief as decreasing it. Concerningly, the bunking AI was rated more positively, and increased trust in AI, more than the debunking AI. Surprisingly, we found that using standard GPT-4o produced very similar effects, such that the guardrails imposed by OpenAI did little to revent the LLM from promoting conspiracy beliefs. Encouragingly, however, a corrective conversation reversed these newly induced conspiracy beliefs, and simply prompting GPT-4o to only use accurate information dramatically reduced its ability to increase conspiracy beliefs. Our findings demonstrate that LLMs possess potent abilities to promote both truth and falsehood, but that potential solutions may exist to help mitigate this risk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been shown to be persuasive across a variety of context. But it remains unclear whether this persuasive power advantages truth over falsehood, or if LLMs can promote misbeliefs just as easily as refuting them. Here, we investigate this question across three pre-registered experiments in which participants (N = 2,724 Americans) discussed a conspiracy theory they were uncertain about with GPT-4o, and the model was instructed to either argue against (\"debunking\") or for (\"bunking\") that conspiracy. When using a \"jailbroken\" GPT-4o variant with guardrails removed, the AI was as effective at increasing conspiracy belief as decreasing it. Concerningly, the bunking AI was rated more positively, and increased trust in AI, more than the debunking AI. Surprisingly, we found that using standard GPT-4o produced very similar effects, such that the guardrails imposed by OpenAI did little to revent the LLM from promoting conspiracy beliefs. Encouragingly, however, a corrective conversation reversed these newly induced conspiracy beliefs, and simply prompting GPT-4o to only use accurate information dramatically reduced its ability to increase conspiracy beliefs. Our findings demonstrate that LLMs possess potent abilities to promote both truth and falsehood, but that potential solutions may exist to help mitigate this risk."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:56:05Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    56,
                    5,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Thomas H. Costello"
                    },
                    {
                        "name": "Kellin Pelrine"
                    },
                    {
                        "name": "Matthew Kowal"
                    },
                    {
                        "name": "Antonio A. Arechar"
                    },
                    {
                        "name": "Jean-Franois Godbout"
                    },
                    {
                        "name": "Adam Gleave"
                    },
                    {
                        "name": "David Rand"
                    },
                    {
                        "name": "Gordon Pennycook"
                    }
                ],
                "author_detail": {
                    "name": "Gordon Pennycook"
                },
                "author": "Gordon Pennycook"
            },
            {
                "id": "http://arxiv.org/abs/2601.05047v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05047v1",
                "title": "Challenges and Research Directions for Large Language Model Inference Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges and Research Directions for Large Language Model Inference Hardware"
                },
                "updated": "2026-01-08T15:52:11Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    52,
                    11,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05047v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/MC.2026.3652916",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Large Language Model (LLM) inference is hard. The autoregressive Decode phase of the underlying Transformer model makes LLM inference fundamentally different from training. Exacerbated by recent AI trends, the primary challenges are memory and interconnect rather than compute. To address these challenges, we highlight four architecture research opportunities: High Bandwidth Flash for 10X memory capacity with HBM-like bandwidth; Processing-Near-Memory and 3D memory-logic stacking for high memory bandwidth; and low-latency interconnect to speedup communication. While our focus is datacenter AI, we also review their applicability for mobile devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is hard. The autoregressive Decode phase of the underlying Transformer model makes LLM inference fundamentally different from training. Exacerbated by recent AI trends, the primary challenges are memory and interconnect rather than compute. To address these challenges, we highlight four architecture research opportunities: High Bandwidth Flash for 10X memory capacity with HBM-like bandwidth; Processing-Near-Memory and 3D memory-logic stacking for high memory bandwidth; and low-latency interconnect to speedup communication. While our focus is datacenter AI, we also review their applicability for mobile devices."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:52:11Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    52,
                    11,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Accepted for publication by IEEE Computer, 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "David Patterson"
                    }
                ],
                "author_detail": {
                    "name": "David Patterson"
                },
                "author": "David Patterson",
                "arxiv_doi": "10.1109/MC.2026.3652916"
            },
            {
                "id": "http://arxiv.org/abs/2412.17189v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.17189v4",
                "title": "Talking with Tables for Better LLM Factual Data Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Talking with Tables for Better LLM Factual Data Interactions"
                },
                "updated": "2026-01-08T15:45:17Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    45,
                    17,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.17189v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.17189v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) often struggle with requests related to information retrieval and data manipulation that frequently arise in real-world scenarios under multiple conditions. In this paper, we demonstrate that leveraging tabular structures in LLM interactions, is more effective than utilizing other structures for handling prevalent requests that operate over factual data. Through comprehensive evaluations across various scenarios and request types, we show that providing tabular structures yields a 40.29\\% average performance gain along with better robustness and token efficiency. Through attention-value analysis, we discover that tables help LLMs better locate relevant information, explaining these improvements. Beyond tables and text, we evaluate whether (1) blending structuredness within text, such as providing templates or fixing the order of attributes, and (2) other representative structures, such as knowledge graphs and JSON are helpful. We observe that utilizing tables offers the best balance between efficiency and effectiveness. The method remains robust to task complexity and adapts to unstructured sources through text-to-table conversion. Overall, we highlight the untapped potential of tabular representations for future LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle with requests related to information retrieval and data manipulation that frequently arise in real-world scenarios under multiple conditions. In this paper, we demonstrate that leveraging tabular structures in LLM interactions, is more effective than utilizing other structures for handling prevalent requests that operate over factual data. Through comprehensive evaluations across various scenarios and request types, we show that providing tabular structures yields a 40.29\\% average performance gain along with better robustness and token efficiency. Through attention-value analysis, we discover that tables help LLMs better locate relevant information, explaining these improvements. Beyond tables and text, we evaluate whether (1) blending structuredness within text, such as providing templates or fixing the order of attributes, and (2) other representative structures, such as knowledge graphs and JSON are helpful. We observe that utilizing tables offers the best balance between efficiency and effectiveness. The method remains robust to task complexity and adapts to unstructured sources through text-to-table conversion. Overall, we highlight the untapped potential of tabular representations for future LLM applications."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-22T23:31:03Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    23,
                    31,
                    3,
                    6,
                    357,
                    0
                ],
                "arxiv_comment": "20 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jio Oh"
                    },
                    {
                        "name": "Geon Heo"
                    },
                    {
                        "name": "Seungjun Oh"
                    },
                    {
                        "name": "Hyunjin Kim"
                    },
                    {
                        "name": "JinYeong Bak"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Steven Euijong Whang"
                    }
                ],
                "author_detail": {
                    "name": "Steven Euijong Whang"
                },
                "author": "Steven Euijong Whang"
            },
            {
                "id": "http://arxiv.org/abs/2601.05039v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05039v1",
                "title": "FinDeepForecast: A Live Multi-Agent System for Benchmarking Deep Research Agents in Financial Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinDeepForecast: A Live Multi-Agent System for Benchmarking Deep Research Agents in Financial Forecasting"
                },
                "updated": "2026-01-08T15:45:09Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    45,
                    9,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05039v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deep Research (DR) Agents powered by advanced Large Language Models (LLMs) have fundamentally shifted the paradigm for completing complex research tasks. Yet, a comprehensive and live evaluation of their forecasting performance on real-world, research-oriented tasks in high-stakes domains (e.g., finance) remains underexplored. We introduce FinDeepForecast, the first live, end-to-end multi-agent system for automatically evaluating DR agents by continuously generating research-oriented financial forecasting tasks. This system is equipped with a dual-track taxonomy, enabling the dynamic generation of recurrent and non-recurrent forecasting tasks at both corporate and macro levels. With this system, we generate FinDeepForecastBench, a weekly evaluation benchmark over a ten-week horizon, encompassing 8 global economies and 1,314 listed companies, and evaluate 13 representative methods. Extensive experiments show that, while DR agents consistently outperform strong baselines, their performance still falls short of genuine forward-looking financial reasoning. We expect the proposed FinDeepForecast system to consistently facilitate future advancements of DR agents in research-oriented financial forecasting tasks. The benchmark and leaderboard are publicly available on the OpenFinArena Platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Research (DR) Agents powered by advanced Large Language Models (LLMs) have fundamentally shifted the paradigm for completing complex research tasks. Yet, a comprehensive and live evaluation of their forecasting performance on real-world, research-oriented tasks in high-stakes domains (e.g., finance) remains underexplored. We introduce FinDeepForecast, the first live, end-to-end multi-agent system for automatically evaluating DR agents by continuously generating research-oriented financial forecasting tasks. This system is equipped with a dual-track taxonomy, enabling the dynamic generation of recurrent and non-recurrent forecasting tasks at both corporate and macro levels. With this system, we generate FinDeepForecastBench, a weekly evaluation benchmark over a ten-week horizon, encompassing 8 global economies and 1,314 listed companies, and evaluate 13 representative methods. Extensive experiments show that, while DR agents consistently outperform strong baselines, their performance still falls short of genuine forward-looking financial reasoning. We expect the proposed FinDeepForecast system to consistently facilitate future advancements of DR agents in research-oriented financial forecasting tasks. The benchmark and leaderboard are publicly available on the OpenFinArena Platform."
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:45:09Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    45,
                    9,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA"
                },
                "authors": [
                    {
                        "name": "Xiangyu Li"
                    },
                    {
                        "name": "Xuan Yao"
                    },
                    {
                        "name": "Guohao Qi"
                    },
                    {
                        "name": "Fengbin Zhu"
                    },
                    {
                        "name": "Kelvin J. L. Koa"
                    },
                    {
                        "name": "Xiang Yao Ng"
                    },
                    {
                        "name": "Ziyang Liu"
                    },
                    {
                        "name": "Xingyu Ni"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Yonghui Yang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Huanbo Luan"
                    },
                    {
                        "name": "Xiaofen Xing"
                    },
                    {
                        "name": "Xiangmin Xu"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Ke-Wei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ke-Wei Huang"
                },
                "author": "Ke-Wei Huang"
            },
            {
                "id": "http://arxiv.org/abs/2502.13691v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.13691v3",
                "title": "Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora"
                },
                "updated": "2026-01-08T15:44:53Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    44,
                    53,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.13691v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.13691v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As large language models (LLMs) converge towards similar capabilities, the key to advancing their performance lies in identifying and incorporating valuable new information sources. However, evaluating which text collections are worth the substantial investment required for digitization, preprocessing, and integration into LLM systems remains a significant challenge. We present a novel approach to this challenge: an automated pipeline that evaluates the potential information gain from text collections without requiring model training or fine-tuning. Our method generates multiple choice questions (MCQs) from texts and measures an LLM's performance both with and without access to the source material. The performance gap between these conditions serves as a proxy for the collection's information potential. We validate our approach using five strategically selected datasets: EPFL PhD manuscripts, a private collection of Venetian historical records, two sets of Wikipedia articles on related topics, and a synthetic baseline dataset. Our results demonstrate that this method effectively identifies collections containing valuable novel information, providing a practical tool for prioritizing data acquisition and integration efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) converge towards similar capabilities, the key to advancing their performance lies in identifying and incorporating valuable new information sources. However, evaluating which text collections are worth the substantial investment required for digitization, preprocessing, and integration into LLM systems remains a significant challenge. We present a novel approach to this challenge: an automated pipeline that evaluates the potential information gain from text collections without requiring model training or fine-tuning. Our method generates multiple choice questions (MCQs) from texts and measures an LLM's performance both with and without access to the source material. The performance gap between these conditions serves as a proxy for the collection's information potential. We validate our approach using five strategically selected datasets: EPFL PhD manuscripts, a private collection of Venetian historical records, two sets of Wikipedia articles on related topics, and a synthetic baseline dataset. Our results demonstrate that this method effectively identifies collections containing valuable novel information, providing a practical tool for prioritizing data acquisition and integration efforts."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-19T13:03:06Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    3,
                    6,
                    2,
                    50,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tristan Karch"
                    },
                    {
                        "name": "Luca Engel"
                    },
                    {
                        "name": "Philippe Schwaller"
                    },
                    {
                        "name": "Frdric Kaplan"
                    }
                ],
                "author_detail": {
                    "name": "Frdric Kaplan"
                },
                "author": "Frdric Kaplan"
            },
            {
                "id": "http://arxiv.org/abs/2601.05038v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05038v1",
                "title": "ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG"
                },
                "updated": "2026-01-08T15:44:52Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    44,
                    52,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05038v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried ''compressing'' these documents into smaller summaries or mathematical embeddings, there is a catch: the more you compress the data, the more the LLM struggles to understand it. To address this challenge, we propose ArcAligner (Adaptive recursive context *Aligner*), a lightweight module integrated into the language model layers to help the model better utilize highly compressed context representations for downstream generation. It uses an adaptive ''gating'' system that only adds extra processing power when the information is complex, keeping the system fast. Across knowledge-intensive QA benchmarks, ArcAligner consistently beats compression baselines at comparable compression rates, especially on multi-hop and long-tail settings. The source code is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried ''compressing'' these documents into smaller summaries or mathematical embeddings, there is a catch: the more you compress the data, the more the LLM struggles to understand it. To address this challenge, we propose ArcAligner (Adaptive recursive context *Aligner*), a lightweight module integrated into the language model layers to help the model better utilize highly compressed context representations for downstream generation. It uses an adaptive ''gating'' system that only adds extra processing power when the information is complex, keeping the system fast. Across knowledge-intensive QA benchmarks, ArcAligner consistently beats compression baselines at comparable compression rates, especially on multi-hop and long-tail settings. The source code is publicly available."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:44:52Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    44,
                    52,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Code is available at https://github.com/liunian-Jay/ArcAligner.git",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Sendong Zhao"
                    },
                    {
                        "name": "Bairui Hu"
                    },
                    {
                        "name": "Haochun Wang"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin"
            },
            {
                "id": "http://arxiv.org/abs/2601.05035v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05035v1",
                "title": "Patch-based Representation and Learning for Efficient Deformation Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patch-based Representation and Learning for Efficient Deformation Modeling"
                },
                "updated": "2026-01-08T15:43:57Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    43,
                    57,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05035v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this paper, we present a patch-based representation of surfaces, PolyFit, which is obtained by fitting jet functions locally on surface patches. Such a representation can be learned efficiently in a supervised fashion from both analytic functions and real data. Once learned, it can be generalized to various types of surfaces. Using PolyFit, the surfaces can be efficiently deformed by updating a compact set of jet coefficients rather than optimizing per-vertex degrees of freedom for many downstream tasks in computer vision and graphics. We demonstrate the capabilities of our proposed methodologies with two applications: 1) Shape-from-template (SfT): where the goal is to deform the input 3D template of an object as seen in image/video. Using PolyFit, we adopt test-time optimization that delivers competitive accuracy while being markedly faster than offline physics-based solvers, and outperforms recent physics-guided neural simulators in accuracy at modest additional runtime. 2) Garment draping. We train a self-supervised, mesh- and garment-agnostic model that generalizes across resolutions and garment types, delivering up to an order-of-magnitude faster inference than strong baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a patch-based representation of surfaces, PolyFit, which is obtained by fitting jet functions locally on surface patches. Such a representation can be learned efficiently in a supervised fashion from both analytic functions and real data. Once learned, it can be generalized to various types of surfaces. Using PolyFit, the surfaces can be efficiently deformed by updating a compact set of jet coefficients rather than optimizing per-vertex degrees of freedom for many downstream tasks in computer vision and graphics. We demonstrate the capabilities of our proposed methodologies with two applications: 1) Shape-from-template (SfT): where the goal is to deform the input 3D template of an object as seen in image/video. Using PolyFit, we adopt test-time optimization that delivers competitive accuracy while being markedly faster than offline physics-based solvers, and outperforms recent physics-guided neural simulators in accuracy at modest additional runtime. 2) Garment draping. We train a self-supervised, mesh- and garment-agnostic model that generalizes across resolutions and garment types, delivering up to an order-of-magnitude faster inference than strong baselines."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:43:57Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    43,
                    57,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ruochen Chen"
                    },
                    {
                        "name": "Thuy Tran"
                    },
                    {
                        "name": "Shaifali Parashar"
                    }
                ],
                "author_detail": {
                    "name": "Shaifali Parashar"
                },
                "author": "Shaifali Parashar"
            },
            {
                "id": "http://arxiv.org/abs/2601.05027v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05027v1",
                "title": "OptiSet: Unified Optimizing Set Selection and Ranking for Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OptiSet: Unified Optimizing Set Selection and Ranking for Retrieval-Augmented Generation"
                },
                "updated": "2026-01-08T15:35:01Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    35,
                    1,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05027v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-Augmented Generation (RAG) improves generation quality by incorporating evidence retrieved from large external corpora. However, most existing methods rely on statically selecting top-k passages based on individual relevance, which fails to exploit combinatorial gains among passages and often introduces substantial redundancy. To address this limitation, we propose OptiSet, a set-centric framework that unifies set selection and set-level ranking for RAG. OptiSet adopts an \"Expand-then-Refine\" paradigm: it first expands a query into multiple perspectives to enable a diverse candidate pool and then refines the candidate pool via re-selection to form a compact evidence set. We then devise a self-synthesis strategy without strong LLM supervision to derive preference labels from the set conditional utility changes of the generator, thereby identifying complementary and redundant evidence. Finally, we introduce a set-list wise training strategy that jointly optimizes set selection and set-level ranking, enabling the model to favor compact, high-gain evidence sets. Extensive experiments demonstrate that OptiSet improves performance on complex combinatorial problems and makes generation more efficient. The source code is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) improves generation quality by incorporating evidence retrieved from large external corpora. However, most existing methods rely on statically selecting top-k passages based on individual relevance, which fails to exploit combinatorial gains among passages and often introduces substantial redundancy. To address this limitation, we propose OptiSet, a set-centric framework that unifies set selection and set-level ranking for RAG. OptiSet adopts an \"Expand-then-Refine\" paradigm: it first expands a query into multiple perspectives to enable a diverse candidate pool and then refines the candidate pool via re-selection to form a compact evidence set. We then devise a self-synthesis strategy without strong LLM supervision to derive preference labels from the set conditional utility changes of the generator, thereby identifying complementary and redundant evidence. Finally, we introduce a set-list wise training strategy that jointly optimizes set selection and set-level ranking, enabling the model to favor compact, high-gain evidence sets. Extensive experiments demonstrate that OptiSet improves performance on complex combinatorial problems and makes generation more efficient. The source code is publicly available."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:35:01Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    35,
                    1,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Code is available at https://github.com/liunian-Jay/OptiSet.git",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Sendong Zhao"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Bairui Hu"
                    },
                    {
                        "name": "Yanrui Du"
                    },
                    {
                        "name": "Haochun Wang"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin"
            },
            {
                "id": "http://arxiv.org/abs/2601.05022v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05022v1",
                "title": "Knowledge-to-Data: LLM-Driven Synthesis of Structured Network Traffic for Testbed-Free IDS Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-to-Data: LLM-Driven Synthesis of Structured Network Traffic for Testbed-Free IDS Evaluation"
                },
                "updated": "2026-01-08T15:31:33Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    31,
                    33,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05022v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Realistic, large-scale, and well-labeled cybersecurity datasets are essential for training and evaluating Intrusion Detection Systems (IDS). However, they remain difficult to obtain due to privacy constraints, data sensitivity, and the cost of building controlled collection environments such as testbeds and cyber ranges. This paper investigates whether Large Language Models (LLMs) can operate as controlled knowledge-to-data engines for generating structured synthetic network traffic datasets suitable for IDS research. We propose a methodology that combines protocol documentation, attack semantics, and explicit statistical rules to condition LLMs without fine-tuning or access to raw samples. Using the AWID3 IEEE~802.11 benchmark as a demanding case study, we generate labeled datasets with four state-of-the-art LLMs and assess fidelity through a multi-level validation framework including global similarity metrics, per-feature distribution testing, structural comparison, and cross-domain classification. Results show that, under explicit constraints, LLM-generated datasets can closely approximate the statistical and structural characteristics of real network traffic, enabling gradient-boosting classifiers to achieve F1-scores up to 0.956 when evaluated on real samples. Overall, the findings suggest that constrained LLM-driven generation can facilitate on-demand IDS experimentation, providing a testbed-free, privacy-preserving alternative that overcomes the traditional bottlenecks of physical traffic collection and manual labeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realistic, large-scale, and well-labeled cybersecurity datasets are essential for training and evaluating Intrusion Detection Systems (IDS). However, they remain difficult to obtain due to privacy constraints, data sensitivity, and the cost of building controlled collection environments such as testbeds and cyber ranges. This paper investigates whether Large Language Models (LLMs) can operate as controlled knowledge-to-data engines for generating structured synthetic network traffic datasets suitable for IDS research. We propose a methodology that combines protocol documentation, attack semantics, and explicit statistical rules to condition LLMs without fine-tuning or access to raw samples. Using the AWID3 IEEE~802.11 benchmark as a demanding case study, we generate labeled datasets with four state-of-the-art LLMs and assess fidelity through a multi-level validation framework including global similarity metrics, per-feature distribution testing, structural comparison, and cross-domain classification. Results show that, under explicit constraints, LLM-generated datasets can closely approximate the statistical and structural characteristics of real network traffic, enabling gradient-boosting classifiers to achieve F1-scores up to 0.956 when evaluated on real samples. Overall, the findings suggest that constrained LLM-driven generation can facilitate on-demand IDS experimentation, providing a testbed-free, privacy-preserving alternative that overcomes the traditional bottlenecks of physical traffic collection and manual labeling."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:31:33Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    31,
                    33,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Konstantinos E. Kampourakis"
                    },
                    {
                        "name": "Vyron Kampourakis"
                    },
                    {
                        "name": "Efstratios Chatzoglou"
                    },
                    {
                        "name": "Georgios Kambourakis"
                    },
                    {
                        "name": "Stefanos Gritzalis"
                    }
                ],
                "author_detail": {
                    "name": "Stefanos Gritzalis"
                },
                "author": "Stefanos Gritzalis"
            },
            {
                "id": "http://arxiv.org/abs/2601.05020v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05020v1",
                "title": "Scalable neural pushbroom architectures for real-time denoising of hyperspectral images onboard satellites",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable neural pushbroom architectures for real-time denoising of hyperspectral images onboard satellites"
                },
                "updated": "2026-01-08T15:28:39Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    28,
                    39,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05020v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The next generation of Earth observation satellites will seek to deploy intelligent models directly onboard the payload in order to minimize the latency incurred by the transmission and processing chain of the ground segment, for time-critical applications. Designing neural architectures for onboard execution, particularly for satellite-based hyperspectral imagers, poses novel challenges due to the unique constraints of this environment and imaging system that are largely unexplored by the traditional computer vision literature. In this paper, we show that this setting requires addressing three competing objectives, namely high-quality inference with low complexity, dynamic power scalability and fault tolerance. We focus on the problem of hyperspectral image denoising, which is a critical task to enable effective downstream inference, and highlights the constraints of the onboard processing scenario. We propose a neural network design that addresses the three aforementioned objectives with several novel contributions. In particular, we propose a mixture of denoisers that can be resilient to radiation-induced faults as well as allowing for time-varying power scaling. Moreover, each denoiser employs an innovative architecture where an image is processed line-by-line in a causal way, with a memory of past lines, in order to match the acquisition process of pushbroom hyperspectral sensors and greatly limit memory requirements. We show that the proposed architecture can run in real-time, i.e., process one line in the time it takes to acquire the next one, on low-power hardware and provide competitive denoising quality with respect to significantly more complex state-of-the-art models. We also show that the power scalability and fault tolerance objectives provide a design space with multiple tradeoffs between those properties and denoising quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The next generation of Earth observation satellites will seek to deploy intelligent models directly onboard the payload in order to minimize the latency incurred by the transmission and processing chain of the ground segment, for time-critical applications. Designing neural architectures for onboard execution, particularly for satellite-based hyperspectral imagers, poses novel challenges due to the unique constraints of this environment and imaging system that are largely unexplored by the traditional computer vision literature. In this paper, we show that this setting requires addressing three competing objectives, namely high-quality inference with low complexity, dynamic power scalability and fault tolerance. We focus on the problem of hyperspectral image denoising, which is a critical task to enable effective downstream inference, and highlights the constraints of the onboard processing scenario. We propose a neural network design that addresses the three aforementioned objectives with several novel contributions. In particular, we propose a mixture of denoisers that can be resilient to radiation-induced faults as well as allowing for time-varying power scaling. Moreover, each denoiser employs an innovative architecture where an image is processed line-by-line in a causal way, with a memory of past lines, in order to match the acquisition process of pushbroom hyperspectral sensors and greatly limit memory requirements. We show that the proposed architecture can run in real-time, i.e., process one line in the time it takes to acquire the next one, on low-power hardware and provide competitive denoising quality with respect to significantly more complex state-of-the-art models. We also show that the power scalability and fault tolerance objectives provide a design space with multiple tradeoffs between those properties and denoising quality."
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:28:39Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    28,
                    39,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV"
                },
                "authors": [
                    {
                        "name": "Ziyao Yi"
                    },
                    {
                        "name": "Davide Piccinini"
                    },
                    {
                        "name": "Diego Valsesia"
                    },
                    {
                        "name": "Tiziano Bianchi"
                    },
                    {
                        "name": "Enrico Magli"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Magli"
                },
                "author": "Enrico Magli"
            },
            {
                "id": "http://arxiv.org/abs/2601.05017v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05017v1",
                "title": "HMVI: Unifying Heterogeneous Attributes with Natural Neighbors for Missing Value Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HMVI: Unifying Heterogeneous Attributes with Natural Neighbors for Missing Value Inference"
                },
                "updated": "2026-01-08T15:18:36Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    18,
                    36,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05017v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Missing value imputation is a fundamental challenge in machine intelligence, heavily dependent on data completeness. Current imputation methods often handle numerical and categorical attributes independently, overlooking critical interdependencies among heterogeneous features. To address these limitations, we propose a novel imputation approach that explicitly models cross-type feature dependencies within a unified framework. Our method leverages both complete and incomplete instances to ensure accurate and consistent imputation in tabular data. Extensive experimental results demonstrate that the proposed approach achieves superior performance over existing techniques and significantly enhances downstream machine learning tasks, providing a robust solution for real-world systems with missing data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missing value imputation is a fundamental challenge in machine intelligence, heavily dependent on data completeness. Current imputation methods often handle numerical and categorical attributes independently, overlooking critical interdependencies among heterogeneous features. To address these limitations, we propose a novel imputation approach that explicitly models cross-type feature dependencies within a unified framework. Our method leverages both complete and incomplete instances to ensure accurate and consistent imputation in tabular data. Extensive experimental results demonstrate that the proposed approach achieves superior performance over existing techniques and significantly enhances downstream machine learning tasks, providing a robust solution for real-world systems with missing data."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:18:36Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    18,
                    36,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Submitted to ICASSP 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xiaopeng Luo"
                    },
                    {
                        "name": "Zexi Tan"
                    },
                    {
                        "name": "Zhuowei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhuowei Wang"
                },
                "author": "Zhuowei Wang"
            },
            {
                "id": "http://arxiv.org/abs/2505.01895v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.01895v2",
                "title": "Score-Based Modeling of Effective Langevin Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Score-Based Modeling of Effective Langevin Dynamics"
                },
                "updated": "2026-01-08T15:11:12Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    11,
                    12,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.01895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.01895v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce a constructive framework to learn effective Langevin equations from stationary time series that reproduce, by construction, both the observed steady-state density and temporal correlations of resolved variables. The drift is parameterized in terms of the score function--the gradient of the logarithm of the steady-state distribution--and a constant mobility matrix whose symmetric part controls dissipation and diffusion and whose antisymmetric part encodes nonequilibrium circulation. The score is learned from samples using denoising score matching, while the constant coefficients are inferred from short-lag correlation identities estimated via a clustering-based finite-volume discretization on a data-adaptive state-space partition. We validate the approach on low-dimensional stochastic benchmarks and on partially observed Kuramoto--Sivashinsky dynamics, where the resulting Markovian surrogate captures the marginal invariant measure and temporal correlations of the resolved modes. The resulting Langevin models define explicit reduced generators that enable efficient sampling and forecasting of resolved statistics without direct simulation of the underlying full dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a constructive framework to learn effective Langevin equations from stationary time series that reproduce, by construction, both the observed steady-state density and temporal correlations of resolved variables. The drift is parameterized in terms of the score function--the gradient of the logarithm of the steady-state distribution--and a constant mobility matrix whose symmetric part controls dissipation and diffusion and whose antisymmetric part encodes nonequilibrium circulation. The score is learned from samples using denoising score matching, while the constant coefficients are inferred from short-lag correlation identities estimated via a clustering-based finite-volume discretization on a data-adaptive state-space partition. We validate the approach on low-dimensional stochastic benchmarks and on partially observed Kuramoto--Sivashinsky dynamics, where the resulting Markovian surrogate captures the marginal invariant measure and temporal correlations of the resolved modes. The resulting Langevin models define explicit reduced generators that enable efficient sampling and forecasting of resolved statistics without direct simulation of the underlying full dynamics."
                },
                "tags": [
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-03T19:09:33Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    19,
                    9,
                    33,
                    5,
                    123,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "nlin.CD"
                },
                "authors": [
                    {
                        "name": "Ludovico Theo Giorgini"
                    }
                ],
                "author_detail": {
                    "name": "Ludovico Theo Giorgini"
                },
                "author": "Ludovico Theo Giorgini"
            },
            {
                "id": "http://arxiv.org/abs/2601.05009v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05009v1",
                "title": "An Empirical Investigation of Robustness in Large Language Models under Tabular Distortions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Investigation of Robustness in Large Language Models under Tabular Distortions"
                },
                "updated": "2026-01-08T15:10:32Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    10,
                    32,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05009v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We investigate how large language models (LLMs) fail when tabular data in an otherwise canonical representation is subjected to semantic and structural distortions. Our findings reveal that LLMs lack an inherent ability to detect and correct subtle distortions in table representations. Only when provided with an explicit prior, via a system prompt, do models partially adjust their reasoning strategies and correct some distortions, though not consistently or completely. To study this phenomenon, we introduce a small, expert-curated dataset that explicitly evaluates LLMs on table question answering (TQA) tasks requiring an additional error-correction step prior to analysis. Our results reveal systematic differences in how LLMs ingest and interpret tabular information under distortion, with even SoTA models such as GPT-5.2 model exhibiting a drop of minimum 22% accuracy under distortion. These findings raise important questions for future research, particularly regarding when and how models should autonomously decide to realign tabular inputs, analogous to human behavior, without relying on explicit prompts or tabular data pre-processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate how large language models (LLMs) fail when tabular data in an otherwise canonical representation is subjected to semantic and structural distortions. Our findings reveal that LLMs lack an inherent ability to detect and correct subtle distortions in table representations. Only when provided with an explicit prior, via a system prompt, do models partially adjust their reasoning strategies and correct some distortions, though not consistently or completely. To study this phenomenon, we introduce a small, expert-curated dataset that explicitly evaluates LLMs on table question answering (TQA) tasks requiring an additional error-correction step prior to analysis. Our results reveal systematic differences in how LLMs ingest and interpret tabular information under distortion, with even SoTA models such as GPT-5.2 model exhibiting a drop of minimum 22% accuracy under distortion. These findings raise important questions for future research, particularly regarding when and how models should autonomously decide to realign tabular inputs, analogous to human behavior, without relying on explicit prompts or tabular data pre-processing."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:10:32Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    10,
                    32,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "4 pages, 1 figure, 1 table",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Avik Dutta"
                    },
                    {
                        "name": "Harshit Nigam"
                    },
                    {
                        "name": "Hosein Hasanbeig"
                    },
                    {
                        "name": "Arjun Radhakrishna"
                    },
                    {
                        "name": "Sumit Gulwani"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Gulwani"
                },
                "author": "Sumit Gulwani"
            },
            {
                "id": "http://arxiv.org/abs/2511.13368v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13368v2",
                "title": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning"
                },
                "updated": "2026-01-08T15:10:27Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    10,
                    27,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13368v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages remains poorly understood. We conduct a controlled LoRA fine-tuning study across multiple open-weight LLM families and scales, using a standardised grid of 11 languages and four benchmarks. We fine-tune each model on a single task-language source and measure transfer when evaluated on all other task-language target pairs. We decompose transfer into three regimes: (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language). Single-source fine-tuning yields a net positive uplift across regimes, but the gains are strongly asymmetric. Matched-Task (Cross-Language) transfer emerges as the most effective and predictable regime, driven principally by the identity of the target language rather than model architecture. We identify a stable hierarchy where high-resource languages and broad semantic tasks act as efficient recipients that absorb gains from diverse sources, while specialised tasks and lower-resource languages are more isolated. These results imply that effective fine-tuning requires navigating donor-recipient roles to maximise downstream gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages remains poorly understood. We conduct a controlled LoRA fine-tuning study across multiple open-weight LLM families and scales, using a standardised grid of 11 languages and four benchmarks. We fine-tune each model on a single task-language source and measure transfer when evaluated on all other task-language target pairs. We decompose transfer into three regimes: (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language). Single-source fine-tuning yields a net positive uplift across regimes, but the gains are strongly asymmetric. Matched-Task (Cross-Language) transfer emerges as the most effective and predictable regime, driven principally by the identity of the target language rather than model architecture. We identify a stable hierarchy where high-resource languages and broad semantic tasks act as efficient recipients that absorb gains from diverse sources, while specialised tasks and lower-resource languages are more isolated. These results imply that effective fine-tuning requires navigating donor-recipient roles to maximise downstream gains."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:41:31Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    41,
                    31,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kajetan Dymkiewicz"
                    },
                    {
                        "name": "Ivan Vulic"
                    },
                    {
                        "name": "Helen Yannakoudakis"
                    },
                    {
                        "name": "Eilam Shapira"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Anna Korhonen"
                    }
                ],
                "author_detail": {
                    "name": "Anna Korhonen"
                },
                "author": "Anna Korhonen"
            },
            {
                "id": "http://arxiv.org/abs/2601.05004v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05004v1",
                "title": "Can Large Language Models Resolve Semantic Discrepancy in Self-Destructive Subcultures? Evidence from Jirai Kei",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Resolve Semantic Discrepancy in Self-Destructive Subcultures? Evidence from Jirai Kei"
                },
                "updated": "2026-01-08T15:02:41Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    2,
                    41,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05004v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Self-destructive behaviors are linked to complex psychological states and can be challenging to diagnose. These behaviors may be even harder to identify within subcultural groups due to their unique expressions. As large language models (LLMs) are applied across various fields, some researchers have begun exploring their application for detecting self-destructive behaviors. Motivated by this, we investigate self-destructive behavior detection within subcultures using current LLM-based methods. However, these methods have two main challenges: (1) Knowledge Lag: Subcultural slang evolves rapidly, faster than LLMs' training cycles; and (2) Semantic Misalignment: it is challenging to grasp the specific and nuanced expressions unique to subcultures. To address these issues, we proposed Subcultural Alignment Solver (SAS), a multi-agent framework that incorporates automatic retrieval and subculture alignment, significantly enhancing the performance of LLMs in detecting self-destructive behavior. Our experimental results show that SAS outperforms the current advanced multi-agent framework OWL. Notably, it competes well with fine-tuned LLMs. We hope that SAS will advance the field of self-destructive behavior detection in subcultural contexts and serve as a valuable resource for future researchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-destructive behaviors are linked to complex psychological states and can be challenging to diagnose. These behaviors may be even harder to identify within subcultural groups due to their unique expressions. As large language models (LLMs) are applied across various fields, some researchers have begun exploring their application for detecting self-destructive behaviors. Motivated by this, we investigate self-destructive behavior detection within subcultures using current LLM-based methods. However, these methods have two main challenges: (1) Knowledge Lag: Subcultural slang evolves rapidly, faster than LLMs' training cycles; and (2) Semantic Misalignment: it is challenging to grasp the specific and nuanced expressions unique to subcultures. To address these issues, we proposed Subcultural Alignment Solver (SAS), a multi-agent framework that incorporates automatic retrieval and subculture alignment, significantly enhancing the performance of LLMs in detecting self-destructive behavior. Our experimental results show that SAS outperforms the current advanced multi-agent framework OWL. Notably, it competes well with fine-tuned LLMs. We hope that SAS will advance the field of self-destructive behavior detection in subcultural contexts and serve as a valuable resource for future researchers."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:02:41Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    2,
                    41,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Preprint",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Xilin Tao"
                    },
                    {
                        "name": "Siyi Yao"
                    },
                    {
                        "name": "Jiageng Wu"
                    },
                    {
                        "name": "Yuntao Zou"
                    },
                    {
                        "name": "Zhuotao Tian"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Dagang Li"
                    }
                ],
                "author_detail": {
                    "name": "Dagang Li"
                },
                "author": "Dagang Li"
            },
            {
                "id": "http://arxiv.org/abs/2402.10424v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2402.10424v2",
                "title": "Pelican Soup Framework: A Theoretical Framework for Language Model Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pelican Soup Framework: A Theoretical Framework for Language Model Capabilities"
                },
                "updated": "2026-01-08T14:58:52Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    58,
                    52,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2402.10424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2402.10424v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this work, we propose a simple theoretical framework, Pelican Soup, aiming to better understand how pretraining allows LLMs to (1) generalize to unseen instructions and (2) perform in-context learning, even when the verbalizers are irrelevant to the task. To this end, in our framework, we introduce the notion of \"knowledge base\" and \"reference-sense association\" and a simple formalism for natural language processing tasks. Our framework demonstrates how linguistic, psychology, and philosophy studies can inform our understanding of the language model and is connected to several other existing theoretical results. As an illustration of the usage of our framework, we derive a bound on in-context learning loss with our framework. Finally, we support our framework with empirical experiments and provide possible future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a simple theoretical framework, Pelican Soup, aiming to better understand how pretraining allows LLMs to (1) generalize to unseen instructions and (2) perform in-context learning, even when the verbalizers are irrelevant to the task. To this end, in our framework, we introduce the notion of \"knowledge base\" and \"reference-sense association\" and a simple formalism for natural language processing tasks. Our framework demonstrates how linguistic, psychology, and philosophy studies can inform our understanding of the language model and is connected to several other existing theoretical results. As an illustration of the usage of our framework, we derive a bound on in-context learning loss with our framework. Finally, we support our framework with empirical experiments and provide possible future research directions."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-02-16T03:20:14Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    3,
                    20,
                    14,
                    4,
                    47,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ting-Rui Chiang"
                    },
                    {
                        "name": "Dani Yogatama"
                    }
                ],
                "author_detail": {
                    "name": "Dani Yogatama"
                },
                "author": "Dani Yogatama"
            },
            {
                "id": "http://arxiv.org/abs/2601.03042v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.03042v2",
                "title": "BaseCal: Unsupervised Confidence Calibration via Base Model Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaseCal: Unsupervised Confidence Calibration via Base Model Signals"
                },
                "updated": "2026-01-08T14:57:18Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    57,
                    18,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.03042v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.03042v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reliable confidence is essential for trusting the outputs of LLMs, yet widely deployed post-trained LLMs (PoLLMs) typically compromise this trust with severe overconfidence. In contrast, we observe that their corresponding base LLMs often remain well-calibrated. This naturally motivates us to calibrate PoLLM confidence using the base LLM as a reference. This work proposes two ways to achieve this. A straightforward solution, BaseCal-ReEval, evaluates PoLLM's responses by feeding them into the base LLM to get average probabilities as confidence. While effective, this approach introduces additional inference overhead. To address this, we propose BaseCal-Proj, which trains a lightweight projection to map the final-layer hidden states of PoLLMs back to those of their base LLMs. These projected states are then processed by the base LLM's output layer to derive base-calibrated confidence for PoLLM's responses. Notably, BaseCal is an unsupervised, plug-and-play solution that operates without human labels or LLM modifications. Experiments across five datasets and three LLM families demonstrate the effectiveness of BaseCal, reducing Expected Calibration Error (ECE) by an average of 42.90\\% compared to the best unsupervised baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable confidence is essential for trusting the outputs of LLMs, yet widely deployed post-trained LLMs (PoLLMs) typically compromise this trust with severe overconfidence. In contrast, we observe that their corresponding base LLMs often remain well-calibrated. This naturally motivates us to calibrate PoLLM confidence using the base LLM as a reference. This work proposes two ways to achieve this. A straightforward solution, BaseCal-ReEval, evaluates PoLLM's responses by feeding them into the base LLM to get average probabilities as confidence. While effective, this approach introduces additional inference overhead. To address this, we propose BaseCal-Proj, which trains a lightweight projection to map the final-layer hidden states of PoLLMs back to those of their base LLMs. These projected states are then processed by the base LLM's output layer to derive base-calibrated confidence for PoLLM's responses. Notably, BaseCal is an unsupervised, plug-and-play solution that operates without human labels or LLM modifications. Experiments across five datasets and three LLM families demonstrate the effectiveness of BaseCal, reducing Expected Calibration Error (ECE) by an average of 42.90\\% compared to the best unsupervised baselines."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-06T14:22:21Z",
                "published_parsed": [
                    2026,
                    1,
                    6,
                    14,
                    22,
                    21,
                    1,
                    6,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hexiang Tan"
                    },
                    {
                        "name": "Wanli Yang"
                    },
                    {
                        "name": "Junwei Zhang"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Rui Tang"
                    },
                    {
                        "name": "Du Su"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Yuanzhuo Wang"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng"
            },
            {
                "id": "http://arxiv.org/abs/2601.04992v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04992v1",
                "title": "Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization"
                },
                "updated": "2026-01-08T14:49:10Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    49,
                    10,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04992v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Supervised fine-tuning (SFT) on chain-of-thought (CoT) trajectories demonstrations is a common approach for enabling reasoning in large language models. Standard practices typically only retain trajectories with correct final answers (positives) while ignoring the rest (negatives). We argue that this paradigm discards substantial supervision and exacerbates overfitting, limiting out-of-domain (OOD) generalization. Specifically, we surprisingly find that incorporating negative trajectories into SFT yields substantial OOD generalization gains over positive-only training, as these trajectories often retain valid intermediate reasoning despite incorrect final answers. To understand this effect in depth, we systematically analyze data, training dynamics, and inference behavior, identifying 22 recurring patterns in negative chains that serve a dual role: they moderate loss descent to mitigate overfitting during training and boost policy entropy by 35.67% during inference to facilitate exploration. Motivated by these observations, we further propose Gain-based LOss Weighting (GLOW), an adaptive, sample-aware scheme that exploits such distinctive training dynamics by rescaling per-sample loss based on inter-epoch progress. Empirically, GLOW efficiently leverages unfiltered trajectories, yielding a 5.51% OOD gain over positive-only SFT on Qwen2.5-7B and boosting MMLU from 72.82% to 76.47% as an RL initialization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) on chain-of-thought (CoT) trajectories demonstrations is a common approach for enabling reasoning in large language models. Standard practices typically only retain trajectories with correct final answers (positives) while ignoring the rest (negatives). We argue that this paradigm discards substantial supervision and exacerbates overfitting, limiting out-of-domain (OOD) generalization. Specifically, we surprisingly find that incorporating negative trajectories into SFT yields substantial OOD generalization gains over positive-only training, as these trajectories often retain valid intermediate reasoning despite incorrect final answers. To understand this effect in depth, we systematically analyze data, training dynamics, and inference behavior, identifying 22 recurring patterns in negative chains that serve a dual role: they moderate loss descent to mitigate overfitting during training and boost policy entropy by 35.67% during inference to facilitate exploration. Motivated by these observations, we further propose Gain-based LOss Weighting (GLOW), an adaptive, sample-aware scheme that exploits such distinctive training dynamics by rescaling per-sample loss based on inter-epoch progress. Empirically, GLOW efficiently leverages unfiltered trajectories, yielding a 5.51% OOD gain over positive-only SFT on Qwen2.5-7B and boosting MMLU from 72.82% to 76.47% as an RL initialization."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T14:49:10Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    49,
                    10,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Code and data are available at https://github.com/Eureka-Maggie/GLOW",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Xueyun Tian"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Bingbing Xu"
                    },
                    {
                        "name": "Nuoyan Lyu"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Heng Dong"
                    },
                    {
                        "name": "Zheng Chu"
                    },
                    {
                        "name": "Yuanzhuo Wang"
                    },
                    {
                        "name": "Huawei Shen"
                    }
                ],
                "author_detail": {
                    "name": "Huawei Shen"
                },
                "arxiv_affiliation": "University of Chinese Academy of Sciences, Beijing, China",
                "author": "Huawei Shen"
            },
            {
                "id": "http://arxiv.org/abs/2601.04988v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04988v1",
                "title": "Through Thick and Thin: The Cosmic Evolution of Disk Scale Height",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through Thick and Thin: The Cosmic Evolution of Disk Scale Height"
                },
                "updated": "2026-01-08T14:43:01Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    43,
                    1,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04988v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "To investigate the formation and evolution of vertical structures in disk galaxies, we measure global $\\operatorname{sech}^2$ scale heights, averaging thin and thick components when present, for 2631 edge-on disk galaxies with $M_*>10^{10} M_\\odot$ at $0< z < 3.5$ from the JWST COSMOS-Web survey. We show that dust extinction systematically overestimates scale heights at shorter rest-frame wavelengths, and therefore adopt a fixed rest-frame wavelength of 1 $$m. After further correcting for projection-induced bias using a new accurate method, we find that the median disk scale height increases from $0.56\\pm0.03$ kpc at $z=3.25$ to $0.84\\pm0.04$ kpc at $z=1.25$, and subsequently decreases to $0.67\\pm0.06$ kpc at $z=0.25$. The disk length-to-height ratio remains constant at $2.7\\pm0.2$ for $z>1.5$, but rises to $4.0\\pm0.4$ at $z=0.25$. These results imply that the high-redshift progenitors of present-day thick disks were of intermediate thickness, neither thin nor thick, yet dynamically hot and dense. The observed radial variation of scale height is consistent with the artificial flaring expected from observational effects, disfavoring minor mergers as the primary mechanism of disk thickening. Instead, we suggest that the high-redshift intermediate-thickness disks were single-component systems that increased their vertical scale height through decreasing surface mass density and/or violent gravitational instabilities, eventually producing thick disks. Thin-disk growth begins at $z\\approx2$ and dominates at $z\\lesssim1$, yielding a vertically more compact system with decreasing scale heights from $z\\approx1$ to $0$. The inferred thin-disk mass fraction increases from $0.1\\pm0.03$ at $z=1$ to $0.6\\pm0.1$ at $z=0$. Together, these findings reveal a continuous evolutionary link between high-redshift single-component disks and present-day thick thin disk systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To investigate the formation and evolution of vertical structures in disk galaxies, we measure global $\\operatorname{sech}^2$ scale heights, averaging thin and thick components when present, for 2631 edge-on disk galaxies with $M_*>10^{10} M_\\odot$ at $0< z < 3.5$ from the JWST COSMOS-Web survey. We show that dust extinction systematically overestimates scale heights at shorter rest-frame wavelengths, and therefore adopt a fixed rest-frame wavelength of 1 $$m. After further correcting for projection-induced bias using a new accurate method, we find that the median disk scale height increases from $0.56\\pm0.03$ kpc at $z=3.25$ to $0.84\\pm0.04$ kpc at $z=1.25$, and subsequently decreases to $0.67\\pm0.06$ kpc at $z=0.25$. The disk length-to-height ratio remains constant at $2.7\\pm0.2$ for $z>1.5$, but rises to $4.0\\pm0.4$ at $z=0.25$. These results imply that the high-redshift progenitors of present-day thick disks were of intermediate thickness, neither thin nor thick, yet dynamically hot and dense. The observed radial variation of scale height is consistent with the artificial flaring expected from observational effects, disfavoring minor mergers as the primary mechanism of disk thickening. Instead, we suggest that the high-redshift intermediate-thickness disks were single-component systems that increased their vertical scale height through decreasing surface mass density and/or violent gravitational instabilities, eventually producing thick disks. Thin-disk growth begins at $z\\approx2$ and dominates at $z\\lesssim1$, yielding a vertically more compact system with decreasing scale heights from $z\\approx1$ to $0$. The inferred thin-disk mass fraction increases from $0.1\\pm0.03$ at $z=1$ to $0.6\\pm0.1$ at $z=0$. Together, these findings reveal a continuous evolutionary link between high-redshift single-component disks and present-day thick thin disk systems."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T14:43:01Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    43,
                    1,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "32 pages, 22 figures, and 3 tables. Submitted to ApJS. Revised in response to a positive referee report with only minor comments",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "Si-Yue Yu"
                    },
                    {
                        "name": "Luis C. Ho"
                    },
                    {
                        "name": "Takafumi Tsukui"
                    },
                    {
                        "name": "John D. Silverman"
                    },
                    {
                        "name": "Marc Huertas-Company"
                    },
                    {
                        "name": "Anton M. Koekemoer"
                    },
                    {
                        "name": "Maximilien Franco"
                    },
                    {
                        "name": "Richard Massey"
                    },
                    {
                        "name": "Lilan Yang"
                    },
                    {
                        "name": "Rafael C. Arango-Toro"
                    },
                    {
                        "name": "Andreas L. Faisst"
                    },
                    {
                        "name": "Ghassem Gozaliasl"
                    },
                    {
                        "name": "Kartik Sheth"
                    },
                    {
                        "name": "Jeyhan S. Kartaltepe"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Aryana Haghjoo"
                    },
                    {
                        "name": "Xuheng Ding"
                    },
                    {
                        "name": "Zhaoxuan Liu"
                    },
                    {
                        "name": "Jacqueline McCleary"
                    }
                ],
                "author_detail": {
                    "name": "Jacqueline McCleary"
                },
                "author": "Jacqueline McCleary"
            },
            {
                "id": "http://arxiv.org/abs/2601.04983v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04983v1",
                "title": "Quantum Neural Network Training and Inference with Low Resolution Control Electronics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Neural Network Training and Inference with Low Resolution Control Electronics"
                },
                "updated": "2026-01-08T14:37:56Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    37,
                    56,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04983v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Scaling quantum computers requires tight integration of cryogenic control electronics with quantum processors, where Digital-to-Analog Converters (DACs) face severe power and area constraints. We investigate quantum neural network (QNN) training and inference under finite DAC resolution constraints across various DAC resolutions. Pre-trained QNNs achieve accuracy nearly indistinguishable from infinite-precision baselines when deployed on quantum systems with 6-bit DAC control electronics, exhibiting an elbow curve with diminishing returns beyond 4 bits. However, training under quantization reveals gradient deadlock below 12-bit resolution as gradient magnitudes fall below quantization step sizes. We introduce temperature-controlled stochasticity that overcomes this through probabilistic parameter updates, enabling successful training at 4-10 bit resolutions that remarkably matches or exceeds infinite-precision baseline performance. Our findings demonstrate that low-resolution control electronics need not compromise QML performance, enabling significant power and area reduction in cryogenic control systems for practical deployment as quantum hardware scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling quantum computers requires tight integration of cryogenic control electronics with quantum processors, where Digital-to-Analog Converters (DACs) face severe power and area constraints. We investigate quantum neural network (QNN) training and inference under finite DAC resolution constraints across various DAC resolutions. Pre-trained QNNs achieve accuracy nearly indistinguishable from infinite-precision baselines when deployed on quantum systems with 6-bit DAC control electronics, exhibiting an elbow curve with diminishing returns beyond 4 bits. However, training under quantization reveals gradient deadlock below 12-bit resolution as gradient magnitudes fall below quantization step sizes. We introduce temperature-controlled stochasticity that overcomes this through probabilistic parameter updates, enabling successful training at 4-10 bit resolutions that remarkably matches or exceeds infinite-precision baseline performance. Our findings demonstrate that low-resolution control electronics need not compromise QML performance, enabling significant power and area reduction in cryogenic control systems for practical deployment as quantum hardware scales."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T14:37:56Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    37,
                    56,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "5 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Rupayan Bhattacharjee"
                    },
                    {
                        "name": "Sergi Abadal"
                    },
                    {
                        "name": "Carmen G. Almudever"
                    },
                    {
                        "name": "Eduard Alarcon"
                    }
                ],
                "author_detail": {
                    "name": "Eduard Alarcon"
                },
                "author": "Eduard Alarcon"
            },
            {
                "id": "http://arxiv.org/abs/2502.18770v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.18770v4",
                "title": "Reward Shaping to Mitigate Reward Hacking in RLHF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Shaping to Mitigate Reward Hacking in RLHF"
                },
                "updated": "2026-01-08T14:33:47Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    33,
                    47,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.18770v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.18770v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to \\emph{reward hacking}, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. Although reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests two key design principles: (1) the RL reward should be bounded, and (2) the RL reward benefits from rapid initial growth followed by gradual convergence. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model as the signal for reinforcement learning. Moreover, PAR exhibits two critical variance-reduction properties that contribute to stabilizing the RLHF training process and effectively extending the tolerance window for early stopping. We evaluated PAR on the base model Gemma2-2B using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR's superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. The code is available at https://github.com/PorUna-byte/PAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to \\emph{reward hacking}, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. Although reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests two key design principles: (1) the RL reward should be bounded, and (2) the RL reward benefits from rapid initial growth followed by gradual convergence. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model as the signal for reinforcement learning. Moreover, PAR exhibits two critical variance-reduction properties that contribute to stabilizing the RLHF training process and effectively extending the tolerance window for early stopping. We evaluated PAR on the base model Gemma2-2B using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR's superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. The code is available at https://github.com/PorUna-byte/PAR."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-26T02:57:59Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    57,
                    59,
                    2,
                    57,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jiayi Fu"
                    },
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Chengyuan Yao"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Qi Han"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao"
            },
            {
                "id": "http://arxiv.org/abs/2510.26699v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.26699v3",
                "title": "Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment"
                },
                "updated": "2026-01-08T14:23:27Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    23,
                    27,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.26699v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.26699v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3786167.3788411",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Keeping software systems up to date is essential to avoid technical debt, security vulnerabilities, and the rigidity typical of legacy systems. However, updating libraries and frameworks remains a time consuming and error-prone process. Recent advances in Large Language Models (LLMs) and agentic coding systems offer new opportunities for automating such maintenance tasks. In this paper, we evaluate the update of a well-known Python library, SQLAlchemy, across a dataset of ten client applications. For this task, we use the Github's Copilot Agent Mode, an autonomous AI systema capable of planning and executing multi-step migration workflows. To assess the effectiveness of the automated migration, we also introduce Migration Coverage, a metric that quantifies the proportion of API usage points correctly migrated. The results of our study show that the LLM agent was capable of migrating functionalities and API usages between SQLAlchemy versions (migration coverage: 100%, median), but failed to maintain the application functionality, leading to a low test-pass rate (39.75%, median).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keeping software systems up to date is essential to avoid technical debt, security vulnerabilities, and the rigidity typical of legacy systems. However, updating libraries and frameworks remains a time consuming and error-prone process. Recent advances in Large Language Models (LLMs) and agentic coding systems offer new opportunities for automating such maintenance tasks. In this paper, we evaluate the update of a well-known Python library, SQLAlchemy, across a dataset of ten client applications. For this task, we use the Github's Copilot Agent Mode, an autonomous AI systema capable of planning and executing multi-step migration workflows. To assess the effectiveness of the automated migration, we also introduce Migration Coverage, a metric that quantifies the proportion of API usage points correctly migrated. The results of our study show that the LLM agent was capable of migrating functionalities and API usages between SQLAlchemy versions (migration coverage: 100%, median), but failed to maintain the application functionality, leading to a low test-pass rate (39.75%, median)."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-30T17:05:13Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    5,
                    13,
                    3,
                    303,
                    0
                ],
                "arxiv_comment": "Accepted at 1st International Workshop on Agentic Engineering (AGENT 2026, colocated with ICSE), pages 1-5",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Aylton Almeida"
                    },
                    {
                        "name": "Laerte Xavier"
                    },
                    {
                        "name": "Marco Tulio Valente"
                    }
                ],
                "author_detail": {
                    "name": "Marco Tulio Valente"
                },
                "author": "Marco Tulio Valente",
                "arxiv_doi": "10.1145/3786167.3788411"
            },
            {
                "id": "http://arxiv.org/abs/2601.04973v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04973v1",
                "title": "ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning"
                },
                "updated": "2026-01-08T14:22:58Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    22,
                    58,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04973v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent breakthroughs in Large Reasoning Models (LRMs) have demonstrated that extensive Chain-of-Thought (CoT) generation is critical for enabling intricate cognitive behaviors, such as self-verification and backtracking, to solve complex tasks. However, this capability often leads to ``overthinking'', where models generate redundant reasoning paths that inflate computational costs without improving accuracy. While Supervised Fine-Tuning (SFT) on reasoning traces is a standard paradigm for the 'cold start' phase, applying existing compression techniques to these traces often compromises logical coherence or incurs prohibitive sampling costs. In this paper, we introduce ConMax (Confidence-Maximizing Compression), a novel reinforcement learning framework designed to automatically compress reasoning traces while preserving essential reasoning patterns. ConMax formulates compression as a reward-driven optimization problem, training a policy to prune redundancy by maximizing a weighted combination of answer confidence for predictive fidelity and thinking confidence for reasoning validity through a frozen auxiliary LRM. Extensive experiments across five reasoning datasets demonstrate that ConMax achieves a superior efficiency-performance trade-off. Specifically, it reduces inference length by 43% over strong baselines at the cost of a mere 0.7% dip in accuracy, proving its effectiveness in generating high-quality, efficient training data for LRMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in Large Reasoning Models (LRMs) have demonstrated that extensive Chain-of-Thought (CoT) generation is critical for enabling intricate cognitive behaviors, such as self-verification and backtracking, to solve complex tasks. However, this capability often leads to ``overthinking'', where models generate redundant reasoning paths that inflate computational costs without improving accuracy. While Supervised Fine-Tuning (SFT) on reasoning traces is a standard paradigm for the 'cold start' phase, applying existing compression techniques to these traces often compromises logical coherence or incurs prohibitive sampling costs. In this paper, we introduce ConMax (Confidence-Maximizing Compression), a novel reinforcement learning framework designed to automatically compress reasoning traces while preserving essential reasoning patterns. ConMax formulates compression as a reward-driven optimization problem, training a policy to prune redundancy by maximizing a weighted combination of answer confidence for predictive fidelity and thinking confidence for reasoning validity through a frozen auxiliary LRM. Extensive experiments across five reasoning datasets demonstrate that ConMax achieves a superior efficiency-performance trade-off. Specifically, it reduces inference length by 43% over strong baselines at the cost of a mere 0.7% dip in accuracy, proving its effectiveness in generating high-quality, efficient training data for LRMs."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T14:22:58Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    22,
                    58,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Minda Hu"
                    },
                    {
                        "name": "Zexuan Qiu"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Bo Zhou"
                    },
                    {
                        "name": "Irwin King"
                    }
                ],
                "author_detail": {
                    "name": "Irwin King"
                },
                "author": "Irwin King"
            },
            {
                "id": "http://arxiv.org/abs/2511.14195v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14195v2",
                "title": "N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator"
                },
                "updated": "2026-01-08T14:19:17Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    19,
                    17,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14195v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Evaluating the safety robustness of LLMs is critical for their deployment. However, mainstream Red Teaming methods rely on online generation and black-box output analysis. These approaches are not only costly but also suffer from feedback latency, making them unsuitable for agile diagnostics after training a new model. To address this, we propose N-GLARE (A Non-Generative, Latent Representation-Efficient LLM Safety Evaluator). N-GLARE operates entirely on the model's latent representations, bypassing the need for full text generation. It characterizes hidden layer dynamics by analyzing the APT (Angular-Probabilistic Trajectory) of latent representations and introducing the JSS (Jensen-Shannon Separability) metric. Experiments on over 40 models and 20 red teaming strategies demonstrate that the JSS metric exhibits high consistency with the safety rankings derived from Red Teaming. N-GLARE reproduces the discriminative trends of large-scale red-teaming tests at less than 1\\% of the token cost and the runtime cost, providing an efficient output-free evaluation proxy for real-time diagnostics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the safety robustness of LLMs is critical for their deployment. However, mainstream Red Teaming methods rely on online generation and black-box output analysis. These approaches are not only costly but also suffer from feedback latency, making them unsuitable for agile diagnostics after training a new model. To address this, we propose N-GLARE (A Non-Generative, Latent Representation-Efficient LLM Safety Evaluator). N-GLARE operates entirely on the model's latent representations, bypassing the need for full text generation. It characterizes hidden layer dynamics by analyzing the APT (Angular-Probabilistic Trajectory) of latent representations and introducing the JSS (Jensen-Shannon Separability) metric. Experiments on over 40 models and 20 red teaming strategies demonstrate that the JSS metric exhibits high consistency with the safety rankings derived from Red Teaming. N-GLARE reproduces the discriminative trends of large-scale red-teaming tests at less than 1\\% of the token cost and the runtime cost, providing an efficient output-free evaluation proxy for real-time diagnostics."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T07:03:58Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    7,
                    3,
                    58,
                    1,
                    322,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zheyu Lin"
                    },
                    {
                        "name": "Jirui Yang"
                    },
                    {
                        "name": "Yukui Qiu"
                    },
                    {
                        "name": "Hengqi Guo"
                    },
                    {
                        "name": "Yubing Bao"
                    },
                    {
                        "name": "Yao Guan"
                    }
                ],
                "author_detail": {
                    "name": "Yao Guan"
                },
                "author": "Yao Guan"
            },
            {
                "id": "http://arxiv.org/abs/2508.05988v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.05988v2",
                "title": "Pruning the Unsurprising: Efficient LLM Reasoning via First-Token Surprisal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning the Unsurprising: Efficient LLM Reasoning via First-Token Surprisal"
                },
                "updated": "2026-01-08T14:09:36Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    9,
                    36,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.05988v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.05988v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities by scaling up the length of Chain-of-Thought (CoT). However, excessively long reasoning traces pose substantial challenges for training cost and inference latency. While various CoT compression approaches have emerged to address this challenge, they face inherent trade-offs: token-level methods often disrupt syntactic and logical coherence, while step-level methods based on perplexity fail to reliably capture the logically critical reasoning steps because of the dilution of logical information. In this paper, we propose ASAP (Anchor-guided, SurprisAl-based Pruning), a novel coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided pruning to preserve the core reasoning structure, which efficiently reduces the search space for subsequent processing. Leveraging the insight that logical branching choices are concentrated at the onset of reasoning steps, it then enables logic-aware pruning by selecting logically essential reasoning steps based on a novel first-token surprisal metric. Finally, ASAP distills the models to autonomously generate and leverage these concise CoTs at inference time, enabling efficient reasoning. Experiments show that ASAP achieves state-of-the-art accuracy across multiple benchmarks while substantially reducing training and inference costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities by scaling up the length of Chain-of-Thought (CoT). However, excessively long reasoning traces pose substantial challenges for training cost and inference latency. While various CoT compression approaches have emerged to address this challenge, they face inherent trade-offs: token-level methods often disrupt syntactic and logical coherence, while step-level methods based on perplexity fail to reliably capture the logically critical reasoning steps because of the dilution of logical information. In this paper, we propose ASAP (Anchor-guided, SurprisAl-based Pruning), a novel coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided pruning to preserve the core reasoning structure, which efficiently reduces the search space for subsequent processing. Leveraging the insight that logical branching choices are concentrated at the onset of reasoning steps, it then enables logic-aware pruning by selecting logically essential reasoning steps based on a novel first-token surprisal metric. Finally, ASAP distills the models to autonomously generate and leverage these concise CoTs at inference time, enabling efficient reasoning. Experiments show that ASAP achieves state-of-the-art accuracy across multiple benchmarks while substantially reducing training and inference costs."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-08T03:46:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    3,
                    46,
                    21,
                    4,
                    220,
                    0
                ],
                "arxiv_comment": "Code and model available at https://github.com/Zengwh02/ASAP",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Wenhao Zeng"
                    },
                    {
                        "name": "Yaoning Wang"
                    },
                    {
                        "name": "Chao Hu"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Chengcheng Wan"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Xiaodong Gu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong Gu"
                },
                "author": "Xiaodong Gu"
            },
            {
                "id": "http://arxiv.org/abs/2601.04963v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04963v1",
                "title": "Text as a Universal Interface for Transferable Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text as a Universal Interface for Transferable Personalization"
                },
                "updated": "2026-01-08T14:09:17Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    9,
                    17,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04963v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box'' profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box'' profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T14:09:17Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    9,
                    17,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yuting Liu"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Jiang-Ming Yang"
                    },
                    {
                        "name": "Jianzhe Zhao"
                    },
                    {
                        "name": "Guibing Guo"
                    }
                ],
                "author_detail": {
                    "name": "Guibing Guo"
                },
                "author": "Guibing Guo"
            },
            {
                "id": "http://arxiv.org/abs/2501.02505v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2501.02505v4",
                "title": "Estimation of partial rankings from sparse, noisy comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation of partial rankings from sparse, noisy comparisons"
                },
                "updated": "2026-01-08T14:07:51Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    7,
                    51,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2501.02505v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2501.02505v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Ranking items based on pairwise comparisons is common, from using match outcomes to rank sports teams to using purchase or survey data to rank consumer products. Statistical inference-based methods such as the Bradley-Terry model, which extract rankings based on an underlying generative model, have emerged as flexible and powerful tools to tackle ranking in empirical data. In situations with limited and/or noisy comparisons, it is often challenging to confidently distinguish the performance of different items based on the evidence available in the data. However, most inference-based ranking methods choose to assign each item to a unique rank or score, suggesting a meaningful distinction when there is none. Here, we develop a principled nonparametric Bayesian method, adaptable to any statistical ranking method, for learning partial rankings (rankings with ties) that distinguishes among the ranks of different items only when there is sufficient evidence available in the data. We develop a fast agglomerative algorithm to perform Maximum A Posteriori (MAP) inference of partial rankings under our framework and examine the performance of our method on a variety of real and synthetic network datasets, finding that it frequently gives a more parsimonious summary of the data than traditional ranking, particularly when observations are sparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ranking items based on pairwise comparisons is common, from using match outcomes to rank sports teams to using purchase or survey data to rank consumer products. Statistical inference-based methods such as the Bradley-Terry model, which extract rankings based on an underlying generative model, have emerged as flexible and powerful tools to tackle ranking in empirical data. In situations with limited and/or noisy comparisons, it is often challenging to confidently distinguish the performance of different items based on the evidence available in the data. However, most inference-based ranking methods choose to assign each item to a unique rank or score, suggesting a meaningful distinction when there is none. Here, we develop a principled nonparametric Bayesian method, adaptable to any statistical ranking method, for learning partial rankings (rankings with ties) that distinguishes among the ranks of different items only when there is sufficient evidence available in the data. We develop a fast agglomerative algorithm to perform Maximum A Posteriori (MAP) inference of partial rankings under our framework and examine the performance of our method on a variety of real and synthetic network datasets, finding that it frequently gives a more parsimonious summary of the data than traditional ranking, particularly when observations are sparse."
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-01-05T11:04:30Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    11,
                    4,
                    30,
                    6,
                    5,
                    0
                ],
                "arxiv_comment": "36 pages, 22 figures, 2 tables",
                "arxiv_primary_category": {
                    "term": "physics.soc-ph"
                },
                "authors": [
                    {
                        "name": "Sebastian Morel-Balbi"
                    },
                    {
                        "name": "Alec Kirkley"
                    }
                ],
                "author_detail": {
                    "name": "Alec Kirkley"
                },
                "author": "Alec Kirkley"
            },
            {
                "id": "http://arxiv.org/abs/2601.04960v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04960v1",
                "title": "A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction"
                },
                "updated": "2026-01-08T14:07:30Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    7,
                    30,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04960v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents a unified spoken language model for emotional intelligence, enhanced by a novel data construction strategy termed Injected Emotional-Attribution Thinking (IEAT). IEAT incorporates user emotional states and their underlying causes into the model's internal reasoning process, enabling emotion-aware reasoning to be internalized rather than treated as explicit supervision. The model is trained with a two-stage progressive strategy. The first stage performs speech-text alignment and emotional attribute modeling via self-distillation, while the second stage conducts end-to-end cross-modal joint optimization to ensure consistency between textual and spoken emotional expressions. Experiments on the Human-like Spoken Dialogue Systems Challenge (HumDial) Emotional Intelligence benchmark demonstrate that the proposed approach achieves top-ranked performance across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a unified spoken language model for emotional intelligence, enhanced by a novel data construction strategy termed Injected Emotional-Attribution Thinking (IEAT). IEAT incorporates user emotional states and their underlying causes into the model's internal reasoning process, enabling emotion-aware reasoning to be internalized rather than treated as explicit supervision. The model is trained with a two-stage progressive strategy. The first stage performs speech-text alignment and emotional attribute modeling via self-distillation, while the second stage conducts end-to-end cross-modal joint optimization to ensure consistency between textual and spoken emotional expressions. Experiments on the Human-like Spoken Dialogue Systems Challenge (HumDial) Emotional Intelligence benchmark demonstrate that the proposed approach achieves top-ranked performance across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T14:07:30Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    7,
                    30,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Zehan Li"
                    },
                    {
                        "name": "Yaodong Song"
                    },
                    {
                        "name": "Hongjie Chen"
                    },
                    {
                        "name": "Jian Kang"
                    },
                    {
                        "name": "Jie Lian"
                    },
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Yongxiang Li"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li"
            },
            {
                "id": "http://arxiv.org/abs/2601.04954v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04954v1",
                "title": "Precision over Diversity: High-Precision Reward Generalizes to Robust Instruction Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precision over Diversity: High-Precision Reward Generalizes to Robust Instruction Following"
                },
                "updated": "2026-01-08T14:00:51Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    0,
                    51,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04954v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A central belief in scaling reinforcement learning with verifiable rewards for instruction following (IF) tasks is that, a diverse mixture of verifiable hard and unverifiable soft constraints is essential for generalizing to unseen instructions. In this work, we challenge this prevailing consensus through a systematic empirical investigation. Counter-intuitively, we find that models trained on hard-only constraints consistently outperform those trained on mixed datasets. Extensive experiments reveal that reward precision, rather than constraint diversity, is the primary driver of effective alignment. The LLM judge suffers from a low recall rate in detecting false response, which leads to severe reward hacking, thereby undermining the benefits of diversity. Furthermore, analysis of the attention mechanism reveals that high-precision rewards develop a transferable meta-skill for IF. Motivated by these insights, we propose a simple yet effective data-centric refinement strategy that prioritizes reward precision. Evaluated on five benchmarks, our approach outperforms competitive baselines by 13.4\\% in performance while achieving a 58\\% reduction in training time, maintaining strong generalization beyond instruction following. Our findings advocate for a paradigm shift: moving away from the indiscriminate pursuit of data diversity toward high-precision rewards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central belief in scaling reinforcement learning with verifiable rewards for instruction following (IF) tasks is that, a diverse mixture of verifiable hard and unverifiable soft constraints is essential for generalizing to unseen instructions. In this work, we challenge this prevailing consensus through a systematic empirical investigation. Counter-intuitively, we find that models trained on hard-only constraints consistently outperform those trained on mixed datasets. Extensive experiments reveal that reward precision, rather than constraint diversity, is the primary driver of effective alignment. The LLM judge suffers from a low recall rate in detecting false response, which leads to severe reward hacking, thereby undermining the benefits of diversity. Furthermore, analysis of the attention mechanism reveals that high-precision rewards develop a transferable meta-skill for IF. Motivated by these insights, we propose a simple yet effective data-centric refinement strategy that prioritizes reward precision. Evaluated on five benchmarks, our approach outperforms competitive baselines by 13.4\\% in performance while achieving a 58\\% reduction in training time, maintaining strong generalization beyond instruction following. Our findings advocate for a paradigm shift: moving away from the indiscriminate pursuit of data diversity toward high-precision rewards."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T14:00:51Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    0,
                    51,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "ACL under review 13 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yirong Zeng"
                    },
                    {
                        "name": "Yufei Liu"
                    },
                    {
                        "name": "Xiao Ding"
                    },
                    {
                        "name": "Yutai Hou"
                    },
                    {
                        "name": "Yuxian Wang"
                    },
                    {
                        "name": "Haonan Song"
                    },
                    {
                        "name": "Wu Ning"
                    },
                    {
                        "name": "Dandan Tu"
                    },
                    {
                        "name": "Qixun Zhang"
                    },
                    {
                        "name": "Bibo Cai"
                    },
                    {
                        "name": "Yuxiang He"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu"
            },
            {
                "id": "http://arxiv.org/abs/2601.04946v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04946v1",
                "title": "Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics"
                },
                "updated": "2026-01-08T13:49:14Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    49,
                    14,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04946v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automatic metrics are now central to evaluating text-to-image models, often substituting for human judgment in benchmarking and large-scale filtering. However, it remains unclear whether these metrics truly prioritize semantic correctness or instead favor visually and socially prototypical images learned from biased data distributions. We identify and study \\emph{prototypicality bias} as a systematic failure mode in multimodal evaluation. We introduce a controlled contrastive benchmark \\textsc{\\textbf{ProtoBias}} (\\textit{\\textbf{Proto}typical \\textbf{Bias}}), spanning Animals, Objects, and Demography images, where semantically correct but non-prototypical images are paired with subtly incorrect yet prototypical adversarial counterparts. This setup enables a directional evaluation of whether metrics follow textual semantics or default to prototypes. Our results show that widely used metrics, including CLIPScore, PickScore, and VQA-based scores, frequently misrank these pairs, while even LLM-as-Judge systems exhibit uneven robustness in socially grounded cases. Human evaluations consistently favour semantic correctness with larger decision margins. Motivated by these findings, we propose \\textbf{\\textsc{ProtoScore}}, a robust 7B-parameter metric that substantially reduces failure rates and suppresses misranking, while running at orders of magnitude faster than the inference time of GPT-5, approaching the robustness of much larger closed-source judges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic metrics are now central to evaluating text-to-image models, often substituting for human judgment in benchmarking and large-scale filtering. However, it remains unclear whether these metrics truly prioritize semantic correctness or instead favor visually and socially prototypical images learned from biased data distributions. We identify and study \\emph{prototypicality bias} as a systematic failure mode in multimodal evaluation. We introduce a controlled contrastive benchmark \\textsc{\\textbf{ProtoBias}} (\\textit{\\textbf{Proto}typical \\textbf{Bias}}), spanning Animals, Objects, and Demography images, where semantically correct but non-prototypical images are paired with subtly incorrect yet prototypical adversarial counterparts. This setup enables a directional evaluation of whether metrics follow textual semantics or default to prototypes. Our results show that widely used metrics, including CLIPScore, PickScore, and VQA-based scores, frequently misrank these pairs, while even LLM-as-Judge systems exhibit uneven robustness in socially grounded cases. Human evaluations consistently favour semantic correctness with larger decision margins. Motivated by these findings, we propose \\textbf{\\textsc{ProtoScore}}, a robust 7B-parameter metric that substantially reduces failure rates and suppresses misranking, while running at orders of magnitude faster than the inference time of GPT-5, approaching the robustness of much larger closed-source judges."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T13:49:14Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    49,
                    14,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "First version",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Subhadeep Roy"
                    },
                    {
                        "name": "Gagan Bhatia"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger"
            },
            {
                "id": "http://arxiv.org/abs/2601.04940v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04940v1",
                "title": "CurricuLLM: Designing Personalized and Workforce-Aligned Cybersecurity Curricula Using Fine-Tuned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CurricuLLM: Designing Personalized and Workforce-Aligned Cybersecurity Curricula Using Fine-Tuned LLMs"
                },
                "updated": "2026-01-08T13:43:15Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    43,
                    15,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04940v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The cybersecurity landscape is constantly evolving, driven by increased digitalization and new cybersecurity threats. Cybersecurity programs often fail to equip graduates with skills demanded by the workforce, particularly concerning recent developments in cybersecurity, as curriculum design is costly and labor-intensive. To address this misalignment, we present a novel Large Language Model (LLM)-based framework for automated design and analysis of cybersecurity curricula, called CurricuLLM. Our approach provides three key contributions: (1) automation of personalized curriculum design, (2) a data-driven pipeline aligned with industry demands, and (3) a comprehensive methodology for leveraging fine-tuned LLMs in curriculum development.\n  CurricuLLM utilizes a two-tier approach consisting of PreprocessLM, which standardizes input data, and ClassifyLM, which assigns course content to nine Knowledge Areas in cybersecurity. We systematically evaluated multiple Natural Language Processing (NLP) architectures and fine-tuning strategies, ultimately selecting the Bidirectional Encoder Representations from Transformers (BERT) model as ClassifyLM, fine-tuned on foundational cybersecurity concepts and workforce competencies.\n  We are the first to validate our method with human experts who analyzed real-world cybersecurity curricula and frameworks, motivating that CurricuLLM is an efficient solution to replace labor-intensive curriculum analysis. Moreover, once course content has been classified, it can be integrated with established cybersecurity role-based weights, enabling alignment of the educational program with specific job roles, workforce categories, or general market needs. This lays the foundation for personalized, workforce-aligned cybersecurity curricula that prepare students for the evolving demands in cybersecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cybersecurity landscape is constantly evolving, driven by increased digitalization and new cybersecurity threats. Cybersecurity programs often fail to equip graduates with skills demanded by the workforce, particularly concerning recent developments in cybersecurity, as curriculum design is costly and labor-intensive. To address this misalignment, we present a novel Large Language Model (LLM)-based framework for automated design and analysis of cybersecurity curricula, called CurricuLLM. Our approach provides three key contributions: (1) automation of personalized curriculum design, (2) a data-driven pipeline aligned with industry demands, and (3) a comprehensive methodology for leveraging fine-tuned LLMs in curriculum development.\n  CurricuLLM utilizes a two-tier approach consisting of PreprocessLM, which standardizes input data, and ClassifyLM, which assigns course content to nine Knowledge Areas in cybersecurity. We systematically evaluated multiple Natural Language Processing (NLP) architectures and fine-tuning strategies, ultimately selecting the Bidirectional Encoder Representations from Transformers (BERT) model as ClassifyLM, fine-tuned on foundational cybersecurity concepts and workforce competencies.\n  We are the first to validate our method with human experts who analyzed real-world cybersecurity curricula and frameworks, motivating that CurricuLLM is an efficient solution to replace labor-intensive curriculum analysis. Moreover, once course content has been classified, it can be integrated with established cybersecurity role-based weights, enabling alignment of the educational program with specific job roles, workforce categories, or general market needs. This lays the foundation for personalized, workforce-aligned cybersecurity curricula that prepare students for the evolving demands in cybersecurity."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T13:43:15Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    43,
                    15,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Arthur Nijdam"
                    },
                    {
                        "name": "Harri Khknen"
                    },
                    {
                        "name": "Valtteri Niemi"
                    },
                    {
                        "name": "Paul Stankovski Wagner"
                    },
                    {
                        "name": "Sara Ramezanian"
                    }
                ],
                "author_detail": {
                    "name": "Sara Ramezanian"
                },
                "author": "Sara Ramezanian"
            },
            {
                "id": "http://arxiv.org/abs/2601.04932v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04932v1",
                "title": "GenProve: Learning to Generate Text with Fine-Grained Provenance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenProve: Learning to Generate Text with Fine-Grained Provenance"
                },
                "updated": "2026-01-08T13:30:30Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    30,
                    30,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04932v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLM) often hallucinate, and while adding citations is a common solution, it is frequently insufficient for accountability as users struggle to verify how a cited source supports a generated claim. Existing methods are typically coarse-grained and fail to distinguish between direct quotes and complex reasoning. In this paper, we introduce Generation-time Fine-grained Provenance, a task where models must generate fluent answers while simultaneously producing structured, sentence-level provenance triples. To enable this, we present ReFInE (Relation-aware Fine-grained Interpretability & Evidence), a dataset featuring expert verified annotations that distinguish between Quotation, Compression, and Inference. Building on ReFInE, we propose GenProve, a framework that combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO). By optimizing a composite reward for answer fidelity and provenance correctness, GenProve significantly outperforms 14 strong LLMs in joint evaluation. Crucially, our analysis uncovers a reasoning gap where models excel at surface-level quotation but struggle significantly with inference-based provenance, suggesting that verifiable reasoning remains a frontier challenge distinct from surface-level citation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLM) often hallucinate, and while adding citations is a common solution, it is frequently insufficient for accountability as users struggle to verify how a cited source supports a generated claim. Existing methods are typically coarse-grained and fail to distinguish between direct quotes and complex reasoning. In this paper, we introduce Generation-time Fine-grained Provenance, a task where models must generate fluent answers while simultaneously producing structured, sentence-level provenance triples. To enable this, we present ReFInE (Relation-aware Fine-grained Interpretability & Evidence), a dataset featuring expert verified annotations that distinguish between Quotation, Compression, and Inference. Building on ReFInE, we propose GenProve, a framework that combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO). By optimizing a composite reward for answer fidelity and provenance correctness, GenProve significantly outperforms 14 strong LLMs in joint evaluation. Crucially, our analysis uncovers a reasoning gap where models excel at surface-level quotation but struggle significantly with inference-based provenance, suggesting that verifiable reasoning remains a frontier challenge distinct from surface-level citation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T13:30:30Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    30,
                    30,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jingxuan Wei"
                    },
                    {
                        "name": "Xingyue Wang"
                    },
                    {
                        "name": "Yanghaoyu Liao"
                    },
                    {
                        "name": "Jie Dong"
                    },
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Caijun Jia"
                    },
                    {
                        "name": "Bihui Yu"
                    },
                    {
                        "name": "Junnan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Junnan Zhu"
                },
                "author": "Junnan Zhu"
            },
            {
                "id": "http://arxiv.org/abs/2506.06842v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.06842v2",
                "title": "PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation"
                },
                "updated": "2026-01-08T13:25:34Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    25,
                    34,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.06842v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.06842v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Disinformation detection is a key aspect of media literacy. Psychological studies have shown that knowledge of persuasive fallacies helps individuals detect disinformation. Inspired by these findings, we experimented with large language models (LLMs) to test whether infusing persuasion knowledge enhances disinformation detection. As a result, we introduce the Persuasion-Augmented Chain of Thought (PCoT), a novel approach that leverages persuasion to improve disinformation detection in zero-shot classification. We extensively evaluate PCoT on online news and social media posts. Moreover, we publish two novel, up-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets enable the evaluation of PCoT on content entirely unseen by the LLMs used in our experiments, as the content was published after the models' knowledge cutoffs. We show that, on average, PCoT outperforms competitive methods by 15% across five LLMs and five datasets. These findings highlight the value of persuasion in strengthening zero-shot disinformation detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disinformation detection is a key aspect of media literacy. Psychological studies have shown that knowledge of persuasive fallacies helps individuals detect disinformation. Inspired by these findings, we experimented with large language models (LLMs) to test whether infusing persuasion knowledge enhances disinformation detection. As a result, we introduce the Persuasion-Augmented Chain of Thought (PCoT), a novel approach that leverages persuasion to improve disinformation detection in zero-shot classification. We extensively evaluate PCoT on online news and social media posts. Moreover, we publish two novel, up-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets enable the evaluation of PCoT on content entirely unseen by the LLMs used in our experiments, as the content was published after the models' knowledge cutoffs. We show that, on average, PCoT outperforms competitive methods by 15% across five LLMs and five datasets. These findings highlight the value of persuasion in strengthening zero-shot disinformation detection."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-07T15:46:02Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    15,
                    46,
                    2,
                    5,
                    158,
                    0
                ],
                "arxiv_comment": "Accepted to ACL 2025 Main Conference",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Arkadiusz Modzelewski"
                    },
                    {
                        "name": "Witold Sosnowski"
                    },
                    {
                        "name": "Tiziano Labruna"
                    },
                    {
                        "name": "Adam Wierzbicki"
                    },
                    {
                        "name": "Giovanni Da San Martino"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Da San Martino"
                },
                "author": "Giovanni Da San Martino"
            },
            {
                "id": "http://arxiv.org/abs/2601.03997v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.03997v2",
                "title": "VotIE: Information Extraction from Meeting Minutes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VotIE: Information Extraction from Meeting Minutes"
                },
                "updated": "2026-01-08T13:24:16Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    24,
                    16,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.03997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.03997v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Municipal meeting minutes record key decisions in local democratic processes. Unlike parliamentary proceedings, which typically adhere to standardized formats, they encode voting outcomes in highly heterogeneous, free-form narrative text that varies widely across municipalities, posing significant challenges for automated extraction. In this paper, we introduce VotIE (Voting Information Extraction), a new information extraction task aimed at identifying structured voting events in narrative deliberative records, and establish the first benchmark for this task using Portuguese municipal minutes, building on the recently introduced CitiLink corpus. Our experiments yield two key findings. First, under standard in-domain evaluation, fine-tuned encoders, specifically XLM-R-CRF, achieve the strongest performance, reaching 93.2\\% macro F1, outperforming generative approaches. Second, in a cross-municipality setting that evaluates transfer to unseen administrative contexts, these models suffer substantial performance degradation, whereas few-shot LLMs demonstrate greater robustness, with significantly smaller declines in performance. Despite this generalization advantage, the high computational cost of generative models currently constrains their practicality. As a result, lightweight fine-tuned encoders remain a more practical option for large-scale, real-world deployment. To support reproducible research in administrative NLP, we publicly release our benchmark, trained models, and evaluation framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Municipal meeting minutes record key decisions in local democratic processes. Unlike parliamentary proceedings, which typically adhere to standardized formats, they encode voting outcomes in highly heterogeneous, free-form narrative text that varies widely across municipalities, posing significant challenges for automated extraction. In this paper, we introduce VotIE (Voting Information Extraction), a new information extraction task aimed at identifying structured voting events in narrative deliberative records, and establish the first benchmark for this task using Portuguese municipal minutes, building on the recently introduced CitiLink corpus. Our experiments yield two key findings. First, under standard in-domain evaluation, fine-tuned encoders, specifically XLM-R-CRF, achieve the strongest performance, reaching 93.2\\% macro F1, outperforming generative approaches. Second, in a cross-municipality setting that evaluates transfer to unseen administrative contexts, these models suffer substantial performance degradation, whereas few-shot LLMs demonstrate greater robustness, with significantly smaller declines in performance. Despite this generalization advantage, the high computational cost of generative models currently constrains their practicality. As a result, lightweight fine-tuned encoders remain a more practical option for large-scale, real-world deployment. To support reproducible research in administrative NLP, we publicly release our benchmark, trained models, and evaluation framework."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-07T15:06:53Z",
                "published_parsed": [
                    2026,
                    1,
                    7,
                    15,
                    6,
                    53,
                    2,
                    7,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jos Pedro Evans"
                    },
                    {
                        "name": "Lus Filipe Cunha"
                    },
                    {
                        "name": "Purificao Silvano"
                    },
                    {
                        "name": "Alpio Jorge"
                    },
                    {
                        "name": "Nuno Guimares"
                    },
                    {
                        "name": "Srgio Nunes"
                    },
                    {
                        "name": "Ricardo Campos"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Campos"
                },
                "author": "Ricardo Campos"
            },
            {
                "id": "http://arxiv.org/abs/2601.04925v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04925v1",
                "title": "Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences"
                },
                "updated": "2026-01-08T13:22:25Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    22,
                    25,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04925v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) can generate highly persuasive text, raising concerns about their misuse for propaganda, manipulation, and other harmful purposes. This leads us to our central question: Is LLM-generated persuasion more difficult to automatically detect than human-written persuasion? To address this, we categorize controllable generation approaches for producing persuasive content with LLMs and introduce Persuaficial, a high-quality multilingual benchmark covering six languages: English, German, Polish, Italian, French and Russian. Using this benchmark, we conduct extensive empirical evaluations comparing human-authored and LLM-generated persuasive texts. We find that although overtly persuasive LLM-generated texts can be easier to detect than human-written ones, subtle LLM-generated persuasion consistently degrades automatic detection performance. Beyond detection performance, we provide the first comprehensive linguistic analysis contrasting human and LLM-generated persuasive texts, offering insights that may guide the development of more interpretable and robust detection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can generate highly persuasive text, raising concerns about their misuse for propaganda, manipulation, and other harmful purposes. This leads us to our central question: Is LLM-generated persuasion more difficult to automatically detect than human-written persuasion? To address this, we categorize controllable generation approaches for producing persuasive content with LLMs and introduce Persuaficial, a high-quality multilingual benchmark covering six languages: English, German, Polish, Italian, French and Russian. Using this benchmark, we conduct extensive empirical evaluations comparing human-authored and LLM-generated persuasive texts. We find that although overtly persuasive LLM-generated texts can be easier to detect than human-written ones, subtle LLM-generated persuasion consistently degrades automatic detection performance. Beyond detection performance, we provide the first comprehensive linguistic analysis contrasting human and LLM-generated persuasive texts, offering insights that may guide the development of more interpretable and robust detection tools."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T13:22:25Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    22,
                    25,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Preprint; Paper is currently under review at a major NLP conference",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Arkadiusz Modzelewski"
                    },
                    {
                        "name": "Pawe Golik"
                    },
                    {
                        "name": "Anna Koos"
                    },
                    {
                        "name": "Giovanni Da San Martino"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Da San Martino"
                },
                "author": "Giovanni Da San Martino"
            },
            {
                "id": "http://arxiv.org/abs/2601.04920v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04920v1",
                "title": "Conversational AI for Rapid Scientific Prototyping: A Case Study on ESA's ELOPE Competition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational AI for Rapid Scientific Prototyping: A Case Study on ESA's ELOPE Competition"
                },
                "updated": "2026-01-08T13:17:50Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    17,
                    50,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04920v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are increasingly used as coding partners, yet their role in accelerating scientific discovery remains underexplored. This paper presents a case study of using ChatGPT for rapid prototyping in ESA's ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition. The competition required participants to process event camera data to estimate lunar lander trajectories. Despite joining late, we achieved second place with a score of 0.01282, highlighting the potential of human-AI collaboration in competitive scientific settings. ChatGPT contributed not only executable code but also algorithmic reasoning, data handling routines, and methodological suggestions, such as using fixed number of events instead of fixed time spans for windowing. At the same time, we observed limitations: the model often introduced unnecessary structural changes, gets confused by intermediate discussions about alternative ideas, occasionally produced critical errors and forgets important aspects in longer scientific discussions. By analyzing these strengths and shortcomings, we show how conversational AI can both accelerate development and support conceptual insight in scientific research. We argue that structured integration of LLMs into the scientific workflow can enhance rapid prototyping by proposing best practices for AI-assisted scientific work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used as coding partners, yet their role in accelerating scientific discovery remains underexplored. This paper presents a case study of using ChatGPT for rapid prototyping in ESA's ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition. The competition required participants to process event camera data to estimate lunar lander trajectories. Despite joining late, we achieved second place with a score of 0.01282, highlighting the potential of human-AI collaboration in competitive scientific settings. ChatGPT contributed not only executable code but also algorithmic reasoning, data handling routines, and methodological suggestions, such as using fixed number of events instead of fixed time spans for windowing. At the same time, we observed limitations: the model often introduced unnecessary structural changes, gets confused by intermediate discussions about alternative ideas, occasionally produced critical errors and forgets important aspects in longer scientific discussions. By analyzing these strengths and shortcomings, we show how conversational AI can both accelerate development and support conceptual insight in scientific research. We argue that structured integration of LLMs into the scientific workflow can enhance rapid prototyping by proposing best practices for AI-assisted scientific work."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T13:17:50Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    17,
                    50,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Nils Einecke"
                    }
                ],
                "author_detail": {
                    "name": "Nils Einecke"
                },
                "author": "Nils Einecke"
            },
            {
                "id": "http://arxiv.org/abs/2601.04918v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04918v1",
                "title": "Breaking Robustness Barriers in Cognitive Diagnosis: A One-Shot Neural Architecture Search Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Robustness Barriers in Cognitive Diagnosis: A One-Shot Neural Architecture Search Perspective"
                },
                "updated": "2026-01-08T13:17:40Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    17,
                    40,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04918v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the advancement of network technologies, intelligent tutoring systems (ITS) have emerged to deliver increasingly precise and tailored personalized learning services. Cognitive diagnosis (CD) has emerged as a core research task in ITS, aiming to infer learners' mastery of specific knowledge concepts by modeling the mapping between learning behavior data and knowledge states. However, existing research prioritizes model performance enhancement while neglecting the pervasive noise contamination in observed response data, significantly hindering practical deployment. Furthermore, current cognitive diagnosis models (CDMs) rely heavily on researchers' domain expertise for structural design, which fails to exhaustively explore architectural possibilities, thus leaving model architectures' full potential untapped. To address this issue, we propose OSCD, an evolutionary multi-objective One-Shot neural architecture search method for Cognitive Diagnosis, designed to efficiently and robustly improve the model's capability in assessing learner proficiency. Specifically, OSCD operates through two distinct stages: training and searching. During the training stage, we construct a search space encompassing diverse architectural combinations and train a weight-sharing supernet represented via the complete binary tree topology, enabling comprehensive exploration of potential architectures beyond manual design priors. In the searching stage, we formulate the optimal architecture search under heterogeneous noise scenarios as a multi-objective optimization problem (MOP), and develop an optimization framework integrating a Pareto-optimal solution search strategy with cross-scenario performance evaluation for resolution. Extensive experiments on real-world educational datasets validate the effectiveness and robustness of the optimal architectures discovered by our OSCD model for CD tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of network technologies, intelligent tutoring systems (ITS) have emerged to deliver increasingly precise and tailored personalized learning services. Cognitive diagnosis (CD) has emerged as a core research task in ITS, aiming to infer learners' mastery of specific knowledge concepts by modeling the mapping between learning behavior data and knowledge states. However, existing research prioritizes model performance enhancement while neglecting the pervasive noise contamination in observed response data, significantly hindering practical deployment. Furthermore, current cognitive diagnosis models (CDMs) rely heavily on researchers' domain expertise for structural design, which fails to exhaustively explore architectural possibilities, thus leaving model architectures' full potential untapped. To address this issue, we propose OSCD, an evolutionary multi-objective One-Shot neural architecture search method for Cognitive Diagnosis, designed to efficiently and robustly improve the model's capability in assessing learner proficiency. Specifically, OSCD operates through two distinct stages: training and searching. During the training stage, we construct a search space encompassing diverse architectural combinations and train a weight-sharing supernet represented via the complete binary tree topology, enabling comprehensive exploration of potential architectures beyond manual design priors. In the searching stage, we formulate the optimal architecture search under heterogeneous noise scenarios as a multi-objective optimization problem (MOP), and develop an optimization framework integrating a Pareto-optimal solution search strategy with cross-scenario performance evaluation for resolution. Extensive experiments on real-world educational datasets validate the effectiveness and robustness of the optimal architectures discovered by our OSCD model for CD tasks."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T13:17:40Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    17,
                    40,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "KDD2026, 15 pages",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Ziwen Wang"
                    },
                    {
                        "name": "Shangshang Yang"
                    },
                    {
                        "name": "Xiaoshan Yu"
                    },
                    {
                        "name": "Haiping Ma"
                    },
                    {
                        "name": "Xingyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xingyi Zhang"
                },
                "author": "Xingyi Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2502.02106v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.02106v2",
                "title": "A new stochastic SIS-type modelling framework for analysing epidemic dynamics in continuous space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new stochastic SIS-type modelling framework for analysing epidemic dynamics in continuous space"
                },
                "updated": "2026-01-08T13:11:53Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    11,
                    53,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.02106v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.02106v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose a new stochastic epidemiological model defined in a continuous space of arbitrary dimension, based on SIS dynamics implemented in a spatial $$-Fleming-Viot (SLFV) process. The model can be described by as little as three parameters, and is dual to a spatial branching process with competition linked to genealogies of infected individuals. Therefore, it is a possible modelling framework to develop computationally tractable inference tools for epidemics in a continuous space using demographic and genetic data.We provide mathematical constructions of the process based on well-posed martingale problems as well as driving space-time Poisson point processes. With these devices and the duality relation in hand, we unveil some of the drivers of the transition between extinction and survival of the epidemic. In particular, we show that extinction is in large parts independent of the initial condition, and identify a strong candidate for the reproduction number R 0 of the epidemic in such a model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new stochastic epidemiological model defined in a continuous space of arbitrary dimension, based on SIS dynamics implemented in a spatial $$-Fleming-Viot (SLFV) process. The model can be described by as little as three parameters, and is dual to a spatial branching process with competition linked to genealogies of infected individuals. Therefore, it is a possible modelling framework to develop computationally tractable inference tools for epidemics in a continuous space using demographic and genetic data.We provide mathematical constructions of the process based on well-posed martingale problems as well as driving space-time Poisson point processes. With these devices and the duality relation in hand, we unveil some of the drivers of the transition between extinction and survival of the epidemic. In particular, we show that extinction is in large parts independent of the initial condition, and identify a strong candidate for the reproduction number R 0 of the epidemic in such a model."
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-04T08:41:16Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    41,
                    16,
                    1,
                    35,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "math.PR"
                },
                "authors": [
                    {
                        "name": "Apolline Louvet"
                    },
                    {
                        "name": "Bastian Wiederhold"
                    }
                ],
                "author_detail": {
                    "name": "Bastian Wiederhold"
                },
                "arxiv_affiliation": "BioSP, TUM",
                "author": "Bastian Wiederhold"
            },
            {
                "id": "http://arxiv.org/abs/2601.04913v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04913v1",
                "title": "Bayesian Additive Regression Tree Copula Processes for Scalable Distributional Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Additive Regression Tree Copula Processes for Scalable Distributional Prediction"
                },
                "updated": "2026-01-08T13:11:37Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    11,
                    37,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04913v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We show how to construct the implied copula process of response values from a Bayesian additive regression tree (BART) model with prior on the leaf node variances. This copula process, defined on the covariate space, can be paired with any marginal distribution for the dependent variable to construct a flexible distributional BART model. Bayesian inference is performed via Markov chain Monte Carlo on an augmented posterior, where we show that key sampling steps can be realized as those of Chipman et al. (2010), preserving scalability and computational efficiency even though the copula process is high dimensional. The posterior predictive distribution from the copula process model is derived in closed form as the push-forward of the posterior predictive distribution of the underlying BART model with an optimal transport map. Under suitable conditions, we establish posterior consistency for the regression function and posterior means and prove convergence in distribution of the predictive process and conditional expectation. Simulation studies demonstrate improved accuracy of distributional predictions compared to the original BART model and leading benchmarks. Applications to five real datasets with 506 to 515,345 observations and 8 to 90 covariates further highlight the efficacy and scalability of our proposed BART copula process model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show how to construct the implied copula process of response values from a Bayesian additive regression tree (BART) model with prior on the leaf node variances. This copula process, defined on the covariate space, can be paired with any marginal distribution for the dependent variable to construct a flexible distributional BART model. Bayesian inference is performed via Markov chain Monte Carlo on an augmented posterior, where we show that key sampling steps can be realized as those of Chipman et al. (2010), preserving scalability and computational efficiency even though the copula process is high dimensional. The posterior predictive distribution from the copula process model is derived in closed form as the push-forward of the posterior predictive distribution of the underlying BART model with an optimal transport map. Under suitable conditions, we establish posterior consistency for the regression function and posterior means and prove convergence in distribution of the predictive process and conditional expectation. Simulation studies demonstrate improved accuracy of distributional predictions compared to the original BART model and leading benchmarks. Applications to five real datasets with 506 to 515,345 observations and 8 to 90 covariates further highlight the efficacy and scalability of our proposed BART copula process model."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T13:11:37Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    11,
                    37,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Jan Martin Wenkel"
                    },
                    {
                        "name": "Michael Stanley Smith"
                    },
                    {
                        "name": "Nadja Klein"
                    }
                ],
                "author_detail": {
                    "name": "Nadja Klein"
                },
                "author": "Nadja Klein"
            },
            {
                "id": "http://arxiv.org/abs/2510.15949v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.15949v2",
                "title": "ATLAS: Adaptive Trading with LLM AgentS Through Dynamic Prompt Optimization and Multi-Agent Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ATLAS: Adaptive Trading with LLM AgentS Through Dynamic Prompt Optimization and Multi-Agent Coordination"
                },
                "updated": "2026-01-08T13:08:59Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    8,
                    59,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.15949v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.15949v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models show promise for financial decision-making, yet deploying them as autonomous trading agents raises fundamental challenges: how to adapt instructions when rewards arrive late and obscured by market noise, how to synthesize heterogeneous information streams into coherent decisions, and how to bridge the gap between model outputs and executable market actions. We present ATLAS (Adaptive Trading with LLM AgentS), a unified multi-agent framework that integrates structured information from markets, news, and corporate fundamentals to support robust trading decisions. Within ATLAS, the central trading agent operates in an order-aware action space, ensuring that outputs correspond to executable market orders rather than abstract signals. The agent can incorporate feedback while trading using Adaptive-OPRO, a novel prompt-optimization technique that dynamically adapts the prompt by incorporating real-time, stochastic feedback, leading to increasing performance over time. Across regime-specific equity studies and multiple LLM families, Adaptive-OPRO consistently outperforms fixed prompts, while reflection-based feedback fails to provide systematic gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models show promise for financial decision-making, yet deploying them as autonomous trading agents raises fundamental challenges: how to adapt instructions when rewards arrive late and obscured by market noise, how to synthesize heterogeneous information streams into coherent decisions, and how to bridge the gap between model outputs and executable market actions. We present ATLAS (Adaptive Trading with LLM AgentS), a unified multi-agent framework that integrates structured information from markets, news, and corporate fundamentals to support robust trading decisions. Within ATLAS, the central trading agent operates in an order-aware action space, ensuring that outputs correspond to executable market orders rather than abstract signals. The agent can incorporate feedback while trading using Adaptive-OPRO, a novel prompt-optimization technique that dynamically adapts the prompt by incorporating real-time, stochastic feedback, leading to increasing performance over time. Across regime-specific equity studies and multiple LLM families, Adaptive-OPRO consistently outperforms fixed prompts, while reflection-based feedback fails to provide systematic gains."
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-10T13:01:51Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    13,
                    1,
                    51,
                    4,
                    283,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR"
                },
                "authors": [
                    {
                        "name": "Charidimos Papadakis"
                    },
                    {
                        "name": "Angeliki Dimitriou"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Maria Lymperaiou"
                    },
                    {
                        "name": "Konstantinos Thomas"
                    },
                    {
                        "name": "Giorgos Stamou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Stamou"
                },
                "author": "Giorgos Stamou"
            },
            {
                "id": "http://arxiv.org/abs/2601.04906v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04906v1",
                "title": "Inference for concave distribution functions under measurement error",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for concave distribution functions under measurement error"
                },
                "updated": "2026-01-08T13:05:00Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    5,
                    0,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04906v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose an estimator of a concave cumulative distribution function under the measurement error model, where the non-negative variables of interest are perturbed by additive independent random noise. The estimator is defined as the least concave majorant on the positive half-line of the deconvolution estimator of the distribution function. We show its uniform consistency and its square root convergence in law in $\\ell_\\infty(\\mathbb R)$. To assess the validity of the concavity assumption, we construct a test for the nonparametric null hypothesis that the distribution function is concave on the positive half-line, against the alternative that it is not. We calibrate the test using bootstrap methods. The theoretical justification for calibration led us to establish a bootstrap version of Theorem 1 in Shl and Trabs (2012), a Donsker-type result from which we obtain, as a special case, the limiting behavior of the deconvolution estimator of the distribution function in a bootstrap setting with measurement error. Combining this Donsker-type theorem with the functional delta method, we show that the test statistic and its bootstrap version have the same limiting distribution under the null hypothesis, whereas under the alternative, the bootstrap statistic is stochastically smaller. Consequently, the power of the test tends to one, for any fixed alternative, as the sample size tends to infinity. In addition to the theoretical results for the estimator and the test, we investigate their finite-sample performance in simulation studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an estimator of a concave cumulative distribution function under the measurement error model, where the non-negative variables of interest are perturbed by additive independent random noise. The estimator is defined as the least concave majorant on the positive half-line of the deconvolution estimator of the distribution function. We show its uniform consistency and its square root convergence in law in $\\ell_\\infty(\\mathbb R)$. To assess the validity of the concavity assumption, we construct a test for the nonparametric null hypothesis that the distribution function is concave on the positive half-line, against the alternative that it is not. We calibrate the test using bootstrap methods. The theoretical justification for calibration led us to establish a bootstrap version of Theorem 1 in Shl and Trabs (2012), a Donsker-type result from which we obtain, as a special case, the limiting behavior of the deconvolution estimator of the distribution function in a bootstrap setting with measurement error. Combining this Donsker-type theorem with the functional delta method, we show that the test statistic and its bootstrap version have the same limiting distribution under the null hypothesis, whereas under the alternative, the bootstrap statistic is stochastically smaller. Consequently, the power of the test tends to one, for any fixed alternative, as the sample size tends to infinity. In addition to the theoretical results for the estimator and the test, we investigate their finite-sample performance in simulation studies."
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T13:05:00Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    5,
                    0,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "math.ST"
                },
                "authors": [
                    {
                        "name": "Mohammed Es-Salih Benjrada"
                    },
                    {
                        "name": "Cecile Durot"
                    },
                    {
                        "name": "Tommaso Lando"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Lando"
                },
                "author": "Tommaso Lando"
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2601.05248v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05248v1",
                "title": "LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model"
                },
                "updated": "2026-01-08T18:59:53Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    59,
                    53,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05248v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. However, explicit reasoning typically incurs non-negligible inference latency, which constrains the temporal resolution required for robotic manipulation. Moreover, such reasoning is confined to the linguistic space, imposing a representational bottleneck that struggles to faithfully capture ineffable physical attributes. To mitigate these limitations, we propose LaST$_0$, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize. Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories. Furthermore, LaST$_0$ adopts a dual-system architecture implemented via a Mixture-of-Transformers design, where a reasoning expert conducts low-frequency latent inference and an acting expert generates high-frequency actions conditioned on robotics-oriented latent representations. To facilitate coordination, LaST$_0$ is trained with heterogeneous operation frequencies, enabling adaptive switching between reasoning and action inference rates during deployment. Across ten simulated and six real-world manipulation tasks, LaST$_0$ improves mean success rates by 8% and 13% over prior VLA methods, respectively, while achieving substantially faster inference. Project website: https://sites.google.com/view/last0",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. However, explicit reasoning typically incurs non-negligible inference latency, which constrains the temporal resolution required for robotic manipulation. Moreover, such reasoning is confined to the linguistic space, imposing a representational bottleneck that struggles to faithfully capture ineffable physical attributes. To mitigate these limitations, we propose LaST$_0$, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize. Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories. Furthermore, LaST$_0$ adopts a dual-system architecture implemented via a Mixture-of-Transformers design, where a reasoning expert conducts low-frequency latent inference and an acting expert generates high-frequency actions conditioned on robotics-oriented latent representations. To facilitate coordination, LaST$_0$ is trained with heterogeneous operation frequencies, enabling adaptive switching between reasoning and action inference rates during deployment. Across ten simulated and six real-world manipulation tasks, LaST$_0$ improves mean success rates by 8% and 13% over prior VLA methods, respectively, while achieving substantially faster inference. Project website: https://sites.google.com/view/last0"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:59:53Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    59,
                    53,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Zhuoyang Liu"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Chengkai Hou"
                    },
                    {
                        "name": "Chenyang Gu"
                    },
                    {
                        "name": "Jiale Yu"
                    },
                    {
                        "name": "Xiangju Mi"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Zhengping Che"
                    },
                    {
                        "name": "Jian Tang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2501.09135v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2501.09135v3",
                "title": "HAFix: History-Augmented Large Language Models for Bug Fixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAFix: History-Augmented Large Language Models for Bug Fixing"
                },
                "updated": "2026-01-08T18:43:43Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    43,
                    43,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2501.09135v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2501.09135v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent studies have explored the performance of Large Language Models (LLMs) on various Software Engineering (SE) tasks, such as code generation and bug fixing. However, these approaches typically rely on the context data from the current snapshot of the project, overlooking the potential of rich historical data residing in real-world software repositories. Additionally, the impact of prompt styles on LLM performance for SE tasks within a historical context remains underexplored. To address these gaps, we propose HAFix, which stands for History-Augmented LLMs on Bug Fixing, a novel approach that leverages seven individual historical heuristics associated with bugs and aggregates the results of these heuristics (HAFix-Agg) to enhance LLMs' bug-fixing capabilities. To empirically evaluate HAFix, we employ three Code LLMs (i.e., Code Llama, DeepSeek-Coder and DeepSeek-Coder-V2-Lite models) on 51 single-line Python bugs from BugsInPy and 116 single-line Java bugs from Defects4J. Our evaluation demonstrates that multiple HAFix heuristics achieve statistically significant improvements compared to a non-historical baseline inspired by GitHub Copilot. Furthermore, the aggregated HAFix variant HAFix-Agg achieves substantial improvements by combining the complementary strengths of individual heuristics, increasing bug-fixing rates by an average of 45.05% on BugsInPy and 49.92% on Defects4J relative to the corresponding baseline. Moreover, within the context of historical heuristics, we identify the Instruction prompt style as the most effective template compared to the InstructionLabel and InstructionMask for LLMs in bug fixing. Finally, we evaluate the cost of HAFix in terms of inference time and token usage, and provide a pragmatic trade-off analysis of the cost and bug-fixing performance, offering valuable insights for the practical deployment of our approach in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have explored the performance of Large Language Models (LLMs) on various Software Engineering (SE) tasks, such as code generation and bug fixing. However, these approaches typically rely on the context data from the current snapshot of the project, overlooking the potential of rich historical data residing in real-world software repositories. Additionally, the impact of prompt styles on LLM performance for SE tasks within a historical context remains underexplored. To address these gaps, we propose HAFix, which stands for History-Augmented LLMs on Bug Fixing, a novel approach that leverages seven individual historical heuristics associated with bugs and aggregates the results of these heuristics (HAFix-Agg) to enhance LLMs' bug-fixing capabilities. To empirically evaluate HAFix, we employ three Code LLMs (i.e., Code Llama, DeepSeek-Coder and DeepSeek-Coder-V2-Lite models) on 51 single-line Python bugs from BugsInPy and 116 single-line Java bugs from Defects4J. Our evaluation demonstrates that multiple HAFix heuristics achieve statistically significant improvements compared to a non-historical baseline inspired by GitHub Copilot. Furthermore, the aggregated HAFix variant HAFix-Agg achieves substantial improvements by combining the complementary strengths of individual heuristics, increasing bug-fixing rates by an average of 45.05% on BugsInPy and 49.92% on Defects4J relative to the corresponding baseline. Moreover, within the context of historical heuristics, we identify the Instruction prompt style as the most effective template compared to the InstructionLabel and InstructionMask for LLMs in bug fixing. Finally, we evaluate the cost of HAFix in terms of inference time and token usage, and provide a pragmatic trade-off analysis of the cost and bug-fixing performance, offering valuable insights for the practical deployment of our approach in real-world scenarios."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-01-15T20:39:32Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    39,
                    32,
                    2,
                    15,
                    0
                ],
                "arxiv_comment": "Evaluated HAFix on two datasets (BugsInPy, Defects4J) and three LLMs (CodeLlama, DeepSeek-Coder, DeepSeek-Coder-V2) and optimized the figures and tables for better readability",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Yu Shi"
                    },
                    {
                        "name": "Abdul Ali Bangash"
                    },
                    {
                        "name": "Emad Fallahzadeh"
                    },
                    {
                        "name": "Bram Adams"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan"
            },
            {
                "id": "http://arxiv.org/abs/2601.05215v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05215v1",
                "title": "MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents"
                },
                "updated": "2026-01-08T18:39:52Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    39,
                    52,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05215v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present \\textsc{MineNPC-Task}, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world \\emph{Minecraft}. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan/act/memory events-including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts and reports outcomes relative to the total number of attempted subtasks, derived from in-world evidence.\n  As an initial snapshot, we instantiate the framework with GPT-4o and evaluate \\textbf{216} subtasks across \\textbf{8} experienced players. We observe recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, alongside recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively, while highlighting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and harness to support transparent, reproducible evaluation of future memory-aware embodied agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \\textsc{MineNPC-Task}, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world \\emph{Minecraft}. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan/act/memory events-including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts and reports outcomes relative to the total number of attempted subtasks, derived from in-world evidence.\n  As an initial snapshot, we instantiate the framework with GPT-4o and evaluate \\textbf{216} subtasks across \\textbf{8} experienced players. We observe recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, alongside recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively, while highlighting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and harness to support transparent, reproducible evaluation of future memory-aware embodied agents."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:39:52Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    39,
                    52,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Tamil Sudaravan Mohan Doss"
                    },
                    {
                        "name": "Michael Xu"
                    },
                    {
                        "name": "Sudha Rao"
                    },
                    {
                        "name": "Andrew D. Wilson"
                    },
                    {
                        "name": "Balasaravanan Thoravi Kumaravel"
                    }
                ],
                "author_detail": {
                    "name": "Balasaravanan Thoravi Kumaravel"
                },
                "author": "Balasaravanan Thoravi Kumaravel"
            },
            {
                "id": "http://arxiv.org/abs/2601.05214v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05214v1",
                "title": "Internal Representations as Indicators of Hallucinations in Agent Tool Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal Representations as Indicators of Hallucinations in Agent Tool Selection"
                },
                "updated": "2026-01-08T18:38:45Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    38,
                    45,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05214v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:38:45Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    38,
                    45,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Kait Healy"
                    },
                    {
                        "name": "Bharathi Srinivasan"
                    },
                    {
                        "name": "Visakh Madathil"
                    },
                    {
                        "name": "Jing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Wu"
                },
                "author": "Jing Wu"
            },
            {
                "id": "http://arxiv.org/abs/2601.05205v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05205v1",
                "title": "EARL: Energy-Aware Optimization of Liquid State Machines for Pervasive AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EARL: Energy-Aware Optimization of Liquid State Machines for Pervasive AI"
                },
                "updated": "2026-01-08T18:31:11Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    31,
                    11,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05205v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Pervasive AI increasingly depends on on-device learning systems that deliver low-latency and energy-efficient computation under strict resource constraints. Liquid State Machines (LSMs) offer a promising approach for low-power temporal processing in pervasive and neuromorphic systems, but their deployment remains challenging due to high hyperparameter sensitivity and the computational cost of traditional optimization methods that ignore energy constraints. This work presents EARL, an energy-aware reinforcement learning framework that integrates Bayesian optimization with an adaptive reinforcement learning based selection policy to jointly optimize accuracy and energy consumption. EARL employs surrogate modeling for global exploration, reinforcement learning for dynamic candidate prioritization, and an early termination mechanism to eliminate redundant evaluations, substantially reducing computational overhead. Experiments on three benchmark datasets demonstrate that EARL achieves 6 to 15 percent higher accuracy, 60 to 80 percent lower energy consumption, and up to an order of magnitude reduction in optimization time compared to leading hyperparameter tuning frameworks. These results highlight the effectiveness of energy-aware adaptive search in improving the efficiency and scalability of LSMs for resource-constrained on-device AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pervasive AI increasingly depends on on-device learning systems that deliver low-latency and energy-efficient computation under strict resource constraints. Liquid State Machines (LSMs) offer a promising approach for low-power temporal processing in pervasive and neuromorphic systems, but their deployment remains challenging due to high hyperparameter sensitivity and the computational cost of traditional optimization methods that ignore energy constraints. This work presents EARL, an energy-aware reinforcement learning framework that integrates Bayesian optimization with an adaptive reinforcement learning based selection policy to jointly optimize accuracy and energy consumption. EARL employs surrogate modeling for global exploration, reinforcement learning for dynamic candidate prioritization, and an early termination mechanism to eliminate redundant evaluations, substantially reducing computational overhead. Experiments on three benchmark datasets demonstrate that EARL achieves 6 to 15 percent higher accuracy, 60 to 80 percent lower energy consumption, and up to an order of magnitude reduction in optimization time compared to leading hyperparameter tuning frameworks. These results highlight the effectiveness of energy-aware adaptive search in improving the efficiency and scalability of LSMs for resource-constrained on-device AI applications."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:31:11Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    31,
                    11,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "6 pages, 9 figures, 2 Tables, conference [Submitted in PerConAI-2026]",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zain Iqbal"
                    },
                    {
                        "name": "Lorenzo Valerio"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Valerio"
                },
                "author": "Lorenzo Valerio"
            },
            {
                "id": "http://arxiv.org/abs/2601.01266v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.01266v2",
                "title": "From Policy to Logic for Efficient and Interpretable Coverage Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Policy to Logic for Efficient and Interpretable Coverage Assessment"
                },
                "updated": "2026-01-08T18:28:40Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    28,
                    40,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.01266v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.01266v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-03T19:24:51Z",
                "published_parsed": [
                    2026,
                    1,
                    3,
                    19,
                    24,
                    51,
                    5,
                    3,
                    0
                ],
                "arxiv_comment": "Accepted at AIMedHealth @ AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Rhitabrat Pokharel"
                    },
                    {
                        "name": "Hamid Reza Hassanzadeh"
                    },
                    {
                        "name": "Ameeta Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Ameeta Agrawal"
                },
                "author": "Ameeta Agrawal"
            },
            {
                "id": "http://arxiv.org/abs/2601.05194v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05194v1",
                "title": "An interpretable data-driven approach to optimizing clinical fall risk assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An interpretable data-driven approach to optimizing clinical fall risk assessment"
                },
                "updated": "2026-01-08T18:17:31Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    17,
                    31,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05194v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this study, we aim to better align fall risk prediction from the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically meaningful measures via a data-driven modelling approach. We conducted a retrospective cohort analysis of 54,209 inpatient admissions from three Johns Hopkins Health System hospitals between March 2022 and October 2023. A total of 20,208 admissions were included as high fall risk encounters, and 13,941 were included as low fall risk encounters. To incorporate clinical knowledge and maintain interpretability, we employed constrained score optimization (CSO) models to reweight the JHFRAT scoring weights, while preserving its additive structure and clinical thresholds. Recalibration refers to adjusting item weights so that the resulting score can order encounters more consistently by the study's risk labels, and without changing the tool's form factor or deployment workflow. The model demonstrated significant improvements in predictive performance over the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). This performance improvement translates to protecting an additional 35 high-risk patients per week across the Johns Hopkins Health System. The constrained score optimization models performed similarly with and without the EHR variables. Although the benchmark black-box model (XGBoost), improves upon the performance metrics of the knowledge-based constrained logistic regression (AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk labeling. This evidence-based approach provides a robust foundation for health systems to systematically enhance inpatient fall prevention protocols and patient safety using data-driven optimization techniques, contributing to improved risk assessment and resource allocation in healthcare settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we aim to better align fall risk prediction from the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically meaningful measures via a data-driven modelling approach. We conducted a retrospective cohort analysis of 54,209 inpatient admissions from three Johns Hopkins Health System hospitals between March 2022 and October 2023. A total of 20,208 admissions were included as high fall risk encounters, and 13,941 were included as low fall risk encounters. To incorporate clinical knowledge and maintain interpretability, we employed constrained score optimization (CSO) models to reweight the JHFRAT scoring weights, while preserving its additive structure and clinical thresholds. Recalibration refers to adjusting item weights so that the resulting score can order encounters more consistently by the study's risk labels, and without changing the tool's form factor or deployment workflow. The model demonstrated significant improvements in predictive performance over the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). This performance improvement translates to protecting an additional 35 high-risk patients per week across the Johns Hopkins Health System. The constrained score optimization models performed similarly with and without the EHR variables. Although the benchmark black-box model (XGBoost), improves upon the performance metrics of the knowledge-based constrained logistic regression (AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk labeling. This evidence-based approach provides a robust foundation for health systems to systematically enhance inpatient fall prevention protocols and patient safety using data-driven optimization techniques, contributing to improved risk assessment and resource allocation in healthcare settings."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:17:31Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    17,
                    31,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2510.20714",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Fardin Ganjkhanloo"
                    },
                    {
                        "name": "Emmett Springer"
                    },
                    {
                        "name": "Erik H. Hoyer"
                    },
                    {
                        "name": "Daniel L. Young"
                    },
                    {
                        "name": "Holley Farley"
                    },
                    {
                        "name": "Kimia Ghobadi"
                    }
                ],
                "author_detail": {
                    "name": "Kimia Ghobadi"
                },
                "author": "Kimia Ghobadi"
            },
            {
                "id": "http://arxiv.org/abs/2601.05192v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05192v1",
                "title": "LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation"
                },
                "updated": "2026-01-08T18:15:34Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    15,
                    34,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05192v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:15:34Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    15,
                    34,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Samy Haffoudhi"
                    },
                    {
                        "name": "Fabian M. Suchanek"
                    },
                    {
                        "name": "Nils Holzenberger"
                    }
                ],
                "author_detail": {
                    "name": "Nils Holzenberger"
                },
                "author": "Nils Holzenberger"
            },
            {
                "id": "http://arxiv.org/abs/2601.05187v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05187v1",
                "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning"
                },
                "updated": "2026-01-08T18:10:35Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    10,
                    35,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05187v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:10:35Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    10,
                    35,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yanchang Liang"
                    },
                    {
                        "name": "Xiaowei Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Zhao"
                },
                "author": "Xiaowei Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2601.05184v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05184v1",
                "title": "Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop"
                },
                "updated": "2026-01-08T18:08:15Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    8,
                    15,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05184v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid advancement of large language models (LLMs) has led to growing interest in using synthetic data to train future models. However, this creates a self-consuming retraining loop, where models are trained on their own outputs and may cause performance drops and induce emerging biases. In real-world applications, previously deployed LLMs may influence the data they generate, leading to a dynamic system driven by user feedback. For example, if a model continues to underserve users from a group, less query data will be collected from this particular demographic of users. In this study, we introduce the concept of \\textbf{S}elf-\\textbf{C}onsuming \\textbf{P}erformative \\textbf{L}oop (\\textbf{SCPL}) and investigate the role of synthetic data in shaping bias during these dynamic iterative training processes under controlled performative feedback. This controlled setting is motivated by the inaccessibility of real-world user preference data from dynamic production systems, and enables us to isolate and analyze feedback-driven bias evolution in a principled manner. We focus on two types of loops, including the typical retraining setting and the incremental fine-tuning setting, which is largely underexplored. Through experiments on three real-world tasks, we find that the performative loop increases preference bias and decreases disparate bias. We design a reward-based rejection sampling strategy to mitigate the bias, moving towards more trustworthy self-improving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has led to growing interest in using synthetic data to train future models. However, this creates a self-consuming retraining loop, where models are trained on their own outputs and may cause performance drops and induce emerging biases. In real-world applications, previously deployed LLMs may influence the data they generate, leading to a dynamic system driven by user feedback. For example, if a model continues to underserve users from a group, less query data will be collected from this particular demographic of users. In this study, we introduce the concept of \\textbf{S}elf-\\textbf{C}onsuming \\textbf{P}erformative \\textbf{L}oop (\\textbf{SCPL}) and investigate the role of synthetic data in shaping bias during these dynamic iterative training processes under controlled performative feedback. This controlled setting is motivated by the inaccessibility of real-world user preference data from dynamic production systems, and enables us to isolate and analyze feedback-driven bias evolution in a principled manner. We focus on two types of loops, including the typical retraining setting and the incremental fine-tuning setting, which is largely underexplored. Through experiments on three real-world tasks, we find that the performative loop increases preference bias and decreases disparate bias. We design a reward-based rejection sampling strategy to mitigate the bias, moving towards more trustworthy self-improving systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:08:15Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    8,
                    15,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yaxuan Wang"
                    },
                    {
                        "name": "Zhongteng Cai"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Xueru Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu"
            },
            {
                "id": "http://arxiv.org/abs/2505.21072v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.21072v3",
                "title": "Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation"
                },
                "updated": "2026-01-08T18:06:58Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    6,
                    58,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.21072v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.21072v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) enhanced with retrieval, an approach known as Retrieval-Augmented Generation (RAG), have achieved strong performance in open-domain question answering. However, RAG remains prone to hallucinations: factually incorrect outputs may arise from inaccuracies in the model's internal knowledge and the retrieved context. Existing approaches to mitigating hallucinations often conflate factuality with faithfulness to the retrieved evidence, incorrectly labeling factually correct statements as hallucinations if they are not explicitly supported by the retrieval. In this paper, we introduce FRANQ, a new method for hallucination detection in RAG outputs. FRANQ applies distinct uncertainty quantification (UQ) techniques to estimate factuality, conditioning on whether a statement is faithful to the retrieved context. To evaluate FRANQ and competing UQ methods, we construct a new long-form question answering dataset annotated for both factuality and faithfulness, combining automated labeling with manual validation of challenging cases. Extensive experiments across multiple datasets, tasks, and LLMs show that FRANQ achieves more accurate detection of factual errors in RAG-generated responses compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) enhanced with retrieval, an approach known as Retrieval-Augmented Generation (RAG), have achieved strong performance in open-domain question answering. However, RAG remains prone to hallucinations: factually incorrect outputs may arise from inaccuracies in the model's internal knowledge and the retrieved context. Existing approaches to mitigating hallucinations often conflate factuality with faithfulness to the retrieved evidence, incorrectly labeling factually correct statements as hallucinations if they are not explicitly supported by the retrieval. In this paper, we introduce FRANQ, a new method for hallucination detection in RAG outputs. FRANQ applies distinct uncertainty quantification (UQ) techniques to estimate factuality, conditioning on whether a statement is faithful to the retrieved context. To evaluate FRANQ and competing UQ methods, we construct a new long-form question answering dataset annotated for both factuality and faithfulness, combining automated labeling with manual validation of challenging cases. Extensive experiments across multiple datasets, tasks, and LLMs show that FRANQ achieves more accurate detection of factual errors in RAG-generated responses compared to existing approaches."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-27T11:56:59Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    11,
                    56,
                    59,
                    1,
                    147,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ekaterina Fadeeva"
                    },
                    {
                        "name": "Aleksandr Rubashevskii"
                    },
                    {
                        "name": "Dzianis Piatrashyn"
                    },
                    {
                        "name": "Roman Vashurin"
                    },
                    {
                        "name": "Shehzaad Dhuliawala"
                    },
                    {
                        "name": "Artem Shelmanov"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Maxim Panov"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Panov"
                },
                "author": "Maxim Panov"
            },
            {
                "id": "http://arxiv.org/abs/2601.05178v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05178v1",
                "title": "Multi-band Carrier Phase Positioning toward 6G: Performance Bounds and Efficient Estimators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-band Carrier Phase Positioning toward 6G: Performance Bounds and Efficient Estimators"
                },
                "updated": "2026-01-08T18:03:28Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    3,
                    28,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05178v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In addition to satellite systems, carrier phase positioning (CPP) is gaining attraction also in terrestrial mobile networks, particularly in 5G New Radio evolution toward 6G. One key challenge is to resolve the integer ambiguity problem, as the carrier phase provides only relative position information. This work introduces and studies a multi-band CPP scenario with intra- and inter-band carrier aggregation (CA) opportunities across FR1, mmWave-FR2, and emerging 6G FR3 bands. Specifically, we derive multi-band CPP performance bounds, showcasing the superiority of multi-band CPP for high-precision localization in current and future mobile networks, while noting also practical imperfections such as clock offsets between the user equipment (UE) and the network as well as mutual clock imperfections between the network nodes. A wide collection of numerical results is provided, covering the impacts of the available carrier bandwidth, number of aggregated carriers, transmit power, and the number of network nodes or base stations. The offered results highlight that only two carriers suffice to substantially facilitate resolving the integer ambiguity problem while also largely enhancing the robustness of positioning against imperfections imposed by the network-side clocks and multi-path propagation. In addition, we also propose a two-stage practical estimator that achieves the derived bounds under all realistic bandwidth and transmit power conditions. Furthermore, we show that with an additional search-based refinement step, the proposed estimator becomes particularly suitable for narrowband Internet of Things applications operating efficiently even under narrow carrier bandwidths. Finally, both the derived bounds and the proposed estimators are extended to scenarios where the bands assigned to each base station are nonuniform or fully disjoint, enhancing the practical deployment flexibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In addition to satellite systems, carrier phase positioning (CPP) is gaining attraction also in terrestrial mobile networks, particularly in 5G New Radio evolution toward 6G. One key challenge is to resolve the integer ambiguity problem, as the carrier phase provides only relative position information. This work introduces and studies a multi-band CPP scenario with intra- and inter-band carrier aggregation (CA) opportunities across FR1, mmWave-FR2, and emerging 6G FR3 bands. Specifically, we derive multi-band CPP performance bounds, showcasing the superiority of multi-band CPP for high-precision localization in current and future mobile networks, while noting also practical imperfections such as clock offsets between the user equipment (UE) and the network as well as mutual clock imperfections between the network nodes. A wide collection of numerical results is provided, covering the impacts of the available carrier bandwidth, number of aggregated carriers, transmit power, and the number of network nodes or base stations. The offered results highlight that only two carriers suffice to substantially facilitate resolving the integer ambiguity problem while also largely enhancing the robustness of positioning against imperfections imposed by the network-side clocks and multi-path propagation. In addition, we also propose a two-stage practical estimator that achieves the derived bounds under all realistic bandwidth and transmit power conditions. Furthermore, we show that with an additional search-based refinement step, the proposed estimator becomes particularly suitable for narrowband Internet of Things applications operating efficiently even under narrow carrier bandwidths. Finally, both the derived bounds and the proposed estimators are extended to scenarios where the bands assigned to each base station are nonuniform or fully disjoint, enhancing the practical deployment flexibility."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T18:03:28Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    18,
                    3,
                    28,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "13 pages, 10 figures, under review in IEEE Transactions on Wireless Communications",
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Ehsan Shourezari"
                    },
                    {
                        "name": "Ossi Kaltiokallio"
                    },
                    {
                        "name": "Mehmet C. Ilter"
                    },
                    {
                        "name": "Jukka Talvitie"
                    },
                    {
                        "name": "Gonzalo Seco-Granados"
                    },
                    {
                        "name": "Henk Wymeersch"
                    },
                    {
                        "name": "Mikko Valkama"
                    }
                ],
                "author_detail": {
                    "name": "Mikko Valkama"
                },
                "author": "Mikko Valkama"
            },
            {
                "id": "http://arxiv.org/abs/2601.05172v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05172v1",
                "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoV: Chain-of-View Prompting for Spatial Reasoning"
                },
                "updated": "2026-01-08T17:59:42Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    59,
                    42,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05172v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T17:59:42Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    59,
                    42,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Haoyu Zhao"
                    },
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Weijie Wang"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Ruihan Zhu"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang"
            },
            {
                "id": "http://arxiv.org/abs/2601.05170v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05170v1",
                "title": "Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference"
                },
                "updated": "2026-01-08T17:58:52Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    58,
                    52,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05170v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of the meta-inferential properties they entail. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of the meta-inferential properties they entail. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T17:58:52Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    58,
                    52,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Rasmus Blanck"
                    },
                    {
                        "name": "Bill Noble"
                    },
                    {
                        "name": "Stergios Chatzikyriakidis"
                    }
                ],
                "author_detail": {
                    "name": "Stergios Chatzikyriakidis"
                },
                "author": "Stergios Chatzikyriakidis"
            },
            {
                "id": "http://arxiv.org/abs/2601.05167v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05167v1",
                "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RelayLLM: Efficient Reasoning via Collaborative Decoding"
                },
                "updated": "2026-01-08T17:56:16Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    56,
                    16,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05167v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T17:56:16Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    56,
                    16,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Chengsong Huang"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Langlin Huang"
                    },
                    {
                        "name": "Jinyuan Li"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Jiaxin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxin Huang"
                },
                "author": "Jiaxin Huang"
            },
            {
                "id": "http://arxiv.org/abs/2509.18693v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.18693v3",
                "title": "MVT: Mask-Grounded Vision-Language Models for Taxonomy-Aligned Land-Cover Tagging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVT: Mask-Grounded Vision-Language Models for Taxonomy-Aligned Land-Cover Tagging"
                },
                "updated": "2026-01-08T17:56:05Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    56,
                    5,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.18693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.18693v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Land-cover understanding in remote sensing increasingly demands class-agnostic systems that generalize across datasets while remaining spatially precise and interpretable. We study a geometry-first discovery-and-interpretation setting under domain shift, where candidate regions are delineated class-agnostically and supervision avoids lexical class names via anonymized identifiers. Complementary to open-set recognition and open-world learning, we focus on coupling class-agnostic mask evidence with taxonomy-grounded scene interpretation, rather than unknown rejection or continual class expansion. We propose MVT, a three-stage framework that (i) extracts boundary-faithful region masks using SAM2 with domain adaptation, (ii) performs mask-grounded semantic tagging and scene description generation via dual-step LoRA fine-tuning of multimodal LLMs, and (iii) evaluates outputs with LLM-as-judge scoring calibrated by stratified expert ratings. On cross-dataset segmentation transfer (train on OpenEarthMap, evaluate on LoveDA), domain-adapted SAM2 improves mask quality; meanwhile, dual-step MLLM fine-tuning yields more accurate taxonomy-aligned tags and more informative mask-grounded scene descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Land-cover understanding in remote sensing increasingly demands class-agnostic systems that generalize across datasets while remaining spatially precise and interpretable. We study a geometry-first discovery-and-interpretation setting under domain shift, where candidate regions are delineated class-agnostically and supervision avoids lexical class names via anonymized identifiers. Complementary to open-set recognition and open-world learning, we focus on coupling class-agnostic mask evidence with taxonomy-grounded scene interpretation, rather than unknown rejection or continual class expansion. We propose MVT, a three-stage framework that (i) extracts boundary-faithful region masks using SAM2 with domain adaptation, (ii) performs mask-grounded semantic tagging and scene description generation via dual-step LoRA fine-tuning of multimodal LLMs, and (iii) evaluates outputs with LLM-as-judge scoring calibrated by stratified expert ratings. On cross-dataset segmentation transfer (train on OpenEarthMap, evaluate on LoveDA), domain-adapted SAM2 improves mask quality; meanwhile, dual-step MLLM fine-tuning yields more accurate taxonomy-aligned tags and more informative mask-grounded scene descriptions."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-23T06:23:56Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    6,
                    23,
                    56,
                    1,
                    266,
                    0
                ],
                "arxiv_comment": "The project is available at https://charlescsyyy.github.io/MVT",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Siyi Chen"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Weicong Pang"
                    },
                    {
                        "name": "Ruiming Yang"
                    },
                    {
                        "name": "Ziru Chen"
                    },
                    {
                        "name": "Renjun Gao"
                    },
                    {
                        "name": "Alexis Kai Hon Lau"
                    },
                    {
                        "name": "Dasa Gu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Cheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Li"
                },
                "author": "Cheng Li"
            },
            {
                "id": "http://arxiv.org/abs/2601.05162v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05162v1",
                "title": "GenAI-DrawIO-Creator: A Framework for Automated Diagram Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenAI-DrawIO-Creator: A Framework for Automated Diagram Generation"
                },
                "updated": "2026-01-08T17:51:35Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    51,
                    35,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05162v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diagrams are crucial for communicating complex information, yet creating and modifying them remains a labor-intensive task. We present GenAI-DrawIO-Creator, a novel framework that leverages Large Language Models (LLMs) to automate diagram generation and manipulation in the structured XML format used by draw.io. Our system integrates Claude 3.7 to reason about structured visual data and produce valid diagram representations. Key contributions include a high-level system design enabling real-time diagram updates, specialized prompt engineering and error-checking to ensure well-formed XML outputs. We demonstrate a working prototype capable of generating accurate diagrams (such as network architectures and flowcharts) from natural language or code, and even replicating diagrams from images. Simulated evaluations show that our approach significantly reduces diagram creation time and produces outputs with high structural fidelity. Our results highlight the promise of Claude 3.7 in handling structured visual reasoning tasks and lay the groundwork for future research in AI-assisted diagramming applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagrams are crucial for communicating complex information, yet creating and modifying them remains a labor-intensive task. We present GenAI-DrawIO-Creator, a novel framework that leverages Large Language Models (LLMs) to automate diagram generation and manipulation in the structured XML format used by draw.io. Our system integrates Claude 3.7 to reason about structured visual data and produce valid diagram representations. Key contributions include a high-level system design enabling real-time diagram updates, specialized prompt engineering and error-checking to ensure well-formed XML outputs. We demonstrate a working prototype capable of generating accurate diagrams (such as network architectures and flowcharts) from natural language or code, and even replicating diagrams from images. Simulated evaluations show that our approach significantly reduces diagram creation time and produces outputs with high structural fidelity. Our results highlight the promise of Claude 3.7 in handling structured visual reasoning tasks and lay the groundwork for future research in AI-assisted diagramming applications."
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T17:51:35Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    51,
                    35,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Jinze Yu"
                    },
                    {
                        "name": "Dayuan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Dayuan Jiang"
                },
                "author": "Dayuan Jiang"
            },
            {
                "id": "http://arxiv.org/abs/2601.05148v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05148v1",
                "title": "Atlas 2 -- Foundation models for clinical deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atlas 2 -- Foundation models for clinical deployment"
                },
                "updated": "2026-01-08T17:37:00Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    37,
                    0,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05148v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Pathology foundation models substantially advanced the possibilities in computational pathology -- yet tradeoffs in terms of performance, robustness, and computational requirements remained, which limited their clinical deployment. In this report, we present Atlas 2, Atlas 2-B, and Atlas 2-S, three pathology vision foundation models which bridge these shortcomings by showing state-of-the-art performance in prediction performance, robustness, and resource efficiency in a comprehensive evaluation across eighty public benchmarks. Our models were trained on the largest pathology foundation model dataset to date comprising 5.5 million histopathology whole slide images, collected from three medical institutions Charit - Universttsmedizin Berlin, LMU Munich, and Mayo Clinic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pathology foundation models substantially advanced the possibilities in computational pathology -- yet tradeoffs in terms of performance, robustness, and computational requirements remained, which limited their clinical deployment. In this report, we present Atlas 2, Atlas 2-B, and Atlas 2-S, three pathology vision foundation models which bridge these shortcomings by showing state-of-the-art performance in prediction performance, robustness, and resource efficiency in a comprehensive evaluation across eighty public benchmarks. Our models were trained on the largest pathology foundation model dataset to date comprising 5.5 million histopathology whole slide images, collected from three medical institutions Charit - Universttsmedizin Berlin, LMU Munich, and Mayo Clinic."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T17:37:00Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    37,
                    0,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Maximilian Alber"
                    },
                    {
                        "name": "Timo Milbich"
                    },
                    {
                        "name": "Alexandra Carpen-Amarie"
                    },
                    {
                        "name": "Stephan Tietz"
                    },
                    {
                        "name": "Jonas Dippel"
                    },
                    {
                        "name": "Lukas Muttenthaler"
                    },
                    {
                        "name": "Beatriz Perez Cancer"
                    },
                    {
                        "name": "Alessandro Benetti"
                    },
                    {
                        "name": "Panos Korfiatis"
                    },
                    {
                        "name": "Elias Eulig"
                    },
                    {
                        "name": "Jrme Lscher"
                    },
                    {
                        "name": "Jiasen Wu"
                    },
                    {
                        "name": "Sayed Abid Hashimi"
                    },
                    {
                        "name": "Gabriel Dernbach"
                    },
                    {
                        "name": "Simon Schallenberg"
                    },
                    {
                        "name": "Neelay Shah"
                    },
                    {
                        "name": "Moritz Krgener"
                    },
                    {
                        "name": "Aniruddh Jammoria"
                    },
                    {
                        "name": "Jake Matras"
                    },
                    {
                        "name": "Patrick Duffy"
                    },
                    {
                        "name": "Matt Redlon"
                    },
                    {
                        "name": "Philipp Jurmeister"
                    },
                    {
                        "name": "David Horst"
                    },
                    {
                        "name": "Lukas Ruff"
                    },
                    {
                        "name": "Klaus-Robert Mller"
                    },
                    {
                        "name": "Frederick Klauschen"
                    },
                    {
                        "name": "Andrew Norgan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Norgan"
                },
                "author": "Andrew Norgan"
            },
            {
                "id": "http://arxiv.org/abs/2601.04083v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04083v2",
                "title": "Cells on Autopilot: Adaptive Cell (Re)Selection via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cells on Autopilot: Adaptive Cell (Re)Selection via Reinforcement Learning"
                },
                "updated": "2026-01-08T17:32:37Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    32,
                    37,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04083v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04083v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The widespread deployment of 5G networks, together with the coexistence of 4G/LTE networks, provides mobile devices a diverse set of candidate cells to connect to. However, associating mobile devices to cells to maximize overall network performance, a.k.a. cell (re)selection, remains a key challenge for mobile operators. Today, cell (re)selection parameters are typically configured manually based on operator experience and rarely adapted to dynamic network conditions. In this work, we ask: Can an agent automatically learn and adapt cell (re)selection parameters to consistently improve network performance? We present a reinforcement learning (RL)-based framework called CellPilot that adaptively tunes cell (re)selection parameters by learning spatiotemporal patterns of mobile network dynamics. Our study with real-world data demonstrates that even a lightweight RL agent can outperform conventional heuristic reconfigurations by up to 167%, while generalizing effectively across different network scenarios. These results indicate that data-driven approaches can significantly improve cell (re)selection configurations and enhance mobile network performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of 5G networks, together with the coexistence of 4G/LTE networks, provides mobile devices a diverse set of candidate cells to connect to. However, associating mobile devices to cells to maximize overall network performance, a.k.a. cell (re)selection, remains a key challenge for mobile operators. Today, cell (re)selection parameters are typically configured manually based on operator experience and rarely adapted to dynamic network conditions. In this work, we ask: Can an agent automatically learn and adapt cell (re)selection parameters to consistently improve network performance? We present a reinforcement learning (RL)-based framework called CellPilot that adaptively tunes cell (re)selection parameters by learning spatiotemporal patterns of mobile network dynamics. Our study with real-world data demonstrates that even a lightweight RL agent can outperform conventional heuristic reconfigurations by up to 167%, while generalizing effectively across different network scenarios. These results indicate that data-driven approaches can significantly improve cell (re)selection configurations and enhance mobile network performance."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-07T16:51:33Z",
                "published_parsed": [
                    2026,
                    1,
                    7,
                    16,
                    51,
                    33,
                    2,
                    7,
                    0
                ],
                "arxiv_comment": "11 pages, 12 figures, v2: Corrected performance numbers in the conclusion; no change to methodology",
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Marvin Illian"
                    },
                    {
                        "name": "Ramin Khalili"
                    },
                    {
                        "name": "Antonio A. de A. Rocha"
                    },
                    {
                        "name": "Lin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lin Wang"
                },
                "author": "Lin Wang"
            },
            {
                "id": "http://arxiv.org/abs/2601.05144v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05144v1",
                "title": "Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models"
                },
                "updated": "2026-01-08T17:32:22Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    32,
                    22,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05144v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reasoning Large Language Models (RLLMs) excelling in complex tasks present unique challenges for digital watermarking, as existing methods often disrupt logical coherence or incur high computational costs. Token-based watermarking techniques can corrupt the reasoning flow by applying pseudo-random biases, while semantic-aware approaches improve quality but introduce significant latency or require auxiliary models. This paper introduces ReasonMark, a novel watermarking framework specifically designed for reasoning-intensive LLMs. Our approach decouples generation into an undisturbed Thinking Phase and a watermarked Answering Phase. We propose a Criticality Score to identify semantically pivotal tokens from the reasoning trace, which are distilled into a Principal Semantic Vector (PSV). The PSV then guides a semantically-adaptive mechanism that modulates watermark strength based on token-PSV alignment, ensuring robustness without compromising logical integrity. Extensive experiments show ReasonMark surpasses state-of-the-art methods by reducing text Perplexity by 0.35, increasing translation BLEU score by 0.164, and raising mathematical accuracy by 0.67 points. These advancements are achieved alongside a 0.34% higher watermark detection AUC and stronger robustness to attacks, all with a negligible increase in latency. This work enables the traceable and trustworthy deployment of reasoning LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Large Language Models (RLLMs) excelling in complex tasks present unique challenges for digital watermarking, as existing methods often disrupt logical coherence or incur high computational costs. Token-based watermarking techniques can corrupt the reasoning flow by applying pseudo-random biases, while semantic-aware approaches improve quality but introduce significant latency or require auxiliary models. This paper introduces ReasonMark, a novel watermarking framework specifically designed for reasoning-intensive LLMs. Our approach decouples generation into an undisturbed Thinking Phase and a watermarked Answering Phase. We propose a Criticality Score to identify semantically pivotal tokens from the reasoning trace, which are distilled into a Principal Semantic Vector (PSV). The PSV then guides a semantically-adaptive mechanism that modulates watermark strength based on token-PSV alignment, ensuring robustness without compromising logical integrity. Extensive experiments show ReasonMark surpasses state-of-the-art methods by reducing text Perplexity by 0.35, increasing translation BLEU score by 0.164, and raising mathematical accuracy by 0.67 points. These advancements are achieved alongside a 0.34% higher watermark detection AUC and stronger robustness to attacks, all with a negligible increase in latency. This work enables the traceable and trustworthy deployment of reasoning LLMs in real-world applications."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T17:32:22Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    32,
                    22,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Shuliang Liu"
                    },
                    {
                        "name": "Xingyu Li"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Bingchen Duan"
                    },
                    {
                        "name": "Qi Zheng"
                    },
                    {
                        "name": "Dong Fang"
                    },
                    {
                        "name": "Lingfeng Su"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu"
            },
            {
                "id": "http://arxiv.org/abs/2503.04739v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.04739v2",
                "title": "A Framework for Responsible AI Systems: Building Societal Trust through Domain Definition, Trustworthy AI Design, Auditability, Accountability, and Governance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Responsible AI Systems: Building Societal Trust through Domain Definition, Trustworthy AI Design, Auditability, Accountability, and Governance"
                },
                "updated": "2026-01-08T17:22:46Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    22,
                    46,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.04739v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.04739v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Responsible Artificial Intelligence (RAI) addresses the ethical and regulatory challenges of deploying AI systems in high-risk scenarios. This paper proposes a comprehensive framework for the design of an RAI system (RAIS) that integrates five key dimensions: domain definition, trustworthy AI design, auditability, accountability, and governance. Unlike prior work that treats these components in isolation, our proposal emphasizes their inter-dependencies and iterative feedback loops, enabling proactive and reactive accountability throughout the AI lifecycle. Beyond presenting the framework, we synthesize recent developments in global AI governance and analyze limitations in existing principles-based approaches, highlighting fragmentation, implementation gaps, and the need for participatory governance. The paper also identifies critical challenges and research directions for the RAIS framework, including sector-specific adaptation and operationalization, to support certification, post-deployment monitoring, and risk-based auditing. By bridging technical design and institutional responsibility, this work offers a practical blueprint for embedding responsibility throughout the AI lifecycle, enabling transparent, ethically aligned, and legally compliant AI-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsible Artificial Intelligence (RAI) addresses the ethical and regulatory challenges of deploying AI systems in high-risk scenarios. This paper proposes a comprehensive framework for the design of an RAI system (RAIS) that integrates five key dimensions: domain definition, trustworthy AI design, auditability, accountability, and governance. Unlike prior work that treats these components in isolation, our proposal emphasizes their inter-dependencies and iterative feedback loops, enabling proactive and reactive accountability throughout the AI lifecycle. Beyond presenting the framework, we synthesize recent developments in global AI governance and analyze limitations in existing principles-based approaches, highlighting fragmentation, implementation gaps, and the need for participatory governance. The paper also identifies critical challenges and research directions for the RAIS framework, including sector-specific adaptation and operationalization, to support certification, post-deployment monitoring, and risk-based auditing. By bridging technical design and institutional responsibility, this work offers a practical blueprint for embedding responsibility throughout the AI lifecycle, enabling transparent, ethically aligned, and legally compliant AI-based systems."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-04T14:47:30Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    47,
                    30,
                    1,
                    35,
                    0
                ],
                "arxiv_comment": "27 pages, 9 figures, 2 tables",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Andrs Herrera-Poyatos"
                    },
                    {
                        "name": "Javier Del Ser"
                    },
                    {
                        "name": "Marcos Lpez de Prado"
                    },
                    {
                        "name": "Fei-Yue Wang"
                    },
                    {
                        "name": "Enrique Herrera-Viedma"
                    },
                    {
                        "name": "Francisco Herrera"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Herrera"
                },
                "author": "Francisco Herrera"
            },
            {
                "id": "http://arxiv.org/abs/2503.19850v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.19850v3",
                "title": "FALCONEye: Finding Answers and Localizing Content in ONE-hour-long videos with multi-modal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FALCONEye: Finding Answers and Localizing Content in ONE-hour-long videos with multi-modal LLMs"
                },
                "updated": "2026-01-08T17:17:54Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    17,
                    54,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.19850v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.19850v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Finding information in hour-long videos is a challenging task even for top-performing Vision Language Models (VLMs), as encoding visual content quickly exceeds available context windows. To tackle this challenge, we present FALCONEye, a novel video agent based on a training-free, model-agnostic meta-architecture composed of a VLM and a Large Language Model (LLM). FALCONEye answers open-ended questions using an exploration-based search algorithm guided by calibrated confidence from the VLM's answers. We also introduce the FALCON-Bench benchmark, extending Question Answering problem to Video Answer Search-requiring models to return both the answer and its supporting temporal window for open-ended questions in hour-long videos. With just a 7B VLM and a lightweight LLM, FALCONEye outscores all open-source 7B VLMs and comparable agents in FALCON-Bench. It further demonstrates its generalization capability in MLVU benchmark with shorter videos and different tasks, surpassing GPT-4o on single-detail tasks while slashing inference cost by roughly an order of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding information in hour-long videos is a challenging task even for top-performing Vision Language Models (VLMs), as encoding visual content quickly exceeds available context windows. To tackle this challenge, we present FALCONEye, a novel video agent based on a training-free, model-agnostic meta-architecture composed of a VLM and a Large Language Model (LLM). FALCONEye answers open-ended questions using an exploration-based search algorithm guided by calibrated confidence from the VLM's answers. We also introduce the FALCON-Bench benchmark, extending Question Answering problem to Video Answer Search-requiring models to return both the answer and its supporting temporal window for open-ended questions in hour-long videos. With just a 7B VLM and a lightweight LLM, FALCONEye outscores all open-source 7B VLMs and comparable agents in FALCON-Bench. It further demonstrates its generalization capability in MLVU benchmark with shorter videos and different tasks, surpassing GPT-4o on single-detail tasks while slashing inference cost by roughly an order of magnitude."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-25T17:17:19Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    17,
                    19,
                    1,
                    84,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Carlos Plou"
                    },
                    {
                        "name": "Cesar Borja"
                    },
                    {
                        "name": "Ruben Martinez-Cantin"
                    },
                    {
                        "name": "Ana C. Murillo"
                    }
                ],
                "author_detail": {
                    "name": "Ana C. Murillo"
                },
                "author": "Ana C. Murillo"
            },
            {
                "id": "http://arxiv.org/abs/2601.05114v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05114v1",
                "title": "Evaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior"
                },
                "updated": "2026-01-08T17:02:22Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    2,
                    22,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05114v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-as-judge systems promise scalable, consistent evaluation. We find the opposite: judges are consistent, but not with each other; they are consistent with themselves. Across 3,240 evaluations (9 judges x 120 unique video x pack items x 3 independent runs), inter-judge agreement is near-zero (Krippendorff's  = 0.042). On two dimensions, judges disagree more than random noise would predict ( < 0). Yet this disagreement isn't chaos; it's structured. A classifier identifies which judge produced an evaluation with 77.1% accuracy from rubric scores alone, rising to 89.9% with disposition features. Within model families, the signal is even stronger: GPT-4.1 and GPT-5.2 are distinguishable with 99.6% accuracy. We call this the reliability paradox: judges cannot agree on what constitutes quality, yet their disagreement patterns are so stable they function as fingerprints. Each judge implements a distinct, stable theory of quality: an \"evaluative disposition\" that shapes how it interprets any rubric. We characterize these dispositions along multiple axes: harshness/leniency, dimension emphasis, within-judge stability (ICC), and evidence behavior (receipt validity, semantic linkage via NLI, and shotgun index). The implication is stark: LLM judges are not interchangeable instruments measuring a shared construct. They are distinct measurement devices, each encoding its own implicit theory of quality. Averaging their scores produces a synthetic verdict that corresponds to no judge's actual values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-judge systems promise scalable, consistent evaluation. We find the opposite: judges are consistent, but not with each other; they are consistent with themselves. Across 3,240 evaluations (9 judges x 120 unique video x pack items x 3 independent runs), inter-judge agreement is near-zero (Krippendorff's  = 0.042). On two dimensions, judges disagree more than random noise would predict ( < 0). Yet this disagreement isn't chaos; it's structured. A classifier identifies which judge produced an evaluation with 77.1% accuracy from rubric scores alone, rising to 89.9% with disposition features. Within model families, the signal is even stronger: GPT-4.1 and GPT-5.2 are distinguishable with 99.6% accuracy. We call this the reliability paradox: judges cannot agree on what constitutes quality, yet their disagreement patterns are so stable they function as fingerprints. Each judge implements a distinct, stable theory of quality: an \"evaluative disposition\" that shapes how it interprets any rubric. We characterize these dispositions along multiple axes: harshness/leniency, dimension emphasis, within-judge stability (ICC), and evidence behavior (receipt validity, semantic linkage via NLI, and shotgun index). The implication is stark: LLM judges are not interchangeable instruments measuring a shared construct. They are distinct measurement devices, each encoding its own implicit theory of quality. Averaging their scores produces a synthetic verdict that corresponds to no judge's actual values."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T17:02:22Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    2,
                    22,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "23 pages, 6 figures, code and artifacts at : https://github.com/wajid-nasser/evaluative-fingerprints",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Wajid Nasser"
                    }
                ],
                "author_detail": {
                    "name": "Wajid Nasser"
                },
                "author": "Wajid Nasser"
            },
            {
                "id": "http://arxiv.org/abs/2507.11939v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.11939v2",
                "title": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering"
                },
                "updated": "2026-01-08T17:00:25Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    17,
                    0,
                    25,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.11939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.11939v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Charts are a universally adopted medium for data communication, yet existing chart understanding benchmarks are overwhelmingly English-centric, limiting their accessibility and relevance to global audiences. To address this limitation, we introduce PolyChartQA, the first large-scale multilingual benchmark for chart question answering, comprising 22,606 charts and 26,151 QA pairs across 10 diverse languages. PolyChartQA is constructed through a scalable pipeline that enables efficient multilingual chart generation via data translation and code reuse, supported by LLM-based translation and rigorous quality control. We systematically evaluate multilingual chart understanding with PolyChartQA on state-of-the-art LVLMs and reveal a significant performance gap between English and other languages, particularly low-resource ones. Additionally, we introduce a companion multilingual chart question answering training set, PolyChartQA-Train, on which fine-tuning LVLMs yields substantial gains in multilingual chart understanding across diverse model sizes and architectures. Together, our benchmark provides a foundation for developing globally inclusive vision-language models capable of understanding charts across diverse linguistic contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charts are a universally adopted medium for data communication, yet existing chart understanding benchmarks are overwhelmingly English-centric, limiting their accessibility and relevance to global audiences. To address this limitation, we introduce PolyChartQA, the first large-scale multilingual benchmark for chart question answering, comprising 22,606 charts and 26,151 QA pairs across 10 diverse languages. PolyChartQA is constructed through a scalable pipeline that enables efficient multilingual chart generation via data translation and code reuse, supported by LLM-based translation and rigorous quality control. We systematically evaluate multilingual chart understanding with PolyChartQA on state-of-the-art LVLMs and reveal a significant performance gap between English and other languages, particularly low-resource ones. Additionally, we introduce a companion multilingual chart question answering training set, PolyChartQA-Train, on which fine-tuning LVLMs yields substantial gains in multilingual chart understanding across diverse model sizes and architectures. Together, our benchmark provides a foundation for developing globally inclusive vision-language models capable of understanding charts across diverse linguistic contexts."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-16T06:09:02Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    9,
                    2,
                    2,
                    197,
                    0
                ],
                "arxiv_comment": "Work in Progress",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yichen Xu"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Jianzhe Ma"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Qin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Qin Jin"
                },
                "author": "Qin Jin"
            },
            {
                "id": "http://arxiv.org/abs/2601.05111v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05111v1",
                "title": "Agent-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-as-a-Judge"
                },
                "updated": "2026-01-08T16:58:10Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    58,
                    10,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05111v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:58:10Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    58,
                    10,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Runyang You"
                    },
                    {
                        "name": "Hongru Cai"
                    },
                    {
                        "name": "Caiqi Zhang"
                    },
                    {
                        "name": "Qiancheng Xu"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li"
            },
            {
                "id": "http://arxiv.org/abs/2601.05109v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05109v1",
                "title": "Nalar: An agent serving framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nalar: An agent serving framework"
                },
                "updated": "2026-01-08T16:56:40Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    56,
                    40,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05109v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-driven agentic applications increasingly automate complex, multi-step tasks, but serving them efficiently remains challenging due to heterogeneous components, dynamic and model-driven control flow, long-running state, and unpredictable latencies. Nalar is a ground-up agent-serving framework that cleanly separates workflow specification from execution while providing the runtime visibility and control needed for robust performance. Nalar preserves full Python expressiveness, using lightweight auto-generated stubs that turn agent and tool invocations into futures carrying dependency and context metadata. A managed state layer decouples logical state from physical placement, enabling safe reuse, migration, and consistent retry behavior. A two-level control architecture combines global policy computation with local event-driven enforcement to support adaptive routing, scheduling, and resource management across evolving workflows. Together, these mechanisms allow Nalar to deliver scalable, efficient, and policy-driven serving of heterogeneous agentic applications without burdening developers with orchestration logic. Across three agentic workloads, Nalar cuts tail latency by 34--74\\%, achieves up to $2.9\\times$ speedups, sustains 80 RPS where baselines fail, and scales to 130K futures with sub-500 ms control overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-driven agentic applications increasingly automate complex, multi-step tasks, but serving them efficiently remains challenging due to heterogeneous components, dynamic and model-driven control flow, long-running state, and unpredictable latencies. Nalar is a ground-up agent-serving framework that cleanly separates workflow specification from execution while providing the runtime visibility and control needed for robust performance. Nalar preserves full Python expressiveness, using lightweight auto-generated stubs that turn agent and tool invocations into futures carrying dependency and context metadata. A managed state layer decouples logical state from physical placement, enabling safe reuse, migration, and consistent retry behavior. A two-level control architecture combines global policy computation with local event-driven enforcement to support adaptive routing, scheduling, and resource management across evolving workflows. Together, these mechanisms allow Nalar to deliver scalable, efficient, and policy-driven serving of heterogeneous agentic applications without burdening developers with orchestration logic. Across three agentic workloads, Nalar cuts tail latency by 34--74\\%, achieves up to $2.9\\times$ speedups, sustains 80 RPS where baselines fail, and scales to 130K futures with sub-500 ms control overhead."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:56:40Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    56,
                    40,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Marco Laju"
                    },
                    {
                        "name": "Donghyun Son"
                    },
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Nitin Kedia"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Jayanth Srinivasa"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella"
            },
            {
                "id": "http://arxiv.org/abs/2601.05107v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05107v1",
                "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction"
                },
                "updated": "2026-01-08T16:54:30Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    54,
                    30,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05107v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \\textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \\textbf{Stee}rable \\textbf{M}emory Agent, \\texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \\textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \\textbf{Stee}rable \\textbf{M}emory Agent, \\texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:54:30Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    54,
                    30,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Muzhao Tian"
                    },
                    {
                        "name": "Zisu Huang"
                    },
                    {
                        "name": "Xiaohua Wang"
                    },
                    {
                        "name": "Jingwen Xu"
                    },
                    {
                        "name": "Zhengkang Guo"
                    },
                    {
                        "name": "Qi Qian"
                    },
                    {
                        "name": "Yuanzhe Shen"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Jiakang Yuan"
                    },
                    {
                        "name": "Changze Lv"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqing Zheng"
                },
                "author": "Xiaoqing Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2601.05106v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05106v1",
                "title": "Token-Level LLM Collaboration via FusionRoute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Level LLM Collaboration via FusionRoute"
                },
                "updated": "2026-01-08T16:53:16Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    53,
                    16,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05106v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:53:16Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    53,
                    16,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "25 pages",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Nuoya Xiong"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Hanqing Zeng"
                    },
                    {
                        "name": "Zhaorun Chen"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Shuchao Bi"
                    },
                    {
                        "name": "Lizhu Zhang"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhuokai Zhao"
                },
                "author": "Zhuokai Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.16282v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16282v2",
                "title": "CALM: A CKA-Guided Adaptive Layer-Wise Modularization Framework for LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALM: A CKA-Guided Adaptive Layer-Wise Modularization Framework for LLM Quantization"
                },
                "updated": "2026-01-08T16:51:18Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    51,
                    18,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16282v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16282v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CALM (A CKA-guided Adaptive Layer-wise Modularization)a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. CALM independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMsincluding LLaMA and Qwenin terms of perplexity (PPL) and downstream task performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CALM (A CKA-guided Adaptive Layer-wise Modularization)a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. CALM independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMsincluding LLaMA and Qwenin terms of perplexity (PPL) and downstream task performance."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T08:01:19Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    8,
                    1,
                    19,
                    3,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jinhao Zhang"
                    },
                    {
                        "name": "Yunquan Zhang"
                    },
                    {
                        "name": "Daning Chen"
                    },
                    {
                        "name": "JunSun"
                    },
                    {
                        "name": "Zicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Zicheng Yan"
                },
                "author": "Zicheng Yan"
            },
            {
                "id": "http://arxiv.org/abs/2601.05103v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05103v1",
                "title": "Semantically Orthogonal Framework for Citation Classification: Disentangling Intent and Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantically Orthogonal Framework for Citation Classification: Disentangling Intent and Content"
                },
                "updated": "2026-01-08T16:48:36Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    48,
                    36,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05103v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1007/978-3-032-05409-8_12",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Understanding the role of citations is essential for research assessment and citation-aware digital libraries. However, existing citation classification frameworks often conflate citation intent (why a work is cited) with cited content type (what part is cited), limiting their effectiveness in auto classification due to a dilemma between fine-grained type distinctions and practical classification reliability. We introduce SOFT, a Semantically Orthogonal Framework with Two dimensions that explicitly separates citation intent from cited content type, drawing inspiration from semantic role theory. We systematically re-annotate the ACL-ARC dataset using SOFT and release a cross-disciplinary test set sampled from ACT2. Evaluation with both zero-shot and fine-tuned Large Language Models demonstrates that SOFT enables higher agreement between human annotators and LLMs, and supports stronger classification performance and robust cross-domain generalization compared to ACL-ARC and SciCite annotation frameworks. These results confirm SOFT's value as a clear, reusable annotation standard, improving clarity, consistency, and generalizability for digital libraries and scholarly communication infrastructures. All code and data are publicly available on GitHub https://github.com/zhiyintan/SOFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the role of citations is essential for research assessment and citation-aware digital libraries. However, existing citation classification frameworks often conflate citation intent (why a work is cited) with cited content type (what part is cited), limiting their effectiveness in auto classification due to a dilemma between fine-grained type distinctions and practical classification reliability. We introduce SOFT, a Semantically Orthogonal Framework with Two dimensions that explicitly separates citation intent from cited content type, drawing inspiration from semantic role theory. We systematically re-annotate the ACL-ARC dataset using SOFT and release a cross-disciplinary test set sampled from ACT2. Evaluation with both zero-shot and fine-tuned Large Language Models demonstrates that SOFT enables higher agreement between human annotators and LLMs, and supports stronger classification performance and robust cross-domain generalization compared to ACL-ARC and SciCite annotation frameworks. These results confirm SOFT's value as a clear, reusable annotation standard, improving clarity, consistency, and generalizability for digital libraries and scholarly communication infrastructures. All code and data are publicly available on GitHub https://github.com/zhiyintan/SOFT."
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:48:36Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    48,
                    36,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Accepted at the 29th International Conference on Theory and Practice of Digital Libraries (TPDL 2025)",
                "arxiv_primary_category": {
                    "term": "cs.DL"
                },
                "authors": [
                    {
                        "name": "Changxu Duan"
                    },
                    {
                        "name": "Zhiyin Tan"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyin Tan"
                },
                "author": "Zhiyin Tan",
                "arxiv_doi": "10.1007/978-3-032-05409-8_12"
            },
            {
                "id": "http://arxiv.org/abs/2601.05101v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05101v1",
                "title": "Arabic Prompts with English Tools: A Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabic Prompts with English Tools: A Benchmark"
                },
                "updated": "2026-01-08T16:47:09Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    47,
                    9,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05101v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are now integral to numerous industries, increasingly serving as the core reasoning engine for autonomous agents that perform complex tasks through tool-use. While the development of Arabic-native LLMs is accelerating, the benchmarks for evaluating their capabilities lag behind, with most existing frameworks focusing on English. A critical and overlooked area is tool-calling, where the performance of models prompted in non-English languages like Arabic is poorly understood, especially since these models are often pretrained on predominantly English data. This paper addresses this critical gap by introducing the first dedicated benchmark for evaluating the tool-calling and agentic capabilities of LLMs in the Arabic language. Our work provides a standardized framework to measure the functional accuracy and robustness of models in Arabic agentic workflows. Our findings reveal a huge performance gap: when users interact in Arabic, tool-calling accuracy drops by an average of 5-10\\%, regardless of whether the tool descriptions themselves are in Arabic or English. By shedding light on these critical challenges, this benchmark aims to foster the development of more reliable and linguistically equitable AI agents for Arabic-speaking users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are now integral to numerous industries, increasingly serving as the core reasoning engine for autonomous agents that perform complex tasks through tool-use. While the development of Arabic-native LLMs is accelerating, the benchmarks for evaluating their capabilities lag behind, with most existing frameworks focusing on English. A critical and overlooked area is tool-calling, where the performance of models prompted in non-English languages like Arabic is poorly understood, especially since these models are often pretrained on predominantly English data. This paper addresses this critical gap by introducing the first dedicated benchmark for evaluating the tool-calling and agentic capabilities of LLMs in the Arabic language. Our work provides a standardized framework to measure the functional accuracy and robustness of models in Arabic agentic workflows. Our findings reveal a huge performance gap: when users interact in Arabic, tool-calling accuracy drops by an average of 5-10\\%, regardless of whether the tool descriptions themselves are in Arabic or English. By shedding light on these critical challenges, this benchmark aims to foster the development of more reliable and linguistically equitable AI agents for Arabic-speaking users."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:47:09Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    47,
                    9,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "10 pages, 10 figures, LLMs, Big Data, and Multilinguality for All (LLMs4All) Workshop at IEEE BigData 2025 Conference, Macau, December 10, 2025",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Konstantin Kubrak"
                    },
                    {
                        "name": "Ahmed El-Moselhy"
                    },
                    {
                        "name": "Ammar Alsulami"
                    },
                    {
                        "name": "Remaz Altuwaim"
                    },
                    {
                        "name": "Hassan Ismail Fawaz"
                    },
                    {
                        "name": "Faisal Alsaby"
                    }
                ],
                "author_detail": {
                    "name": "Faisal Alsaby"
                },
                "author": "Faisal Alsaby"
            },
            {
                "id": "http://arxiv.org/abs/2506.15480v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.15480v2",
                "title": "Instruction Tuning with and without Context: Behavioral Shifts and Downstream Impact",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Tuning with and without Context: Behavioral Shifts and Downstream Impact"
                },
                "updated": "2026-01-08T16:32:25Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    32,
                    25,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.15480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.15480v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Instruction tuning is a widely used approach to improve the instruction-following ability of large language models (LLMs). Instruction-tuning datasets typically include a mixture of context-augmented and context-free examples, yet prior work has largely combined these data types without examining their distinct effects. In this paper, we investigate how training LLMs with or without context affects model behavior and downstream performance. First, in the text domain, we show that LLMs trained with context attend more strongly to the provided knowledge, achieving better grounding. We also observe that context-augmented training shifts how LLMs use knowledge: models store and leverage less on parametric knowledge and instead depend more on the provided context. Second, we observe that using LLM trained with context-augmented data as the backbone for vision-language models reduces hallucination and improves grounding in the visual domain. Finally, we explore practical strategies for real-world deployments where context availability varies. We show that maintaining separate context-augmented and context-free models and routing inputs between them yields more robust overall performance than training a single mixed model, as it better preserves their complementary strengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning is a widely used approach to improve the instruction-following ability of large language models (LLMs). Instruction-tuning datasets typically include a mixture of context-augmented and context-free examples, yet prior work has largely combined these data types without examining their distinct effects. In this paper, we investigate how training LLMs with or without context affects model behavior and downstream performance. First, in the text domain, we show that LLMs trained with context attend more strongly to the provided knowledge, achieving better grounding. We also observe that context-augmented training shifts how LLMs use knowledge: models store and leverage less on parametric knowledge and instead depend more on the provided context. Second, we observe that using LLM trained with context-augmented data as the backbone for vision-language models reduces hallucination and improves grounding in the visual domain. Finally, we explore practical strategies for real-world deployments where context availability varies. We show that maintaining separate context-augmented and context-free models and routing inputs between them yields more robust overall performance than training a single mixed model, as it better preserves their complementary strengths."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-18T14:13:56Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    14,
                    13,
                    56,
                    2,
                    169,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hyunji Lee"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Yunjae Won"
                    },
                    {
                        "name": "Hanseok Oh"
                    },
                    {
                        "name": "Geewook Kim"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Minjoon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Minjoon Seo"
                },
                "author": "Minjoon Seo"
            },
            {
                "id": "http://arxiv.org/abs/2506.03259v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.03259v2",
                "title": "Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems"
                },
                "updated": "2026-01-08T16:22:24Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    22,
                    24,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.03259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.03259v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Purpose: This study aims to evaluate the effectiveness of large language models (LLMs) in automating disease annotation of CT radiology reports. We compare a rule-based algorithm (RBA), RadBERT, and three lightweight open-weight LLMs for multi-disease labeling of chest, abdomen, and pelvis (CAP) CT reports.\n  Materials and Methods: This retrospective study analyzed 40,833 chest-abdomen-pelvis (CAP) CT reports from 29,540 patients, with 1,789 reports manually annotated across three organ systems. External validation was conducted using the CT RATE dataset. Three open-weight LLMs were tested with zero-shot prompting. Performance was evaluated using Cohen's Kappa ($$) and micro/macro-averaged F1 scores.\n  Results: In the internal test set of 12,197 CAP reports from 8,854 patients, Llama-3.1 8B and Gemma-3 27B showed the highest agreement ($$ median: 0.87). On the manually annotated set, Gemma-3 27B achieved the top macro-F1 (0.82), followed by Llama-3.1 8B (0.79), while the RBA scored lowest (0.64). On the CT RATE dataset (lungs/pleura labels only), Llama-3.1 8B performed best (0.91), with Gemma-3 27B close behind (0.89). Performance differences were mainly due to differing labeling practices, especially for labels with high subjectivity such as atelectasis.\n  Conclusion: Lightweight LLMs outperform rule-based methods for CT report annotation and generalize across organ systems with zero-shot prompting. However, binary labels alone cannot capture the full nuance of report language. LLMs can provide a flexible, efficient solution aligned with clinical judgment and user needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: This study aims to evaluate the effectiveness of large language models (LLMs) in automating disease annotation of CT radiology reports. We compare a rule-based algorithm (RBA), RadBERT, and three lightweight open-weight LLMs for multi-disease labeling of chest, abdomen, and pelvis (CAP) CT reports.\n  Materials and Methods: This retrospective study analyzed 40,833 chest-abdomen-pelvis (CAP) CT reports from 29,540 patients, with 1,789 reports manually annotated across three organ systems. External validation was conducted using the CT RATE dataset. Three open-weight LLMs were tested with zero-shot prompting. Performance was evaluated using Cohen's Kappa ($$) and micro/macro-averaged F1 scores.\n  Results: In the internal test set of 12,197 CAP reports from 8,854 patients, Llama-3.1 8B and Gemma-3 27B showed the highest agreement ($$ median: 0.87). On the manually annotated set, Gemma-3 27B achieved the top macro-F1 (0.82), followed by Llama-3.1 8B (0.79), while the RBA scored lowest (0.64). On the CT RATE dataset (lungs/pleura labels only), Llama-3.1 8B performed best (0.91), with Gemma-3 27B close behind (0.89). Performance differences were mainly due to differing labeling practices, especially for labels with high subjectivity such as atelectasis.\n  Conclusion: Lightweight LLMs outperform rule-based methods for CT report annotation and generalize across organ systems with zero-shot prompting. However, binary labels alone cannot capture the full nuance of report language. LLMs can provide a flexible, efficient solution aligned with clinical judgment and user needs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-03T18:00:08Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    0,
                    8,
                    1,
                    154,
                    0
                ],
                "arxiv_comment": "18 pages, 9 figures, to be submitted in Radiology: Artificial Intelligence",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Michael E. Garcia-Alcoser"
                    },
                    {
                        "name": "Mobina GhojoghNejad"
                    },
                    {
                        "name": "Fakrul Islam Tushar"
                    },
                    {
                        "name": "David Kim"
                    },
                    {
                        "name": "Kyle J. Lafata"
                    },
                    {
                        "name": "Geoffrey D. Rubin"
                    },
                    {
                        "name": "Joseph Y. Lo"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Y. Lo"
                },
                "author": "Joseph Y. Lo"
            },
            {
                "id": "http://arxiv.org/abs/2601.05076v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05076v1",
                "title": "Chain-of-Sanitized-Thoughts: Plugging PII Leakage in CoT of Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Sanitized-Thoughts: Plugging PII Leakage in CoT of Large Reasoning Models"
                },
                "updated": "2026-01-08T16:19:43Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    19,
                    43,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05076v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Reasoning Models (LRMs) improve performance, reliability, and interpretability by generating explicit chain-of-thought (CoT) reasoning, but this transparency introduces a serious privacy risk: intermediate reasoning often leaks personally identifiable information (PII) even when final answers are sanitized. We study how to induce privacy-first reasoning, where models reason without exposing sensitive information, using deployable interventions rather than post-hoc redaction. We introduce PII-CoT-Bench, a supervised dataset with privacy-aware CoT annotations, and a category-balanced evaluation benchmark covering realistic and adversarial leakage scenarios. Our results reveal a capability-dependent trend: state-of-the-art models benefit most from prompt-based controls, whereas weaker models require fine-tuning to achieve meaningful leakage reduction. Across models and categories, both approaches substantially reduce PII exposure with minimal degradation in utility, demonstrating that private reasoning can be achieved without sacrificing performance. Overall, we show that private CoT reasoning can be achieved with minimal utility loss, providing practical guidance for building privacy-preserving reasoning systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) improve performance, reliability, and interpretability by generating explicit chain-of-thought (CoT) reasoning, but this transparency introduces a serious privacy risk: intermediate reasoning often leaks personally identifiable information (PII) even when final answers are sanitized. We study how to induce privacy-first reasoning, where models reason without exposing sensitive information, using deployable interventions rather than post-hoc redaction. We introduce PII-CoT-Bench, a supervised dataset with privacy-aware CoT annotations, and a category-balanced evaluation benchmark covering realistic and adversarial leakage scenarios. Our results reveal a capability-dependent trend: state-of-the-art models benefit most from prompt-based controls, whereas weaker models require fine-tuning to achieve meaningful leakage reduction. Across models and categories, both approaches substantially reduce PII exposure with minimal degradation in utility, demonstrating that private reasoning can be achieved without sacrificing performance. Overall, we show that private CoT reasoning can be achieved with minimal utility loss, providing practical guidance for building privacy-preserving reasoning systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:19:43Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    19,
                    43,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "12 pages, 6 figures, 1 table",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Arghyadeep Das"
                    },
                    {
                        "name": "Sai Sreenivas Chintha"
                    },
                    {
                        "name": "Rishiraj Girmal"
                    },
                    {
                        "name": "Kinjal Pandey"
                    },
                    {
                        "name": "Sharvi Endait"
                    }
                ],
                "author_detail": {
                    "name": "Sharvi Endait"
                },
                "author": "Sharvi Endait"
            },
            {
                "id": "http://arxiv.org/abs/2601.05075v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05075v1",
                "title": "SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment"
                },
                "updated": "2026-01-08T16:19:24Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    19,
                    24,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05075v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:19:24Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    19,
                    24,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ziyang Chen"
                    },
                    {
                        "name": "Zhenxuan Huang"
                    },
                    {
                        "name": "Yile Wang"
                    },
                    {
                        "name": "Weiqin Wang"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Hui Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hui Huang"
                },
                "author": "Hui Huang"
            },
            {
                "id": "http://arxiv.org/abs/2510.17722v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.17722v2",
                "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues"
                },
                "updated": "2026-01-08T16:16:20Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    16,
                    20,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.17722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.17722v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. However, existing evaluation benchmarks remain limited to single-turn question answering, overlooking the complexity of multi-turn dialogues in real-world scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video understanding benchmark for evaluating MLLMs in multi-turn dialogues. Specifically, our MT-Video-Bench mainly assesses 6 core competencies that focus on perceptivity and interactivity, encompassing 1,000 meticulously curated multi-turn dialogues from diverse domains. These capabilities are rigorously aligned with real-world applications, such as interactive sports analysis and multi-turn video-based intelligent tutoring. With MT-Video-Bench, we extensively evaluate various state-of-the-art open-source and closed-source MLLMs, revealing their significant performance discrepancies and limitations in handling multi-turn video dialogues. The benchmark will be publicly available to foster future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. However, existing evaluation benchmarks remain limited to single-turn question answering, overlooking the complexity of multi-turn dialogues in real-world scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video understanding benchmark for evaluating MLLMs in multi-turn dialogues. Specifically, our MT-Video-Bench mainly assesses 6 core competencies that focus on perceptivity and interactivity, encompassing 1,000 meticulously curated multi-turn dialogues from diverse domains. These capabilities are rigorously aligned with real-world applications, such as interactive sports analysis and multi-turn video-based intelligent tutoring. With MT-Video-Bench, we extensively evaluate various state-of-the-art open-source and closed-source MLLMs, revealing their significant performance discrepancies and limitations in handling multi-turn video dialogues. The benchmark will be publicly available to foster future research."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-20T16:38:40Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    16,
                    38,
                    40,
                    0,
                    293,
                    0
                ],
                "arxiv_comment": "Project Website: https://github.com/NJU-LINK/MT-Video-Bench",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yaning Pan"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Guohui Zhang"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Yongqian Wen"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Haoxuan Hu"
                    },
                    {
                        "name": "Zhiyu Pan"
                    },
                    {
                        "name": "Yibing Huang"
                    },
                    {
                        "name": "Zhidong Gan"
                    },
                    {
                        "name": "Yonghong Lin"
                    },
                    {
                        "name": "An Ping"
                    },
                    {
                        "name": "Shihao Li"
                    },
                    {
                        "name": "Yanghai Wang"
                    },
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Jiaheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Liu"
                },
                "author": "Jiaheng Liu"
            },
            {
                "id": "http://arxiv.org/abs/2601.05062v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05062v1",
                "title": "Compositional Steering of Large Language Models with Steering Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Steering of Large Language Models with Steering Tokens"
                },
                "updated": "2026-01-08T16:08:44Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    8,
                    44,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05062v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \\textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \\emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \\textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \\textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \\textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \\textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \\emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \\textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \\textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \\textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T16:08:44Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    8,
                    44,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Gorjan Radevski"
                    },
                    {
                        "name": "Kiril Gashteovski"
                    },
                    {
                        "name": "Giwon Hong"
                    },
                    {
                        "name": "Carolin Lawrence"
                    },
                    {
                        "name": "Goran Glava"
                    }
                ],
                "author_detail": {
                    "name": "Goran Glava"
                },
                "author": "Goran Glava"
            },
            {
                "id": "http://arxiv.org/abs/2510.21007v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.21007v3",
                "title": "Can Confidence Estimates Decide When Chain-of-Thought Is Necessary for LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Confidence Estimates Decide When Chain-of-Thought Is Necessary for LLMs?"
                },
                "updated": "2026-01-08T16:05:16Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    16,
                    5,
                    16,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.21007v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.21007v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Chain-of-thought (CoT) prompting is a common technique for improving the reasoning abilities of large language models (LLMs). However, extended reasoning is often unnecessary and substantially increases token usage. As such, a key question becomes how to optimally allocate compute to when reasoning is actually needed. We study this through confidence-gated CoT, where a model produces a direct answer and a confidence estimate to decide whether to invoke CoT. We present an evaluation framework together with the first systematic study of confidence signals for this decision. We evaluate four representative confidence measures and compare them with random gating and an oracle upper bound. Experiments across two model families and diverse reasoning tasks show that existing training-free confidence measures can reduce redundant reasoning. However, we also find that the utility of individual confidence measures is inconsistent across settings. Through our evaluation framework and analysis, our study provides practical guidance toward developing and evaluating models that selectively use CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) prompting is a common technique for improving the reasoning abilities of large language models (LLMs). However, extended reasoning is often unnecessary and substantially increases token usage. As such, a key question becomes how to optimally allocate compute to when reasoning is actually needed. We study this through confidence-gated CoT, where a model produces a direct answer and a confidence estimate to decide whether to invoke CoT. We present an evaluation framework together with the first systematic study of confidence signals for this decision. We evaluate four representative confidence measures and compare them with random gating and an oracle upper bound. Experiments across two model families and diverse reasoning tasks show that existing training-free confidence measures can reduce redundant reasoning. However, we also find that the utility of individual confidence measures is inconsistent across settings. Through our evaluation framework and analysis, our study provides practical guidance toward developing and evaluating models that selectively use CoT."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-23T21:33:28Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    21,
                    33,
                    28,
                    3,
                    296,
                    0
                ],
                "arxiv_comment": "Under Review",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Samuel Lewis-Lim"
                    },
                    {
                        "name": "Xingwei Tan"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Nikolaos Aletras"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Aletras"
                },
                "author": "Nikolaos Aletras"
            },
            {
                "id": "http://arxiv.org/abs/2601.05053v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05053v1",
                "title": "Reinforced Efficient Reasoning via Semantically Diverse Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Efficient Reasoning via Semantically Diverse Exploration"
                },
                "updated": "2026-01-08T15:56:44Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    56,
                    44,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05053v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:56:44Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    56,
                    44,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Ziqi Zhao"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Jiahong Zou"
                    },
                    {
                        "name": "Liu Yang"
                    },
                    {
                        "name": "Zhiwei Xu"
                    },
                    {
                        "name": "Xuri Ge"
                    },
                    {
                        "name": "Zhumin Chen"
                    },
                    {
                        "name": "Xinyu Ma"
                    },
                    {
                        "name": "Daiting Shi"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xin Xin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xin"
                },
                "author": "Xin Xin"
            },
            {
                "id": "http://arxiv.org/abs/2601.05051v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05051v1",
                "title": "Publishing FAIR and Machine-actionable Reviews in Materials Science: The Case for Symbolic Knowledge in Neuro-symbolic Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Publishing FAIR and Machine-actionable Reviews in Materials Science: The Case for Symbolic Knowledge in Neuro-symbolic Artificial Intelligence"
                },
                "updated": "2026-01-08T15:56:17Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    56,
                    17,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05051v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Scientific reviews are central to knowledge integration in materials science, yet their key insights remain locked in narrative text and static PDF tables, limiting reuse by humans and machines alike. This article presents a case study in atomic layer deposition and etching (ALD/E) where we publish review tables as FAIR, machine-actionable comparisons in the Open Research Knowledge Graph (ORKG), turning them into structured, queryable knowledge. Building on this, we contrast symbolic querying over ORKG with large language model-based querying, and argue that a curated symbolic layer should remain the backbone of reliable neurosymbolic AI in materials science, with LLMs serving as complementary, symbolically grounded interfaces rather than standalone sources of truth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific reviews are central to knowledge integration in materials science, yet their key insights remain locked in narrative text and static PDF tables, limiting reuse by humans and machines alike. This article presents a case study in atomic layer deposition and etching (ALD/E) where we publish review tables as FAIR, machine-actionable comparisons in the Open Research Knowledge Graph (ORKG), turning them into structured, queryable knowledge. Building on this, we contrast symbolic querying over ORKG with large language model-based querying, and argue that a curated symbolic layer should remain the backbone of reliable neurosymbolic AI in materials science, with LLMs serving as complementary, symbolically grounded interfaces rather than standalone sources of truth."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:56:17Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    56,
                    17,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "35 pages, 11 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Soren Auer"
                    },
                    {
                        "name": "Eleni Poupaki"
                    },
                    {
                        "name": "Alex Watkins"
                    },
                    {
                        "name": "Anjana Devi"
                    },
                    {
                        "name": "Riikka L. Puurunen"
                    },
                    {
                        "name": "Bora Karasulu"
                    },
                    {
                        "name": "Adrie Mackus"
                    },
                    {
                        "name": "Erwin Kessels"
                    }
                ],
                "author_detail": {
                    "name": "Erwin Kessels"
                },
                "author": "Erwin Kessels"
            },
            {
                "id": "http://arxiv.org/abs/2601.05050v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05050v1",
                "title": "Large language models can effectively convince people to believe conspiracies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can effectively convince people to believe conspiracies"
                },
                "updated": "2026-01-08T15:56:05Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    56,
                    5,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05050v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have been shown to be persuasive across a variety of context. But it remains unclear whether this persuasive power advantages truth over falsehood, or if LLMs can promote misbeliefs just as easily as refuting them. Here, we investigate this question across three pre-registered experiments in which participants (N = 2,724 Americans) discussed a conspiracy theory they were uncertain about with GPT-4o, and the model was instructed to either argue against (\"debunking\") or for (\"bunking\") that conspiracy. When using a \"jailbroken\" GPT-4o variant with guardrails removed, the AI was as effective at increasing conspiracy belief as decreasing it. Concerningly, the bunking AI was rated more positively, and increased trust in AI, more than the debunking AI. Surprisingly, we found that using standard GPT-4o produced very similar effects, such that the guardrails imposed by OpenAI did little to revent the LLM from promoting conspiracy beliefs. Encouragingly, however, a corrective conversation reversed these newly induced conspiracy beliefs, and simply prompting GPT-4o to only use accurate information dramatically reduced its ability to increase conspiracy beliefs. Our findings demonstrate that LLMs possess potent abilities to promote both truth and falsehood, but that potential solutions may exist to help mitigate this risk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been shown to be persuasive across a variety of context. But it remains unclear whether this persuasive power advantages truth over falsehood, or if LLMs can promote misbeliefs just as easily as refuting them. Here, we investigate this question across three pre-registered experiments in which participants (N = 2,724 Americans) discussed a conspiracy theory they were uncertain about with GPT-4o, and the model was instructed to either argue against (\"debunking\") or for (\"bunking\") that conspiracy. When using a \"jailbroken\" GPT-4o variant with guardrails removed, the AI was as effective at increasing conspiracy belief as decreasing it. Concerningly, the bunking AI was rated more positively, and increased trust in AI, more than the debunking AI. Surprisingly, we found that using standard GPT-4o produced very similar effects, such that the guardrails imposed by OpenAI did little to revent the LLM from promoting conspiracy beliefs. Encouragingly, however, a corrective conversation reversed these newly induced conspiracy beliefs, and simply prompting GPT-4o to only use accurate information dramatically reduced its ability to increase conspiracy beliefs. Our findings demonstrate that LLMs possess potent abilities to promote both truth and falsehood, but that potential solutions may exist to help mitigate this risk."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:56:05Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    56,
                    5,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Thomas H. Costello"
                    },
                    {
                        "name": "Kellin Pelrine"
                    },
                    {
                        "name": "Matthew Kowal"
                    },
                    {
                        "name": "Antonio A. Arechar"
                    },
                    {
                        "name": "Jean-Franois Godbout"
                    },
                    {
                        "name": "Adam Gleave"
                    },
                    {
                        "name": "David Rand"
                    },
                    {
                        "name": "Gordon Pennycook"
                    }
                ],
                "author_detail": {
                    "name": "Gordon Pennycook"
                },
                "author": "Gordon Pennycook"
            },
            {
                "id": "http://arxiv.org/abs/2601.05047v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05047v1",
                "title": "Challenges and Research Directions for Large Language Model Inference Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges and Research Directions for Large Language Model Inference Hardware"
                },
                "updated": "2026-01-08T15:52:11Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    52,
                    11,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05047v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/MC.2026.3652916",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Large Language Model (LLM) inference is hard. The autoregressive Decode phase of the underlying Transformer model makes LLM inference fundamentally different from training. Exacerbated by recent AI trends, the primary challenges are memory and interconnect rather than compute. To address these challenges, we highlight four architecture research opportunities: High Bandwidth Flash for 10X memory capacity with HBM-like bandwidth; Processing-Near-Memory and 3D memory-logic stacking for high memory bandwidth; and low-latency interconnect to speedup communication. While our focus is datacenter AI, we also review their applicability for mobile devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is hard. The autoregressive Decode phase of the underlying Transformer model makes LLM inference fundamentally different from training. Exacerbated by recent AI trends, the primary challenges are memory and interconnect rather than compute. To address these challenges, we highlight four architecture research opportunities: High Bandwidth Flash for 10X memory capacity with HBM-like bandwidth; Processing-Near-Memory and 3D memory-logic stacking for high memory bandwidth; and low-latency interconnect to speedup communication. While our focus is datacenter AI, we also review their applicability for mobile devices."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:52:11Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    52,
                    11,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Accepted for publication by IEEE Computer, 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "David Patterson"
                    }
                ],
                "author_detail": {
                    "name": "David Patterson"
                },
                "author": "David Patterson",
                "arxiv_doi": "10.1109/MC.2026.3652916"
            },
            {
                "id": "http://arxiv.org/abs/2412.17189v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.17189v4",
                "title": "Talking with Tables for Better LLM Factual Data Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Talking with Tables for Better LLM Factual Data Interactions"
                },
                "updated": "2026-01-08T15:45:17Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    45,
                    17,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.17189v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.17189v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) often struggle with requests related to information retrieval and data manipulation that frequently arise in real-world scenarios under multiple conditions. In this paper, we demonstrate that leveraging tabular structures in LLM interactions, is more effective than utilizing other structures for handling prevalent requests that operate over factual data. Through comprehensive evaluations across various scenarios and request types, we show that providing tabular structures yields a 40.29\\% average performance gain along with better robustness and token efficiency. Through attention-value analysis, we discover that tables help LLMs better locate relevant information, explaining these improvements. Beyond tables and text, we evaluate whether (1) blending structuredness within text, such as providing templates or fixing the order of attributes, and (2) other representative structures, such as knowledge graphs and JSON are helpful. We observe that utilizing tables offers the best balance between efficiency and effectiveness. The method remains robust to task complexity and adapts to unstructured sources through text-to-table conversion. Overall, we highlight the untapped potential of tabular representations for future LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle with requests related to information retrieval and data manipulation that frequently arise in real-world scenarios under multiple conditions. In this paper, we demonstrate that leveraging tabular structures in LLM interactions, is more effective than utilizing other structures for handling prevalent requests that operate over factual data. Through comprehensive evaluations across various scenarios and request types, we show that providing tabular structures yields a 40.29\\% average performance gain along with better robustness and token efficiency. Through attention-value analysis, we discover that tables help LLMs better locate relevant information, explaining these improvements. Beyond tables and text, we evaluate whether (1) blending structuredness within text, such as providing templates or fixing the order of attributes, and (2) other representative structures, such as knowledge graphs and JSON are helpful. We observe that utilizing tables offers the best balance between efficiency and effectiveness. The method remains robust to task complexity and adapts to unstructured sources through text-to-table conversion. Overall, we highlight the untapped potential of tabular representations for future LLM applications."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-22T23:31:03Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    23,
                    31,
                    3,
                    6,
                    357,
                    0
                ],
                "arxiv_comment": "20 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jio Oh"
                    },
                    {
                        "name": "Geon Heo"
                    },
                    {
                        "name": "Seungjun Oh"
                    },
                    {
                        "name": "Hyunjin Kim"
                    },
                    {
                        "name": "JinYeong Bak"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Steven Euijong Whang"
                    }
                ],
                "author_detail": {
                    "name": "Steven Euijong Whang"
                },
                "author": "Steven Euijong Whang"
            },
            {
                "id": "http://arxiv.org/abs/2601.05039v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05039v1",
                "title": "FinDeepForecast: A Live Multi-Agent System for Benchmarking Deep Research Agents in Financial Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinDeepForecast: A Live Multi-Agent System for Benchmarking Deep Research Agents in Financial Forecasting"
                },
                "updated": "2026-01-08T15:45:09Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    45,
                    9,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05039v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deep Research (DR) Agents powered by advanced Large Language Models (LLMs) have fundamentally shifted the paradigm for completing complex research tasks. Yet, a comprehensive and live evaluation of their forecasting performance on real-world, research-oriented tasks in high-stakes domains (e.g., finance) remains underexplored. We introduce FinDeepForecast, the first live, end-to-end multi-agent system for automatically evaluating DR agents by continuously generating research-oriented financial forecasting tasks. This system is equipped with a dual-track taxonomy, enabling the dynamic generation of recurrent and non-recurrent forecasting tasks at both corporate and macro levels. With this system, we generate FinDeepForecastBench, a weekly evaluation benchmark over a ten-week horizon, encompassing 8 global economies and 1,314 listed companies, and evaluate 13 representative methods. Extensive experiments show that, while DR agents consistently outperform strong baselines, their performance still falls short of genuine forward-looking financial reasoning. We expect the proposed FinDeepForecast system to consistently facilitate future advancements of DR agents in research-oriented financial forecasting tasks. The benchmark and leaderboard are publicly available on the OpenFinArena Platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Research (DR) Agents powered by advanced Large Language Models (LLMs) have fundamentally shifted the paradigm for completing complex research tasks. Yet, a comprehensive and live evaluation of their forecasting performance on real-world, research-oriented tasks in high-stakes domains (e.g., finance) remains underexplored. We introduce FinDeepForecast, the first live, end-to-end multi-agent system for automatically evaluating DR agents by continuously generating research-oriented financial forecasting tasks. This system is equipped with a dual-track taxonomy, enabling the dynamic generation of recurrent and non-recurrent forecasting tasks at both corporate and macro levels. With this system, we generate FinDeepForecastBench, a weekly evaluation benchmark over a ten-week horizon, encompassing 8 global economies and 1,314 listed companies, and evaluate 13 representative methods. Extensive experiments show that, while DR agents consistently outperform strong baselines, their performance still falls short of genuine forward-looking financial reasoning. We expect the proposed FinDeepForecast system to consistently facilitate future advancements of DR agents in research-oriented financial forecasting tasks. The benchmark and leaderboard are publicly available on the OpenFinArena Platform."
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:45:09Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    45,
                    9,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA"
                },
                "authors": [
                    {
                        "name": "Xiangyu Li"
                    },
                    {
                        "name": "Xuan Yao"
                    },
                    {
                        "name": "Guohao Qi"
                    },
                    {
                        "name": "Fengbin Zhu"
                    },
                    {
                        "name": "Kelvin J. L. Koa"
                    },
                    {
                        "name": "Xiang Yao Ng"
                    },
                    {
                        "name": "Ziyang Liu"
                    },
                    {
                        "name": "Xingyu Ni"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Yonghui Yang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Huanbo Luan"
                    },
                    {
                        "name": "Xiaofen Xing"
                    },
                    {
                        "name": "Xiangmin Xu"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Ke-Wei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ke-Wei Huang"
                },
                "author": "Ke-Wei Huang"
            },
            {
                "id": "http://arxiv.org/abs/2502.13691v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.13691v3",
                "title": "Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora"
                },
                "updated": "2026-01-08T15:44:53Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    44,
                    53,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.13691v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.13691v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As large language models (LLMs) converge towards similar capabilities, the key to advancing their performance lies in identifying and incorporating valuable new information sources. However, evaluating which text collections are worth the substantial investment required for digitization, preprocessing, and integration into LLM systems remains a significant challenge. We present a novel approach to this challenge: an automated pipeline that evaluates the potential information gain from text collections without requiring model training or fine-tuning. Our method generates multiple choice questions (MCQs) from texts and measures an LLM's performance both with and without access to the source material. The performance gap between these conditions serves as a proxy for the collection's information potential. We validate our approach using five strategically selected datasets: EPFL PhD manuscripts, a private collection of Venetian historical records, two sets of Wikipedia articles on related topics, and a synthetic baseline dataset. Our results demonstrate that this method effectively identifies collections containing valuable novel information, providing a practical tool for prioritizing data acquisition and integration efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) converge towards similar capabilities, the key to advancing their performance lies in identifying and incorporating valuable new information sources. However, evaluating which text collections are worth the substantial investment required for digitization, preprocessing, and integration into LLM systems remains a significant challenge. We present a novel approach to this challenge: an automated pipeline that evaluates the potential information gain from text collections without requiring model training or fine-tuning. Our method generates multiple choice questions (MCQs) from texts and measures an LLM's performance both with and without access to the source material. The performance gap between these conditions serves as a proxy for the collection's information potential. We validate our approach using five strategically selected datasets: EPFL PhD manuscripts, a private collection of Venetian historical records, two sets of Wikipedia articles on related topics, and a synthetic baseline dataset. Our results demonstrate that this method effectively identifies collections containing valuable novel information, providing a practical tool for prioritizing data acquisition and integration efforts."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-19T13:03:06Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    3,
                    6,
                    2,
                    50,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tristan Karch"
                    },
                    {
                        "name": "Luca Engel"
                    },
                    {
                        "name": "Philippe Schwaller"
                    },
                    {
                        "name": "Frdric Kaplan"
                    }
                ],
                "author_detail": {
                    "name": "Frdric Kaplan"
                },
                "author": "Frdric Kaplan"
            },
            {
                "id": "http://arxiv.org/abs/2601.05038v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05038v1",
                "title": "ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG"
                },
                "updated": "2026-01-08T15:44:52Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    44,
                    52,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05038v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried ''compressing'' these documents into smaller summaries or mathematical embeddings, there is a catch: the more you compress the data, the more the LLM struggles to understand it. To address this challenge, we propose ArcAligner (Adaptive recursive context *Aligner*), a lightweight module integrated into the language model layers to help the model better utilize highly compressed context representations for downstream generation. It uses an adaptive ''gating'' system that only adds extra processing power when the information is complex, keeping the system fast. Across knowledge-intensive QA benchmarks, ArcAligner consistently beats compression baselines at comparable compression rates, especially on multi-hop and long-tail settings. The source code is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried ''compressing'' these documents into smaller summaries or mathematical embeddings, there is a catch: the more you compress the data, the more the LLM struggles to understand it. To address this challenge, we propose ArcAligner (Adaptive recursive context *Aligner*), a lightweight module integrated into the language model layers to help the model better utilize highly compressed context representations for downstream generation. It uses an adaptive ''gating'' system that only adds extra processing power when the information is complex, keeping the system fast. Across knowledge-intensive QA benchmarks, ArcAligner consistently beats compression baselines at comparable compression rates, especially on multi-hop and long-tail settings. The source code is publicly available."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:44:52Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    44,
                    52,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Code is available at https://github.com/liunian-Jay/ArcAligner.git",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Sendong Zhao"
                    },
                    {
                        "name": "Bairui Hu"
                    },
                    {
                        "name": "Haochun Wang"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin"
            },
            {
                "id": "http://arxiv.org/abs/2601.05027v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05027v1",
                "title": "OptiSet: Unified Optimizing Set Selection and Ranking for Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OptiSet: Unified Optimizing Set Selection and Ranking for Retrieval-Augmented Generation"
                },
                "updated": "2026-01-08T15:35:01Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    35,
                    1,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05027v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-Augmented Generation (RAG) improves generation quality by incorporating evidence retrieved from large external corpora. However, most existing methods rely on statically selecting top-k passages based on individual relevance, which fails to exploit combinatorial gains among passages and often introduces substantial redundancy. To address this limitation, we propose OptiSet, a set-centric framework that unifies set selection and set-level ranking for RAG. OptiSet adopts an \"Expand-then-Refine\" paradigm: it first expands a query into multiple perspectives to enable a diverse candidate pool and then refines the candidate pool via re-selection to form a compact evidence set. We then devise a self-synthesis strategy without strong LLM supervision to derive preference labels from the set conditional utility changes of the generator, thereby identifying complementary and redundant evidence. Finally, we introduce a set-list wise training strategy that jointly optimizes set selection and set-level ranking, enabling the model to favor compact, high-gain evidence sets. Extensive experiments demonstrate that OptiSet improves performance on complex combinatorial problems and makes generation more efficient. The source code is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) improves generation quality by incorporating evidence retrieved from large external corpora. However, most existing methods rely on statically selecting top-k passages based on individual relevance, which fails to exploit combinatorial gains among passages and often introduces substantial redundancy. To address this limitation, we propose OptiSet, a set-centric framework that unifies set selection and set-level ranking for RAG. OptiSet adopts an \"Expand-then-Refine\" paradigm: it first expands a query into multiple perspectives to enable a diverse candidate pool and then refines the candidate pool via re-selection to form a compact evidence set. We then devise a self-synthesis strategy without strong LLM supervision to derive preference labels from the set conditional utility changes of the generator, thereby identifying complementary and redundant evidence. Finally, we introduce a set-list wise training strategy that jointly optimizes set selection and set-level ranking, enabling the model to favor compact, high-gain evidence sets. Extensive experiments demonstrate that OptiSet improves performance on complex combinatorial problems and makes generation more efficient. The source code is publicly available."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:35:01Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    35,
                    1,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Code is available at https://github.com/liunian-Jay/OptiSet.git",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Sendong Zhao"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Bairui Hu"
                    },
                    {
                        "name": "Yanrui Du"
                    },
                    {
                        "name": "Haochun Wang"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin"
            },
            {
                "id": "http://arxiv.org/abs/2601.05022v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05022v1",
                "title": "Knowledge-to-Data: LLM-Driven Synthesis of Structured Network Traffic for Testbed-Free IDS Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-to-Data: LLM-Driven Synthesis of Structured Network Traffic for Testbed-Free IDS Evaluation"
                },
                "updated": "2026-01-08T15:31:33Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    31,
                    33,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05022v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Realistic, large-scale, and well-labeled cybersecurity datasets are essential for training and evaluating Intrusion Detection Systems (IDS). However, they remain difficult to obtain due to privacy constraints, data sensitivity, and the cost of building controlled collection environments such as testbeds and cyber ranges. This paper investigates whether Large Language Models (LLMs) can operate as controlled knowledge-to-data engines for generating structured synthetic network traffic datasets suitable for IDS research. We propose a methodology that combines protocol documentation, attack semantics, and explicit statistical rules to condition LLMs without fine-tuning or access to raw samples. Using the AWID3 IEEE~802.11 benchmark as a demanding case study, we generate labeled datasets with four state-of-the-art LLMs and assess fidelity through a multi-level validation framework including global similarity metrics, per-feature distribution testing, structural comparison, and cross-domain classification. Results show that, under explicit constraints, LLM-generated datasets can closely approximate the statistical and structural characteristics of real network traffic, enabling gradient-boosting classifiers to achieve F1-scores up to 0.956 when evaluated on real samples. Overall, the findings suggest that constrained LLM-driven generation can facilitate on-demand IDS experimentation, providing a testbed-free, privacy-preserving alternative that overcomes the traditional bottlenecks of physical traffic collection and manual labeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realistic, large-scale, and well-labeled cybersecurity datasets are essential for training and evaluating Intrusion Detection Systems (IDS). However, they remain difficult to obtain due to privacy constraints, data sensitivity, and the cost of building controlled collection environments such as testbeds and cyber ranges. This paper investigates whether Large Language Models (LLMs) can operate as controlled knowledge-to-data engines for generating structured synthetic network traffic datasets suitable for IDS research. We propose a methodology that combines protocol documentation, attack semantics, and explicit statistical rules to condition LLMs without fine-tuning or access to raw samples. Using the AWID3 IEEE~802.11 benchmark as a demanding case study, we generate labeled datasets with four state-of-the-art LLMs and assess fidelity through a multi-level validation framework including global similarity metrics, per-feature distribution testing, structural comparison, and cross-domain classification. Results show that, under explicit constraints, LLM-generated datasets can closely approximate the statistical and structural characteristics of real network traffic, enabling gradient-boosting classifiers to achieve F1-scores up to 0.956 when evaluated on real samples. Overall, the findings suggest that constrained LLM-driven generation can facilitate on-demand IDS experimentation, providing a testbed-free, privacy-preserving alternative that overcomes the traditional bottlenecks of physical traffic collection and manual labeling."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:31:33Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    31,
                    33,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Konstantinos E. Kampourakis"
                    },
                    {
                        "name": "Vyron Kampourakis"
                    },
                    {
                        "name": "Efstratios Chatzoglou"
                    },
                    {
                        "name": "Georgios Kambourakis"
                    },
                    {
                        "name": "Stefanos Gritzalis"
                    }
                ],
                "author_detail": {
                    "name": "Stefanos Gritzalis"
                },
                "author": "Stefanos Gritzalis"
            },
            {
                "id": "http://arxiv.org/abs/2601.05009v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05009v1",
                "title": "An Empirical Investigation of Robustness in Large Language Models under Tabular Distortions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Investigation of Robustness in Large Language Models under Tabular Distortions"
                },
                "updated": "2026-01-08T15:10:32Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    10,
                    32,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05009v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We investigate how large language models (LLMs) fail when tabular data in an otherwise canonical representation is subjected to semantic and structural distortions. Our findings reveal that LLMs lack an inherent ability to detect and correct subtle distortions in table representations. Only when provided with an explicit prior, via a system prompt, do models partially adjust their reasoning strategies and correct some distortions, though not consistently or completely. To study this phenomenon, we introduce a small, expert-curated dataset that explicitly evaluates LLMs on table question answering (TQA) tasks requiring an additional error-correction step prior to analysis. Our results reveal systematic differences in how LLMs ingest and interpret tabular information under distortion, with even SoTA models such as GPT-5.2 model exhibiting a drop of minimum 22% accuracy under distortion. These findings raise important questions for future research, particularly regarding when and how models should autonomously decide to realign tabular inputs, analogous to human behavior, without relying on explicit prompts or tabular data pre-processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate how large language models (LLMs) fail when tabular data in an otherwise canonical representation is subjected to semantic and structural distortions. Our findings reveal that LLMs lack an inherent ability to detect and correct subtle distortions in table representations. Only when provided with an explicit prior, via a system prompt, do models partially adjust their reasoning strategies and correct some distortions, though not consistently or completely. To study this phenomenon, we introduce a small, expert-curated dataset that explicitly evaluates LLMs on table question answering (TQA) tasks requiring an additional error-correction step prior to analysis. Our results reveal systematic differences in how LLMs ingest and interpret tabular information under distortion, with even SoTA models such as GPT-5.2 model exhibiting a drop of minimum 22% accuracy under distortion. These findings raise important questions for future research, particularly regarding when and how models should autonomously decide to realign tabular inputs, analogous to human behavior, without relying on explicit prompts or tabular data pre-processing."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:10:32Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    10,
                    32,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "4 pages, 1 figure, 1 table",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Avik Dutta"
                    },
                    {
                        "name": "Harshit Nigam"
                    },
                    {
                        "name": "Hosein Hasanbeig"
                    },
                    {
                        "name": "Arjun Radhakrishna"
                    },
                    {
                        "name": "Sumit Gulwani"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Gulwani"
                },
                "author": "Sumit Gulwani"
            },
            {
                "id": "http://arxiv.org/abs/2511.13368v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13368v2",
                "title": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning"
                },
                "updated": "2026-01-08T15:10:27Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    10,
                    27,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13368v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages remains poorly understood. We conduct a controlled LoRA fine-tuning study across multiple open-weight LLM families and scales, using a standardised grid of 11 languages and four benchmarks. We fine-tune each model on a single task-language source and measure transfer when evaluated on all other task-language target pairs. We decompose transfer into three regimes: (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language). Single-source fine-tuning yields a net positive uplift across regimes, but the gains are strongly asymmetric. Matched-Task (Cross-Language) transfer emerges as the most effective and predictable regime, driven principally by the identity of the target language rather than model architecture. We identify a stable hierarchy where high-resource languages and broad semantic tasks act as efficient recipients that absorb gains from diverse sources, while specialised tasks and lower-resource languages are more isolated. These results imply that effective fine-tuning requires navigating donor-recipient roles to maximise downstream gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages remains poorly understood. We conduct a controlled LoRA fine-tuning study across multiple open-weight LLM families and scales, using a standardised grid of 11 languages and four benchmarks. We fine-tune each model on a single task-language source and measure transfer when evaluated on all other task-language target pairs. We decompose transfer into three regimes: (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language). Single-source fine-tuning yields a net positive uplift across regimes, but the gains are strongly asymmetric. Matched-Task (Cross-Language) transfer emerges as the most effective and predictable regime, driven principally by the identity of the target language rather than model architecture. We identify a stable hierarchy where high-resource languages and broad semantic tasks act as efficient recipients that absorb gains from diverse sources, while specialised tasks and lower-resource languages are more isolated. These results imply that effective fine-tuning requires navigating donor-recipient roles to maximise downstream gains."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:41:31Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    41,
                    31,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kajetan Dymkiewicz"
                    },
                    {
                        "name": "Ivan Vulic"
                    },
                    {
                        "name": "Helen Yannakoudakis"
                    },
                    {
                        "name": "Eilam Shapira"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Anna Korhonen"
                    }
                ],
                "author_detail": {
                    "name": "Anna Korhonen"
                },
                "author": "Anna Korhonen"
            },
            {
                "id": "http://arxiv.org/abs/2601.05004v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.05004v1",
                "title": "Can Large Language Models Resolve Semantic Discrepancy in Self-Destructive Subcultures? Evidence from Jirai Kei",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Resolve Semantic Discrepancy in Self-Destructive Subcultures? Evidence from Jirai Kei"
                },
                "updated": "2026-01-08T15:02:41Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    2,
                    41,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.05004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.05004v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Self-destructive behaviors are linked to complex psychological states and can be challenging to diagnose. These behaviors may be even harder to identify within subcultural groups due to their unique expressions. As large language models (LLMs) are applied across various fields, some researchers have begun exploring their application for detecting self-destructive behaviors. Motivated by this, we investigate self-destructive behavior detection within subcultures using current LLM-based methods. However, these methods have two main challenges: (1) Knowledge Lag: Subcultural slang evolves rapidly, faster than LLMs' training cycles; and (2) Semantic Misalignment: it is challenging to grasp the specific and nuanced expressions unique to subcultures. To address these issues, we proposed Subcultural Alignment Solver (SAS), a multi-agent framework that incorporates automatic retrieval and subculture alignment, significantly enhancing the performance of LLMs in detecting self-destructive behavior. Our experimental results show that SAS outperforms the current advanced multi-agent framework OWL. Notably, it competes well with fine-tuned LLMs. We hope that SAS will advance the field of self-destructive behavior detection in subcultural contexts and serve as a valuable resource for future researchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-destructive behaviors are linked to complex psychological states and can be challenging to diagnose. These behaviors may be even harder to identify within subcultural groups due to their unique expressions. As large language models (LLMs) are applied across various fields, some researchers have begun exploring their application for detecting self-destructive behaviors. Motivated by this, we investigate self-destructive behavior detection within subcultures using current LLM-based methods. However, these methods have two main challenges: (1) Knowledge Lag: Subcultural slang evolves rapidly, faster than LLMs' training cycles; and (2) Semantic Misalignment: it is challenging to grasp the specific and nuanced expressions unique to subcultures. To address these issues, we proposed Subcultural Alignment Solver (SAS), a multi-agent framework that incorporates automatic retrieval and subculture alignment, significantly enhancing the performance of LLMs in detecting self-destructive behavior. Our experimental results show that SAS outperforms the current advanced multi-agent framework OWL. Notably, it competes well with fine-tuned LLMs. We hope that SAS will advance the field of self-destructive behavior detection in subcultural contexts and serve as a valuable resource for future researchers."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T15:02:41Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    15,
                    2,
                    41,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Preprint",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Xilin Tao"
                    },
                    {
                        "name": "Siyi Yao"
                    },
                    {
                        "name": "Jiageng Wu"
                    },
                    {
                        "name": "Yuntao Zou"
                    },
                    {
                        "name": "Zhuotao Tian"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Dagang Li"
                    }
                ],
                "author_detail": {
                    "name": "Dagang Li"
                },
                "author": "Dagang Li"
            },
            {
                "id": "http://arxiv.org/abs/2402.10424v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2402.10424v2",
                "title": "Pelican Soup Framework: A Theoretical Framework for Language Model Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pelican Soup Framework: A Theoretical Framework for Language Model Capabilities"
                },
                "updated": "2026-01-08T14:58:52Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    58,
                    52,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2402.10424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2402.10424v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this work, we propose a simple theoretical framework, Pelican Soup, aiming to better understand how pretraining allows LLMs to (1) generalize to unseen instructions and (2) perform in-context learning, even when the verbalizers are irrelevant to the task. To this end, in our framework, we introduce the notion of \"knowledge base\" and \"reference-sense association\" and a simple formalism for natural language processing tasks. Our framework demonstrates how linguistic, psychology, and philosophy studies can inform our understanding of the language model and is connected to several other existing theoretical results. As an illustration of the usage of our framework, we derive a bound on in-context learning loss with our framework. Finally, we support our framework with empirical experiments and provide possible future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a simple theoretical framework, Pelican Soup, aiming to better understand how pretraining allows LLMs to (1) generalize to unseen instructions and (2) perform in-context learning, even when the verbalizers are irrelevant to the task. To this end, in our framework, we introduce the notion of \"knowledge base\" and \"reference-sense association\" and a simple formalism for natural language processing tasks. Our framework demonstrates how linguistic, psychology, and philosophy studies can inform our understanding of the language model and is connected to several other existing theoretical results. As an illustration of the usage of our framework, we derive a bound on in-context learning loss with our framework. Finally, we support our framework with empirical experiments and provide possible future research directions."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-02-16T03:20:14Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    3,
                    20,
                    14,
                    4,
                    47,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ting-Rui Chiang"
                    },
                    {
                        "name": "Dani Yogatama"
                    }
                ],
                "author_detail": {
                    "name": "Dani Yogatama"
                },
                "author": "Dani Yogatama"
            },
            {
                "id": "http://arxiv.org/abs/2601.03042v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.03042v2",
                "title": "BaseCal: Unsupervised Confidence Calibration via Base Model Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaseCal: Unsupervised Confidence Calibration via Base Model Signals"
                },
                "updated": "2026-01-08T14:57:18Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    57,
                    18,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.03042v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.03042v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reliable confidence is essential for trusting the outputs of LLMs, yet widely deployed post-trained LLMs (PoLLMs) typically compromise this trust with severe overconfidence. In contrast, we observe that their corresponding base LLMs often remain well-calibrated. This naturally motivates us to calibrate PoLLM confidence using the base LLM as a reference. This work proposes two ways to achieve this. A straightforward solution, BaseCal-ReEval, evaluates PoLLM's responses by feeding them into the base LLM to get average probabilities as confidence. While effective, this approach introduces additional inference overhead. To address this, we propose BaseCal-Proj, which trains a lightweight projection to map the final-layer hidden states of PoLLMs back to those of their base LLMs. These projected states are then processed by the base LLM's output layer to derive base-calibrated confidence for PoLLM's responses. Notably, BaseCal is an unsupervised, plug-and-play solution that operates without human labels or LLM modifications. Experiments across five datasets and three LLM families demonstrate the effectiveness of BaseCal, reducing Expected Calibration Error (ECE) by an average of 42.90\\% compared to the best unsupervised baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable confidence is essential for trusting the outputs of LLMs, yet widely deployed post-trained LLMs (PoLLMs) typically compromise this trust with severe overconfidence. In contrast, we observe that their corresponding base LLMs often remain well-calibrated. This naturally motivates us to calibrate PoLLM confidence using the base LLM as a reference. This work proposes two ways to achieve this. A straightforward solution, BaseCal-ReEval, evaluates PoLLM's responses by feeding them into the base LLM to get average probabilities as confidence. While effective, this approach introduces additional inference overhead. To address this, we propose BaseCal-Proj, which trains a lightweight projection to map the final-layer hidden states of PoLLMs back to those of their base LLMs. These projected states are then processed by the base LLM's output layer to derive base-calibrated confidence for PoLLM's responses. Notably, BaseCal is an unsupervised, plug-and-play solution that operates without human labels or LLM modifications. Experiments across five datasets and three LLM families demonstrate the effectiveness of BaseCal, reducing Expected Calibration Error (ECE) by an average of 42.90\\% compared to the best unsupervised baselines."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-06T14:22:21Z",
                "published_parsed": [
                    2026,
                    1,
                    6,
                    14,
                    22,
                    21,
                    1,
                    6,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hexiang Tan"
                    },
                    {
                        "name": "Wanli Yang"
                    },
                    {
                        "name": "Junwei Zhang"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Rui Tang"
                    },
                    {
                        "name": "Du Su"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Yuanzhuo Wang"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng"
            },
            {
                "id": "http://arxiv.org/abs/2601.04983v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04983v1",
                "title": "Quantum Neural Network Training and Inference with Low Resolution Control Electronics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Neural Network Training and Inference with Low Resolution Control Electronics"
                },
                "updated": "2026-01-08T14:37:56Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    37,
                    56,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04983v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Scaling quantum computers requires tight integration of cryogenic control electronics with quantum processors, where Digital-to-Analog Converters (DACs) face severe power and area constraints. We investigate quantum neural network (QNN) training and inference under finite DAC resolution constraints across various DAC resolutions. Pre-trained QNNs achieve accuracy nearly indistinguishable from infinite-precision baselines when deployed on quantum systems with 6-bit DAC control electronics, exhibiting an elbow curve with diminishing returns beyond 4 bits. However, training under quantization reveals gradient deadlock below 12-bit resolution as gradient magnitudes fall below quantization step sizes. We introduce temperature-controlled stochasticity that overcomes this through probabilistic parameter updates, enabling successful training at 4-10 bit resolutions that remarkably matches or exceeds infinite-precision baseline performance. Our findings demonstrate that low-resolution control electronics need not compromise QML performance, enabling significant power and area reduction in cryogenic control systems for practical deployment as quantum hardware scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling quantum computers requires tight integration of cryogenic control electronics with quantum processors, where Digital-to-Analog Converters (DACs) face severe power and area constraints. We investigate quantum neural network (QNN) training and inference under finite DAC resolution constraints across various DAC resolutions. Pre-trained QNNs achieve accuracy nearly indistinguishable from infinite-precision baselines when deployed on quantum systems with 6-bit DAC control electronics, exhibiting an elbow curve with diminishing returns beyond 4 bits. However, training under quantization reveals gradient deadlock below 12-bit resolution as gradient magnitudes fall below quantization step sizes. We introduce temperature-controlled stochasticity that overcomes this through probabilistic parameter updates, enabling successful training at 4-10 bit resolutions that remarkably matches or exceeds infinite-precision baseline performance. Our findings demonstrate that low-resolution control electronics need not compromise QML performance, enabling significant power and area reduction in cryogenic control systems for practical deployment as quantum hardware scales."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T14:37:56Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    37,
                    56,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "5 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Rupayan Bhattacharjee"
                    },
                    {
                        "name": "Sergi Abadal"
                    },
                    {
                        "name": "Carmen G. Almudever"
                    },
                    {
                        "name": "Eduard Alarcon"
                    }
                ],
                "author_detail": {
                    "name": "Eduard Alarcon"
                },
                "author": "Eduard Alarcon"
            },
            {
                "id": "http://arxiv.org/abs/2502.18770v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.18770v4",
                "title": "Reward Shaping to Mitigate Reward Hacking in RLHF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Shaping to Mitigate Reward Hacking in RLHF"
                },
                "updated": "2026-01-08T14:33:47Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    33,
                    47,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.18770v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.18770v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to \\emph{reward hacking}, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. Although reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests two key design principles: (1) the RL reward should be bounded, and (2) the RL reward benefits from rapid initial growth followed by gradual convergence. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model as the signal for reinforcement learning. Moreover, PAR exhibits two critical variance-reduction properties that contribute to stabilizing the RLHF training process and effectively extending the tolerance window for early stopping. We evaluated PAR on the base model Gemma2-2B using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR's superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. The code is available at https://github.com/PorUna-byte/PAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to \\emph{reward hacking}, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. Although reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests two key design principles: (1) the RL reward should be bounded, and (2) the RL reward benefits from rapid initial growth followed by gradual convergence. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model as the signal for reinforcement learning. Moreover, PAR exhibits two critical variance-reduction properties that contribute to stabilizing the RLHF training process and effectively extending the tolerance window for early stopping. We evaluated PAR on the base model Gemma2-2B using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR's superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. The code is available at https://github.com/PorUna-byte/PAR."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-26T02:57:59Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    57,
                    59,
                    2,
                    57,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jiayi Fu"
                    },
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Chengyuan Yao"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Qi Han"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao"
            },
            {
                "id": "http://arxiv.org/abs/2510.26699v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.26699v3",
                "title": "Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment"
                },
                "updated": "2026-01-08T14:23:27Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    23,
                    27,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.26699v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.26699v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3786167.3788411",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Keeping software systems up to date is essential to avoid technical debt, security vulnerabilities, and the rigidity typical of legacy systems. However, updating libraries and frameworks remains a time consuming and error-prone process. Recent advances in Large Language Models (LLMs) and agentic coding systems offer new opportunities for automating such maintenance tasks. In this paper, we evaluate the update of a well-known Python library, SQLAlchemy, across a dataset of ten client applications. For this task, we use the Github's Copilot Agent Mode, an autonomous AI systema capable of planning and executing multi-step migration workflows. To assess the effectiveness of the automated migration, we also introduce Migration Coverage, a metric that quantifies the proportion of API usage points correctly migrated. The results of our study show that the LLM agent was capable of migrating functionalities and API usages between SQLAlchemy versions (migration coverage: 100%, median), but failed to maintain the application functionality, leading to a low test-pass rate (39.75%, median).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keeping software systems up to date is essential to avoid technical debt, security vulnerabilities, and the rigidity typical of legacy systems. However, updating libraries and frameworks remains a time consuming and error-prone process. Recent advances in Large Language Models (LLMs) and agentic coding systems offer new opportunities for automating such maintenance tasks. In this paper, we evaluate the update of a well-known Python library, SQLAlchemy, across a dataset of ten client applications. For this task, we use the Github's Copilot Agent Mode, an autonomous AI systema capable of planning and executing multi-step migration workflows. To assess the effectiveness of the automated migration, we also introduce Migration Coverage, a metric that quantifies the proportion of API usage points correctly migrated. The results of our study show that the LLM agent was capable of migrating functionalities and API usages between SQLAlchemy versions (migration coverage: 100%, median), but failed to maintain the application functionality, leading to a low test-pass rate (39.75%, median)."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-30T17:05:13Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    5,
                    13,
                    3,
                    303,
                    0
                ],
                "arxiv_comment": "Accepted at 1st International Workshop on Agentic Engineering (AGENT 2026, colocated with ICSE), pages 1-5",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Aylton Almeida"
                    },
                    {
                        "name": "Laerte Xavier"
                    },
                    {
                        "name": "Marco Tulio Valente"
                    }
                ],
                "author_detail": {
                    "name": "Marco Tulio Valente"
                },
                "author": "Marco Tulio Valente",
                "arxiv_doi": "10.1145/3786167.3788411"
            },
            {
                "id": "http://arxiv.org/abs/2511.14195v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14195v2",
                "title": "N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator"
                },
                "updated": "2026-01-08T14:19:17Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    19,
                    17,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14195v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Evaluating the safety robustness of LLMs is critical for their deployment. However, mainstream Red Teaming methods rely on online generation and black-box output analysis. These approaches are not only costly but also suffer from feedback latency, making them unsuitable for agile diagnostics after training a new model. To address this, we propose N-GLARE (A Non-Generative, Latent Representation-Efficient LLM Safety Evaluator). N-GLARE operates entirely on the model's latent representations, bypassing the need for full text generation. It characterizes hidden layer dynamics by analyzing the APT (Angular-Probabilistic Trajectory) of latent representations and introducing the JSS (Jensen-Shannon Separability) metric. Experiments on over 40 models and 20 red teaming strategies demonstrate that the JSS metric exhibits high consistency with the safety rankings derived from Red Teaming. N-GLARE reproduces the discriminative trends of large-scale red-teaming tests at less than 1\\% of the token cost and the runtime cost, providing an efficient output-free evaluation proxy for real-time diagnostics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the safety robustness of LLMs is critical for their deployment. However, mainstream Red Teaming methods rely on online generation and black-box output analysis. These approaches are not only costly but also suffer from feedback latency, making them unsuitable for agile diagnostics after training a new model. To address this, we propose N-GLARE (A Non-Generative, Latent Representation-Efficient LLM Safety Evaluator). N-GLARE operates entirely on the model's latent representations, bypassing the need for full text generation. It characterizes hidden layer dynamics by analyzing the APT (Angular-Probabilistic Trajectory) of latent representations and introducing the JSS (Jensen-Shannon Separability) metric. Experiments on over 40 models and 20 red teaming strategies demonstrate that the JSS metric exhibits high consistency with the safety rankings derived from Red Teaming. N-GLARE reproduces the discriminative trends of large-scale red-teaming tests at less than 1\\% of the token cost and the runtime cost, providing an efficient output-free evaluation proxy for real-time diagnostics."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T07:03:58Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    7,
                    3,
                    58,
                    1,
                    322,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zheyu Lin"
                    },
                    {
                        "name": "Jirui Yang"
                    },
                    {
                        "name": "Yukui Qiu"
                    },
                    {
                        "name": "Hengqi Guo"
                    },
                    {
                        "name": "Yubing Bao"
                    },
                    {
                        "name": "Yao Guan"
                    }
                ],
                "author_detail": {
                    "name": "Yao Guan"
                },
                "author": "Yao Guan"
            },
            {
                "id": "http://arxiv.org/abs/2508.05988v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.05988v2",
                "title": "Pruning the Unsurprising: Efficient LLM Reasoning via First-Token Surprisal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning the Unsurprising: Efficient LLM Reasoning via First-Token Surprisal"
                },
                "updated": "2026-01-08T14:09:36Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    9,
                    36,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.05988v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.05988v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities by scaling up the length of Chain-of-Thought (CoT). However, excessively long reasoning traces pose substantial challenges for training cost and inference latency. While various CoT compression approaches have emerged to address this challenge, they face inherent trade-offs: token-level methods often disrupt syntactic and logical coherence, while step-level methods based on perplexity fail to reliably capture the logically critical reasoning steps because of the dilution of logical information. In this paper, we propose ASAP (Anchor-guided, SurprisAl-based Pruning), a novel coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided pruning to preserve the core reasoning structure, which efficiently reduces the search space for subsequent processing. Leveraging the insight that logical branching choices are concentrated at the onset of reasoning steps, it then enables logic-aware pruning by selecting logically essential reasoning steps based on a novel first-token surprisal metric. Finally, ASAP distills the models to autonomously generate and leverage these concise CoTs at inference time, enabling efficient reasoning. Experiments show that ASAP achieves state-of-the-art accuracy across multiple benchmarks while substantially reducing training and inference costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities by scaling up the length of Chain-of-Thought (CoT). However, excessively long reasoning traces pose substantial challenges for training cost and inference latency. While various CoT compression approaches have emerged to address this challenge, they face inherent trade-offs: token-level methods often disrupt syntactic and logical coherence, while step-level methods based on perplexity fail to reliably capture the logically critical reasoning steps because of the dilution of logical information. In this paper, we propose ASAP (Anchor-guided, SurprisAl-based Pruning), a novel coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided pruning to preserve the core reasoning structure, which efficiently reduces the search space for subsequent processing. Leveraging the insight that logical branching choices are concentrated at the onset of reasoning steps, it then enables logic-aware pruning by selecting logically essential reasoning steps based on a novel first-token surprisal metric. Finally, ASAP distills the models to autonomously generate and leverage these concise CoTs at inference time, enabling efficient reasoning. Experiments show that ASAP achieves state-of-the-art accuracy across multiple benchmarks while substantially reducing training and inference costs."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-08T03:46:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    3,
                    46,
                    21,
                    4,
                    220,
                    0
                ],
                "arxiv_comment": "Code and model available at https://github.com/Zengwh02/ASAP",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Wenhao Zeng"
                    },
                    {
                        "name": "Yaoning Wang"
                    },
                    {
                        "name": "Chao Hu"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Chengcheng Wan"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Xiaodong Gu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong Gu"
                },
                "author": "Xiaodong Gu"
            },
            {
                "id": "http://arxiv.org/abs/2601.04963v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04963v1",
                "title": "Text as a Universal Interface for Transferable Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text as a Universal Interface for Transferable Personalization"
                },
                "updated": "2026-01-08T14:09:17Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    9,
                    17,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04963v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box'' profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box'' profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T14:09:17Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    9,
                    17,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yuting Liu"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Jiang-Ming Yang"
                    },
                    {
                        "name": "Jianzhe Zhao"
                    },
                    {
                        "name": "Guibing Guo"
                    }
                ],
                "author_detail": {
                    "name": "Guibing Guo"
                },
                "author": "Guibing Guo"
            },
            {
                "id": "http://arxiv.org/abs/2601.04960v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04960v1",
                "title": "A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction"
                },
                "updated": "2026-01-08T14:07:30Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    7,
                    30,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04960v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents a unified spoken language model for emotional intelligence, enhanced by a novel data construction strategy termed Injected Emotional-Attribution Thinking (IEAT). IEAT incorporates user emotional states and their underlying causes into the model's internal reasoning process, enabling emotion-aware reasoning to be internalized rather than treated as explicit supervision. The model is trained with a two-stage progressive strategy. The first stage performs speech-text alignment and emotional attribute modeling via self-distillation, while the second stage conducts end-to-end cross-modal joint optimization to ensure consistency between textual and spoken emotional expressions. Experiments on the Human-like Spoken Dialogue Systems Challenge (HumDial) Emotional Intelligence benchmark demonstrate that the proposed approach achieves top-ranked performance across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a unified spoken language model for emotional intelligence, enhanced by a novel data construction strategy termed Injected Emotional-Attribution Thinking (IEAT). IEAT incorporates user emotional states and their underlying causes into the model's internal reasoning process, enabling emotion-aware reasoning to be internalized rather than treated as explicit supervision. The model is trained with a two-stage progressive strategy. The first stage performs speech-text alignment and emotional attribute modeling via self-distillation, while the second stage conducts end-to-end cross-modal joint optimization to ensure consistency between textual and spoken emotional expressions. Experiments on the Human-like Spoken Dialogue Systems Challenge (HumDial) Emotional Intelligence benchmark demonstrate that the proposed approach achieves top-ranked performance across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T14:07:30Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    7,
                    30,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Zehan Li"
                    },
                    {
                        "name": "Yaodong Song"
                    },
                    {
                        "name": "Hongjie Chen"
                    },
                    {
                        "name": "Jian Kang"
                    },
                    {
                        "name": "Jie Lian"
                    },
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Yongxiang Li"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li"
            },
            {
                "id": "http://arxiv.org/abs/2601.04957v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04957v1",
                "title": "Safe Reinforcement Learning Beyond Baseline Control: A Hierarchical Framework for Space Triangle Tethered Formation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe Reinforcement Learning Beyond Baseline Control: A Hierarchical Framework for Space Triangle Tethered Formation System"
                },
                "updated": "2026-01-08T14:02:34Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    2,
                    34,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04957v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Triangular tethered formation system (TTFS) provide a promising platform for deep space exploration and distributed sensing due to its intrinsic spatial-orientation stability and capability of adjusting distances among node satellites through deployment and retrieval of tethers. However, due to the coupled tether-satellite dynamics and disturbance sensitivity of TTFS, traditional control methods struggle to achieve a balanced trade-off among configuration accuracy requirements, tension constraints, and energy efficiency consumption throughout the deployment process.In this paper, a novel model-reference reinforcement learning control framework is proposed for TTFS. By integrating baseline model-based control with a Soft Actor-Critic (SAC) compensator, the proposed method simultaneously achieves high-precision tracking, fuel efficiency, and compliance with tension limits. A hierarchical training scheme is developed to address the convergence difficulties arising from strongly coupled states in centralized training, while tailored reward functions, reset conditions, and normalization criteria are designed to accelerate training convergence. Closed-loop stability of the overall control law is rigorously proven using Lyapunov methods. Simulation results demonstrate that the proposed controller reduces steady-state tracking errors by over 96% for tethers and 99% for node satellites, while cutting fuel consumption by two orders of magnitude compared with the baseline method. These results validate the effectiveness and stability of the proposed approach for TTFS deployment control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Triangular tethered formation system (TTFS) provide a promising platform for deep space exploration and distributed sensing due to its intrinsic spatial-orientation stability and capability of adjusting distances among node satellites through deployment and retrieval of tethers. However, due to the coupled tether-satellite dynamics and disturbance sensitivity of TTFS, traditional control methods struggle to achieve a balanced trade-off among configuration accuracy requirements, tension constraints, and energy efficiency consumption throughout the deployment process.In this paper, a novel model-reference reinforcement learning control framework is proposed for TTFS. By integrating baseline model-based control with a Soft Actor-Critic (SAC) compensator, the proposed method simultaneously achieves high-precision tracking, fuel efficiency, and compliance with tension limits. A hierarchical training scheme is developed to address the convergence difficulties arising from strongly coupled states in centralized training, while tailored reward functions, reset conditions, and normalization criteria are designed to accelerate training convergence. Closed-loop stability of the overall control law is rigorously proven using Lyapunov methods. Simulation results demonstrate that the proposed controller reduces steady-state tracking errors by over 96% for tethers and 99% for node satellites, while cutting fuel consumption by two orders of magnitude compared with the baseline method. These results validate the effectiveness and stability of the proposed approach for TTFS deployment control."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T14:02:34Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    2,
                    34,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Xinyi Tao"
                    },
                    {
                        "name": "Panfeng Huang"
                    },
                    {
                        "name": "Fan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Fan Zhang"
                },
                "author": "Fan Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2505.18911v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.18911v3",
                "title": "Topological Quenching of Noise in a Free-Running Moebius Microcomb",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topological Quenching of Noise in a Free-Running Moebius Microcomb"
                },
                "updated": "2026-01-08T14:00:58Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    0,
                    58,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.18911v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.18911v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Microcombs require ultralow-noise repetition rates to enable next-generation applications in metrology, high-speed communications, microwave photonics, and sensing, where spectral purity is a central performance metric. Best-performing sources operate actively locked at \"quiet points\" in parameter space, fixed by device and material properties. Creating broad, low-noise operating regions with relaxed constraints-especially in simplified free-running architectures that avoid electronics-heavy control-remains an open challenge. Here, we demonstrate a symmetry-protected topological Mbius soliton molecule that enables intrinsically low phase noise in a fully free-running microcomb, operating without any external referencing or control. Using a microresonator-filtered laser, we implement a Mbius geometry via interleaved microcavity modes. Upon the formation of a topological Mbius soliton molecule, the free-running laser exhibits over 15 dB of phase-noise suppression across 10 Hz-10 kHz at a 100 GHz repetition rate, yielding -63 dBc/Hz phase noise at 1 kHz and an Allan deviation of 4x10^-10 at 10 s average time-without any external control. We show that the Mbius structure brings dynamic robustness to the comb, and we demonstrate a symmetry-protected topological regime that enables long-term drift-invariant operation. Our results establish a route to intrinsically noise-quenched microcombs operating in a fully free-running configuration, governed by internal physical principles and suitable for field-deployable, low-noise photonic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microcombs require ultralow-noise repetition rates to enable next-generation applications in metrology, high-speed communications, microwave photonics, and sensing, where spectral purity is a central performance metric. Best-performing sources operate actively locked at \"quiet points\" in parameter space, fixed by device and material properties. Creating broad, low-noise operating regions with relaxed constraints-especially in simplified free-running architectures that avoid electronics-heavy control-remains an open challenge. Here, we demonstrate a symmetry-protected topological Mbius soliton molecule that enables intrinsically low phase noise in a fully free-running microcomb, operating without any external referencing or control. Using a microresonator-filtered laser, we implement a Mbius geometry via interleaved microcavity modes. Upon the formation of a topological Mbius soliton molecule, the free-running laser exhibits over 15 dB of phase-noise suppression across 10 Hz-10 kHz at a 100 GHz repetition rate, yielding -63 dBc/Hz phase noise at 1 kHz and an Allan deviation of 4x10^-10 at 10 s average time-without any external control. We show that the Mbius structure brings dynamic robustness to the comb, and we demonstrate a symmetry-protected topological regime that enables long-term drift-invariant operation. Our results establish a route to intrinsically noise-quenched microcombs operating in a fully free-running configuration, governed by internal physical principles and suitable for field-deployable, low-noise photonic systems."
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-25T00:27:47Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    0,
                    27,
                    47,
                    6,
                    145,
                    0
                ],
                "arxiv_comment": "28 pages",
                "arxiv_primary_category": {
                    "term": "physics.optics"
                },
                "authors": [
                    {
                        "name": "Debayan Das"
                    },
                    {
                        "name": "Antonio Cutrona"
                    },
                    {
                        "name": "Andrew C. Cooper"
                    },
                    {
                        "name": "Luana Olivieri"
                    },
                    {
                        "name": "Alexander G. Balanov"
                    },
                    {
                        "name": "Sai Tak Chu"
                    },
                    {
                        "name": "Brent E. Little"
                    },
                    {
                        "name": "Roberto Morandotti"
                    },
                    {
                        "name": "David J. Moss"
                    },
                    {
                        "name": "Juan Sebastian Totero Gongora"
                    },
                    {
                        "name": "Marco Peccianti"
                    },
                    {
                        "name": "Gian-Luca Oppo"
                    },
                    {
                        "name": "Alessia Pasquazi"
                    }
                ],
                "author_detail": {
                    "name": "Alessia Pasquazi"
                },
                "author": "Alessia Pasquazi"
            },
            {
                "id": "http://arxiv.org/abs/2601.04954v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04954v1",
                "title": "Precision over Diversity: High-Precision Reward Generalizes to Robust Instruction Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precision over Diversity: High-Precision Reward Generalizes to Robust Instruction Following"
                },
                "updated": "2026-01-08T14:00:51Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    0,
                    51,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04954v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A central belief in scaling reinforcement learning with verifiable rewards for instruction following (IF) tasks is that, a diverse mixture of verifiable hard and unverifiable soft constraints is essential for generalizing to unseen instructions. In this work, we challenge this prevailing consensus through a systematic empirical investigation. Counter-intuitively, we find that models trained on hard-only constraints consistently outperform those trained on mixed datasets. Extensive experiments reveal that reward precision, rather than constraint diversity, is the primary driver of effective alignment. The LLM judge suffers from a low recall rate in detecting false response, which leads to severe reward hacking, thereby undermining the benefits of diversity. Furthermore, analysis of the attention mechanism reveals that high-precision rewards develop a transferable meta-skill for IF. Motivated by these insights, we propose a simple yet effective data-centric refinement strategy that prioritizes reward precision. Evaluated on five benchmarks, our approach outperforms competitive baselines by 13.4\\% in performance while achieving a 58\\% reduction in training time, maintaining strong generalization beyond instruction following. Our findings advocate for a paradigm shift: moving away from the indiscriminate pursuit of data diversity toward high-precision rewards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central belief in scaling reinforcement learning with verifiable rewards for instruction following (IF) tasks is that, a diverse mixture of verifiable hard and unverifiable soft constraints is essential for generalizing to unseen instructions. In this work, we challenge this prevailing consensus through a systematic empirical investigation. Counter-intuitively, we find that models trained on hard-only constraints consistently outperform those trained on mixed datasets. Extensive experiments reveal that reward precision, rather than constraint diversity, is the primary driver of effective alignment. The LLM judge suffers from a low recall rate in detecting false response, which leads to severe reward hacking, thereby undermining the benefits of diversity. Furthermore, analysis of the attention mechanism reveals that high-precision rewards develop a transferable meta-skill for IF. Motivated by these insights, we propose a simple yet effective data-centric refinement strategy that prioritizes reward precision. Evaluated on five benchmarks, our approach outperforms competitive baselines by 13.4\\% in performance while achieving a 58\\% reduction in training time, maintaining strong generalization beyond instruction following. Our findings advocate for a paradigm shift: moving away from the indiscriminate pursuit of data diversity toward high-precision rewards."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T14:00:51Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    14,
                    0,
                    51,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "ACL under review 13 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yirong Zeng"
                    },
                    {
                        "name": "Yufei Liu"
                    },
                    {
                        "name": "Xiao Ding"
                    },
                    {
                        "name": "Yutai Hou"
                    },
                    {
                        "name": "Yuxian Wang"
                    },
                    {
                        "name": "Haonan Song"
                    },
                    {
                        "name": "Wu Ning"
                    },
                    {
                        "name": "Dandan Tu"
                    },
                    {
                        "name": "Qixun Zhang"
                    },
                    {
                        "name": "Bibo Cai"
                    },
                    {
                        "name": "Yuxiang He"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu"
            },
            {
                "id": "http://arxiv.org/abs/2601.04946v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04946v1",
                "title": "Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics"
                },
                "updated": "2026-01-08T13:49:14Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    49,
                    14,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04946v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automatic metrics are now central to evaluating text-to-image models, often substituting for human judgment in benchmarking and large-scale filtering. However, it remains unclear whether these metrics truly prioritize semantic correctness or instead favor visually and socially prototypical images learned from biased data distributions. We identify and study \\emph{prototypicality bias} as a systematic failure mode in multimodal evaluation. We introduce a controlled contrastive benchmark \\textsc{\\textbf{ProtoBias}} (\\textit{\\textbf{Proto}typical \\textbf{Bias}}), spanning Animals, Objects, and Demography images, where semantically correct but non-prototypical images are paired with subtly incorrect yet prototypical adversarial counterparts. This setup enables a directional evaluation of whether metrics follow textual semantics or default to prototypes. Our results show that widely used metrics, including CLIPScore, PickScore, and VQA-based scores, frequently misrank these pairs, while even LLM-as-Judge systems exhibit uneven robustness in socially grounded cases. Human evaluations consistently favour semantic correctness with larger decision margins. Motivated by these findings, we propose \\textbf{\\textsc{ProtoScore}}, a robust 7B-parameter metric that substantially reduces failure rates and suppresses misranking, while running at orders of magnitude faster than the inference time of GPT-5, approaching the robustness of much larger closed-source judges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic metrics are now central to evaluating text-to-image models, often substituting for human judgment in benchmarking and large-scale filtering. However, it remains unclear whether these metrics truly prioritize semantic correctness or instead favor visually and socially prototypical images learned from biased data distributions. We identify and study \\emph{prototypicality bias} as a systematic failure mode in multimodal evaluation. We introduce a controlled contrastive benchmark \\textsc{\\textbf{ProtoBias}} (\\textit{\\textbf{Proto}typical \\textbf{Bias}}), spanning Animals, Objects, and Demography images, where semantically correct but non-prototypical images are paired with subtly incorrect yet prototypical adversarial counterparts. This setup enables a directional evaluation of whether metrics follow textual semantics or default to prototypes. Our results show that widely used metrics, including CLIPScore, PickScore, and VQA-based scores, frequently misrank these pairs, while even LLM-as-Judge systems exhibit uneven robustness in socially grounded cases. Human evaluations consistently favour semantic correctness with larger decision margins. Motivated by these findings, we propose \\textbf{\\textsc{ProtoScore}}, a robust 7B-parameter metric that substantially reduces failure rates and suppresses misranking, while running at orders of magnitude faster than the inference time of GPT-5, approaching the robustness of much larger closed-source judges."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T13:49:14Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    49,
                    14,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "First version",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Subhadeep Roy"
                    },
                    {
                        "name": "Gagan Bhatia"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger"
            },
            {
                "id": "http://arxiv.org/abs/2601.04940v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04940v1",
                "title": "CurricuLLM: Designing Personalized and Workforce-Aligned Cybersecurity Curricula Using Fine-Tuned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CurricuLLM: Designing Personalized and Workforce-Aligned Cybersecurity Curricula Using Fine-Tuned LLMs"
                },
                "updated": "2026-01-08T13:43:15Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    43,
                    15,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04940v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The cybersecurity landscape is constantly evolving, driven by increased digitalization and new cybersecurity threats. Cybersecurity programs often fail to equip graduates with skills demanded by the workforce, particularly concerning recent developments in cybersecurity, as curriculum design is costly and labor-intensive. To address this misalignment, we present a novel Large Language Model (LLM)-based framework for automated design and analysis of cybersecurity curricula, called CurricuLLM. Our approach provides three key contributions: (1) automation of personalized curriculum design, (2) a data-driven pipeline aligned with industry demands, and (3) a comprehensive methodology for leveraging fine-tuned LLMs in curriculum development.\n  CurricuLLM utilizes a two-tier approach consisting of PreprocessLM, which standardizes input data, and ClassifyLM, which assigns course content to nine Knowledge Areas in cybersecurity. We systematically evaluated multiple Natural Language Processing (NLP) architectures and fine-tuning strategies, ultimately selecting the Bidirectional Encoder Representations from Transformers (BERT) model as ClassifyLM, fine-tuned on foundational cybersecurity concepts and workforce competencies.\n  We are the first to validate our method with human experts who analyzed real-world cybersecurity curricula and frameworks, motivating that CurricuLLM is an efficient solution to replace labor-intensive curriculum analysis. Moreover, once course content has been classified, it can be integrated with established cybersecurity role-based weights, enabling alignment of the educational program with specific job roles, workforce categories, or general market needs. This lays the foundation for personalized, workforce-aligned cybersecurity curricula that prepare students for the evolving demands in cybersecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cybersecurity landscape is constantly evolving, driven by increased digitalization and new cybersecurity threats. Cybersecurity programs often fail to equip graduates with skills demanded by the workforce, particularly concerning recent developments in cybersecurity, as curriculum design is costly and labor-intensive. To address this misalignment, we present a novel Large Language Model (LLM)-based framework for automated design and analysis of cybersecurity curricula, called CurricuLLM. Our approach provides three key contributions: (1) automation of personalized curriculum design, (2) a data-driven pipeline aligned with industry demands, and (3) a comprehensive methodology for leveraging fine-tuned LLMs in curriculum development.\n  CurricuLLM utilizes a two-tier approach consisting of PreprocessLM, which standardizes input data, and ClassifyLM, which assigns course content to nine Knowledge Areas in cybersecurity. We systematically evaluated multiple Natural Language Processing (NLP) architectures and fine-tuning strategies, ultimately selecting the Bidirectional Encoder Representations from Transformers (BERT) model as ClassifyLM, fine-tuned on foundational cybersecurity concepts and workforce competencies.\n  We are the first to validate our method with human experts who analyzed real-world cybersecurity curricula and frameworks, motivating that CurricuLLM is an efficient solution to replace labor-intensive curriculum analysis. Moreover, once course content has been classified, it can be integrated with established cybersecurity role-based weights, enabling alignment of the educational program with specific job roles, workforce categories, or general market needs. This lays the foundation for personalized, workforce-aligned cybersecurity curricula that prepare students for the evolving demands in cybersecurity."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T13:43:15Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    43,
                    15,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Arthur Nijdam"
                    },
                    {
                        "name": "Harri Khknen"
                    },
                    {
                        "name": "Valtteri Niemi"
                    },
                    {
                        "name": "Paul Stankovski Wagner"
                    },
                    {
                        "name": "Sara Ramezanian"
                    }
                ],
                "author_detail": {
                    "name": "Sara Ramezanian"
                },
                "author": "Sara Ramezanian"
            },
            {
                "id": "http://arxiv.org/abs/2601.04932v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04932v1",
                "title": "GenProve: Learning to Generate Text with Fine-Grained Provenance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenProve: Learning to Generate Text with Fine-Grained Provenance"
                },
                "updated": "2026-01-08T13:30:30Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    30,
                    30,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04932v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLM) often hallucinate, and while adding citations is a common solution, it is frequently insufficient for accountability as users struggle to verify how a cited source supports a generated claim. Existing methods are typically coarse-grained and fail to distinguish between direct quotes and complex reasoning. In this paper, we introduce Generation-time Fine-grained Provenance, a task where models must generate fluent answers while simultaneously producing structured, sentence-level provenance triples. To enable this, we present ReFInE (Relation-aware Fine-grained Interpretability & Evidence), a dataset featuring expert verified annotations that distinguish between Quotation, Compression, and Inference. Building on ReFInE, we propose GenProve, a framework that combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO). By optimizing a composite reward for answer fidelity and provenance correctness, GenProve significantly outperforms 14 strong LLMs in joint evaluation. Crucially, our analysis uncovers a reasoning gap where models excel at surface-level quotation but struggle significantly with inference-based provenance, suggesting that verifiable reasoning remains a frontier challenge distinct from surface-level citation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLM) often hallucinate, and while adding citations is a common solution, it is frequently insufficient for accountability as users struggle to verify how a cited source supports a generated claim. Existing methods are typically coarse-grained and fail to distinguish between direct quotes and complex reasoning. In this paper, we introduce Generation-time Fine-grained Provenance, a task where models must generate fluent answers while simultaneously producing structured, sentence-level provenance triples. To enable this, we present ReFInE (Relation-aware Fine-grained Interpretability & Evidence), a dataset featuring expert verified annotations that distinguish between Quotation, Compression, and Inference. Building on ReFInE, we propose GenProve, a framework that combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO). By optimizing a composite reward for answer fidelity and provenance correctness, GenProve significantly outperforms 14 strong LLMs in joint evaluation. Crucially, our analysis uncovers a reasoning gap where models excel at surface-level quotation but struggle significantly with inference-based provenance, suggesting that verifiable reasoning remains a frontier challenge distinct from surface-level citation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T13:30:30Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    30,
                    30,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jingxuan Wei"
                    },
                    {
                        "name": "Xingyue Wang"
                    },
                    {
                        "name": "Yanghaoyu Liao"
                    },
                    {
                        "name": "Jie Dong"
                    },
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Caijun Jia"
                    },
                    {
                        "name": "Bihui Yu"
                    },
                    {
                        "name": "Junnan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Junnan Zhu"
                },
                "author": "Junnan Zhu"
            },
            {
                "id": "http://arxiv.org/abs/2506.06842v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.06842v2",
                "title": "PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation"
                },
                "updated": "2026-01-08T13:25:34Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    25,
                    34,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.06842v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.06842v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Disinformation detection is a key aspect of media literacy. Psychological studies have shown that knowledge of persuasive fallacies helps individuals detect disinformation. Inspired by these findings, we experimented with large language models (LLMs) to test whether infusing persuasion knowledge enhances disinformation detection. As a result, we introduce the Persuasion-Augmented Chain of Thought (PCoT), a novel approach that leverages persuasion to improve disinformation detection in zero-shot classification. We extensively evaluate PCoT on online news and social media posts. Moreover, we publish two novel, up-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets enable the evaluation of PCoT on content entirely unseen by the LLMs used in our experiments, as the content was published after the models' knowledge cutoffs. We show that, on average, PCoT outperforms competitive methods by 15% across five LLMs and five datasets. These findings highlight the value of persuasion in strengthening zero-shot disinformation detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disinformation detection is a key aspect of media literacy. Psychological studies have shown that knowledge of persuasive fallacies helps individuals detect disinformation. Inspired by these findings, we experimented with large language models (LLMs) to test whether infusing persuasion knowledge enhances disinformation detection. As a result, we introduce the Persuasion-Augmented Chain of Thought (PCoT), a novel approach that leverages persuasion to improve disinformation detection in zero-shot classification. We extensively evaluate PCoT on online news and social media posts. Moreover, we publish two novel, up-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets enable the evaluation of PCoT on content entirely unseen by the LLMs used in our experiments, as the content was published after the models' knowledge cutoffs. We show that, on average, PCoT outperforms competitive methods by 15% across five LLMs and five datasets. These findings highlight the value of persuasion in strengthening zero-shot disinformation detection."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-07T15:46:02Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    15,
                    46,
                    2,
                    5,
                    158,
                    0
                ],
                "arxiv_comment": "Accepted to ACL 2025 Main Conference",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Arkadiusz Modzelewski"
                    },
                    {
                        "name": "Witold Sosnowski"
                    },
                    {
                        "name": "Tiziano Labruna"
                    },
                    {
                        "name": "Adam Wierzbicki"
                    },
                    {
                        "name": "Giovanni Da San Martino"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Da San Martino"
                },
                "author": "Giovanni Da San Martino"
            },
            {
                "id": "http://arxiv.org/abs/2601.03997v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.03997v2",
                "title": "VotIE: Information Extraction from Meeting Minutes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VotIE: Information Extraction from Meeting Minutes"
                },
                "updated": "2026-01-08T13:24:16Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    24,
                    16,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.03997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.03997v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Municipal meeting minutes record key decisions in local democratic processes. Unlike parliamentary proceedings, which typically adhere to standardized formats, they encode voting outcomes in highly heterogeneous, free-form narrative text that varies widely across municipalities, posing significant challenges for automated extraction. In this paper, we introduce VotIE (Voting Information Extraction), a new information extraction task aimed at identifying structured voting events in narrative deliberative records, and establish the first benchmark for this task using Portuguese municipal minutes, building on the recently introduced CitiLink corpus. Our experiments yield two key findings. First, under standard in-domain evaluation, fine-tuned encoders, specifically XLM-R-CRF, achieve the strongest performance, reaching 93.2\\% macro F1, outperforming generative approaches. Second, in a cross-municipality setting that evaluates transfer to unseen administrative contexts, these models suffer substantial performance degradation, whereas few-shot LLMs demonstrate greater robustness, with significantly smaller declines in performance. Despite this generalization advantage, the high computational cost of generative models currently constrains their practicality. As a result, lightweight fine-tuned encoders remain a more practical option for large-scale, real-world deployment. To support reproducible research in administrative NLP, we publicly release our benchmark, trained models, and evaluation framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Municipal meeting minutes record key decisions in local democratic processes. Unlike parliamentary proceedings, which typically adhere to standardized formats, they encode voting outcomes in highly heterogeneous, free-form narrative text that varies widely across municipalities, posing significant challenges for automated extraction. In this paper, we introduce VotIE (Voting Information Extraction), a new information extraction task aimed at identifying structured voting events in narrative deliberative records, and establish the first benchmark for this task using Portuguese municipal minutes, building on the recently introduced CitiLink corpus. Our experiments yield two key findings. First, under standard in-domain evaluation, fine-tuned encoders, specifically XLM-R-CRF, achieve the strongest performance, reaching 93.2\\% macro F1, outperforming generative approaches. Second, in a cross-municipality setting that evaluates transfer to unseen administrative contexts, these models suffer substantial performance degradation, whereas few-shot LLMs demonstrate greater robustness, with significantly smaller declines in performance. Despite this generalization advantage, the high computational cost of generative models currently constrains their practicality. As a result, lightweight fine-tuned encoders remain a more practical option for large-scale, real-world deployment. To support reproducible research in administrative NLP, we publicly release our benchmark, trained models, and evaluation framework."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-07T15:06:53Z",
                "published_parsed": [
                    2026,
                    1,
                    7,
                    15,
                    6,
                    53,
                    2,
                    7,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jos Pedro Evans"
                    },
                    {
                        "name": "Lus Filipe Cunha"
                    },
                    {
                        "name": "Purificao Silvano"
                    },
                    {
                        "name": "Alpio Jorge"
                    },
                    {
                        "name": "Nuno Guimares"
                    },
                    {
                        "name": "Srgio Nunes"
                    },
                    {
                        "name": "Ricardo Campos"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Campos"
                },
                "author": "Ricardo Campos"
            },
            {
                "id": "http://arxiv.org/abs/2601.04925v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04925v1",
                "title": "Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences"
                },
                "updated": "2026-01-08T13:22:25Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    22,
                    25,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04925v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) can generate highly persuasive text, raising concerns about their misuse for propaganda, manipulation, and other harmful purposes. This leads us to our central question: Is LLM-generated persuasion more difficult to automatically detect than human-written persuasion? To address this, we categorize controllable generation approaches for producing persuasive content with LLMs and introduce Persuaficial, a high-quality multilingual benchmark covering six languages: English, German, Polish, Italian, French and Russian. Using this benchmark, we conduct extensive empirical evaluations comparing human-authored and LLM-generated persuasive texts. We find that although overtly persuasive LLM-generated texts can be easier to detect than human-written ones, subtle LLM-generated persuasion consistently degrades automatic detection performance. Beyond detection performance, we provide the first comprehensive linguistic analysis contrasting human and LLM-generated persuasive texts, offering insights that may guide the development of more interpretable and robust detection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can generate highly persuasive text, raising concerns about their misuse for propaganda, manipulation, and other harmful purposes. This leads us to our central question: Is LLM-generated persuasion more difficult to automatically detect than human-written persuasion? To address this, we categorize controllable generation approaches for producing persuasive content with LLMs and introduce Persuaficial, a high-quality multilingual benchmark covering six languages: English, German, Polish, Italian, French and Russian. Using this benchmark, we conduct extensive empirical evaluations comparing human-authored and LLM-generated persuasive texts. We find that although overtly persuasive LLM-generated texts can be easier to detect than human-written ones, subtle LLM-generated persuasion consistently degrades automatic detection performance. Beyond detection performance, we provide the first comprehensive linguistic analysis contrasting human and LLM-generated persuasive texts, offering insights that may guide the development of more interpretable and robust detection tools."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T13:22:25Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    22,
                    25,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Preprint; Paper is currently under review at a major NLP conference",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Arkadiusz Modzelewski"
                    },
                    {
                        "name": "Pawe Golik"
                    },
                    {
                        "name": "Anna Koos"
                    },
                    {
                        "name": "Giovanni Da San Martino"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Da San Martino"
                },
                "author": "Giovanni Da San Martino"
            },
            {
                "id": "http://arxiv.org/abs/2601.04920v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04920v1",
                "title": "Conversational AI for Rapid Scientific Prototyping: A Case Study on ESA's ELOPE Competition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational AI for Rapid Scientific Prototyping: A Case Study on ESA's ELOPE Competition"
                },
                "updated": "2026-01-08T13:17:50Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    17,
                    50,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04920v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are increasingly used as coding partners, yet their role in accelerating scientific discovery remains underexplored. This paper presents a case study of using ChatGPT for rapid prototyping in ESA's ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition. The competition required participants to process event camera data to estimate lunar lander trajectories. Despite joining late, we achieved second place with a score of 0.01282, highlighting the potential of human-AI collaboration in competitive scientific settings. ChatGPT contributed not only executable code but also algorithmic reasoning, data handling routines, and methodological suggestions, such as using fixed number of events instead of fixed time spans for windowing. At the same time, we observed limitations: the model often introduced unnecessary structural changes, gets confused by intermediate discussions about alternative ideas, occasionally produced critical errors and forgets important aspects in longer scientific discussions. By analyzing these strengths and shortcomings, we show how conversational AI can both accelerate development and support conceptual insight in scientific research. We argue that structured integration of LLMs into the scientific workflow can enhance rapid prototyping by proposing best practices for AI-assisted scientific work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used as coding partners, yet their role in accelerating scientific discovery remains underexplored. This paper presents a case study of using ChatGPT for rapid prototyping in ESA's ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition. The competition required participants to process event camera data to estimate lunar lander trajectories. Despite joining late, we achieved second place with a score of 0.01282, highlighting the potential of human-AI collaboration in competitive scientific settings. ChatGPT contributed not only executable code but also algorithmic reasoning, data handling routines, and methodological suggestions, such as using fixed number of events instead of fixed time spans for windowing. At the same time, we observed limitations: the model often introduced unnecessary structural changes, gets confused by intermediate discussions about alternative ideas, occasionally produced critical errors and forgets important aspects in longer scientific discussions. By analyzing these strengths and shortcomings, we show how conversational AI can both accelerate development and support conceptual insight in scientific research. We argue that structured integration of LLMs into the scientific workflow can enhance rapid prototyping by proposing best practices for AI-assisted scientific work."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T13:17:50Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    17,
                    50,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Nils Einecke"
                    }
                ],
                "author_detail": {
                    "name": "Nils Einecke"
                },
                "author": "Nils Einecke"
            },
            {
                "id": "http://arxiv.org/abs/2601.04918v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04918v1",
                "title": "Breaking Robustness Barriers in Cognitive Diagnosis: A One-Shot Neural Architecture Search Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Robustness Barriers in Cognitive Diagnosis: A One-Shot Neural Architecture Search Perspective"
                },
                "updated": "2026-01-08T13:17:40Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    17,
                    40,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04918v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the advancement of network technologies, intelligent tutoring systems (ITS) have emerged to deliver increasingly precise and tailored personalized learning services. Cognitive diagnosis (CD) has emerged as a core research task in ITS, aiming to infer learners' mastery of specific knowledge concepts by modeling the mapping between learning behavior data and knowledge states. However, existing research prioritizes model performance enhancement while neglecting the pervasive noise contamination in observed response data, significantly hindering practical deployment. Furthermore, current cognitive diagnosis models (CDMs) rely heavily on researchers' domain expertise for structural design, which fails to exhaustively explore architectural possibilities, thus leaving model architectures' full potential untapped. To address this issue, we propose OSCD, an evolutionary multi-objective One-Shot neural architecture search method for Cognitive Diagnosis, designed to efficiently and robustly improve the model's capability in assessing learner proficiency. Specifically, OSCD operates through two distinct stages: training and searching. During the training stage, we construct a search space encompassing diverse architectural combinations and train a weight-sharing supernet represented via the complete binary tree topology, enabling comprehensive exploration of potential architectures beyond manual design priors. In the searching stage, we formulate the optimal architecture search under heterogeneous noise scenarios as a multi-objective optimization problem (MOP), and develop an optimization framework integrating a Pareto-optimal solution search strategy with cross-scenario performance evaluation for resolution. Extensive experiments on real-world educational datasets validate the effectiveness and robustness of the optimal architectures discovered by our OSCD model for CD tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of network technologies, intelligent tutoring systems (ITS) have emerged to deliver increasingly precise and tailored personalized learning services. Cognitive diagnosis (CD) has emerged as a core research task in ITS, aiming to infer learners' mastery of specific knowledge concepts by modeling the mapping between learning behavior data and knowledge states. However, existing research prioritizes model performance enhancement while neglecting the pervasive noise contamination in observed response data, significantly hindering practical deployment. Furthermore, current cognitive diagnosis models (CDMs) rely heavily on researchers' domain expertise for structural design, which fails to exhaustively explore architectural possibilities, thus leaving model architectures' full potential untapped. To address this issue, we propose OSCD, an evolutionary multi-objective One-Shot neural architecture search method for Cognitive Diagnosis, designed to efficiently and robustly improve the model's capability in assessing learner proficiency. Specifically, OSCD operates through two distinct stages: training and searching. During the training stage, we construct a search space encompassing diverse architectural combinations and train a weight-sharing supernet represented via the complete binary tree topology, enabling comprehensive exploration of potential architectures beyond manual design priors. In the searching stage, we formulate the optimal architecture search under heterogeneous noise scenarios as a multi-objective optimization problem (MOP), and develop an optimization framework integrating a Pareto-optimal solution search strategy with cross-scenario performance evaluation for resolution. Extensive experiments on real-world educational datasets validate the effectiveness and robustness of the optimal architectures discovered by our OSCD model for CD tasks."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T13:17:40Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    17,
                    40,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "KDD2026, 15 pages",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Ziwen Wang"
                    },
                    {
                        "name": "Shangshang Yang"
                    },
                    {
                        "name": "Xiaoshan Yu"
                    },
                    {
                        "name": "Haiping Ma"
                    },
                    {
                        "name": "Xingyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xingyi Zhang"
                },
                "author": "Xingyi Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2510.15949v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.15949v2",
                "title": "ATLAS: Adaptive Trading with LLM AgentS Through Dynamic Prompt Optimization and Multi-Agent Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ATLAS: Adaptive Trading with LLM AgentS Through Dynamic Prompt Optimization and Multi-Agent Coordination"
                },
                "updated": "2026-01-08T13:08:59Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    13,
                    8,
                    59,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.15949v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.15949v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models show promise for financial decision-making, yet deploying them as autonomous trading agents raises fundamental challenges: how to adapt instructions when rewards arrive late and obscured by market noise, how to synthesize heterogeneous information streams into coherent decisions, and how to bridge the gap between model outputs and executable market actions. We present ATLAS (Adaptive Trading with LLM AgentS), a unified multi-agent framework that integrates structured information from markets, news, and corporate fundamentals to support robust trading decisions. Within ATLAS, the central trading agent operates in an order-aware action space, ensuring that outputs correspond to executable market orders rather than abstract signals. The agent can incorporate feedback while trading using Adaptive-OPRO, a novel prompt-optimization technique that dynamically adapts the prompt by incorporating real-time, stochastic feedback, leading to increasing performance over time. Across regime-specific equity studies and multiple LLM families, Adaptive-OPRO consistently outperforms fixed prompts, while reflection-based feedback fails to provide systematic gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models show promise for financial decision-making, yet deploying them as autonomous trading agents raises fundamental challenges: how to adapt instructions when rewards arrive late and obscured by market noise, how to synthesize heterogeneous information streams into coherent decisions, and how to bridge the gap between model outputs and executable market actions. We present ATLAS (Adaptive Trading with LLM AgentS), a unified multi-agent framework that integrates structured information from markets, news, and corporate fundamentals to support robust trading decisions. Within ATLAS, the central trading agent operates in an order-aware action space, ensuring that outputs correspond to executable market orders rather than abstract signals. The agent can incorporate feedback while trading using Adaptive-OPRO, a novel prompt-optimization technique that dynamically adapts the prompt by incorporating real-time, stochastic feedback, leading to increasing performance over time. Across regime-specific equity studies and multiple LLM families, Adaptive-OPRO consistently outperforms fixed prompts, while reflection-based feedback fails to provide systematic gains."
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-10T13:01:51Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    13,
                    1,
                    51,
                    4,
                    283,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR"
                },
                "authors": [
                    {
                        "name": "Charidimos Papadakis"
                    },
                    {
                        "name": "Angeliki Dimitriou"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Maria Lymperaiou"
                    },
                    {
                        "name": "Konstantinos Thomas"
                    },
                    {
                        "name": "Giorgos Stamou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Stamou"
                },
                "author": "Giorgos Stamou"
            },
            {
                "id": "http://arxiv.org/abs/2601.04899v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04899v1",
                "title": "Rotation-Robust Regression with Convolutional Model Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotation-Robust Regression with Convolutional Model Trees"
                },
                "updated": "2026-01-08T12:53:33Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    53,
                    33,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04899v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We study rotation-robust learning for image inputs using Convolutional Model Trees (CMTs) [1], whose split and leaf coefficients can be structured on the image grid and transformed geometrically at deployment time. In a controlled MNIST setting with a rotation-invariant regression target, we introduce three geometry-aware inductive biases for split directions -- convolutional smoothing, a tilt dominance constraint, and importance-based pruning -- and quantify their impact on robustness under in-plane rotations. We further evaluate a deployment-time orientation search that selects a discrete rotation maximizing a forest-level confidence proxy without updating model parameters. Orientation search improves robustness under severe rotations but can be harmful near the canonical orientation when confidence is misaligned with correctness. Finally, we observe consistent trends on MNIST digit recognition implemented as one-vs-rest regression, highlighting both the promise and limitations of confidence-based orientation selection for model-tree ensembles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study rotation-robust learning for image inputs using Convolutional Model Trees (CMTs) [1], whose split and leaf coefficients can be structured on the image grid and transformed geometrically at deployment time. In a controlled MNIST setting with a rotation-invariant regression target, we introduce three geometry-aware inductive biases for split directions -- convolutional smoothing, a tilt dominance constraint, and importance-based pruning -- and quantify their impact on robustness under in-plane rotations. We further evaluate a deployment-time orientation search that selects a discrete rotation maximizing a forest-level confidence proxy without updating model parameters. Orientation search improves robustness under severe rotations but can be harmful near the canonical orientation when confidence is misaligned with correctness. Finally, we observe consistent trends on MNIST digit recognition implemented as one-vs-rest regression, highlighting both the promise and limitations of confidence-based orientation selection for model-tree ensembles."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T12:53:33Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    53,
                    33,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hongyi Li"
                    },
                    {
                        "name": "William Ward Armstrong"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu"
            },
            {
                "id": "http://arxiv.org/abs/2601.04895v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04895v1",
                "title": "DVD: A Robust Method for Detecting Variant Contamination in Large Language Model Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DVD: A Robust Method for Detecting Variant Contamination in Large Language Model Evaluation"
                },
                "updated": "2026-01-08T12:48:40Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    48,
                    40,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04895v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Evaluating large language models (LLMs) is increasingly confounded by \\emph{variant contamination}: the training corpus contains semantically equivalent yet lexically or syntactically altered versions of test items. Unlike verbatim leakage, these paraphrased or structurally transformed variants evade existing detectors based on sampling consistency or perplexity, thereby inflating benchmark scores via memorization rather than genuine reasoning. We formalize this problem and introduce \\textbf{DVD} (\\textbf{D}etection via \\textbf{V}ariance of generation \\textbf{D}istribution), a single-sample detector that models the local output distribution induced by temperature sampling. Our key insight is that contaminated items trigger alternation between a \\emph{memory-adherence} state and a \\emph{perturbation-drift} state, yielding abnormally high variance in the synthetic difficulty of low-probability tokens; uncontaminated items remain in drift with comparatively smooth variance. We construct the first benchmark for variant contamination across two domains Omni-MATH and SuperGPQA by generating and filtering semantically equivalent variants, and simulate contamination via fine-tuning models of different scales and architectures (Qwen2.5 and Llama3.1). Across datasets and models, \\textbf{DVD} consistently outperforms perplexity-based, Min-$k$\\%++, edit-distance (CDD), and embedding-similarity baselines, while exhibiting strong robustness to hyperparameters. Our results establish variance of the generation distribution as a principled and practical fingerprint for detecting variant contamination in LLM evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language models (LLMs) is increasingly confounded by \\emph{variant contamination}: the training corpus contains semantically equivalent yet lexically or syntactically altered versions of test items. Unlike verbatim leakage, these paraphrased or structurally transformed variants evade existing detectors based on sampling consistency or perplexity, thereby inflating benchmark scores via memorization rather than genuine reasoning. We formalize this problem and introduce \\textbf{DVD} (\\textbf{D}etection via \\textbf{V}ariance of generation \\textbf{D}istribution), a single-sample detector that models the local output distribution induced by temperature sampling. Our key insight is that contaminated items trigger alternation between a \\emph{memory-adherence} state and a \\emph{perturbation-drift} state, yielding abnormally high variance in the synthetic difficulty of low-probability tokens; uncontaminated items remain in drift with comparatively smooth variance. We construct the first benchmark for variant contamination across two domains Omni-MATH and SuperGPQA by generating and filtering semantically equivalent variants, and simulate contamination via fine-tuning models of different scales and architectures (Qwen2.5 and Llama3.1). Across datasets and models, \\textbf{DVD} consistently outperforms perplexity-based, Min-$k$\\%++, edit-distance (CDD), and embedding-similarity baselines, while exhibiting strong robustness to hyperparameters. Our results establish variance of the generation distribution as a principled and practical fingerprint for detecting variant contamination in LLM evaluation."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T12:48:40Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    48,
                    40,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Renzhao Liang"
                    },
                    {
                        "name": "Jingru Chen"
                    },
                    {
                        "name": "Bo Jia"
                    },
                    {
                        "name": "Bo Deng"
                    },
                    {
                        "name": "Chenggang Xie"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Ke Jin"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Cunxiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Cunxiang Wang"
                },
                "author": "Cunxiang Wang"
            },
            {
                "id": "http://arxiv.org/abs/2601.04891v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04891v1",
                "title": "Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform"
                },
                "updated": "2026-01-08T12:42:17Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    42,
                    17,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04891v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision Language Models (VLMs) have shown strong performance on multimodal reasoning tasks, yet most evaluations focus on short videos and assume unconstrained computational resources. In industrial settings such as pharmaceutical content understanding, practitioners must process long-form videos under strict GPU, latency, and cost constraints, where many existing approaches fail to scale. In this work, we present an industrial GenAI framework that processes over 200,000 PDFs, 25,326 videos across eight formats (e.g., MP4, M4V, etc.), and 888 multilingual audio files in more than 20 languages. Our study makes three contributions: (i) an industrial large-scale architecture for multimodal reasoning in pharmaceutical domains; (ii) empirical analysis of over 40 VLMs on two leading benchmarks (Video-MME and MMBench) and proprietary dataset of 25,326 videos across 14 disease areas; and (iii) four findings relevant to long-form video reasoning: the role of multimodality, attention mechanism trade-offs, temporal reasoning limits, and challenges of video splitting under GPU constraints. Results show 3-8 times efficiency gains with SDPA attention on commodity GPUs, multimodality improving up to 8/12 task domains (especially length-dependent tasks), and clear bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs. Rather than proposing a new \"A+B\" model, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, and provide actionable guidance for both researchers and practitioners designing scalable multimodal systems for long-form video understanding in industrial domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have shown strong performance on multimodal reasoning tasks, yet most evaluations focus on short videos and assume unconstrained computational resources. In industrial settings such as pharmaceutical content understanding, practitioners must process long-form videos under strict GPU, latency, and cost constraints, where many existing approaches fail to scale. In this work, we present an industrial GenAI framework that processes over 200,000 PDFs, 25,326 videos across eight formats (e.g., MP4, M4V, etc.), and 888 multilingual audio files in more than 20 languages. Our study makes three contributions: (i) an industrial large-scale architecture for multimodal reasoning in pharmaceutical domains; (ii) empirical analysis of over 40 VLMs on two leading benchmarks (Video-MME and MMBench) and proprietary dataset of 25,326 videos across 14 disease areas; and (iii) four findings relevant to long-form video reasoning: the role of multimodality, attention mechanism trade-offs, temporal reasoning limits, and challenges of video splitting under GPU constraints. Results show 3-8 times efficiency gains with SDPA attention on commodity GPUs, multimodality improving up to 8/12 task domains (especially length-dependent tasks), and clear bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs. Rather than proposing a new \"A+B\" model, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, and provide actionable guidance for both researchers and practitioners designing scalable multimodal systems for long-form video understanding in industrial domains."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T12:42:17Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    42,
                    17,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Submitted to the Industry Track of Top Tier Conference; currently under peer review",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Suyash Mishra"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Srikanth Patil"
                    },
                    {
                        "name": "Satyanarayan Pati"
                    },
                    {
                        "name": "Baddu Narendra"
                    }
                ],
                "author_detail": {
                    "name": "Baddu Narendra"
                },
                "author": "Baddu Narendra"
            },
            {
                "id": "http://arxiv.org/abs/2601.04889v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04889v1",
                "title": "Faithful Summarisation under Disagreement via Belief-Level Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithful Summarisation under Disagreement via Belief-Level Aggregation"
                },
                "updated": "2026-01-08T12:40:47Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    40,
                    47,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04889v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Opinion and multi-document summarisation often involve genuinely conflicting viewpoints, yet many existing approaches, particularly LLM-based systems, implicitly smooth disagreement and over-represent majority opinions. This limits the faithfulness of generated summaries in opinion-heavy settings. We introduce a disagreement-aware synthesis pipeline that separates belief-level aggregation from language generation. Documents are first represented as structured belief sets and aggregated using distance-based belief merging operators that explicitly model conflict. Large language models are then used only to realise the aggregated beliefs as natural language summaries. We evaluate the approach across multiple model families and scales, comparing it to methods that perform explicit aggregation during generation. Our results show that while sufficiently large models can match belief-level aggregation when aggregation is handled at generation time, this behaviour is not stable across architectures or capacities. In contrast, belief-level aggregation combined with simple prompting yields consistently strong disagreement-aware performance across models, while maintaining fluent and grounded summaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Opinion and multi-document summarisation often involve genuinely conflicting viewpoints, yet many existing approaches, particularly LLM-based systems, implicitly smooth disagreement and over-represent majority opinions. This limits the faithfulness of generated summaries in opinion-heavy settings. We introduce a disagreement-aware synthesis pipeline that separates belief-level aggregation from language generation. Documents are first represented as structured belief sets and aggregated using distance-based belief merging operators that explicitly model conflict. Large language models are then used only to realise the aggregated beliefs as natural language summaries. We evaluate the approach across multiple model families and scales, comparing it to methods that perform explicit aggregation during generation. Our results show that while sufficiently large models can match belief-level aggregation when aggregation is handled at generation time, this behaviour is not stable across architectures or capacities. In contrast, belief-level aggregation combined with simple prompting yields consistently strong disagreement-aware performance across models, while maintaining fluent and grounded summaries."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T12:40:47Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    40,
                    47,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Favour Yahdii Aghaebe"
                    },
                    {
                        "name": "Tanefa Apekey"
                    },
                    {
                        "name": "Elizabeth Williams"
                    },
                    {
                        "name": "Nafise Sadat Moosavi"
                    }
                ],
                "author_detail": {
                    "name": "Nafise Sadat Moosavi"
                },
                "author": "Nafise Sadat Moosavi"
            },
            {
                "id": "http://arxiv.org/abs/2601.04888v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04888v1",
                "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents"
                },
                "updated": "2026-01-08T12:39:05Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    39,
                    5,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04888v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T12:39:05Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    39,
                    5,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "16 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Tongyu Wen"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou"
            },
            {
                "id": "http://arxiv.org/abs/2512.17435v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17435v2",
                "title": "ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination"
                },
                "updated": "2026-01-08T12:38:18Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    38,
                    18,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17435v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Visual navigation is a fundamental capability for autonomous home-assistance robots, enabling long-horizon tasks such as object search. While recent methods have leveraged Large Language Models (LLMs) to incorporate commonsense reasoning and improve exploration efficiency, their planning remains constrained by textual representations, which cannot adequately capture spatial occupancy or scene geometry--critical factors for navigation decisions. We explore whether Vision-Language Models (VLMs) can achieve mapless visual navigation using only onboard RGB/RGB-D streams, unlocking their potential for spatial perception and planning. We achieve this through an imagination-powered navigation framework, ImagineNav++, which imagines future observation images from candidate robot views and translates navigation planning into a simple best-view image selection problem for VLMs. First, a future-view imagination module distills human navigation preferences to generate semantically meaningful viewpoints with high exploration potential. These imagined views then serve as visual prompts for the VLM to identify the most informative viewpoint. To maintain spatial consistency, we develop a selective foveation memory mechanism, which hierarchically integrates keyframe observations via a sparse-to-dense framework, constructing a compact yet comprehensive memory for long-term spatial reasoning. This approach transforms goal-oriented navigation into a series of tractable point-goal navigation tasks. Extensive experiments on open-vocabulary object and instance navigation benchmarks show that ImagineNav++ achieves SOTA performance in mapless settings, even surpassing most map-based methods, highlighting the importance of scene imagination and memory in VLM-based spatial reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual navigation is a fundamental capability for autonomous home-assistance robots, enabling long-horizon tasks such as object search. While recent methods have leveraged Large Language Models (LLMs) to incorporate commonsense reasoning and improve exploration efficiency, their planning remains constrained by textual representations, which cannot adequately capture spatial occupancy or scene geometry--critical factors for navigation decisions. We explore whether Vision-Language Models (VLMs) can achieve mapless visual navigation using only onboard RGB/RGB-D streams, unlocking their potential for spatial perception and planning. We achieve this through an imagination-powered navigation framework, ImagineNav++, which imagines future observation images from candidate robot views and translates navigation planning into a simple best-view image selection problem for VLMs. First, a future-view imagination module distills human navigation preferences to generate semantically meaningful viewpoints with high exploration potential. These imagined views then serve as visual prompts for the VLM to identify the most informative viewpoint. To maintain spatial consistency, we develop a selective foveation memory mechanism, which hierarchically integrates keyframe observations via a sparse-to-dense framework, constructing a compact yet comprehensive memory for long-term spatial reasoning. This approach transforms goal-oriented navigation into a series of tractable point-goal navigation tasks. Extensive experiments on open-vocabulary object and instance navigation benchmarks show that ImagineNav++ achieves SOTA performance in mapless settings, even surpassing most map-based methods, highlighting the importance of scene imagination and memory in VLM-based spatial reasoning."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T10:40:16Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    10,
                    40,
                    16,
                    4,
                    353,
                    0
                ],
                "arxiv_comment": "17 pages, 10 figures. arXiv admin note: text overlap with arXiv:2410.09874",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Xinxin Zhao"
                    },
                    {
                        "name": "Wenzhe Cai"
                    },
                    {
                        "name": "Changyin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Changyin Sun"
                },
                "author": "Changyin Sun"
            },
            {
                "id": "http://arxiv.org/abs/2601.04885v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04885v1",
                "title": "CuMA: Aligning LLMs with Sparse Cultural Values via Demographic-Aware Mixture of Adapters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CuMA: Aligning LLMs with Sparse Cultural Values via Demographic-Aware Mixture of Adapters"
                },
                "updated": "2026-01-08T12:30:43Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    30,
                    43,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04885v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As Large Language Models (LLMs) serve a global audience, alignment must transition from enforcing universal consensus to respecting cultural pluralism. We demonstrate that dense models, when forced to fit conflicting value distributions, suffer from \\textbf{Mean Collapse}, converging to a generic average that fails to represent diverse groups. We attribute this to \\textbf{Cultural Sparsity}, where gradient interference prevents dense parameters from spanning distinct cultural modes. To resolve this, we propose \\textbf{\\textsc{CuMA}} (\\textbf{Cu}ltural \\textbf{M}ixture of \\textbf{A}dapters), a framework that frames alignment as a \\textbf{conditional capacity separation} problem. By incorporating demographic-aware routing, \\textsc{CuMA} internalizes a \\textit{Latent Cultural Topology} to explicitly disentangle conflicting gradients into specialized expert subspaces. Extensive evaluations on WorldValuesBench, Community Alignment, and PRISM demonstrate that \\textsc{CuMA} achieves state-of-the-art performance, significantly outperforming both dense baselines and semantic-only MoEs. Crucially, our analysis confirms that \\textsc{CuMA} effectively mitigates mean collapse, preserving cultural diversity. Our code is available at https://github.com/Throll/CuMA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) serve a global audience, alignment must transition from enforcing universal consensus to respecting cultural pluralism. We demonstrate that dense models, when forced to fit conflicting value distributions, suffer from \\textbf{Mean Collapse}, converging to a generic average that fails to represent diverse groups. We attribute this to \\textbf{Cultural Sparsity}, where gradient interference prevents dense parameters from spanning distinct cultural modes. To resolve this, we propose \\textbf{\\textsc{CuMA}} (\\textbf{Cu}ltural \\textbf{M}ixture of \\textbf{A}dapters), a framework that frames alignment as a \\textbf{conditional capacity separation} problem. By incorporating demographic-aware routing, \\textsc{CuMA} internalizes a \\textit{Latent Cultural Topology} to explicitly disentangle conflicting gradients into specialized expert subspaces. Extensive evaluations on WorldValuesBench, Community Alignment, and PRISM demonstrate that \\textsc{CuMA} achieves state-of-the-art performance, significantly outperforming both dense baselines and semantic-only MoEs. Crucially, our analysis confirms that \\textsc{CuMA} effectively mitigates mean collapse, preserving cultural diversity. Our code is available at https://github.com/Throll/CuMA."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T12:30:43Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    30,
                    43,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ao Sun"
                    },
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Zhe Tan"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Jiachen Zhu"
                    },
                    {
                        "name": "Shu Su"
                    },
                    {
                        "name": "Yuheng Jia"
                    }
                ],
                "author_detail": {
                    "name": "Yuheng Jia"
                },
                "author": "Yuheng Jia"
            },
            {
                "id": "http://arxiv.org/abs/2601.04879v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04879v1",
                "title": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis"
                },
                "updated": "2026-01-08T12:27:52Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    27,
                    52,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04879v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Synthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at https://github.com/Melmaphother/Mind2Report.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at https://github.com/Melmaphother/Mind2Report."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T12:27:52Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    27,
                    52,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "26 Pages, 9 Figures, 7 Tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Daoyu Wang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Shuo Yu"
                    },
                    {
                        "name": "Xiaoyu Tao"
                    },
                    {
                        "name": "Yuqian Wang"
                    },
                    {
                        "name": "Chengzhong Chu"
                    },
                    {
                        "name": "Yu Duan"
                    },
                    {
                        "name": "Mingkang Long"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen"
            },
            {
                "id": "http://arxiv.org/abs/2601.04878v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04878v1",
                "title": "Higher-Order Knowledge Representations for Agentic Scientific Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Higher-Order Knowledge Representations for Agentic Scientific Reasoning"
                },
                "updated": "2026-01-08T12:25:37Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    25,
                    37,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04878v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Scientific inquiry requires systems-level reasoning that integrates heterogeneous experimental data, cross-domain knowledge, and mechanistic evidence into coherent explanations. While Large Language Models (LLMs) offer inferential capabilities, they often depend on retrieval-augmented contexts that lack structural depth. Traditional Knowledge Graphs (KGs) attempt to bridge this gap, yet their pairwise constraints fail to capture the irreducible higher-order interactions that govern emergent physical behavior. To address this, we introduce a methodology for constructing hypergraph-based knowledge representations that faithfully encode multi-entity relationships. Applied to a corpus of ~1,100 manuscripts on biocomposite scaffolds, our framework constructs a global hypergraph of 161,172 nodes and 320,201 hyperedges, revealing a scale-free topology (power law exponent ~1.23) organized around highly connected conceptual hubs. This representation prevents the combinatorial explosion typical of pairwise expansions and explicitly preserves the co-occurrence context of scientific formulations. We further demonstrate that equipping agentic systems with hypergraph traversal tools, specifically using node-intersection constraints, enables them to bridge semantically distant concepts. By exploiting these higher-order pathways, the system successfully generates grounded mechanistic hypotheses for novel composite materials, such as linking cerium oxide to PCL scaffolds via chitosan intermediates. This work establishes a \"teacherless\" agentic reasoning system where hypergraph topology acts as a verifiable guardrail, accelerating scientific discovery by uncovering relationships obscured by traditional graph methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific inquiry requires systems-level reasoning that integrates heterogeneous experimental data, cross-domain knowledge, and mechanistic evidence into coherent explanations. While Large Language Models (LLMs) offer inferential capabilities, they often depend on retrieval-augmented contexts that lack structural depth. Traditional Knowledge Graphs (KGs) attempt to bridge this gap, yet their pairwise constraints fail to capture the irreducible higher-order interactions that govern emergent physical behavior. To address this, we introduce a methodology for constructing hypergraph-based knowledge representations that faithfully encode multi-entity relationships. Applied to a corpus of ~1,100 manuscripts on biocomposite scaffolds, our framework constructs a global hypergraph of 161,172 nodes and 320,201 hyperedges, revealing a scale-free topology (power law exponent ~1.23) organized around highly connected conceptual hubs. This representation prevents the combinatorial explosion typical of pairwise expansions and explicitly preserves the co-occurrence context of scientific formulations. We further demonstrate that equipping agentic systems with hypergraph traversal tools, specifically using node-intersection constraints, enables them to bridge semantically distant concepts. By exploiting these higher-order pathways, the system successfully generates grounded mechanistic hypotheses for novel composite materials, such as linking cerium oxide to PCL scaffolds via chitosan intermediates. This work establishes a \"teacherless\" agentic reasoning system where hypergraph topology acts as a verifiable guardrail, accelerating scientific discovery by uncovering relationships obscured by traditional graph methods."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T12:25:37Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    25,
                    37,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Isabella A. Stewart"
                    },
                    {
                        "name": "Markus J. Buehler"
                    }
                ],
                "author_detail": {
                    "name": "Markus J. Buehler"
                },
                "author": "Markus J. Buehler"
            },
            {
                "id": "http://arxiv.org/abs/2601.04875v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04875v1",
                "title": "EvolSQL: Structure-Aware Evolution for Scalable Text-to-SQL Data Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolSQL: Structure-Aware Evolution for Scalable Text-to-SQL Data Synthesis"
                },
                "updated": "2026-01-08T12:19:50Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    19,
                    50,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04875v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Training effective Text-to-SQL models remains challenging due to the scarcity of high-quality, diverse, and structurally complex datasets. Existing methods either rely on limited human-annotated corpora, or synthesize datasets directly by simply prompting LLMs without explicit control over SQL structures, often resulting in limited structural diversity and complexity. To address this, we introduce EvolSQL, a structure-aware data synthesis framework that evolves SQL queries from seed data into richer and more semantically diverse forms. EvolSQL starts with an exploratory Query-SQL expansion to broaden question diversity and improve schema coverage, and then applies an adaptive directional evolution strategy using six atomic transformation operators derived from the SQL Abstract Syntax Tree to progressively increase query complexity across relational, predicate, aggregation, and nesting dimensions. An execution-grounded SQL refinement module and schema-aware deduplication further ensure the creation of high-quality, structurally diverse mapping pairs. Experimental results show that a 7B model fine-tuned on our data outperforms one trained on the much larger SynSQL dataset using only 1/18 of the data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training effective Text-to-SQL models remains challenging due to the scarcity of high-quality, diverse, and structurally complex datasets. Existing methods either rely on limited human-annotated corpora, or synthesize datasets directly by simply prompting LLMs without explicit control over SQL structures, often resulting in limited structural diversity and complexity. To address this, we introduce EvolSQL, a structure-aware data synthesis framework that evolves SQL queries from seed data into richer and more semantically diverse forms. EvolSQL starts with an exploratory Query-SQL expansion to broaden question diversity and improve schema coverage, and then applies an adaptive directional evolution strategy using six atomic transformation operators derived from the SQL Abstract Syntax Tree to progressively increase query complexity across relational, predicate, aggregation, and nesting dimensions. An execution-grounded SQL refinement module and schema-aware deduplication further ensure the creation of high-quality, structurally diverse mapping pairs. Experimental results show that a 7B model fine-tuned on our data outperforms one trained on the much larger SynSQL dataset using only 1/18 of the data."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T12:19:50Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    19,
                    50,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "18 pages",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Xuanguang Pan"
                    },
                    {
                        "name": "Chongyang Tao"
                    },
                    {
                        "name": "Jiayuan Bai"
                    },
                    {
                        "name": "Jianling Gao"
                    },
                    {
                        "name": "Zhengwei Tao"
                    },
                    {
                        "name": "Xiansheng Zhou"
                    },
                    {
                        "name": "Gavin Cheung"
                    },
                    {
                        "name": "Shuai Ma"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Ma"
                },
                "author": "Shuai Ma"
            },
            {
                "id": "http://arxiv.org/abs/2511.10643v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10643v3",
                "title": "Black-Box On-Policy Distillation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-Box On-Policy Distillation of Large Language Models"
                },
                "updated": "2026-01-08T12:15:40Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    12,
                    15,
                    40,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10643v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10643v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:58:37Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    58,
                    37,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tianzhu Ye"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Zewen Chi"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei"
            },
            {
                "id": "http://arxiv.org/abs/2601.04861v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04861v1",
                "title": "Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models"
                },
                "updated": "2026-01-08T11:56:09Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    11,
                    56,
                    9,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04861v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\\% while reducing cost by up to 79.78\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\\% while reducing cost by up to 79.78\\%."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T11:56:09Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    11,
                    56,
                    9,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jingbo Wang"
                    },
                    {
                        "name": "Sendong Zhao"
                    },
                    {
                        "name": "Jiatong Liu"
                    },
                    {
                        "name": "Haochun Wang"
                    },
                    {
                        "name": "Wanting Li"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu"
            },
            {
                "id": "http://arxiv.org/abs/2601.04859v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04859v1",
                "title": "A Navigational Approach for Comprehensive RAG via Traversal over Proposition Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Navigational Approach for Comprehensive RAG via Traversal over Proposition Graphs"
                },
                "updated": "2026-01-08T11:50:40Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    11,
                    50,
                    40,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04859v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Standard RAG pipelines based on chunking excel at simple factual retrieval but fail on complex multi-hop queries due to a lack of structural connectivity. Conversely, initial strategies that interleave retrieval with reasoning often lack global corpus awareness, while Knowledge Graph (KG)-based RAG performs strongly on complex multi-hop tasks but suffers on fact-oriented single-hop queries. To bridge this gap, we propose a novel RAG framework: ToPG (Traversal over Proposition Graphs). ToPG models its knowledge base as a heterogeneous graph of propositions, entities, and passages, effectively combining the granular fact density of propositions with graph connectivity. We leverage this structure using iterative Suggestion-Selection cycles, where the Suggestion phase enables a query-aware traversal of the graph, and the Selection phase provides LLM feedback to prune irrelevant propositions and seed the next iteration. Evaluated on three distinct QA tasks (Simple, Complex, and Abstract QA), ToPG demonstrates strong performance across both accuracy- and quality-based metrics. Overall, ToPG shows that query-aware graph traversal combined with factual granularity is a critical component for efficient structured RAG systems. ToPG is available at https://github.com/idiap/ToPG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard RAG pipelines based on chunking excel at simple factual retrieval but fail on complex multi-hop queries due to a lack of structural connectivity. Conversely, initial strategies that interleave retrieval with reasoning often lack global corpus awareness, while Knowledge Graph (KG)-based RAG performs strongly on complex multi-hop tasks but suffers on fact-oriented single-hop queries. To bridge this gap, we propose a novel RAG framework: ToPG (Traversal over Proposition Graphs). ToPG models its knowledge base as a heterogeneous graph of propositions, entities, and passages, effectively combining the granular fact density of propositions with graph connectivity. We leverage this structure using iterative Suggestion-Selection cycles, where the Suggestion phase enables a query-aware traversal of the graph, and the Selection phase provides LLM feedback to prune irrelevant propositions and seed the next iteration. Evaluated on three distinct QA tasks (Simple, Complex, and Abstract QA), ToPG demonstrates strong performance across both accuracy- and quality-based metrics. Overall, ToPG shows that query-aware graph traversal combined with factual granularity is a critical component for efficient structured RAG systems. ToPG is available at https://github.com/idiap/ToPG."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T11:50:40Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    11,
                    50,
                    40,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "23 pages, 10 figures, 6 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Maxime Delmas"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Andr Freitas"
                    }
                ],
                "author_detail": {
                    "name": "Andr Freitas"
                },
                "author": "Andr Freitas"
            },
            {
                "id": "http://arxiv.org/abs/2601.04857v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04857v1",
                "title": "MisSpans: Fine-Grained False Span Identification in Cross-Domain Fake News",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MisSpans: Fine-Grained False Span Identification in Cross-Domain Fake News"
                },
                "updated": "2026-01-08T11:46:30Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    11,
                    46,
                    30,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04857v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Online misinformation is increasingly pervasive, yet most existing benchmarks and methods evaluate veracity at the level of whole claims or paragraphs using coarse binary labels, obscuring how true and false details often co-exist within single sentences. These simplifications also limit interpretability: global explanations cannot identify which specific segments are misleading or differentiate how a detail is false (e.g., distorted vs. fabricated). To address these gaps, we introduce MisSpans, the first multi-domain, human-annotated benchmark for span-level misinformation detection and analysis, consisting of paired real and fake news stories. MisSpans defines three complementary tasks: MisSpansIdentity for pinpointing false spans within sentences, MisSpansType for categorising false spans by misinformation type, and MisSpansExplanation for providing rationales grounded in identified spans. Together, these tasks enable fine-grained localisation, nuanced characterisation beyond true/false and actionable explanations. Expert annotators were guided by standardised guidelines and consistency checks, leading to high inter-annotator agreement. We evaluate 15 representative LLMs, including reasoning-enhanced and non-reasoning variants, under zero-shot and one-shot settings. Results reveal the challenging nature of fine-grained misinformation identification and analysis, and highlight the need for a deeper understanding of how performance may be influenced by multiple interacting factors, including model size and reasoning capabilities, along with domain-specific textual features. This project will be available at https://github.com/lzw108/MisSpans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online misinformation is increasingly pervasive, yet most existing benchmarks and methods evaluate veracity at the level of whole claims or paragraphs using coarse binary labels, obscuring how true and false details often co-exist within single sentences. These simplifications also limit interpretability: global explanations cannot identify which specific segments are misleading or differentiate how a detail is false (e.g., distorted vs. fabricated). To address these gaps, we introduce MisSpans, the first multi-domain, human-annotated benchmark for span-level misinformation detection and analysis, consisting of paired real and fake news stories. MisSpans defines three complementary tasks: MisSpansIdentity for pinpointing false spans within sentences, MisSpansType for categorising false spans by misinformation type, and MisSpansExplanation for providing rationales grounded in identified spans. Together, these tasks enable fine-grained localisation, nuanced characterisation beyond true/false and actionable explanations. Expert annotators were guided by standardised guidelines and consistency checks, leading to high inter-annotator agreement. We evaluate 15 representative LLMs, including reasoning-enhanced and non-reasoning variants, under zero-shot and one-shot settings. Results reveal the challenging nature of fine-grained misinformation identification and analysis, and highlight the need for a deeper understanding of how performance may be influenced by multiple interacting factors, including model size and reasoning capabilities, along with domain-specific textual features. This project will be available at https://github.com/lzw108/MisSpans."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T11:46:30Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    11,
                    46,
                    30,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "Work in progress",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Paul Thompson"
                    },
                    {
                        "name": "Jiaqi Rong"
                    },
                    {
                        "name": "Baojie Qu"
                    },
                    {
                        "name": "Runteng Guo"
                    },
                    {
                        "name": "Min Peng"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Ananiadou"
                },
                "author": "Sophia Ananiadou"
            },
            {
                "id": "http://arxiv.org/abs/2601.04853v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04853v1",
                "title": "RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection"
                },
                "updated": "2026-01-08T11:43:16Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    11,
                    43,
                    16,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04853v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample's semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at https://github.com/lzw108/RAAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample's semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at https://github.com/lzw108/RAAR."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T11:43:16Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    11,
                    43,
                    16,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Runteng Guo"
                    },
                    {
                        "name": "Baojie Qu"
                    },
                    {
                        "name": "Yuechen Jiang"
                    },
                    {
                        "name": "Min Peng"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Ananiadou"
                },
                "author": "Sophia Ananiadou"
            },
            {
                "id": "http://arxiv.org/abs/2409.00826v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2409.00826v2",
                "title": "Digital Homunculi and Institutional Design: Breaking Through the Experimentation Bottleneck",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Homunculi and Institutional Design: Breaking Through the Experimentation Bottleneck"
                },
                "updated": "2026-01-08T11:30:12Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    11,
                    30,
                    12,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2409.00826v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2409.00826v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Democracy research faces a longstanding experimentation bottleneck. Potential institutional innovations remain untested because human-subject studies are slow, expensive, and ethically fraught. This paper argues that digital homunculi, that is, GenAI-powered agents role-playing humans in diverse institutional settings, could offer a way to break through the bottleneck. In contrast to the legacy agent-based modeling, building complexity from transparent simple rules, the digital homunculi methodology aims to extract latent human behavioral knowledge from opaque large language models. To this ends, it designs multi-agent interactions as elicitation devices to trigger in LLMs human-like behavior that can be recorded as synthetic data. However, the validity of synthetic data remains an open question. Success requires that accurate, coherent, transferable models of humans ('little humans' - homunculi) already lurk within GenAI's inscrutable matrices and can be lured out via the social simulation role-play exercise. At the same time, to the extent these attempts are successful, they promise to completely transform the political economy of institutional research from scarcity to abundance. To help mitigate the number of challenges along the way to such success, I propose concrete validation strategies including behavioral back-testing via knowledge cutoffs, and outline infrastructure requirements for rigorous evaluation. The stakes are high: legacy democratic institutions develop at much slower pace than the surrounding technological landscape. If they falter, we lack a repository of tested backup alternatives. Breaking through the experimentation bottleneck must be a priority and digital homunculi may be quickly maturing into a methodology capable of achieving this feat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Democracy research faces a longstanding experimentation bottleneck. Potential institutional innovations remain untested because human-subject studies are slow, expensive, and ethically fraught. This paper argues that digital homunculi, that is, GenAI-powered agents role-playing humans in diverse institutional settings, could offer a way to break through the bottleneck. In contrast to the legacy agent-based modeling, building complexity from transparent simple rules, the digital homunculi methodology aims to extract latent human behavioral knowledge from opaque large language models. To this ends, it designs multi-agent interactions as elicitation devices to trigger in LLMs human-like behavior that can be recorded as synthetic data. However, the validity of synthetic data remains an open question. Success requires that accurate, coherent, transferable models of humans ('little humans' - homunculi) already lurk within GenAI's inscrutable matrices and can be lured out via the social simulation role-play exercise. At the same time, to the extent these attempts are successful, they promise to completely transform the political economy of institutional research from scarcity to abundance. To help mitigate the number of challenges along the way to such success, I propose concrete validation strategies including behavioral back-testing via knowledge cutoffs, and outline infrastructure requirements for rigorous evaluation. The stakes are high: legacy democratic institutions develop at much slower pace than the surrounding technological landscape. If they falter, we lack a repository of tested backup alternatives. Breaking through the experimentation bottleneck must be a priority and digital homunculi may be quickly maturing into a methodology capable of achieving this feat."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-09-01T19:57:32Z",
                "published_parsed": [
                    2024,
                    9,
                    1,
                    19,
                    57,
                    32,
                    6,
                    245,
                    0
                ],
                "arxiv_comment": "28 pages",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Petr Specian"
                    }
                ],
                "author_detail": {
                    "name": "Petr Specian"
                },
                "author": "Petr Specian"
            },
            {
                "id": "http://arxiv.org/abs/2505.21400v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.21400v2",
                "title": "Breaking AR's Sampling Bottleneck: Provable Acceleration via Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking AR's Sampling Bottleneck: Provable Acceleration via Diffusion Language Models"
                },
                "updated": "2026-01-08T11:30:06Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    11,
                    30,
                    6,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.21400v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.21400v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models have emerged as a powerful paradigm for modern generative modeling, demonstrating strong potential for large language models (LLMs). Unlike conventional autoregressive (AR) models that generate tokens sequentially, diffusion models allow for parallel sampling, offering a promising path to accelerate generation and eliminate the left-to-right generation constraints. Despite their empirical success, theoretical understandings of diffusion language models remain underdeveloped. In this work, we develop convergence guarantees for diffusion language models from an information-theoretic perspective. Our analysis demonstrates that the sampling error, measured by the Kullback-Leibler (KL) divergence, decays inversely with the number of iterations $T$ and scales linearly with the mutual information between tokens in the target text sequence. Crucially, our theory covers the regime $T<L$, where $L$ is the text sequence length. This justifies that high-quality samples can be generated with fewer iterations than $L$, thereby breaking the fundamental sampling bottleneck of $L$ steps required by AR models. We further establish matching upper and lower bounds, up to some constant factor, that shows the tightness of our convergence analysis. These results offer novel theoretical insights into the practical effectiveness of diffusion language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a powerful paradigm for modern generative modeling, demonstrating strong potential for large language models (LLMs). Unlike conventional autoregressive (AR) models that generate tokens sequentially, diffusion models allow for parallel sampling, offering a promising path to accelerate generation and eliminate the left-to-right generation constraints. Despite their empirical success, theoretical understandings of diffusion language models remain underdeveloped. In this work, we develop convergence guarantees for diffusion language models from an information-theoretic perspective. Our analysis demonstrates that the sampling error, measured by the Kullback-Leibler (KL) divergence, decays inversely with the number of iterations $T$ and scales linearly with the mutual information between tokens in the target text sequence. Crucially, our theory covers the regime $T<L$, where $L$ is the text sequence length. This justifies that high-quality samples can be generated with fewer iterations than $L$, thereby breaking the fundamental sampling bottleneck of $L$ steps required by AR models. We further establish matching upper and lower bounds, up to some constant factor, that shows the tightness of our convergence analysis. These results offer novel theoretical insights into the practical effectiveness of diffusion language models."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-27T16:24:20Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    16,
                    24,
                    20,
                    1,
                    147,
                    0
                ],
                "arxiv_comment": "This is the full version of a paper published at NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Changxiao Cai"
                    }
                ],
                "author_detail": {
                    "name": "Changxiao Cai"
                },
                "author": "Changxiao Cai"
            },
            {
                "id": "http://arxiv.org/abs/2508.12611v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.12611v3",
                "title": "An LLM + ASP Workflow for Joint Entity-Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM + ASP Workflow for Joint Entity-Relation Extraction"
                },
                "updated": "2026-01-08T11:15:52Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    11,
                    15,
                    52,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.12611v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.12611v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.4204/EPTCS.439.6",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Joint entity-relation extraction (JERE) identifies both entities and their relationships simultaneously. Traditional machine-learning based approaches to performing this task require a large corpus of annotated data and lack the ability to easily incorporate domain specific information in the construction of the model. Therefore, creating a model for JERE is often labor intensive, time consuming, and elaboration intolerant. In this paper, we propose harnessing the capabilities of generative pre-trained large language models (LLMs) and the knowledge representation and reasoning capabilities of Answer Set Programming (ASP) to perform JERE. We present a generic workflow for JERE using LLMs and ASP. The workflow is generic in the sense that it can be applied for JERE in any domain. It takes advantage of LLM's capability in natural language understanding in that it works directly with unannotated text. It exploits the elaboration tolerant feature of ASP in that no modification of its core program is required when additional domain specific knowledge, in the form of type specifications, is found and needs to be used. We demonstrate the usefulness of the proposed workflow through experiments with limited training data on three well-known benchmarks for JERE. The results of our experiments show that the LLM + ASP workflow is better than state-of-the-art JERE systems in several categories with only 10% of training data. It is able to achieve a 2.5 times (35% over 15%) improvement in the Relation Extraction task for the SciERC corpus, one of the most difficult benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint entity-relation extraction (JERE) identifies both entities and their relationships simultaneously. Traditional machine-learning based approaches to performing this task require a large corpus of annotated data and lack the ability to easily incorporate domain specific information in the construction of the model. Therefore, creating a model for JERE is often labor intensive, time consuming, and elaboration intolerant. In this paper, we propose harnessing the capabilities of generative pre-trained large language models (LLMs) and the knowledge representation and reasoning capabilities of Answer Set Programming (ASP) to perform JERE. We present a generic workflow for JERE using LLMs and ASP. The workflow is generic in the sense that it can be applied for JERE in any domain. It takes advantage of LLM's capability in natural language understanding in that it works directly with unannotated text. It exploits the elaboration tolerant feature of ASP in that no modification of its core program is required when additional domain specific knowledge, in the form of type specifications, is found and needs to be used. We demonstrate the usefulness of the proposed workflow through experiments with limited training data on three well-known benchmarks for JERE. The results of our experiments show that the LLM + ASP workflow is better than state-of-the-art JERE systems in several categories with only 10% of training data. It is able to achieve a 2.5 times (35% over 15%) improvement in the Relation Extraction task for the SciERC corpus, one of the most difficult benchmarks."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-18T04:15:35Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    4,
                    15,
                    35,
                    0,
                    230,
                    0
                ],
                "arxiv_comment": "In Proceedings ICLP 2025, arXiv:2601.00047",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "arxiv_journal_ref": "EPTCS 439, 2026, pp. 63-75",
                "authors": [
                    {
                        "name": "Trang Tran"
                    },
                    {
                        "name": "Trung Hoang Le"
                    },
                    {
                        "name": "Huiping Cao"
                    },
                    {
                        "name": "Tran Cao Son"
                    }
                ],
                "author_detail": {
                    "name": "Tran Cao Son"
                },
                "arxiv_affiliation": "New Mexico State University",
                "author": "Tran Cao Son",
                "arxiv_doi": "10.4204/EPTCS.439.6"
            },
            {
                "id": "http://arxiv.org/abs/2503.10509v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.10509v3",
                "title": "From Actions to Words: Towards Abstractive-Textual Policy Summarization in RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Actions to Words: Towards Abstractive-Textual Policy Summarization in RL"
                },
                "updated": "2026-01-08T11:06:58Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    11,
                    6,
                    58,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.10509v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.10509v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Explaining reinforcement learning agents is challenging because policies emerge from complex reward structures and neural representations that are difficult for humans to interpret. Existing approaches often rely on curated demonstrations that expose local behaviors but provide limited insight into an agent's global strategy, leaving users to infer intent from raw observations. We propose SySLLM (Synthesized Summary using Large Language Models), a framework that reframes policy interpretation as a language-generation problem. Instead of visual demonstrations, SySLLM converts spatiotemporal trajectories into structured text and prompts an LLM to generate coherent summaries describing the agent's goals, exploration style, and decision patterns. SySLLM scales to long-horizon, semantically rich environments without task-specific fine-tuning, leveraging LLM world knowledge and compositional reasoning to capture latent behavioral structure across policies. Expert evaluations show strong alignment with human analyses, and a large-scale user study found that 75.5% of participants preferred SySLLM summaries over state-of-the-art demonstration-based explanations. Together, these results position abstractive textual summarization as a paradigm for interpreting complex RL behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining reinforcement learning agents is challenging because policies emerge from complex reward structures and neural representations that are difficult for humans to interpret. Existing approaches often rely on curated demonstrations that expose local behaviors but provide limited insight into an agent's global strategy, leaving users to infer intent from raw observations. We propose SySLLM (Synthesized Summary using Large Language Models), a framework that reframes policy interpretation as a language-generation problem. Instead of visual demonstrations, SySLLM converts spatiotemporal trajectories into structured text and prompts an LLM to generate coherent summaries describing the agent's goals, exploration style, and decision patterns. SySLLM scales to long-horizon, semantically rich environments without task-specific fine-tuning, leveraging LLM world knowledge and compositional reasoning to capture latent behavioral structure across policies. Expert evaluations show strong alignment with human analyses, and a large-scale user study found that 75.5% of participants preferred SySLLM summaries over state-of-the-art demonstration-based explanations. Together, these results position abstractive textual summarization as a paradigm for interpreting complex RL behavior."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-13T16:10:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    10,
                    14,
                    3,
                    72,
                    0
                ],
                "arxiv_comment": "In Proceedings of AAMAS 2026 (The 25th International Conference on Autonomous Agents and Multi-Agent Systems)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Sahar Admoni"
                    },
                    {
                        "name": "Assaf Hallak"
                    },
                    {
                        "name": "Yftah Ziser"
                    },
                    {
                        "name": "Omer Ben-Porat"
                    },
                    {
                        "name": "Ofra Amir"
                    }
                ],
                "author_detail": {
                    "name": "Ofra Amir"
                },
                "author": "Ofra Amir"
            },
            {
                "id": "http://arxiv.org/abs/2601.04823v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04823v1",
                "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation"
                },
                "updated": "2026-01-08T10:58:51Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    10,
                    58,
                    51,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04823v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T10:58:51Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    10,
                    58,
                    51,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Guanzhi Deng"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Ronghao Chen"
                    },
                    {
                        "name": "Huacan Wang"
                    },
                    {
                        "name": "Linqi Song"
                    },
                    {
                        "name": "Lijie Wen"
                    }
                ],
                "author_detail": {
                    "name": "Lijie Wen"
                },
                "author": "Lijie Wen"
            },
            {
                "id": "http://arxiv.org/abs/2601.03034v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.03034v2",
                "title": "NorwAI's Large Language Models: Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NorwAI's Large Language Models: Technical Report"
                },
                "updated": "2026-01-08T10:58:43Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    10,
                    58,
                    43,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.03034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.03034v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Norwegian, spoken by approximately five million people, remains underrepresented in many of the most significant breakthroughs in Natural Language Processing (NLP). To address this gap, the NorLLM team at NorwAI has developed a family of models specifically tailored to Norwegian and other Scandinavian languages, building on diverse Transformer-based architectures such as GPT, Mistral, Llama2, Mixtral and Magistral. These models are either pretrained from scratch or continually pretrained on 25B - 88.45B tokens, using a Norwegian-extended tokenizer and advanced post-training strategies to optimize performance, enhance robustness, and improve adaptability across various real-world tasks. Notably, instruction-tuned variants (e.g., Mistral-7B-Instruct and Mixtral-8x7B-Instruct) showcase strong assistant-style capabilities, underscoring their potential for practical deployment in interactive and domain-specific applications. The NorwAI large language models are openly available to Nordic organizations, companies and students for both research and experimental use. This report provides detailed documentation of the model architectures, training data, tokenizer design, fine-tuning strategies, deployment, and evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Norwegian, spoken by approximately five million people, remains underrepresented in many of the most significant breakthroughs in Natural Language Processing (NLP). To address this gap, the NorLLM team at NorwAI has developed a family of models specifically tailored to Norwegian and other Scandinavian languages, building on diverse Transformer-based architectures such as GPT, Mistral, Llama2, Mixtral and Magistral. These models are either pretrained from scratch or continually pretrained on 25B - 88.45B tokens, using a Norwegian-extended tokenizer and advanced post-training strategies to optimize performance, enhance robustness, and improve adaptability across various real-world tasks. Notably, instruction-tuned variants (e.g., Mistral-7B-Instruct and Mixtral-8x7B-Instruct) showcase strong assistant-style capabilities, underscoring their potential for practical deployment in interactive and domain-specific applications. The NorwAI large language models are openly available to Nordic organizations, companies and students for both research and experimental use. This report provides detailed documentation of the model architectures, training data, tokenizer design, fine-tuning strategies, deployment, and evaluations."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-06T14:06:55Z",
                "published_parsed": [
                    2026,
                    1,
                    6,
                    14,
                    6,
                    55,
                    1,
                    6,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jon Atle Gulla"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Lemei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lemei Zhang"
                },
                "author": "Lemei Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2509.04069v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.04069v2",
                "title": "Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning"
                },
                "updated": "2026-01-08T10:57:57Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    10,
                    57,
                    57,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.04069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.04069v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper proposes an exploration-efficient Deep Reinforcement Learning with Reference policy (DRLR) framework for learning robotics tasks that incorporates demonstrations. The DRLR framework is developed based on an algorithm called Imitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve IBRL by modifying the action selection module. The proposed action selection module provides a calibrated Q-value, which mitigates the bootstrapping error that otherwise leads to inefficient exploration. Furthermore, to prevent the RL policy from converging to a sub-optimal policy, SAC is used as the RL policy instead of TD3. The effectiveness of our method in mitigating bootstrapping error and preventing overfitting is empirically validated by learning two robotics tasks: bucket loading and open drawer, which require extensive interactions with the environment. Simulation results also demonstrate the robustness of the DRLR framework across tasks with both low and high state-action dimensions, and varying demonstration qualities. To evaluate the developed framework on a real-world industrial robotics task, the bucket loading task is deployed on a real wheel loader. The sim2real results validate the successful deployment of the DRLR framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an exploration-efficient Deep Reinforcement Learning with Reference policy (DRLR) framework for learning robotics tasks that incorporates demonstrations. The DRLR framework is developed based on an algorithm called Imitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve IBRL by modifying the action selection module. The proposed action selection module provides a calibrated Q-value, which mitigates the bootstrapping error that otherwise leads to inefficient exploration. Furthermore, to prevent the RL policy from converging to a sub-optimal policy, SAC is used as the RL policy instead of TD3. The effectiveness of our method in mitigating bootstrapping error and preventing overfitting is empirically validated by learning two robotics tasks: bucket loading and open drawer, which require extensive interactions with the environment. Simulation results also demonstrate the robustness of the DRLR framework across tasks with both low and high state-action dimensions, and varying demonstration qualities. To evaluate the developed framework on a real-world industrial robotics task, the bucket loading task is deployed on a real wheel loader. The sim2real results validate the successful deployment of the DRLR framework."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-04T10:02:32Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    2,
                    32,
                    3,
                    247,
                    0
                ],
                "arxiv_comment": "This paper has been accepted for Journal publication in Frontiers in Robotics and AI",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Chengyandan Shen"
                    },
                    {
                        "name": "Christoffer Sloth"
                    }
                ],
                "author_detail": {
                    "name": "Christoffer Sloth"
                },
                "author": "Christoffer Sloth"
            },
            {
                "id": "http://arxiv.org/abs/2601.04820v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04820v1",
                "title": "LGTD: Local-Global Trend Decomposition for Season-Length-Free Time Series Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LGTD: Local-Global Trend Decomposition for Season-Length-Free Time Series Analysis"
                },
                "updated": "2026-01-08T10:56:26Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    10,
                    56,
                    26,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04820v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Time series decomposition into trend, seasonal structure, and residual components is a core primitive for downstream analytics such as anomaly detection, change-point detection, and forecasting. However, most existing seasonal-trend decomposition methods rely on user-specified or estimated season lengths and implicitly assume stable periodic structure. These assumptions limit robustness and deployability in large, heterogeneous collections where recurring patterns may drift, appear intermittently, or exist at multiple time scales.\n  We propose LGTD (Local-Global Trend Decomposition), a season-length-free decomposition framework that represents a time series as the sum of a smooth global trend, adaptive local trends whose recurrence induces implicit (emergent) seasonal structure, and a residual component. Rather than explicitly modeling seasonality through a fixed or estimated period, LGTD treats seasonal structure as an emergent property arising from repeated local trend regimes. Concretely, LGTD first estimates a global trend capturing long-term evolution, then applies AutoTrend, an adaptive error-driven local linear trend inference module, to segment the detrended signal into short-lived piecewise-linear regimes. Residuals are obtained after removing both global and local trends.\n  By eliminating manual season-length specification, LGTD supports automated, low-touch deployment across time series with irregular, drifting, or weakly periodic structure. We analyze computational complexity and show that LGTD scales linearly with series length under mild conditions. Experiments on synthetic benchmarks demonstrate robust and balanced decomposition performance across fixed, transitive, and variable season-length settings, especially where period-based methods degrade.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series decomposition into trend, seasonal structure, and residual components is a core primitive for downstream analytics such as anomaly detection, change-point detection, and forecasting. However, most existing seasonal-trend decomposition methods rely on user-specified or estimated season lengths and implicitly assume stable periodic structure. These assumptions limit robustness and deployability in large, heterogeneous collections where recurring patterns may drift, appear intermittently, or exist at multiple time scales.\n  We propose LGTD (Local-Global Trend Decomposition), a season-length-free decomposition framework that represents a time series as the sum of a smooth global trend, adaptive local trends whose recurrence induces implicit (emergent) seasonal structure, and a residual component. Rather than explicitly modeling seasonality through a fixed or estimated period, LGTD treats seasonal structure as an emergent property arising from repeated local trend regimes. Concretely, LGTD first estimates a global trend capturing long-term evolution, then applies AutoTrend, an adaptive error-driven local linear trend inference module, to segment the detrended signal into short-lived piecewise-linear regimes. Residuals are obtained after removing both global and local trends.\n  By eliminating manual season-length specification, LGTD supports automated, low-touch deployment across time series with irregular, drifting, or weakly periodic structure. We analyze computational complexity and show that LGTD scales linearly with series length under mild conditions. Experiments on synthetic benchmarks demonstrate robust and balanced decomposition performance across fixed, transitive, and variable season-length settings, especially where period-based methods degrade."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T10:56:26Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    10,
                    56,
                    26,
                    3,
                    8,
                    0
                ],
                "arxiv_comment": "First draft",
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Chotanansub Sophaken"
                    },
                    {
                        "name": "Thanadej Rattanakornphan"
                    },
                    {
                        "name": "Piyanon Charoenpoonpanich"
                    },
                    {
                        "name": "Thanapol Phungtua-eng"
                    },
                    {
                        "name": "Chainarong Amornbunchornvej"
                    }
                ],
                "author_detail": {
                    "name": "Chainarong Amornbunchornvej"
                },
                "author": "Chainarong Amornbunchornvej"
            },
            {
                "id": "http://arxiv.org/abs/2601.04819v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04819v1",
                "title": "AECV-Bench: Benchmarking Multimodal Models on Architectural and Engineering Drawings Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AECV-Bench: Benchmarking Multimodal Models on Architectural and Engineering Drawings Understanding"
                },
                "updated": "2026-01-08T10:54:32Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    10,
                    54,
                    32,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04819v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "AEC drawings encode geometry and semantics through symbols, layout conventions, and dense annotation, yet it remains unclear whether modern multimodal and vision-language models can reliably interpret this graphical language. We present AECV-Bench, a benchmark for evaluating multimodal and vision-language models on realistic AEC artefacts via two complementary use cases: (i) object counting on 120 high-quality floor plans (doors, windows, bedrooms, toilets), and (ii) drawing-grounded document QA spanning 192 question-answer pairs that test text extraction (OCR), instance counting, spatial reasoning, and comparative reasoning over common drawing regions. Object-counting performance is reported using per-field exact-match accuracy and MAPE results, while document-QA performance is reported using overall accuracy and per-category breakdowns with an LLM-as-a-judge scoring pipeline and targeted human adjudication for edge cases. Evaluating a broad set of state-of-the-art models under a unified protocol, we observe a stable capability gradient; OCR and text-centric document QA are strongest (up to 0.95 accuracy), spatial reasoning is moderate, and symbol-centric drawing understanding - especially reliable counting of doors and windows - remains unsolved (often 0.40-0.55 accuracy) with substantial proportional errors. These results suggest that current systems function well as document assistants but lack robust drawing literacy, motivating domain-specific representations and tool-augmented, human-in-the-loop workflows for an efficient AEC automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AEC drawings encode geometry and semantics through symbols, layout conventions, and dense annotation, yet it remains unclear whether modern multimodal and vision-language models can reliably interpret this graphical language. We present AECV-Bench, a benchmark for evaluating multimodal and vision-language models on realistic AEC artefacts via two complementary use cases: (i) object counting on 120 high-quality floor plans (doors, windows, bedrooms, toilets), and (ii) drawing-grounded document QA spanning 192 question-answer pairs that test text extraction (OCR), instance counting, spatial reasoning, and comparative reasoning over common drawing regions. Object-counting performance is reported using per-field exact-match accuracy and MAPE results, while document-QA performance is reported using overall accuracy and per-category breakdowns with an LLM-as-a-judge scoring pipeline and targeted human adjudication for edge cases. Evaluating a broad set of state-of-the-art models under a unified protocol, we observe a stable capability gradient; OCR and text-centric document QA are strongest (up to 0.95 accuracy), spatial reasoning is moderate, and symbol-centric drawing understanding - especially reliable counting of doors and windows - remains unsolved (often 0.40-0.55 accuracy) with substantial proportional errors. These results suggest that current systems function well as document assistants but lack robust drawing literacy, motivating domain-specific representations and tool-augmented, human-in-the-loop workflows for an efficient AEC automation."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T10:54:32Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    10,
                    54,
                    32,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Aleksei Kondratenko"
                    },
                    {
                        "name": "Mussie Birhane"
                    },
                    {
                        "name": "Houssame E. Hsain"
                    },
                    {
                        "name": "Guido Maciocci"
                    }
                ],
                "author_detail": {
                    "name": "Guido Maciocci"
                },
                "author": "Guido Maciocci"
            },
            {
                "id": "http://arxiv.org/abs/2601.04801v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2601.04801v1",
                "title": "MPM-LLM4DSE: Reaching the Pareto Frontier in HLS with Multimodal Learning and LLM-Driven Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPM-LLM4DSE: Reaching the Pareto Frontier in HLS with Multimodal Learning and LLM-Driven Exploration"
                },
                "updated": "2026-01-08T10:32:49Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    10,
                    32,
                    49,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2601.04801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2601.04801v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "High-Level Synthesis (HLS) design space exploration (DSE) seeks Pareto-optimal designs within expansive pragma configuration spaces. To accelerate HLS DSE, graph neural networks (GNNs) are commonly employed as surrogates for HLS tools to predict quality of results (QoR) metrics, while multi-objective optimization algorithms expedite the exploration. However, GNN-based prediction methods may not fully capture the rich semantic features inherent in behavioral descriptions, and conventional multi-objective optimization algorithms often do not explicitly account for the domain-specific knowledge regarding how pragma directives influence QoR. To address these limitations, this paper proposes the MPM-LLM4DSE framework, which incorporates a multimodal prediction model (MPM) that simultaneously fuses features from behavioral descriptions and control and data flow graphs. Furthermore, the framework employs a large language model (LLM) as an optimizer, accompanied by a tailored prompt engineering methodology. This methodology incorporates pragma impact analysis on QoR to guide the LLM in generating high-quality configurations (LLM4DSE). Experimental results demonstrate that our multimodal predictive model significantly outperforms state-of-the-art work ProgSG by up to 10.25$\\times$. Furthermore, in DSE tasks, the proposed LLM4DSE achieves an average performance gain of 39.90\\% over prior methods, validating the effectiveness of our prompting methodology. Code and models are available at https://github.com/wslcccc/MPM-LLM4DSE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Level Synthesis (HLS) design space exploration (DSE) seeks Pareto-optimal designs within expansive pragma configuration spaces. To accelerate HLS DSE, graph neural networks (GNNs) are commonly employed as surrogates for HLS tools to predict quality of results (QoR) metrics, while multi-objective optimization algorithms expedite the exploration. However, GNN-based prediction methods may not fully capture the rich semantic features inherent in behavioral descriptions, and conventional multi-objective optimization algorithms often do not explicitly account for the domain-specific knowledge regarding how pragma directives influence QoR. To address these limitations, this paper proposes the MPM-LLM4DSE framework, which incorporates a multimodal prediction model (MPM) that simultaneously fuses features from behavioral descriptions and control and data flow graphs. Furthermore, the framework employs a large language model (LLM) as an optimizer, accompanied by a tailored prompt engineering methodology. This methodology incorporates pragma impact analysis on QoR to guide the LLM in generating high-quality configurations (LLM4DSE). Experimental results demonstrate that our multimodal predictive model significantly outperforms state-of-the-art work ProgSG by up to 10.25$\\times$. Furthermore, in DSE tasks, the proposed LLM4DSE achieves an average performance gain of 39.90\\% over prior methods, validating the effectiveness of our prompting methodology. Code and models are available at https://github.com/wslcccc/MPM-LLM4DSE."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2026-01-08T10:32:49Z",
                "published_parsed": [
                    2026,
                    1,
                    8,
                    10,
                    32,
                    49,
                    3,
                    8,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Shanshan Wang"
                    },
                    {
                        "name": "Chenglong Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Chenglong Xiao"
                },
                "author": "Chenglong Xiao"
            },
            {
                "id": "http://arxiv.org/abs/2512.24334v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.24334v2",
                "title": "OptiVote: Non-Coherent FSO Over-the-Air Majority Vote for Communication-Efficient Distributed Federated Learning in Space Data Centers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OptiVote: Non-Coherent FSO Over-the-Air Majority Vote for Communication-Efficient Distributed Federated Learning in Space Data Centers"
                },
                "updated": "2026-01-08T10:31:52Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    10,
                    31,
                    52,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.24334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.24334v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid deployment of mega-constellations is driving the long-term vision of space data centers (SDCs), where interconnected satellites form in-orbit distributed computing and learning infrastructures. Enabling distributed federated learning in such systems is challenging because iterative training requires frequent aggregation over inter-satellite links that are bandwidth- and energy-constrained, and the link conditions can be highly dynamic. In this work, we exploit over-the-air computation (AirComp) as an in-network aggregation primitive. However, conventional coherent AirComp relies on stringent phase alignment, which is difficult to maintain in space environments due to satellite jitter and Doppler effects. To overcome this limitation, we propose OptiVote, a robust and communication-efficient non-coherent free-space optical (FSO) AirComp framework for federated learning toward Space Data Centers. OptiVote integrates sign stochastic gradient descent (signSGD) with a majority-vote (MV) aggregation principle and pulse-position modulation (PPM), where each satellite conveys local gradient signs by activating orthogonal PPM time slots. The aggregation node performs MV detection via non-coherent energy accumulation, transforming phase-sensitive field superposition into phase-agnostic optical intensity combining, thereby eliminating the need for precise phase synchronization and improving resilience under dynamic impairments. To mitigate aggregation bias induced by heterogeneous FSO channels, we further develop an importance-aware, channel state information (CSI)-free dynamic power control scheme that balances received energies without additional signaling. We provide theoretical analysis by characterizing the aggregate error probability under statistical FSO channels and establishing convergence guarantees for non-convex objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid deployment of mega-constellations is driving the long-term vision of space data centers (SDCs), where interconnected satellites form in-orbit distributed computing and learning infrastructures. Enabling distributed federated learning in such systems is challenging because iterative training requires frequent aggregation over inter-satellite links that are bandwidth- and energy-constrained, and the link conditions can be highly dynamic. In this work, we exploit over-the-air computation (AirComp) as an in-network aggregation primitive. However, conventional coherent AirComp relies on stringent phase alignment, which is difficult to maintain in space environments due to satellite jitter and Doppler effects. To overcome this limitation, we propose OptiVote, a robust and communication-efficient non-coherent free-space optical (FSO) AirComp framework for federated learning toward Space Data Centers. OptiVote integrates sign stochastic gradient descent (signSGD) with a majority-vote (MV) aggregation principle and pulse-position modulation (PPM), where each satellite conveys local gradient signs by activating orthogonal PPM time slots. The aggregation node performs MV detection via non-coherent energy accumulation, transforming phase-sensitive field superposition into phase-agnostic optical intensity combining, thereby eliminating the need for precise phase synchronization and improving resilience under dynamic impairments. To mitigate aggregation bias induced by heterogeneous FSO channels, we further develop an importance-aware, channel state information (CSI)-free dynamic power control scheme that balances received energies without additional signaling. We provide theoretical analysis by characterizing the aggregate error probability under statistical FSO channels and establishing convergence guarantees for non-convex objectives."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-30T16:40:02Z",
                "published_parsed": [
                    2025,
                    12,
                    30,
                    16,
                    40,
                    2,
                    1,
                    364,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Anbang Zhang"
                    },
                    {
                        "name": "Chenyuan Feng"
                    },
                    {
                        "name": "Wai Ho Mow"
                    },
                    {
                        "name": "Jia Ye"
                    },
                    {
                        "name": "Shuaishuai Guo"
                    },
                    {
                        "name": "Geyong Min"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek"
            },
            {
                "id": "http://arxiv.org/abs/2511.05547v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.05547v2",
                "title": "Automated Invoice Data Extraction: Using LLM and OCR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Invoice Data Extraction: Using LLM and OCR"
                },
                "updated": "2026-01-08T10:24:34Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    10,
                    24,
                    34,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.05547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.05547v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Conventional Optical Character Recognition (OCR) systems are challenged by variant invoice layouts, handwritten text, and low-quality scans, which are often caused by strong template dependencies that restrict their flexibility across different document structures and layouts. Newer solutions utilize advanced deep learning models such as Convolutional Neural Networks (CNN) as well as Transformers, and domain-specific models for better layout analysis and accuracy across various sections over varied document types. Large Language Models (LLMs) have revolutionized extraction pipelines at their core with sophisticated entity recognition and semantic comprehension to support complex contextual relationship mapping without direct programming specification. Visual Named Entity Recognition (NER) capabilities permit extraction from invoice images with greater contextual sensitivity and much higher accuracy rates than older approaches. Existing industry best practices utilize hybrid architectures that blend OCR technology and LLM for maximum scalability and minimal human intervention. This work introduces a holistic Artificial Intelligence (AI) platform combining OCR, deep learning, LLMs, and graph analytics to achieve unprecedented extraction quality and consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional Optical Character Recognition (OCR) systems are challenged by variant invoice layouts, handwritten text, and low-quality scans, which are often caused by strong template dependencies that restrict their flexibility across different document structures and layouts. Newer solutions utilize advanced deep learning models such as Convolutional Neural Networks (CNN) as well as Transformers, and domain-specific models for better layout analysis and accuracy across various sections over varied document types. Large Language Models (LLMs) have revolutionized extraction pipelines at their core with sophisticated entity recognition and semantic comprehension to support complex contextual relationship mapping without direct programming specification. Visual Named Entity Recognition (NER) capabilities permit extraction from invoice images with greater contextual sensitivity and much higher accuracy rates than older approaches. Existing industry best practices utilize hybrid architectures that blend OCR technology and LLM for maximum scalability and minimal human intervention. This work introduces a holistic Artificial Intelligence (AI) platform combining OCR, deep learning, LLMs, and graph analytics to achieve unprecedented extraction quality and consistency."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-01T19:05:09Z",
                "published_parsed": [
                    2025,
                    11,
                    1,
                    19,
                    5,
                    9,
                    5,
                    305,
                    0
                ],
                "arxiv_comment": "10 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Khushi Khanchandani"
                    },
                    {
                        "name": "Advait Thakur"
                    },
                    {
                        "name": "Akshita Shetty"
                    },
                    {
                        "name": "Chaitravi Reddy"
                    },
                    {
                        "name": "Ritisa Behera"
                    }
                ],
                "author_detail": {
                    "name": "Ritisa Behera"
                },
                "author": "Ritisa Behera"
            },
            {
                "id": "http://arxiv.org/abs/2508.13593v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.13593v2",
                "title": "Repeater Swarm-Assisted Cellular Systems: Interaction Stability and Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repeater Swarm-Assisted Cellular Systems: Interaction Stability and Performance Analysis"
                },
                "updated": "2026-01-08T10:22:25Z",
                "updated_parsed": [
                    2026,
                    1,
                    8,
                    10,
                    22,
                    25,
                    3,
                    8,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.13593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.13593v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/TWC.2025.3647294",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "We consider a cellular massive MIMO system where swarms of wireless repeaters are deployed to improve coverage. These repeaters are full-duplex relays with small form factors that receive and instantaneously retransmit signals. They can be deployed in a plug-and-play manner at low cost, while being transparent to the network--conceptually they are active channel scatterers with amplification capabilities. Two fundamental questions need to be addressed in repeater deployments: (I) How can we prevent destructive effects of positive feedback caused by inter-repeater interaction (i.e., each repeater receives and amplifies signals from others)? (ii) How much performance improvement can be achieved given that repeaters also inject noise and may introduce more interference? To answer these questions, we first derive a generalized Nyquist stability criterion for the repeater swarm system, and provide an easy-to-check stability condition. Then, we study the uplink performance and develop an efficient iterative algorithm that jointly optimizes the repeater gains, user transmit powers, and receive combining weights to maximize the weighted sum rate while ensuring system stability. Numerical results corroborate our theoretical findings and show that the repeaters can significantly improve the system performance, both in sub-6 GHz and millimeter-wave bands. The results also warrant careful deployment to fully realize the benefits of repeaters, for example, by ensuring a high probability of line-of-sight links between repeaters and the base station.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a cellular massive MIMO system where swarms of wireless repeaters are deployed to improve coverage. These repeaters are full-duplex relays with small form factors that receive and instantaneously retransmit signals. They can be deployed in a plug-and-play manner at low cost, while being transparent to the network--conceptually they are active channel scatterers with amplification capabilities. Two fundamental questions need to be addressed in repeater deployments: (I) How can we prevent destructive effects of positive feedback caused by inter-repeater interaction (i.e., each repeater receives and amplifies signals from others)? (ii) How much performance improvement can be achieved given that repeaters also inject noise and may introduce more interference? To answer these questions, we first derive a generalized Nyquist stability criterion for the repeater swarm system, and provide an easy-to-check stability condition. Then, we study the uplink performance and develop an efficient iterative algorithm that jointly optimizes the repeater gains, user transmit powers, and receive combining weights to maximize the weighted sum rate while ensuring system stability. Numerical results corroborate our theoretical findings and show that the repeaters can significantly improve the system performance, both in sub-6 GHz and millimeter-wave bands. The results also warrant careful deployment to fully realize the benefits of repeaters, for example, by ensuring a high probability of line-of-sight links between repeaters and the base station."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-19T07:59:27Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    7,
                    59,
                    27,
                    1,
                    231,
                    0
                ],
                "arxiv_comment": "16 pages, 13 figures. Accepted for publication in IEEE Transactions on Wireless Communications",
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Jianan Bai"
                    },
                    {
                        "name": "Anubhab Chowdhury"
                    },
                    {
                        "name": "Anders Hansson"
                    },
                    {
                        "name": "Erik G. Larsson"
                    }
                ],
                "author_detail": {
                    "name": "Erik G. Larsson"
                },
                "author": "Erik G. Larsson",
                "arxiv_doi": "10.1109/TWC.2025.3647294"
            }
        ]
    }
]