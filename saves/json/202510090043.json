[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.06175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06175v1",
                "updated": "2025-10-07T17:35:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    35,
                    28,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:35:28Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    35,
                    28,
                    1,
                    280,
                    0
                ],
                "title": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via\n  Outlier-Suppressed Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via\n  Outlier-Suppressed Vector Quantization"
                },
                "summary": "The Key-Value (KV) cache introduces substantial memory overhead during large\nlanguage model (LLM) inference. Although existing vector quantization (VQ)\nmethods reduce KV cache usage and provide flexible representational capacity\nacross bit-widths, they suffer severe performance degradation at ultra-low\nbit-widths due to key cache outliers that hinder effective codebook\nutilization. To address this challenge, we propose VecInfer, a novel VQ method\nfor aggressive KV cache compression while enabling efficient inference. By\napplying smooth and Hadamard transformations, VecInfer suppresses outliers in\nthe key cache, enabling the codebook to comprehensively cover the original data\ndistribution and thereby reducing quantization difficulty. To facilitate\nefficient deployment, we design an optimized CUDA kernel that fuses computation\nwith dequantization to minimize memory access overhead. Extensive evaluations\ndemonstrate that VecInfer consistently outperforms existing quantization\nbaselines across both long-context understanding and mathematical reasoning\ntasks. With only 2-bit quantization, VecInfer achieves performance comparable\nto full precision, while delivering up to $\\mathbf{2.7\\times}$ speedup in\nlarge-batch self-attention computation and $\\mathbf{8.3\\times}$ reduction in\nsingle-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache introduces substantial memory overhead during large\nlanguage model (LLM) inference. Although existing vector quantization (VQ)\nmethods reduce KV cache usage and provide flexible representational capacity\nacross bit-widths, they suffer severe performance degradation at ultra-low\nbit-widths due to key cache outliers that hinder effective codebook\nutilization. To address this challenge, we propose VecInfer, a novel VQ method\nfor aggressive KV cache compression while enabling efficient inference. By\napplying smooth and Hadamard transformations, VecInfer suppresses outliers in\nthe key cache, enabling the codebook to comprehensively cover the original data\ndistribution and thereby reducing quantization difficulty. To facilitate\nefficient deployment, we design an optimized CUDA kernel that fuses computation\nwith dequantization to minimize memory access overhead. Extensive evaluations\ndemonstrate that VecInfer consistently outperforms existing quantization\nbaselines across both long-context understanding and mathematical reasoning\ntasks. With only 2-bit quantization, VecInfer achieves performance comparable\nto full precision, while delivering up to $\\mathbf{2.7\\times}$ speedup in\nlarge-batch self-attention computation and $\\mathbf{8.3\\times}$ reduction in\nsingle-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length."
                },
                "authors": [
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Chenxu Yang"
                    },
                    {
                        "name": "Zhengyang Tong"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05686v1",
                "updated": "2025-10-07T08:43:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    8,
                    43,
                    7,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T08:43:07Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    8,
                    43,
                    7,
                    1,
                    280,
                    0
                ],
                "title": "On Enhancing Delay SLAs in TCP Networks through Joint Routing and\n  Transport Assistant Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Enhancing Delay SLAs in TCP Networks through Joint Routing and\n  Transport Assistant Deployment"
                },
                "summary": "The Transport Control Protocol has long been the primary transport protocol\nfor applications requiring performance and reliability over the Internet.\nUnfortunately, due its retransmission mechanism, TCP incurs high packet\ndelivery delays when segments are lost. To address this issue, previous\nresearch proposed to use a novel network function, namely Transport Assistant,\ndeployed within the network to cache and retransmit lost packets, thus reducing\nretransmission delays. In this paper, we propose to jointly route the flows and\ndeploy TAs in order to minimize packet delivery delays in best-effort networks\n(scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based\nnetworks (scenario 2). We hence formulate the joint routing and TA deployment\nproblem as Integer Linear Program for the two scenarios and propose a heuristic\nsolution for large-scale instances of the problem. Through extensive\nsimulations, we demonstrate the benefits of performing joint routing flows and\nTA deployment in reducing packet delivery delays (up to 16.4%) while minimizing\ndeployment costs (up to 60.98%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transport Control Protocol has long been the primary transport protocol\nfor applications requiring performance and reliability over the Internet.\nUnfortunately, due its retransmission mechanism, TCP incurs high packet\ndelivery delays when segments are lost. To address this issue, previous\nresearch proposed to use a novel network function, namely Transport Assistant,\ndeployed within the network to cache and retransmit lost packets, thus reducing\nretransmission delays. In this paper, we propose to jointly route the flows and\ndeploy TAs in order to minimize packet delivery delays in best-effort networks\n(scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based\nnetworks (scenario 2). We hence formulate the joint routing and TA deployment\nproblem as Integer Linear Program for the two scenarios and propose a heuristic\nsolution for large-scale instances of the problem. Through extensive\nsimulations, we demonstrate the benefits of performing joint routing flows and\nTA deployment in reducing packet delivery delays (up to 16.4%) while minimizing\ndeployment costs (up to 60.98%)."
                },
                "authors": [
                    {
                        "name": "José Gómez-delaHiz"
                    },
                    {
                        "name": "Mohamed Faten Zhani"
                    },
                    {
                        "name": "Jaime Galán-Jiménez"
                    },
                    {
                        "name": "John Kaippallimalil"
                    }
                ],
                "author_detail": {
                    "name": "John Kaippallimalil"
                },
                "author": "John Kaippallimalil",
                "arxiv_comment": "10 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05529v1",
                "updated": "2025-10-07T02:39:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    2,
                    39,
                    35,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T02:39:35Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    2,
                    39,
                    35,
                    1,
                    280,
                    0
                ],
                "title": "H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model\n  Inference"
                },
                "summary": "Autoregressive decoding in large language models (LLMs) requires caching a\ngrowing list of past key-value (KV) pairs, making long-context inference a\nmemory-bound problem. While recent methods have explored quantizing the cache,\nevicting tokens, or using binary sketches for keys (e.g., Loki), these\napproaches often provide an incomplete solution by leaving one component (like\nvalues) uncompressed or by discarding context information. This paper\nintroduces the Hybrid One-Bit KV Cache (H1B-KV), a comprehensive compression\nscheme that radically reduces memory usage without sacrificing context. H1B-KV\nrepresents each key vector using a 1-bit binary sketch, enabling\nhardware-friendly bitwise attention, and further compresses value vectors using\n4-bit quantization. This holistic, hybrid approach allows a 7-billion parameter\nLLM to handle an 8k-token context with under 60 MB of cache memory - a 70x\nreduction. We demonstrate that after a lightweight finetuning, H1B-KV matches\nfull-precision performance not only on perplexity benchmarks but also on\ncomplex downstream tasks like mathematical reasoning (GSM8K), multi-task\nunderstanding (MMLU), and code generation (HumanEval). Our results show H1B-KV\nsignificantly outperforms leading quantization (KIVI), token eviction\n(SparseLLM), and key-only sketching (Loki) methods in quality-per-byte,\nestablishing it as a robust solution for deploying LLMs in memory-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive decoding in large language models (LLMs) requires caching a\ngrowing list of past key-value (KV) pairs, making long-context inference a\nmemory-bound problem. While recent methods have explored quantizing the cache,\nevicting tokens, or using binary sketches for keys (e.g., Loki), these\napproaches often provide an incomplete solution by leaving one component (like\nvalues) uncompressed or by discarding context information. This paper\nintroduces the Hybrid One-Bit KV Cache (H1B-KV), a comprehensive compression\nscheme that radically reduces memory usage without sacrificing context. H1B-KV\nrepresents each key vector using a 1-bit binary sketch, enabling\nhardware-friendly bitwise attention, and further compresses value vectors using\n4-bit quantization. This holistic, hybrid approach allows a 7-billion parameter\nLLM to handle an 8k-token context with under 60 MB of cache memory - a 70x\nreduction. We demonstrate that after a lightweight finetuning, H1B-KV matches\nfull-precision performance not only on perplexity benchmarks but also on\ncomplex downstream tasks like mathematical reasoning (GSM8K), multi-task\nunderstanding (MMLU), and code generation (HumanEval). Our results show H1B-KV\nsignificantly outperforms leading quantization (KIVI), token eviction\n(SparseLLM), and key-only sketching (Loki) methods in quality-per-byte,\nestablishing it as a robust solution for deploying LLMs in memory-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Harshil Vejendla"
                    }
                ],
                "author_detail": {
                    "name": "Harshil Vejendla"
                },
                "author": "Harshil Vejendla",
                "arxiv_comment": "MIT URTC 2025 Technical Paper (Oral), 5 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05476v1",
                "updated": "2025-10-07T00:32:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    0,
                    32,
                    45,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T00:32:45Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    0,
                    32,
                    45,
                    1,
                    280,
                    0
                ],
                "title": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided\n  Inter-Node Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided\n  Inter-Node Communications"
                },
                "summary": "Message Passing Interface (MPI) is a foundational programming model for\nhigh-performance computing. MPI libraries traditionally employ network\ninterconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP\nand RoCE) with complex software stacks for cross-node communication. We present\ncMPI, the first work to optimize MPI point-to-point communication (both\none-sided and two-sided) using CXL memory sharing on a real CXL platform,\ntransforming cross-node communication into memory transactions and data copies\nwithin CXL memory, bypassing traditional network protocols. We analyze\nperformance across various interconnects and find that CXL memory sharing\nachieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in\nsmall- and medium-scale clusters. We address challenges of CXL memory sharing\nfor MPI communication, including data object management over the dax\nrepresentation [50], cache coherence, and atomic operations. Overall, cMPI\noutperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x\nand 72x in latency and bandwidth, respectively, for small messages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Message Passing Interface (MPI) is a foundational programming model for\nhigh-performance computing. MPI libraries traditionally employ network\ninterconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP\nand RoCE) with complex software stacks for cross-node communication. We present\ncMPI, the first work to optimize MPI point-to-point communication (both\none-sided and two-sided) using CXL memory sharing on a real CXL platform,\ntransforming cross-node communication into memory transactions and data copies\nwithin CXL memory, bypassing traditional network protocols. We analyze\nperformance across various interconnects and find that CXL memory sharing\nachieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in\nsmall- and medium-scale clusters. We address challenges of CXL memory sharing\nfor MPI communication, including data object management over the dax\nrepresentation [50], cache coherence, and atomic operations. Overall, cMPI\noutperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x\nand 72x in latency and bandwidth, respectively, for small messages."
                },
                "authors": [
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Bin Ma"
                    },
                    {
                        "name": "Jongryool Kim"
                    },
                    {
                        "name": "Byungil Koh"
                    },
                    {
                        "name": "Hoshik Kim"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "arxiv_doi": "10.1145/3712285.3759816",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759816",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.05476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05373v1",
                "updated": "2025-10-06T21:08:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    21,
                    8,
                    11,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T21:08:11Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    21,
                    8,
                    11,
                    0,
                    279,
                    0
                ],
                "title": "KVLinC : KV Cache Quantization with Hadamard Rotation and Linear\n  Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLinC : KV Cache Quantization with Hadamard Rotation and Linear\n  Correction"
                },
                "summary": "Quantizing the key-value (KV) cache is a promising strategy for improving the\ninference efficiency of large language models (LLMs). However, aggressive\nquantization to very low precision (e.g., 2 bits) introduces significant errors\nin the stored key and value tensors, which propagate through the dot-product\nattention mechanism and ultimately degrade generation quality. To address this,\nwe propose KVLinC, a framework to mitigate attention errors introduced by KV\ncache quantization in the extreme low-precision regime. KVLinC combines a\nHadamard rotation, which reduces quantization error in values, with lightweight\nlinear correction adapters that explicitly compensate for errors introduced by\nquantized keys. Across extensive evaluations on the LLaMA, Qwen2.5, and Qwen3\nmodel families, KVLinC consistently matches or surpasses strong baselines while\nachieving higher KV-cache compression. Furthermore, we implement a custom\nattention kernel that results in upto 2.55x faster inference compared to Flash\nAttention baseline, enabling efficient long-context LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing the key-value (KV) cache is a promising strategy for improving the\ninference efficiency of large language models (LLMs). However, aggressive\nquantization to very low precision (e.g., 2 bits) introduces significant errors\nin the stored key and value tensors, which propagate through the dot-product\nattention mechanism and ultimately degrade generation quality. To address this,\nwe propose KVLinC, a framework to mitigate attention errors introduced by KV\ncache quantization in the extreme low-precision regime. KVLinC combines a\nHadamard rotation, which reduces quantization error in values, with lightweight\nlinear correction adapters that explicitly compensate for errors introduced by\nquantized keys. Across extensive evaluations on the LLaMA, Qwen2.5, and Qwen3\nmodel families, KVLinC consistently matches or surpasses strong baselines while\nachieving higher KV-cache compression. Furthermore, we implement a custom\nattention kernel that results in upto 2.55x faster inference compared to Flash\nAttention baseline, enabling efficient long-context LLM inference."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "14 pages, 7 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05367v1",
                "updated": "2025-10-06T20:54:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    20,
                    54,
                    44,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T20:54:44Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    20,
                    54,
                    44,
                    0,
                    279,
                    0
                ],
                "title": "LightCache: Memory-Efficient, Training-Free Acceleration for Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightCache: Memory-Efficient, Training-Free Acceleration for Video\n  Generation"
                },
                "summary": "Training-free acceleration has emerged as an advanced research area in video\ngeneration based on diffusion models. The redundancy of latents in diffusion\nmodel inference provides a natural entry point for acceleration. In this paper,\nwe decompose the inference process into the encoding, denoising, and decoding\nstages, and observe that cache-based acceleration methods often lead to\nsubstantial memory surges in the latter two stages. To address this problem, we\nanalyze the characteristics of inference across different stages and propose\nstage-specific strategies for reducing memory consumption: 1) Asynchronous\nCache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same\ntime, we ensure that the time overhead introduced by these three strategies\nremains lower than the acceleration gains themselves. Compared with the\nbaseline, our approach achieves faster inference speed and lower memory usage,\nwhile maintaining quality degradation within an acceptable range. The Code is\navailable at https://github.com/NKUShaw/LightCache .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free acceleration has emerged as an advanced research area in video\ngeneration based on diffusion models. The redundancy of latents in diffusion\nmodel inference provides a natural entry point for acceleration. In this paper,\nwe decompose the inference process into the encoding, denoising, and decoding\nstages, and observe that cache-based acceleration methods often lead to\nsubstantial memory surges in the latter two stages. To address this problem, we\nanalyze the characteristics of inference across different stages and propose\nstage-specific strategies for reducing memory consumption: 1) Asynchronous\nCache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same\ntime, we ensure that the time overhead introduced by these three strategies\nremains lower than the acceleration gains themselves. Compared with the\nbaseline, our approach achieves faster inference speed and lower memory usage,\nwhile maintaining quality degradation within an acceptable range. The Code is\navailable at https://github.com/NKUShaw/LightCache ."
                },
                "authors": [
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Kaiyuan Deng"
                    },
                    {
                        "name": "Yushu Wu"
                    },
                    {
                        "name": "Zheng Zhan"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Bo Hui"
                    }
                ],
                "author_detail": {
                    "name": "Bo Hui"
                },
                "author": "Bo Hui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06416v2",
                "updated": "2025-10-06T17:09:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    9,
                    39,
                    0,
                    279,
                    0
                ],
                "published": "2025-04-08T20:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Autoregressive and Diffusion-Based Sequence Generation"
                },
                "summary": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation. See code and resources at\nhttps://hdlm-colm.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation. See code and resources at\nhttps://hdlm-colm.github.io/"
                },
                "authors": [
                    {
                        "name": "Nima Fathi"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Pierre-André Noël"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-André Noël"
                },
                "author": "Pierre-André Noël",
                "arxiv_comment": "Published as a conference paper at COLM 2025 Website:\n  https://hdlm-colm.github.io/",
                "arxiv_journal_ref": "Second Conference on Language Modeling,\n  https://openreview.net/forum?id=rgq9BFXSFl (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19341v2",
                "updated": "2025-10-06T13:23:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    23,
                    4,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-16T09:14:15Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    14,
                    15,
                    1,
                    259,
                    0
                ],
                "title": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks"
                },
                "summary": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework."
                },
                "authors": [
                    {
                        "name": "Yang Fu"
                    },
                    {
                        "name": "Peng Qin"
                    },
                    {
                        "name": "Yueyue Zhang"
                    },
                    {
                        "name": "Pao Cheng"
                    },
                    {
                        "name": "Jun Lu"
                    },
                    {
                        "name": "Yifei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Wang"
                },
                "author": "Yifei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04646v1",
                "updated": "2025-10-06T09:49:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    9,
                    49,
                    14,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T09:49:14Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    9,
                    49,
                    14,
                    0,
                    279,
                    0
                ],
                "title": "Predictive Feature Caching for Training-free Acceleration of Molecular\n  Geometry Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive Feature Caching for Training-free Acceleration of Molecular\n  Geometry Generation"
                },
                "summary": "Flow matching models generate high-fidelity molecular geometries but incur\nsignificant computational costs during inference, requiring hundreds of network\nevaluations. This inference overhead becomes the primary bottleneck when such\nmodels are employed in practice to sample large numbers of molecular\ncandidates. This work discusses a training-free caching strategy that\naccelerates molecular geometry generation by predicting intermediate hidden\nstates across solver steps. The proposed method operates directly on the\nSE(3)-equivariant backbone, is compatible with pretrained models, and is\northogonal to existing training-based accelerations and system-level\noptimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching\nachieves a twofold reduction in wall-clock inference time at matched sample\nquality and a speedup of up to 3x compared to the base model with minimal\nsample quality degradation. Because these gains compound with other\noptimizations, applying caching alongside other general, lossless optimizations\nyield as much as a 7x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow matching models generate high-fidelity molecular geometries but incur\nsignificant computational costs during inference, requiring hundreds of network\nevaluations. This inference overhead becomes the primary bottleneck when such\nmodels are employed in practice to sample large numbers of molecular\ncandidates. This work discusses a training-free caching strategy that\naccelerates molecular geometry generation by predicting intermediate hidden\nstates across solver steps. The proposed method operates directly on the\nSE(3)-equivariant backbone, is compatible with pretrained models, and is\northogonal to existing training-based accelerations and system-level\noptimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching\nachieves a twofold reduction in wall-clock inference time at matched sample\nquality and a speedup of up to 3x compared to the base model with minimal\nsample quality degradation. Because these gains compound with other\noptimizations, applying caching alongside other general, lossless optimizations\nyield as much as a 7x speedup."
                },
                "authors": [
                    {
                        "name": "Johanna Sommer"
                    },
                    {
                        "name": "John Rachwan"
                    },
                    {
                        "name": "Nils Fleischmann"
                    },
                    {
                        "name": "Stephan Günnemann"
                    },
                    {
                        "name": "Bertrand Charpentier"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Charpentier"
                },
                "author": "Bertrand Charpentier",
                "arxiv_comment": "Accepted at the AI for Science Workshop @ NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04525v1",
                "updated": "2025-10-06T06:30:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T06:30:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion"
                },
                "summary": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers."
                },
                "authors": [
                    {
                        "name": "Satoshi Hayakawa"
                    },
                    {
                        "name": "Yuhta Takida"
                    },
                    {
                        "name": "Masaaki Imaizumi"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04492v1",
                "updated": "2025-10-06T05:04:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    5,
                    4,
                    57,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T05:04:57Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    5,
                    4,
                    57,
                    0,
                    279,
                    0
                ],
                "title": "Joint Probing and Scheduling for Cache-Aided Hybrid\n  Satellite-Terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Probing and Scheduling for Cache-Aided Hybrid\n  Satellite-Terrestrial Networks"
                },
                "summary": "Caching is crucial in hybrid satellite-terrestrial networks to reduce\nlatency, optimize throughput, and improve data availability by storing\nfrequently accessed content closer to users, especially in bandwidth-limited\nsatellite systems, requiring strategic Medium Access Control (MAC) layer. This\npaper addresses throughput optimization in satellite-terrestrial integrated\nnetworks through opportunistic cooperative caching. We propose a joint probing\nand scheduling strategy to enhance content retrieval efficiency. The strategy\nleverages the LEO satellite to probe satellite-to-ground links and cache states\nof multiple cooperative terrestrial stations, enabling dynamic user scheduling\nfor content delivery. Using an optimal stopping theoretic approach with two\nlevels of incomplete information, we make real-time decisions on\nsatellite-terrestrial hybrid links and caching probing. Our threshold-based\nstrategy optimizes probing and scheduling, significantly improving average\nsystem throughput by exploiting cooperative caching, satellite-terrestrial link\ntransmission, and time diversity from dynamic user requests. Simulation results\nvalidate the effectiveness and practicality of the proposed strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is crucial in hybrid satellite-terrestrial networks to reduce\nlatency, optimize throughput, and improve data availability by storing\nfrequently accessed content closer to users, especially in bandwidth-limited\nsatellite systems, requiring strategic Medium Access Control (MAC) layer. This\npaper addresses throughput optimization in satellite-terrestrial integrated\nnetworks through opportunistic cooperative caching. We propose a joint probing\nand scheduling strategy to enhance content retrieval efficiency. The strategy\nleverages the LEO satellite to probe satellite-to-ground links and cache states\nof multiple cooperative terrestrial stations, enabling dynamic user scheduling\nfor content delivery. Using an optimal stopping theoretic approach with two\nlevels of incomplete information, we make real-time decisions on\nsatellite-terrestrial hybrid links and caching probing. Our threshold-based\nstrategy optimizes probing and scheduling, significantly improving average\nsystem throughput by exploiting cooperative caching, satellite-terrestrial link\ntransmission, and time diversity from dynamic user requests. Simulation results\nvalidate the effectiveness and practicality of the proposed strategies."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Sumei Sun"
                    }
                ],
                "author_detail": {
                    "name": "Sumei Sun"
                },
                "author": "Sumei Sun",
                "arxiv_comment": "6 pages, IEEE Global Communications Conference (GLOBECOM), December\n  2025, Taipei, Taiwan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v3",
                "updated": "2025-10-06T04:28:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    4,
                    28,
                    5,
                    0,
                    279,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration. Code is available at\nhttps://github.com/aSleepyTree/EB-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration. Code is available at\nhttps://github.com/aSleepyTree/EB-Cache."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04476v1",
                "updated": "2025-10-06T04:24:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    4,
                    24,
                    23,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T04:24:23Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    4,
                    24,
                    23,
                    0,
                    279,
                    0
                ],
                "title": "Compressed Convolutional Attention: Efficient Attention in a Compressed\n  Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressed Convolutional Attention: Efficient Attention in a Compressed\n  Latent Space"
                },
                "summary": "Multi-headed Attention's (MHA) quadratic compute and linearly growing\nKV-cache make long-context transformers expensive to train and serve. Prior\nworks such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)\nshrink the cache, speeding decode, but leave compute, which determines prefill\nand training speed, largely unchanged. We introduce Compressed Convolutional\nAttention (CCA), a novel attention method which down-projects queries, keys,\nand values and performs the entire attention operation inside the shared latent\nspace. This simple design dramatically cuts parameters, KV-cache, and FLOPs all\nat once by the desired compression factor. Because CCA is orthogonal to\nhead-sharing, we combine the two to form Compressed Convolutional Grouped Query\nAttention (CCGQA), which further tightens the compute-bandwidth Pareto frontier\nso that users can tune compression toward either FLOP or memory limits without\nsacrificing quality. Experiments show that CCGQA consistently outperforms both\nGQA and MLA at equal KV-cache compression on dense and MoE models.\nAdditionally, we show that CCGQA outperforms all other attention methods on MoE\nmodels with half the KV-cache of GQA and MLA, achieving an 8x KV-cache\ncompression with no drop in performance compared to standard MHA. CCA and CCGQA\nalso dramatically reduce the FLOP cost of attention which leads to\nsubstantially faster training and prefill than existing methods. On H100 GPUs,\nour fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence\nlength of 16k relative to MHA, and accelerates backward by about 1.3x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-headed Attention's (MHA) quadratic compute and linearly growing\nKV-cache make long-context transformers expensive to train and serve. Prior\nworks such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)\nshrink the cache, speeding decode, but leave compute, which determines prefill\nand training speed, largely unchanged. We introduce Compressed Convolutional\nAttention (CCA), a novel attention method which down-projects queries, keys,\nand values and performs the entire attention operation inside the shared latent\nspace. This simple design dramatically cuts parameters, KV-cache, and FLOPs all\nat once by the desired compression factor. Because CCA is orthogonal to\nhead-sharing, we combine the two to form Compressed Convolutional Grouped Query\nAttention (CCGQA), which further tightens the compute-bandwidth Pareto frontier\nso that users can tune compression toward either FLOP or memory limits without\nsacrificing quality. Experiments show that CCGQA consistently outperforms both\nGQA and MLA at equal KV-cache compression on dense and MoE models.\nAdditionally, we show that CCGQA outperforms all other attention methods on MoE\nmodels with half the KV-cache of GQA and MLA, achieving an 8x KV-cache\ncompression with no drop in performance compared to standard MHA. CCA and CCGQA\nalso dramatically reduce the FLOP cost of attention which leads to\nsubstantially faster training and prefill than existing methods. On H100 GPUs,\nour fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence\nlength of 16k relative to MHA, and accelerates backward by about 1.3x."
                },
                "authors": [
                    {
                        "name": "Tomas Figliolia"
                    },
                    {
                        "name": "Nicholas Alonso"
                    },
                    {
                        "name": "Rishi Iyer"
                    },
                    {
                        "name": "Quentin Anthony"
                    },
                    {
                        "name": "Beren Millidge"
                    }
                ],
                "author_detail": {
                    "name": "Beren Millidge"
                },
                "author": "Beren Millidge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.18149v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.18149v2",
                "updated": "2025-10-06T02:46:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    2,
                    46,
                    1,
                    0,
                    279,
                    0
                ],
                "published": "2024-03-26T23:17:05Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    23,
                    17,
                    5,
                    1,
                    86,
                    0
                ],
                "title": "Code Generation and Conic Constraints for Model-Predictive Control on\n  Microcontrollers with Conic-TinyMPC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Generation and Conic Constraints for Model-Predictive Control on\n  Microcontrollers with Conic-TinyMPC"
                },
                "summary": "Model-predictive control (MPC) is a powerful framework for controlling\ndynamic systems under constraints, but it remains challenging to deploy on\nresource-constrained platforms, especially for problems involving conic\nconstraints. To address this, we extend recent work developing fast,\nstructure-exploiting, cached ADMM solvers for embedded applications, to provide\nsupport for second-order cones, as well as C++ code generation from Python,\nMATLAB, and Julia for easy deployment. Microcontroller benchmarks show that our\nsolver provides up to a two-order-of-magnitude speedup, ranging from 10.6x to\n142.7x, over state-of-the-art embedded solvers on QP and SOCP problems, and\nenables us to fit order-of-magnitude larger problems in memory. We validate our\nsolver's deployed performance through simulation and hardware experiments,\nincluding conically-constrained trajectory tracking on a 27g Crazyflie\nquadrotor. To get started with Conic-TinyMPC, visit our documentation,\nexamples, and the open-source codebase at https://tinympc.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-predictive control (MPC) is a powerful framework for controlling\ndynamic systems under constraints, but it remains challenging to deploy on\nresource-constrained platforms, especially for problems involving conic\nconstraints. To address this, we extend recent work developing fast,\nstructure-exploiting, cached ADMM solvers for embedded applications, to provide\nsupport for second-order cones, as well as C++ code generation from Python,\nMATLAB, and Julia for easy deployment. Microcontroller benchmarks show that our\nsolver provides up to a two-order-of-magnitude speedup, ranging from 10.6x to\n142.7x, over state-of-the-art embedded solvers on QP and SOCP problems, and\nenables us to fit order-of-magnitude larger problems in memory. We validate our\nsolver's deployed performance through simulation and hardware experiments,\nincluding conically-constrained trajectory tracking on a 27g Crazyflie\nquadrotor. To get started with Conic-TinyMPC, visit our documentation,\nexamples, and the open-source codebase at https://tinympc.org."
                },
                "authors": [
                    {
                        "name": "Ishaan Mahajan"
                    },
                    {
                        "name": "Khai Nguyen"
                    },
                    {
                        "name": "Sam Schoedel"
                    },
                    {
                        "name": "Elakhya Nedumaran"
                    },
                    {
                        "name": "Moises Mata"
                    },
                    {
                        "name": "Brian Plancher"
                    },
                    {
                        "name": "Zachary Manchester"
                    }
                ],
                "author_detail": {
                    "name": "Zachary Manchester"
                },
                "author": "Zachary Manchester",
                "arxiv_comment": "First three authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.18149v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.18149v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v6",
                "updated": "2025-10-05T22:17:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    22,
                    17,
                    34,
                    6,
                    278,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "15 pages, 3 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00384v2",
                "updated": "2025-10-05T21:29:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    21,
                    29,
                    28,
                    6,
                    278,
                    0
                ],
                "published": "2025-05-31T04:27:22Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    4,
                    27,
                    22,
                    5,
                    151,
                    0
                ],
                "title": "Learning Semantics, Not Addresses: Runtime Neural Prefetching for Far\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Semantics, Not Addresses: Runtime Neural Prefetching for Far\n  Memory"
                },
                "summary": "Memory prefetching has long boosted CPU caches and is increasingly vital for\nfar-memory systems, where large portions of memory are offloaded to cheaper,\nremote tiers. While effective prefetching requires accurate prediction of\nfuture accesses, prior ML approaches have been limited to simulation or\nsmall-scale hardware. We introduce FarSight, the first Linux-based far-memory\nsystem to leverage deep learning by decoupling application semantics from\nruntime memory layout. This separation enables offline-trained models to\npredict access patterns over a compact ordinal vocabulary, which are resolved\nat runtime through lightweight mappings. Across four data-intensive workloads,\nFarSight delivers up to 3.6x higher performance than the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory prefetching has long boosted CPU caches and is increasingly vital for\nfar-memory systems, where large portions of memory are offloaded to cheaper,\nremote tiers. While effective prefetching requires accurate prediction of\nfuture accesses, prior ML approaches have been limited to simulation or\nsmall-scale hardware. We introduce FarSight, the first Linux-based far-memory\nsystem to leverage deep learning by decoupling application semantics from\nruntime memory layout. This separation enables offline-trained models to\npredict access patterns over a compact ordinal vocabulary, which are resolved\nat runtime through lightweight mappings. Across four data-intensive workloads,\nFarSight delivers up to 3.6x higher performance than the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Yutong Huang"
                    },
                    {
                        "name": "Zhiyuan Guo"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09253v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09253v2",
                "updated": "2025-10-05T18:13:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    18,
                    13,
                    39,
                    6,
                    278,
                    0
                ],
                "published": "2025-01-16T02:40:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving"
                },
                "summary": "The Text-to-Image (T2I) diffusion model has emerged as one of the most widely\nadopted generative models. However, serving diffusion models at the granularity\nof entire images introduces significant challenges, particularly under\nmulti-resolution workloads. First, image-level serving obstructs batching\nacross requests. Second, heterogeneous resolutions exhibit distinct locality\ncharacteristics, making it difficult to apply a uniform cache policy\neffectively.\n  To address these challenges, we present PatchedServe, a Patch Management\nFramework for SLO-Optimized Hybrid-Resolution Diffusion Serving. PatchedServe\nis the first SLO-optimized T2I diffusion serving framework designed to handle\nheterogeneous resolutions. Specifically, it incorporates a novel patch-based\nprocessing workflow that substantially improves throughput for\nhybrid-resolution inputs. Moreover, PatchedServe devises a patch-level cache\nreuse policy to fully exploit diffusion redundancies and integrates an\nSLO-aware scheduling algorithm with lightweight online latency prediction to\nimprove responsiveness. Our evaluation demonstrates that PatchedServe achieves\n30.1 % higher SLO satisfaction than the state-of-the-art diffusion serving\nsystem, while preserving image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Text-to-Image (T2I) diffusion model has emerged as one of the most widely\nadopted generative models. However, serving diffusion models at the granularity\nof entire images introduces significant challenges, particularly under\nmulti-resolution workloads. First, image-level serving obstructs batching\nacross requests. Second, heterogeneous resolutions exhibit distinct locality\ncharacteristics, making it difficult to apply a uniform cache policy\neffectively.\n  To address these challenges, we present PatchedServe, a Patch Management\nFramework for SLO-Optimized Hybrid-Resolution Diffusion Serving. PatchedServe\nis the first SLO-optimized T2I diffusion serving framework designed to handle\nheterogeneous resolutions. Specifically, it incorporates a novel patch-based\nprocessing workflow that substantially improves throughput for\nhybrid-resolution inputs. Moreover, PatchedServe devises a patch-level cache\nreuse policy to fully exploit diffusion redundancies and integrates an\nSLO-aware scheduling algorithm with lightweight online latency prediction to\nimprove responsiveness. Our evaluation demonstrates that PatchedServe achieves\n30.1 % higher SLO satisfaction than the state-of-the-art diffusion serving\nsystem, while preserving image quality."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09253v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09253v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04188v1",
                "updated": "2025-10-05T13:01:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    13,
                    1,
                    8,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T13:01:08Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    13,
                    1,
                    8,
                    6,
                    278,
                    0
                ],
                "title": "Let Features Decide Their Own Solvers: Hybrid Feature Caching for\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let Features Decide Their Own Solvers: Hybrid Feature Caching for\n  Diffusion Transformers"
                },
                "summary": "Diffusion Transformers offer state-of-the-art fidelity in image and video\nsynthesis, but their iterative sampling process remains a major bottleneck due\nto the high cost of transformer forward passes at each timestep. To mitigate\nthis, feature caching has emerged as a training-free acceleration technique\nthat reuses or forecasts hidden representations. However, existing methods\noften apply a uniform caching strategy across all feature dimensions, ignoring\ntheir heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by\nmodeling hidden feature evolution as a mixture of ODEs across dimensions, and\nintroduce HyCa, a Hybrid ODE solver inspired caching framework that applies\ndimension-wise caching strategies. HyCa achieves near-lossless acceleration\nacross diverse domains and models, including 5.55 times speedup on FLUX, 5.56\ntimes speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and\nQwen-Image-Edit without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers offer state-of-the-art fidelity in image and video\nsynthesis, but their iterative sampling process remains a major bottleneck due\nto the high cost of transformer forward passes at each timestep. To mitigate\nthis, feature caching has emerged as a training-free acceleration technique\nthat reuses or forecasts hidden representations. However, existing methods\noften apply a uniform caching strategy across all feature dimensions, ignoring\ntheir heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by\nmodeling hidden feature evolution as a mixture of ODEs across dimensions, and\nintroduce HyCa, a Hybrid ODE solver inspired caching framework that applies\ndimension-wise caching strategies. HyCa achieves near-lossless acceleration\nacross diverse domains and models, including 5.55 times speedup on FLUX, 5.56\ntimes speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and\nQwen-Image-Edit without retraining."
                },
                "authors": [
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Guantao Chen"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05176v1",
                "updated": "2025-10-05T12:09:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    12,
                    9,
                    14,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T12:09:14Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    12,
                    9,
                    14,
                    6,
                    278,
                    0
                ],
                "title": "PatternKV: Flattening KV Representation Expands Quantization Headroom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PatternKV: Flattening KV Representation Expands Quantization Headroom"
                },
                "summary": "KV cache in autoregressive LLMs eliminates redundant recomputation but has\nemerged as the dominant memory and bandwidth bottleneck during inference,\nnotably with long contexts and test-time scaling. KV quantization is a key\nlever for reducing cache cost, but accuracy drops sharply as the native KV\ndistribution lacks flatness and thus maintains a wide quantization range. Prior\nwork focuses on isolating outliers, which caps their error but fails to flatten\nthe overall distribution, leaving performance fragile under low-bit settings.\nIn this work, we show that the K cache maintains a stable structure that\nevolves gradually with context, while the V cache carries latent semantic\nregularities. Building on these insights, we propose PatternKV, a\npattern-aligned residual quantization scheme. It mines representative pattern\nvectors online, aligns each KV vector to its nearest pattern, and quantizes\nonly the residual. This reshaping of the KV distribution flattens the\nquantization target and narrows its range, thereby improving the fidelity of\nlow-bit KV quantization. Across long-context and test-time scaling settings on\nmultiple backbones, PatternKV delivers consistent 2-bit gains, with a 0.08%\naverage 4-bit drop relative to FP16, improves test-time scaling accuracy by 10%\non average, and raises throughput by 1.4x while supporting 1.25x larger\nbatches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache in autoregressive LLMs eliminates redundant recomputation but has\nemerged as the dominant memory and bandwidth bottleneck during inference,\nnotably with long contexts and test-time scaling. KV quantization is a key\nlever for reducing cache cost, but accuracy drops sharply as the native KV\ndistribution lacks flatness and thus maintains a wide quantization range. Prior\nwork focuses on isolating outliers, which caps their error but fails to flatten\nthe overall distribution, leaving performance fragile under low-bit settings.\nIn this work, we show that the K cache maintains a stable structure that\nevolves gradually with context, while the V cache carries latent semantic\nregularities. Building on these insights, we propose PatternKV, a\npattern-aligned residual quantization scheme. It mines representative pattern\nvectors online, aligns each KV vector to its nearest pattern, and quantizes\nonly the residual. This reshaping of the KV distribution flattens the\nquantization target and narrows its range, thereby improving the fidelity of\nlow-bit KV quantization. Across long-context and test-time scaling settings on\nmultiple backbones, PatternKV delivers consistent 2-bit gains, with a 0.08%\naverage 4-bit drop relative to FP16, improves test-time scaling accuracy by 10%\non average, and raises throughput by 1.4x while supporting 1.25x larger\nbatches."
                },
                "authors": [
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Shaoxiong Feng"
                    },
                    {
                        "name": "Peiwen Yuan"
                    },
                    {
                        "name": "Xinglin Wang"
                    },
                    {
                        "name": "Jiayi Shi"
                    },
                    {
                        "name": "Yueqi Zhang"
                    },
                    {
                        "name": "Chuyi Tan"
                    },
                    {
                        "name": "Boyuan Pan"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kan Li"
                    }
                ],
                "author_detail": {
                    "name": "Kan Li"
                },
                "author": "Kan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04153v1",
                "updated": "2025-10-05T11:09:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    11,
                    9,
                    10,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T11:09:10Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    11,
                    9,
                    10,
                    6,
                    278,
                    0
                ],
                "title": "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy\n  Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy\n  Preservation"
                },
                "summary": "Diffusion Models have gained significant popularity due to their remarkable\ncapabilities in image generation, albeit at the cost of intensive computation\nrequirement. Meanwhile, despite their widespread deployment in inference\nservices such as Midjourney, concerns about the potential leakage of sensitive\ninformation in uploaded user prompts have arisen. Existing solutions either\nlack rigorous privacy guarantees or fail to strike an effective balance between\nutility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play\nsafeguard that enables oblivious cloud-device hybrid generation. By oblivious,\neach input prompt is transformed into a set of semantically similar candidate\nprompts that differ only in sensitive attributes (e.g., gender, ethnicity). The\ncloud server processes all candidate prompts without knowing which one is the\nreal one, thus preventing any prompt leakage. To mitigate server cost, only a\nsmall portion of denoising steps is performed upon the large cloud model. The\nintermediate latents are then sent back to the client, which selects the\ntargeted latent and completes the remaining denoising using a small device\nmodel. Additionally, we analyze and incorporate several cache-based\naccelerations that leverage temporal and batch redundancy, effectively reducing\ncomputation cost with minimal utility degradation. Extensive experiments across\nmultiple datasets demonstrate that ObCLIP provides rigorous privacy and\ncomparable utility to cloud models with slightly increased server cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models have gained significant popularity due to their remarkable\ncapabilities in image generation, albeit at the cost of intensive computation\nrequirement. Meanwhile, despite their widespread deployment in inference\nservices such as Midjourney, concerns about the potential leakage of sensitive\ninformation in uploaded user prompts have arisen. Existing solutions either\nlack rigorous privacy guarantees or fail to strike an effective balance between\nutility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play\nsafeguard that enables oblivious cloud-device hybrid generation. By oblivious,\neach input prompt is transformed into a set of semantically similar candidate\nprompts that differ only in sensitive attributes (e.g., gender, ethnicity). The\ncloud server processes all candidate prompts without knowing which one is the\nreal one, thus preventing any prompt leakage. To mitigate server cost, only a\nsmall portion of denoising steps is performed upon the large cloud model. The\nintermediate latents are then sent back to the client, which selects the\ntargeted latent and completes the remaining denoising using a small device\nmodel. Additionally, we analyze and incorporate several cache-based\naccelerations that leverage temporal and batch redundancy, effectively reducing\ncomputation cost with minimal utility degradation. Extensive experiments across\nmultiple datasets demonstrate that ObCLIP provides rigorous privacy and\ncomparable utility to cloud models with slightly increased server cost."
                },
                "authors": [
                    {
                        "name": "Haoqi Wu"
                    },
                    {
                        "name": "Wei Dai"
                    },
                    {
                        "name": "Ming Xu"
                    },
                    {
                        "name": "Li Wang"
                    },
                    {
                        "name": "Qiang Yan"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yan"
                },
                "author": "Qiang Yan",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v3",
                "updated": "2025-10-05T08:34:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    8,
                    34,
                    30,
                    6,
                    278,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Xudong Wang"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04033v1",
                "updated": "2025-10-05T04:52:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    4,
                    52,
                    26,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T04:52:26Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    4,
                    52,
                    26,
                    6,
                    278,
                    0
                ],
                "title": "A global log for medical AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A global log for medical AI"
                },
                "summary": "Modern computer systems often rely on syslog, a simple, universal protocol\nthat records every critical event across heterogeneous infrastructure. However,\nhealthcare's rapidly growing clinical AI stack has no equivalent. As hospitals\nrush to pilot large language models and other AI-based clinical decision\nsupport tools, we still lack a standard way to record how, when, by whom, and\nfor whom these AI models are used. Without that transparency and visibility, it\nis challenging to measure real-world performance and outcomes, detect adverse\nevents, or correct bias or dataset drift. In the spirit of syslog, we introduce\nMedLog, a protocol for event-level logging of clinical AI. Any time an AI model\nis invoked to interact with a human, interface with another algorithm, or act\nindependently, a MedLog record is created. This record consists of nine core\nfields: header, model, user, target, inputs, artifacts, outputs, outcomes, and\nfeedback, providing a structured and consistent record of model activity. To\nencourage early adoption, especially in low-resource settings, and minimize the\ndata footprint, MedLog supports risk-based sampling, lifecycle-aware retention\npolicies, and write-behind caching; detailed traces for complex, agentic, or\nmulti-stage workflows can also be captured under MedLog. MedLog can catalyze\nthe development of new databases and software to store and analyze MedLog\nrecords. Realizing this vision would enable continuous surveillance, auditing,\nand iterative improvement of medical AI, laying the foundation for a new form\nof digital epidemiology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern computer systems often rely on syslog, a simple, universal protocol\nthat records every critical event across heterogeneous infrastructure. However,\nhealthcare's rapidly growing clinical AI stack has no equivalent. As hospitals\nrush to pilot large language models and other AI-based clinical decision\nsupport tools, we still lack a standard way to record how, when, by whom, and\nfor whom these AI models are used. Without that transparency and visibility, it\nis challenging to measure real-world performance and outcomes, detect adverse\nevents, or correct bias or dataset drift. In the spirit of syslog, we introduce\nMedLog, a protocol for event-level logging of clinical AI. Any time an AI model\nis invoked to interact with a human, interface with another algorithm, or act\nindependently, a MedLog record is created. This record consists of nine core\nfields: header, model, user, target, inputs, artifacts, outputs, outcomes, and\nfeedback, providing a structured and consistent record of model activity. To\nencourage early adoption, especially in low-resource settings, and minimize the\ndata footprint, MedLog supports risk-based sampling, lifecycle-aware retention\npolicies, and write-behind caching; detailed traces for complex, agentic, or\nmulti-stage workflows can also be captured under MedLog. MedLog can catalyze\nthe development of new databases and software to store and analyze MedLog\nrecords. Realizing this vision would enable continuous surveillance, auditing,\nand iterative improvement of medical AI, laying the foundation for a new form\nof digital epidemiology."
                },
                "authors": [
                    {
                        "name": "Ayush Noori"
                    },
                    {
                        "name": "Adam Rodman"
                    },
                    {
                        "name": "Alan Karthikesalingam"
                    },
                    {
                        "name": "Bilal A. Mateen"
                    },
                    {
                        "name": "Christopher A. Longhurst"
                    },
                    {
                        "name": "Daniel Yang"
                    },
                    {
                        "name": "Dave deBronkart"
                    },
                    {
                        "name": "Gauden Galea"
                    },
                    {
                        "name": "Harold F. Wolf III"
                    },
                    {
                        "name": "Jacob Waxman"
                    },
                    {
                        "name": "Joshua C. Mandel"
                    },
                    {
                        "name": "Juliana Rotich"
                    },
                    {
                        "name": "Kenneth D. Mandl"
                    },
                    {
                        "name": "Maryam Mustafa"
                    },
                    {
                        "name": "Melissa Miles"
                    },
                    {
                        "name": "Nigam H. Shah"
                    },
                    {
                        "name": "Peter Lee"
                    },
                    {
                        "name": "Robert Korom"
                    },
                    {
                        "name": "Scott Mahoney"
                    },
                    {
                        "name": "Seth Hain"
                    },
                    {
                        "name": "Tien Yin Wong"
                    },
                    {
                        "name": "Trevor Mundel"
                    },
                    {
                        "name": "Vivek Natarajan"
                    },
                    {
                        "name": "Noa Dagan"
                    },
                    {
                        "name": "David A. Clifton"
                    },
                    {
                        "name": "Ran D. Balicer"
                    },
                    {
                        "name": "Isaac S. Kohane"
                    },
                    {
                        "name": "Marinka Zitnik"
                    }
                ],
                "author_detail": {
                    "name": "Marinka Zitnik"
                },
                "author": "Marinka Zitnik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03851v1",
                "updated": "2025-10-04T15:52:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    52,
                    31,
                    5,
                    277,
                    0
                ],
                "published": "2025-10-04T15:52:31Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    52,
                    31,
                    5,
                    277,
                    0
                ],
                "title": "Algorithm Generation via Creative Ideation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithm Generation via Creative Ideation"
                },
                "summary": "Designing system algorithms remains challenging, where the discontinuous\nnature of the solution space often forces system engineers to rely on generic\nheuristics at the expense of performance. We study whether LLMs can practically\ndrive algorithm generation, and find that they are biased towards well-known\ngeneric designs, rather than making the creative leaps needed to navigate the\ndiscontinuous solution space. To address this limitation, we introduce\nMetaMuse, a framework for creative ideation built on three self-reflection\nprinciples: (1) quantifying solution diversity and usefulness in measurable\nperformance space, rather than abstract idea space, (2) steering ideation\nthrough external stimuli, rather than internal randomness, and (3) constructing\nexecutable solutions using waypoint reasoning, rather than free-form\nchain-of-thought. Extensive evaluation shows that MetaMuse can generate\nhigh-performing solutions for two critical problems at a global cloud provider:\ncache replacement (reducing cache misses by up to 35.76%) and online bin\npacking (reducing bin usage by up to 30.93%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing system algorithms remains challenging, where the discontinuous\nnature of the solution space often forces system engineers to rely on generic\nheuristics at the expense of performance. We study whether LLMs can practically\ndrive algorithm generation, and find that they are biased towards well-known\ngeneric designs, rather than making the creative leaps needed to navigate the\ndiscontinuous solution space. To address this limitation, we introduce\nMetaMuse, a framework for creative ideation built on three self-reflection\nprinciples: (1) quantifying solution diversity and usefulness in measurable\nperformance space, rather than abstract idea space, (2) steering ideation\nthrough external stimuli, rather than internal randomness, and (3) constructing\nexecutable solutions using waypoint reasoning, rather than free-form\nchain-of-thought. Extensive evaluation shows that MetaMuse can generate\nhigh-performing solutions for two critical problems at a global cloud provider:\ncache replacement (reducing cache misses by up to 35.76%) and online bin\npacking (reducing bin usage by up to 30.93%)."
                },
                "authors": [
                    {
                        "name": "Ruiying Ma"
                    },
                    {
                        "name": "Chieh-Jan Mike Liang"
                    },
                    {
                        "name": "Yanjie Gao"
                    },
                    {
                        "name": "Francis Y. Yan"
                    }
                ],
                "author_detail": {
                    "name": "Francis Y. Yan"
                },
                "author": "Francis Y. Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03834v1",
                "updated": "2025-10-04T15:25:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    25,
                    4,
                    5,
                    277,
                    0
                ],
                "published": "2025-10-04T15:25:04Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    25,
                    4,
                    5,
                    277,
                    0
                ],
                "title": "Hybrid MBE Route to Adsorption-Controlled Growth of BaTiO3 Membranes\n  with Robust Polarization Switching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid MBE Route to Adsorption-Controlled Growth of BaTiO3 Membranes\n  with Robust Polarization Switching"
                },
                "summary": "Freestanding ferroelectric membranes are promising for flexible electronics,\nnonvolatile memory, photonics, and spintronics, but their synthesis is\nchallenged by the need for reproducibility with precise stoichiometric control.\nHere, we demonstrate the adsorption-controlled growth of single-crystalline,\nepitaxial BaTiO3 films by hybrid molecular beam epitaxy (MBE) on a binary oxide\nsacrificial layer. Using a simple water-droplet lift-off method, we obtained\nsubmillimeter- to millimeter-sized membranes that retained crystallinity, as\nconfirmed by high-resolution X-ray diffraction, and exhibited robust tetragonal\nsymmetry by Raman spectroscopy. Impedance spectroscopy confirmed a high\ndielectric constant of 1340, reflecting the robust dielectric response of the\nmembranes. Ferroelectric functionality was revealed by piezoresponse force\nmicroscopy (PFM) and further verified by polarization-electric field (P-E) loop\nmeasurements with Positive-Up-Negative-Down (PUND). The P-E loops exhibited a\nremnant polarization of 5 microC cm-2 and a coercive field of 63 kV cm-1. These\nresults were interpreted in relation to c- and a-domain configurations. These\nresults establish hybrid MBE as a generalizable route for producing\nstoichiometry-controlled ferroelectric membranes, enabling their integration\ninto next-generation flexible and multifunctional quantum oxide devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Freestanding ferroelectric membranes are promising for flexible electronics,\nnonvolatile memory, photonics, and spintronics, but their synthesis is\nchallenged by the need for reproducibility with precise stoichiometric control.\nHere, we demonstrate the adsorption-controlled growth of single-crystalline,\nepitaxial BaTiO3 films by hybrid molecular beam epitaxy (MBE) on a binary oxide\nsacrificial layer. Using a simple water-droplet lift-off method, we obtained\nsubmillimeter- to millimeter-sized membranes that retained crystallinity, as\nconfirmed by high-resolution X-ray diffraction, and exhibited robust tetragonal\nsymmetry by Raman spectroscopy. Impedance spectroscopy confirmed a high\ndielectric constant of 1340, reflecting the robust dielectric response of the\nmembranes. Ferroelectric functionality was revealed by piezoresponse force\nmicroscopy (PFM) and further verified by polarization-electric field (P-E) loop\nmeasurements with Positive-Up-Negative-Down (PUND). The P-E loops exhibited a\nremnant polarization of 5 microC cm-2 and a coercive field of 63 kV cm-1. These\nresults were interpreted in relation to c- and a-domain configurations. These\nresults establish hybrid MBE as a generalizable route for producing\nstoichiometry-controlled ferroelectric membranes, enabling their integration\ninto next-generation flexible and multifunctional quantum oxide devices."
                },
                "authors": [
                    {
                        "name": "S. Choo"
                    },
                    {
                        "name": "S. Varshney"
                    },
                    {
                        "name": "J. Shah"
                    },
                    {
                        "name": "A. K. Manjeshwar"
                    },
                    {
                        "name": "D. K. Lee"
                    },
                    {
                        "name": "K. A. Mkhoyan"
                    },
                    {
                        "name": "R. D. James"
                    },
                    {
                        "name": "B. Jalan"
                    }
                ],
                "author_detail": {
                    "name": "B. Jalan"
                },
                "author": "B. Jalan",
                "arxiv_comment": "22 pages 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03712v1",
                "updated": "2025-10-04T07:22:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    7,
                    22,
                    39,
                    5,
                    277,
                    0
                ],
                "published": "2025-10-04T07:22:39Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    7,
                    22,
                    39,
                    5,
                    277,
                    0
                ],
                "title": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems"
                },
                "summary": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization."
                },
                "authors": [
                    {
                        "name": "Jahidul Arafat"
                    },
                    {
                        "name": "Kh. M. Moniruzzaman"
                    },
                    {
                        "name": "Shamim Hossain"
                    },
                    {
                        "name": "Fariha Tasmin"
                    },
                    {
                        "name": "Kamrujjaman"
                    },
                    {
                        "name": "Ahsan Habib Tareq"
                    }
                ],
                "author_detail": {
                    "name": "Ahsan Habib Tareq"
                },
                "author": "Ahsan Habib Tareq",
                "arxiv_comment": "26 pages, 12 tables, 4 figures. Academic-industry collaboration.\n  Framework (HYDRA, RAVEN, APEX) for optimization-induced vulnerabilities.\n  Evaluated: 2,160 configs, 12.7TB data, 1,748 scenarios",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M15, 90B25, 68T05, 90C29",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; C.2.4; D.2.5; D.4.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16391v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16391v3",
                "updated": "2025-10-04T05:59:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    5,
                    59,
                    1,
                    5,
                    277,
                    0
                ],
                "published": "2025-07-22T09:35:59Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    9,
                    35,
                    59,
                    1,
                    203,
                    0
                ],
                "title": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing"
                },
                "summary": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models."
                },
                "authors": [
                    {
                        "name": "Chenqi Lin"
                    },
                    {
                        "name": "Kang Yang"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhaohui Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16391v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16391v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08134v3",
                "updated": "2025-10-04T05:28:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    5,
                    28,
                    39,
                    5,
                    277,
                    0
                ],
                "published": "2025-08-11T16:10:00Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    10,
                    0,
                    0,
                    223,
                    0
                ],
                "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control"
                },
                "summary": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement."
                },
                "authors": [
                    {
                        "name": "Zeqian Long"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Kunyu Feng"
                    },
                    {
                        "name": "Xinhua Zhang"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Harry Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Yue Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yue Ma"
                },
                "author": "Yue Ma",
                "arxiv_comment": "Project webpage is available at https://follow-your-shape.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05370v2",
                "updated": "2025-10-04T03:45:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    3,
                    45,
                    40,
                    5,
                    277,
                    0
                ],
                "published": "2025-02-07T22:51:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "Taming Latency-Memory Trade-Off in MoE-Based LLM Serving via\n  Fine-Grained Expert Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Latency-Memory Trade-Off in MoE-Based LLM Serving via\n  Fine-Grained Expert Offloading"
                },
                "summary": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs.\n  To tame the latency-memory trade-off in MoE serving, we present FineMoE, a\nfine-grained expert offloading system for MoE serving that achieves low\ninference latency with memory efficiency. We design FineMoE to extract\nfine-grained expert selection patterns from MoE models and semantic hints from\ninput prompts to efficiently guide expert prefetching, caching, and offloading\ndecisions. FineMoE is prototyped on top of HuggingFace Transformers and\ndeployed on a six-GPU testbed. Experiments with open-source MoE models and\nreal-world workloads show that FineMoE reduces inference latency by 47% and\nimproves expert hit rate by 39% over state-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs.\n  To tame the latency-memory trade-off in MoE serving, we present FineMoE, a\nfine-grained expert offloading system for MoE serving that achieves low\ninference latency with memory efficiency. We design FineMoE to extract\nfine-grained expert selection patterns from MoE models and semantic hints from\ninput prompts to efficiently guide expert prefetching, caching, and offloading\ndecisions. FineMoE is prototyped on top of HuggingFace Transformers and\ndeployed on a six-GPU testbed. Experiments with open-source MoE models and\nreal-world workloads show that FineMoE reduces inference latency by 47% and\nimproves expert hit rate by 39% over state-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Hanfei Yu"
                    },
                    {
                        "name": "Xingqi Cui"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03215v1",
                "updated": "2025-10-03T17:52:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    52,
                    32,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:52:32Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    52,
                    32,
                    4,
                    276,
                    0
                ],
                "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models"
                },
                "summary": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C."
                },
                "authors": [
                    {
                        "name": "Tianyu Fu"
                    },
                    {
                        "name": "Zihan Min"
                    },
                    {
                        "name": "Hanling Zhang"
                    },
                    {
                        "name": "Jichao Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03198v1",
                "updated": "2025-10-03T17:35:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    35,
                    16,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:35:16Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    35,
                    16,
                    4,
                    276,
                    0
                ],
                "title": "Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation\n  on Minecraft",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation\n  on Minecraft"
                },
                "summary": "Autoregressive video diffusion models have proved effective for world\nmodeling and interactive scene generation, with Minecraft gameplay as a\nrepresentative application. To faithfully simulate play, a model must generate\nnatural content while exploring new scenes and preserve spatial consistency\nwhen revisiting explored areas. Under limited computation budgets, it must\ncompress and exploit historical cues within a finite context window, which\nexposes a trade-off: Temporal-only memory lacks long-term spatial consistency,\nwhereas adding spatial memory strengthens consistency but may degrade new scene\ngeneration quality when the model over-relies on insufficient spatial context.\nWe present Memory Forcing, a learning framework that pairs training protocols\nwith a geometry-indexed spatial memory. Hybrid Training exposes distinct\ngameplay regimes, guiding the model to rely on temporal memory during\nexploration and incorporate spatial memory for revisits. Chained Forward\nTraining extends autoregressive training with model rollouts, where chained\npredictions create larger pose variations and encourage reliance on spatial\nmemory for maintaining consistency. Point-to-Frame Retrieval efficiently\nretrieves history by mapping currently visible points to their source frames,\nwhile Incremental 3D Reconstruction maintains and updates an explicit 3D cache.\nExtensive experiments demonstrate that Memory Forcing achieves superior\nlong-term spatial consistency and generative quality across diverse\nenvironments, while maintaining computational efficiency for extended\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive video diffusion models have proved effective for world\nmodeling and interactive scene generation, with Minecraft gameplay as a\nrepresentative application. To faithfully simulate play, a model must generate\nnatural content while exploring new scenes and preserve spatial consistency\nwhen revisiting explored areas. Under limited computation budgets, it must\ncompress and exploit historical cues within a finite context window, which\nexposes a trade-off: Temporal-only memory lacks long-term spatial consistency,\nwhereas adding spatial memory strengthens consistency but may degrade new scene\ngeneration quality when the model over-relies on insufficient spatial context.\nWe present Memory Forcing, a learning framework that pairs training protocols\nwith a geometry-indexed spatial memory. Hybrid Training exposes distinct\ngameplay regimes, guiding the model to rely on temporal memory during\nexploration and incorporate spatial memory for revisits. Chained Forward\nTraining extends autoregressive training with model rollouts, where chained\npredictions create larger pose variations and encourage reliance on spatial\nmemory for maintaining consistency. Point-to-Frame Retrieval efficiently\nretrieves history by mapping currently visible points to their source frames,\nwhile Incremental 3D Reconstruction maintains and updates an explicit 3D cache.\nExtensive experiments demonstrate that Memory Forcing achieves superior\nlong-term spatial consistency and generative quality across diverse\nenvironments, while maintaining computational efficiency for extended\nsequences."
                },
                "authors": [
                    {
                        "name": "Junchao Huang"
                    },
                    {
                        "name": "Xinting Hu"
                    },
                    {
                        "name": "Boyao Han"
                    },
                    {
                        "name": "Shaoshuai Shi"
                    },
                    {
                        "name": "Zhuotao Tian"
                    },
                    {
                        "name": "Tianyu He"
                    },
                    {
                        "name": "Li Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Li Jiang"
                },
                "author": "Li Jiang",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v2",
                "updated": "2025-10-03T15:37:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    37,
                    19,
                    4,
                    276,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures; Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02866v1",
                "updated": "2025-10-03T10:06:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    6,
                    44,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T10:06:44Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    6,
                    44,
                    4,
                    276,
                    0
                ],
                "title": "Life Estimation of HVDC Cable Insulation under Load Cycles: from\n  Macroscopic to Microscopic Charge Conduction Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Life Estimation of HVDC Cable Insulation under Load Cycles: from\n  Macroscopic to Microscopic Charge Conduction Modelling"
                },
                "summary": "This paper goes one step forward in the life estimation of HVDC cable\ninsulation under load cycles by introducing for the first time a microscopic\nmodel of charge conduction and transport i.e., Bipolar Charge Transport BCT\nmodel for electric field calculation inside the insulation thickness. The paper\nfirstly includes the development and the validation of BCT model with that\nfound in literature. Then, the parameters of the developed BCT model are\noptimized using Pulsed Electro-Acoustic PEA space charge measurements. Followed\nby the integration of the developed, validated and optimized model into the\nelectric field calculation for life estimation of a 500 kV DC-XLPE insulated\ncable subjected to Type Test load cycles according to Cigre Techical Brochure\n852. The developed microscopic model is compared to the macroscopic models\nalready found in the literature. The microscopic model shows a comparable\nelectric field inversion similarly to macroscopic models. However, the behavior\nof the microscopic model is noticed to be different under heating and cooling\nload cycles. In hot cable, the maximum electric field stabilizes at different\namplitude and position inside the insulation thickness in both models. This\ninvestigation has been carried out in the framework of the HEU-NEWGEN research\nproject.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper goes one step forward in the life estimation of HVDC cable\ninsulation under load cycles by introducing for the first time a microscopic\nmodel of charge conduction and transport i.e., Bipolar Charge Transport BCT\nmodel for electric field calculation inside the insulation thickness. The paper\nfirstly includes the development and the validation of BCT model with that\nfound in literature. Then, the parameters of the developed BCT model are\noptimized using Pulsed Electro-Acoustic PEA space charge measurements. Followed\nby the integration of the developed, validated and optimized model into the\nelectric field calculation for life estimation of a 500 kV DC-XLPE insulated\ncable subjected to Type Test load cycles according to Cigre Techical Brochure\n852. The developed microscopic model is compared to the macroscopic models\nalready found in the literature. The microscopic model shows a comparable\nelectric field inversion similarly to macroscopic models. However, the behavior\nof the microscopic model is noticed to be different under heating and cooling\nload cycles. In hot cable, the maximum electric field stabilizes at different\namplitude and position inside the insulation thickness in both models. This\ninvestigation has been carried out in the framework of the HEU-NEWGEN research\nproject."
                },
                "authors": [
                    {
                        "name": "Bassel Diban"
                    },
                    {
                        "name": "Giovanni Mazzanti"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Mazzanti"
                },
                "author": "Giovanni Mazzanti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02758v1",
                "updated": "2025-10-03T06:43:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    43,
                    24,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T06:43:24Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    43,
                    24,
                    4,
                    276,
                    0
                ],
                "title": "TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via\n  Preemptive Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via\n  Preemptive Scheduling"
                },
                "summary": "Real-time LLM interactions demand streamed token generations, where text\ntokens are progressively generated and delivered to users while balancing two\nobjectives: responsiveness (i.e., low time-to-first-token) and steady\ngeneration (i.e.,required time-between-tokens). Standard LLM serving systems\nsuffer from the inflexibility caused by non-preemptive request scheduling and\nreactive memory management, leading to poor resource utilization and low\nrequest processing parallelism under request bursts. Therefore, we present\nTokenFlow, a novel LLM serving system with enhanced text streaming performance\nvia preemptive request scheduling and proactive key-value (KV) cache\nmanagement. TokenFlow dynamically prioritizes requests based on real-time token\nbuffer occupancy and token consumption rate, while actively transferring KV\ncache between GPU and CPU memory in the background and overlapping I/O with\ncomputation to minimize request preemption overhead. Extensive experiments on\nLlama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)\ndemonstrate that TokenFlow achieves up to 82.5% higher effective throughput\n(accounting for actual user consumption) while reducing P99 TTFT by up to\n80.2%, without degrading overall token throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time LLM interactions demand streamed token generations, where text\ntokens are progressively generated and delivered to users while balancing two\nobjectives: responsiveness (i.e., low time-to-first-token) and steady\ngeneration (i.e.,required time-between-tokens). Standard LLM serving systems\nsuffer from the inflexibility caused by non-preemptive request scheduling and\nreactive memory management, leading to poor resource utilization and low\nrequest processing parallelism under request bursts. Therefore, we present\nTokenFlow, a novel LLM serving system with enhanced text streaming performance\nvia preemptive request scheduling and proactive key-value (KV) cache\nmanagement. TokenFlow dynamically prioritizes requests based on real-time token\nbuffer occupancy and token consumption rate, while actively transferring KV\ncache between GPU and CPU memory in the background and overlapping I/O with\ncomputation to minimize request preemption overhead. Extensive experiments on\nLlama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)\ndemonstrate that TokenFlow achieves up to 82.5% higher effective throughput\n(accounting for actual user consumption) while reducing P99 TTFT by up to\n80.2%, without degrading overall token throughput."
                },
                "authors": [
                    {
                        "name": "Junyi Chen"
                    },
                    {
                        "name": "Chuheng Du"
                    },
                    {
                        "name": "Renyuan Liu"
                    },
                    {
                        "name": "Shuochao Yao"
                    },
                    {
                        "name": "Dingtian Yan"
                    },
                    {
                        "name": "Jiang Liao"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Accepted by EuroSys 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02750v1",
                "updated": "2025-10-03T06:27:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    27,
                    33,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T06:27:33Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    27,
                    33,
                    4,
                    276,
                    0
                ],
                "title": "Bayesian Test-time Adaptation for Object Recognition and Detection with\n  Vision-language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Test-time Adaptation for Object Recognition and Detection with\n  Vision-language Models"
                },
                "summary": "Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved\nremarkable success in object recognition and detection. However, their\nperformance often degrades under real-world distribution shifts. Test-time\nadaptation (TTA) aims to mitigate this issue by adapting models during\ninference. Existing methods either rely on computationally expensive\nbackpropagation, which hinders real-time deployment, or focus solely on\nlikelihood adaptation, which overlooks the critical role of the prior. Our\nprior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for\nobject recognition by introducing a training-free framework that incorporates\nadaptive priors. Building upon this foundation, we now present Bayesian Class\nAdaptation plus (BCA+), a unified, training-free framework for TTA for both\nobject recognition and detection. BCA+ introduces a dynamic cache that\nadaptively stores and updates class embeddings, spatial scales (for detection),\nand, crucially, adaptive class priors derived from historical predictions. We\nformulate adaptation as a Bayesian inference problem, where final predictions\nare generated by fusing the initial VLM output with a cache-based prediction.\nThis cache-based prediction combines a dynamically updated likelihood\n(measuring feature and scale similarity) and a prior (reflecting the evolving\nclass distribution). This dual-adaptation mechanism, coupled with\nuncertainty-guided fusion, enables BCA+ to correct both the model's semantic\nunderstanding and its contextual confidence. As a training-free method\nrequiring no backpropagation, BCA+ is highly efficient. Extensive experiments\ndemonstrate that BCA+ achieves state-of-the-art performance on both recognition\nand detection benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved\nremarkable success in object recognition and detection. However, their\nperformance often degrades under real-world distribution shifts. Test-time\nadaptation (TTA) aims to mitigate this issue by adapting models during\ninference. Existing methods either rely on computationally expensive\nbackpropagation, which hinders real-time deployment, or focus solely on\nlikelihood adaptation, which overlooks the critical role of the prior. Our\nprior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for\nobject recognition by introducing a training-free framework that incorporates\nadaptive priors. Building upon this foundation, we now present Bayesian Class\nAdaptation plus (BCA+), a unified, training-free framework for TTA for both\nobject recognition and detection. BCA+ introduces a dynamic cache that\nadaptively stores and updates class embeddings, spatial scales (for detection),\nand, crucially, adaptive class priors derived from historical predictions. We\nformulate adaptation as a Bayesian inference problem, where final predictions\nare generated by fusing the initial VLM output with a cache-based prediction.\nThis cache-based prediction combines a dynamically updated likelihood\n(measuring feature and scale similarity) and a prior (reflecting the evolving\nclass distribution). This dual-adaptation mechanism, coupled with\nuncertainty-guided fusion, enables BCA+ to correct both the model's semantic\nunderstanding and its contextual confidence. As a training-free method\nrequiring no backpropagation, BCA+ is highly efficient. Extensive experiments\ndemonstrate that BCA+ achieves state-of-the-art performance on both recognition\nand detection benchmarks."
                },
                "authors": [
                    {
                        "name": "Lihua Zhou"
                    },
                    {
                        "name": "Mao Ye"
                    },
                    {
                        "name": "Shuaifeng Li"
                    },
                    {
                        "name": "Nianxin Li"
                    },
                    {
                        "name": "Jinlin Wu"
                    },
                    {
                        "name": "Xiatian Zhu"
                    },
                    {
                        "name": "Lei Deng"
                    },
                    {
                        "name": "Hongbin Liu"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02084v2",
                "updated": "2025-10-03T05:10:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    5,
                    10,
                    2,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-02T14:50:50Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    50,
                    50,
                    3,
                    275,
                    0
                ],
                "title": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series\n  Forecasting"
                },
                "summary": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries."
                },
                "authors": [
                    {
                        "name": "Kuiye Ding"
                    },
                    {
                        "name": "Fanda Fan"
                    },
                    {
                        "name": "Zheya Wang"
                    },
                    {
                        "name": "Hongxiao Li"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Chunjie Luo"
                    },
                    {
                        "name": "Jianfeng Zhan"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Zhan"
                },
                "author": "Jianfeng Zhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25188v2",
                "updated": "2025-10-03T00:40:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    0,
                    40,
                    49,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-29T17:59:54Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    59,
                    54,
                    0,
                    272,
                    0
                ],
                "title": "Learning to Parallel: Accelerating Diffusion Large Language Models via\n  Learnable Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Parallel: Accelerating Diffusion Large Language Models via\n  Learnable Parallel Decoding"
                },
                "summary": "Autoregressive decoding in large language models (LLMs) requires\n$\\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting\ninference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token\ngeneration through iterative denoising. However, current parallel decoding\nstrategies rely on fixed, input-agnostic heuristics (e.g., confidence\nthresholds), which fail to adapt to input-specific characteristics, resulting\nin suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,\nwe explore a more flexible and dynamic approach to parallel decoding. We\npropose Learning to Parallel Decode (Learn2PD), a framework that trains a\nlightweight and adaptive filter model to predict, for each token position,\nwhether the current prediction matches the final output. This learned filter\napproximates an oracle parallel decoding strategy that unmasks tokens only when\ncorrectly predicted. Importantly, the filter model is learned in a\npost-training manner, requiring only a small amount of computation to optimize\nit (minute-level GPU time). Additionally, we introduce End-of-Text Prediction\n(EoTP) to detect decoding completion at the end of sequence, avoiding redundant\ndecoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that\nour method achieves up to 22.58$\\times$ speedup without any performance drop,\nand up to 57.51$\\times$ when combined with KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive decoding in large language models (LLMs) requires\n$\\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting\ninference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token\ngeneration through iterative denoising. However, current parallel decoding\nstrategies rely on fixed, input-agnostic heuristics (e.g., confidence\nthresholds), which fail to adapt to input-specific characteristics, resulting\nin suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,\nwe explore a more flexible and dynamic approach to parallel decoding. We\npropose Learning to Parallel Decode (Learn2PD), a framework that trains a\nlightweight and adaptive filter model to predict, for each token position,\nwhether the current prediction matches the final output. This learned filter\napproximates an oracle parallel decoding strategy that unmasks tokens only when\ncorrectly predicted. Importantly, the filter model is learned in a\npost-training manner, requiring only a small amount of computation to optimize\nit (minute-level GPU time). Additionally, we introduce End-of-Text Prediction\n(EoTP) to detect decoding completion at the end of sequence, avoiding redundant\ndecoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that\nour method achieves up to 22.58$\\times$ speedup without any performance drop,\nand up to 57.51$\\times$ when combined with KV-Cache."
                },
                "authors": [
                    {
                        "name": "Wenrui Bao"
                    },
                    {
                        "name": "Zhiben Chen"
                    },
                    {
                        "name": "Dan Xu"
                    },
                    {
                        "name": "Yuzhang Shang"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhang Shang"
                },
                "author": "Yuzhang Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02613v1",
                "updated": "2025-10-02T23:16:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    23,
                    16,
                    35,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T23:16:35Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    23,
                    16,
                    35,
                    3,
                    275,
                    0
                ],
                "title": "ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts\n  Models"
                },
                "summary": "Mixture-of-Experts (MoE) models promise efficient scaling of large language\nmodels (LLMs) by activating only a small subset of experts per token, but their\nparallelized inference pipelines make elastic serving challenging. Existing\nstrategies fall short: horizontal scaling provisions entire replicas of the\ncurrent configuration, often tens to hundreds of accelerators, leading to\ncoarse granularity, long provisioning delays, and costly overprovisioning.\nVertical scaling offers finer adjustments but typically requires instance\nrestarts, incurring downtime. These limitations make current approaches\nill-suited for the bursty, short-lived traffic patterns common in cloud\ndeployments.\n  We present ElasticMoE, an elastic scaling framework for MoE LLMs that\nachieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE\ndecouples inference execution from memory operations, enabling scaling steps to\nproceed concurrently with serving. An HBM Management Module (HMM) reuses\nweights and KV caches via zero-copy remapping, while high-bandwidth\npeer-to-peer transfers bring newly added accelerators online without\ninterrupting service. A virtual memory based expert redistribution mechanism\nmigrates MoE experts without costly buffer reallocations, reducing peak memory\nusage during expert parallelism reconfiguration.\n  Our evaluation on Ascend NPUs with three popular MoE LLMs shows that\nElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput\nduring scaling, and significantly improves SLO attainment compared to\nbaselines. By enabling fine-grained, concurrent scaling with minimal\ndisruption, ElasticMoE advances the practicality of deploying massive MoE LLMs\nin dynamic cloud environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models promise efficient scaling of large language\nmodels (LLMs) by activating only a small subset of experts per token, but their\nparallelized inference pipelines make elastic serving challenging. Existing\nstrategies fall short: horizontal scaling provisions entire replicas of the\ncurrent configuration, often tens to hundreds of accelerators, leading to\ncoarse granularity, long provisioning delays, and costly overprovisioning.\nVertical scaling offers finer adjustments but typically requires instance\nrestarts, incurring downtime. These limitations make current approaches\nill-suited for the bursty, short-lived traffic patterns common in cloud\ndeployments.\n  We present ElasticMoE, an elastic scaling framework for MoE LLMs that\nachieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE\ndecouples inference execution from memory operations, enabling scaling steps to\nproceed concurrently with serving. An HBM Management Module (HMM) reuses\nweights and KV caches via zero-copy remapping, while high-bandwidth\npeer-to-peer transfers bring newly added accelerators online without\ninterrupting service. A virtual memory based expert redistribution mechanism\nmigrates MoE experts without costly buffer reallocations, reducing peak memory\nusage during expert parallelism reconfiguration.\n  Our evaluation on Ascend NPUs with three popular MoE LLMs shows that\nElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput\nduring scaling, and significantly improves SLO attainment compared to\nbaselines. By enabling fine-grained, concurrent scaling with minimal\ndisruption, ElasticMoE advances the practicality of deploying massive MoE LLMs\nin dynamic cloud environments."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Haley Li"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Hanieh Sadri"
                    },
                    {
                        "name": "Qintao Zhang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "arxiv_affiliation": "Huawei Technologies Canada",
                "author": "Zhenan Fan",
                "arxiv_comment": "19 pages, 15 figures, Under Submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v5",
                "updated": "2025-10-02T19:25:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    19,
                    25,
                    29,
                    3,
                    275,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence after the aLoRA is invoked. This change crucially allows\naLoRA to accept the base model's KV cache of the input string, meaning that\naLoRA can be instantly activated whenever needed in a chain without recomputing\nthe prior keys and values. This enables building what we call intrinsics, i.e.\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\ntrain a set of aLoRA-based intrinsics models, demonstrating competitive\naccuracy with standard LoRA while significantly improving inference efficiency.\nWe contributed our Activated LoRA implementation to the Huggingface PEFT\nlibrary https://github.com/huggingface/peft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence after the aLoRA is invoked. This change crucially allows\naLoRA to accept the base model's KV cache of the input string, meaning that\naLoRA can be instantly activated whenever needed in a chain without recomputing\nthe prior keys and values. This enables building what we call intrinsics, i.e.\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\ntrain a set of aLoRA-based intrinsics models, demonstrating competitive\naccuracy with standard LoRA while significantly improving inference efficiency.\nWe contributed our Activated LoRA implementation to the Huggingface PEFT\nlibrary https://github.com/huggingface/peft."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v4",
                "updated": "2025-10-02T19:09:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    19,
                    9,
                    19,
                    3,
                    275,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17650v2",
                "updated": "2025-10-02T18:38:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    18,
                    38,
                    0,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-22T11:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "title": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers"
                },
                "summary": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical."
                },
                "authors": [
                    {
                        "name": "Soroush Mahdi"
                    },
                    {
                        "name": "Fardin Ayar"
                    },
                    {
                        "name": "Ehsan Javanmardi"
                    },
                    {
                        "name": "Manabu Tsukada"
                    },
                    {
                        "name": "Mahdi Javanmardi"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Javanmardi"
                },
                "author": "Mahdi Javanmardi",
                "arxiv_comment": "project page: https://soroush-mim.github.io/projects/evict3r/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v3",
                "updated": "2025-10-02T18:20:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    18,
                    20,
                    18,
                    3,
                    275,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02312v1",
                "updated": "2025-10-02T17:59:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    51,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:59:51Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    51,
                    3,
                    275,
                    0
                ],
                "title": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation"
                },
                "summary": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference."
                },
                "authors": [
                    {
                        "name": "Anna Kuzina"
                    },
                    {
                        "name": "Maciej Pioro"
                    },
                    {
                        "name": "Paul N. Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Preprint. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03346v1",
                "updated": "2025-10-02T16:01:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    1,
                    54,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:01:54Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    1,
                    54,
                    3,
                    275,
                    0
                ],
                "title": "KVComm: Enabling Efficient LLM Communication through Selective KV\n  Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComm: Enabling Efficient LLM Communication through Selective KV\n  Sharing"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in multi-agent\nsystems, where effective inter-model communication is crucial. Existing\ncommunication protocols either rely on natural language, incurring high\ninference costs and information loss, or on hidden states, which suffer from\ninformation concentration bias and inefficiency. To address these limitations,\nwe propose KVComm, a novel communication framework that enables efficient\ncommunication between LLMs through selective sharing of KV pairs. KVComm\nleverages the rich information encoded in the KV pairs while avoiding the\npitfalls of hidden states. We introduce a KV layer-wise selection strategy\nbased on attention importance scores with a Gaussian prior to identify the most\ninformative KV pairs for communication. Extensive experiments across diverse\ntasks and model pairs demonstrate that KVComm achieves comparable performance\nto the upper-bound method, which directly merges inputs to one model without\nany communication, while transmitting as few as 30\\% of layers' KV pairs. Our\nstudy highlights the potential of KV pairs as an effective medium for inter-LLM\ncommunication, paving the way for scalable and efficient multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in multi-agent\nsystems, where effective inter-model communication is crucial. Existing\ncommunication protocols either rely on natural language, incurring high\ninference costs and information loss, or on hidden states, which suffer from\ninformation concentration bias and inefficiency. To address these limitations,\nwe propose KVComm, a novel communication framework that enables efficient\ncommunication between LLMs through selective sharing of KV pairs. KVComm\nleverages the rich information encoded in the KV pairs while avoiding the\npitfalls of hidden states. We introduce a KV layer-wise selection strategy\nbased on attention importance scores with a Gaussian prior to identify the most\ninformative KV pairs for communication. Extensive experiments across diverse\ntasks and model pairs demonstrate that KVComm achieves comparable performance\nto the upper-bound method, which directly merges inputs to one model without\nany communication, while transmitting as few as 30\\% of layers' KV pairs. Our\nstudy highlights the potential of KV pairs as an effective medium for inter-LLM\ncommunication, paving the way for scalable and efficient multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Xiangyu Shi"
                    },
                    {
                        "name": "Marco Chiesa"
                    },
                    {
                        "name": "Gerald Q. Maguire Jr."
                    },
                    {
                        "name": "Dejan Kostic"
                    }
                ],
                "author_detail": {
                    "name": "Dejan Kostic"
                },
                "author": "Dejan Kostic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17356v2",
                "updated": "2025-10-02T14:42:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    42,
                    41,
                    3,
                    275,
                    0
                ],
                "published": "2025-08-24T13:30:00Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "title": "DiCache: Let Diffusion Model Determine Its Own Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiCache: Let Diffusion Model Determine Its Own Cache"
                },
                "summary": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine caching timings and adopting handcrafted rules for\nmulti-step cache utilization. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail to cope\nwith diverse samples. In this paper, a strong sample-specific correlation is\nrevealed between the variation patterns of the shallow-layer feature\ndifferences in the diffusion model and those of deep-layer features. Moreover,\nwe have observed that the features from different model layers form similar\ntrajectories. Based on these observations, we present DiCache, a novel\ntraining-free adaptive caching strategy for accelerating diffusion models at\nruntime, answering both when and how to cache within a unified framework.\nSpecifically, DiCache is composed of two principal components: (1) Online Probe\nProfiling Scheme leverages a shallow-layer online probe to obtain an on-the-fly\nindicator for the caching error in real time, enabling the model to dynamically\ncustomize the caching schedule for each sample. (2) Dynamic Cache Trajectory\nAlignment adaptively approximates the deep-layer feature output from multi-step\nhistorical caches based on the shallow-layer feature trajectory, facilitating\nhigher visual quality. Extensive experiments validate DiCache's capability in\nachieving higher efficiency and improved fidelity over state-of-the-art\napproaches on various leading diffusion models including WAN 2.1, HunyuanVideo\nand Flux.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine caching timings and adopting handcrafted rules for\nmulti-step cache utilization. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail to cope\nwith diverse samples. In this paper, a strong sample-specific correlation is\nrevealed between the variation patterns of the shallow-layer feature\ndifferences in the diffusion model and those of deep-layer features. Moreover,\nwe have observed that the features from different model layers form similar\ntrajectories. Based on these observations, we present DiCache, a novel\ntraining-free adaptive caching strategy for accelerating diffusion models at\nruntime, answering both when and how to cache within a unified framework.\nSpecifically, DiCache is composed of two principal components: (1) Online Probe\nProfiling Scheme leverages a shallow-layer online probe to obtain an on-the-fly\nindicator for the caching error in real time, enabling the model to dynamically\ncustomize the caching schedule for each sample. (2) Dynamic Cache Trajectory\nAlignment adaptively approximates the deep-layer feature output from multi-step\nhistorical caches based on the shallow-layer feature trajectory, facilitating\nhigher visual quality. Extensive experiments validate DiCache's capability in\nachieving higher efficiency and improved fidelity over state-of-the-art\napproaches on various leading diffusion models including WAN 2.1, HunyuanVideo\nand Flux."
                },
                "authors": [
                    {
                        "name": "Jiazi Bu"
                    },
                    {
                        "name": "Pengyang Ling"
                    },
                    {
                        "name": "Yujie Zhou"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "arxiv_comment": "Project Page: https://bujiazi.github.io/dicache.github.io/ Code:\n  https://github.com/Bujiazi/DiCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v3",
                "updated": "2025-10-02T14:09:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    9,
                    3,
                    3,
                    275,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization is widely adopted to accelerate inference and reduce memory\nconsumption in large language models (LLMs). While activation-weight joint\nquantization enables efficient low-precision decoding, it suffers from\nsubstantial performance degradation on multi-step reasoning tasks. We propose\nQSpec, a novel quantization paradigm that decouples efficiency from quality by\nintegrating two complementary schemes via speculative decoding: low-precision\njoint quantization for fast drafting and high-precision weight-only\nquantization for accurate verification. QSpec reuses both weights and KV cache\nacross stages, enabling near-zero-cost switching without retraining or\nauxiliary models. Compared to high-precision baselines, QSpec achieves up to\n1.64x speedup without quality degradation, and outperforms state-of-the-art\nspeculative decoding methods by up to 1.55x in batched settings. Furthermore,\nQSpec supports plug-and-play deployment and generalizes well across model\nscales, quantization methods, and workloads. These properties make QSpec a\npractical and scalable solution for high-fidelity quantized LLM serving under\nmemory-constrained scenarios. Our code is available at\nhttps://github.com/hku-netexplo-lab/QSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is widely adopted to accelerate inference and reduce memory\nconsumption in large language models (LLMs). While activation-weight joint\nquantization enables efficient low-precision decoding, it suffers from\nsubstantial performance degradation on multi-step reasoning tasks. We propose\nQSpec, a novel quantization paradigm that decouples efficiency from quality by\nintegrating two complementary schemes via speculative decoding: low-precision\njoint quantization for fast drafting and high-precision weight-only\nquantization for accurate verification. QSpec reuses both weights and KV cache\nacross stages, enabling near-zero-cost switching without retraining or\nauxiliary models. Compared to high-precision baselines, QSpec achieves up to\n1.64x speedup without quality degradation, and outperforms state-of-the-art\nspeculative decoding methods by up to 1.55x in batched settings. Furthermore,\nQSpec supports plug-and-play deployment and generalizes well across model\nscales, quantization methods, and workloads. These properties make QSpec a\npractical and scalable solution for high-fidelity quantized LLM serving under\nmemory-constrained scenarios. Our code is available at\nhttps://github.com/hku-netexplo-lab/QSpec."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "arxiv_journal_ref": "Proceedings of the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01884v1",
                "updated": "2025-10-02T10:49:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    49,
                    54,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T10:49:54Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    49,
                    54,
                    3,
                    275,
                    0
                ],
                "title": "Study of the $^{20}$Ne($p,γ$)$^{21}$Na reaction at LUNA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of the $^{20}$Ne($p,γ$)$^{21}$Na reaction at LUNA"
                },
                "summary": "The NeNa-MgAl cycles are involved in the synthesis of Ne, Na, Mg, and Al\nisotopes. The $^{20}$Ne($p,\\gamma$)$^{21}$Na (Q = 2431.68 keV) reaction is the\nfirst and slowest reaction of the NeNa cycle and it controls the speed at which\nthe entire cycle proceeds. At the state of the art, the uncertainty on the\n20Ne(p,{\\gamma})21Na reaction rate affects the production of the elements in\nthe NeNa cycle. In particular, in the temperature range from 0.1 GK to 1 GK,\nthe rate is dominated by the 366 keV resonance corresponding to the excited\nstate of EX = 2797.5 keV and by the direct capture component. The present study\nfocus on the study of the 366 keV resonance and the direct capture below 400\nkeV. At LUNA (Laboratory for Underground Nuclear Astrophysics) the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction has been measured using the intense\nproton beam delivered by the LUNA 400 kV accelerator and a windowless\ndifferential-pumping gas target. The products of the reaction are detected with\ntwo high-purity germanium detectors. The experimental details and preliminary\nresults on the 366 keV resonance and on the direct capture component at very\nlow energies will be shown, together with their possible impact on the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NeNa-MgAl cycles are involved in the synthesis of Ne, Na, Mg, and Al\nisotopes. The $^{20}$Ne($p,\\gamma$)$^{21}$Na (Q = 2431.68 keV) reaction is the\nfirst and slowest reaction of the NeNa cycle and it controls the speed at which\nthe entire cycle proceeds. At the state of the art, the uncertainty on the\n20Ne(p,{\\gamma})21Na reaction rate affects the production of the elements in\nthe NeNa cycle. In particular, in the temperature range from 0.1 GK to 1 GK,\nthe rate is dominated by the 366 keV resonance corresponding to the excited\nstate of EX = 2797.5 keV and by the direct capture component. The present study\nfocus on the study of the 366 keV resonance and the direct capture below 400\nkeV. At LUNA (Laboratory for Underground Nuclear Astrophysics) the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction has been measured using the intense\nproton beam delivered by the LUNA 400 kV accelerator and a windowless\ndifferential-pumping gas target. The products of the reaction are detected with\ntwo high-purity germanium detectors. The experimental details and preliminary\nresults on the 366 keV resonance and on the direct capture component at very\nlow energies will be shown, together with their possible impact on the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction rate."
                },
                "authors": [
                    {
                        "name": "A. Caciolli"
                    }
                ],
                "author_detail": {
                    "name": "A. Caciolli"
                },
                "arxiv_affiliation": "on behalf of the LUNA collaboration",
                "author": "A. Caciolli",
                "arxiv_doi": "10.1051/epjconf/202429207005",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/epjconf/202429207005",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.01884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "EPJ Web Conf., 292 (2024) 07005",
                "arxiv_primary_category": {
                    "term": "nucl-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20211v2",
                "updated": "2025-10-02T04:11:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    4,
                    11,
                    7,
                    3,
                    275,
                    0
                ],
                "published": "2025-05-26T16:52:40Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    52,
                    40,
                    0,
                    146,
                    0
                ],
                "title": "PiCa: Parameter-Efficient Fine-Tuning with Column Space Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiCa: Parameter-Efficient Fine-Tuning with Column Space Projection"
                },
                "summary": "Fine-tuning large foundation models is essential for building expert models\ntailored to specialized tasks and domains, but fully updating billions of\nparameters is computationally prohibitive. Reducing the number of trainable\nparameters using parameter-efficient fine-tuning is therefore crucial not only\nto reduce training costs but also to mitigate storage, caching, and serving\noverheads during deployment. Prior works, such as Singular Vectors-guided\nFine-Tuning, have shown that exploiting the geometry of pre-trained weights can\nsignificantly improve parameter-efficiency, but they lack a solid theoretical\nfoundation. In this paper, we introduce Parameter-efficient Fine-tuning with\nColumn Space Projection (PiCa), a novel theoretically grounded PEFT method. We\nprove that projecting gradients onto the principal column space of pre-trained\nweights provides an effective inductive bias for adaptation and further enhance\nparameter efficiency through a novel weight-sharing strategy. Across diverse\nNLP and vision tasks, PiCa consistently outperforms state-of-the-art baselines\nunder comparable or smaller parameter budgets, demonstrating both theoretical\nrigor and practical effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large foundation models is essential for building expert models\ntailored to specialized tasks and domains, but fully updating billions of\nparameters is computationally prohibitive. Reducing the number of trainable\nparameters using parameter-efficient fine-tuning is therefore crucial not only\nto reduce training costs but also to mitigate storage, caching, and serving\noverheads during deployment. Prior works, such as Singular Vectors-guided\nFine-Tuning, have shown that exploiting the geometry of pre-trained weights can\nsignificantly improve parameter-efficiency, but they lack a solid theoretical\nfoundation. In this paper, we introduce Parameter-efficient Fine-tuning with\nColumn Space Projection (PiCa), a novel theoretically grounded PEFT method. We\nprove that projecting gradients onto the principal column space of pre-trained\nweights provides an effective inductive bias for adaptation and further enhance\nparameter efficiency through a novel weight-sharing strategy. Across diverse\nNLP and vision tasks, PiCa consistently outperforms state-of-the-art baselines\nunder comparable or smaller parameter budgets, demonstrating both theoretical\nrigor and practical effectiveness."
                },
                "authors": [
                    {
                        "name": "Junseo Hwang"
                    },
                    {
                        "name": "Wonguk Cho"
                    },
                    {
                        "name": "Taesup Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taesup Kim"
                },
                "author": "Taesup Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07447v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07447v4",
                "updated": "2025-10-01T20:30:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    20,
                    30,
                    18,
                    2,
                    274,
                    0
                ],
                "published": "2024-11-12T00:10:34Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    0,
                    10,
                    34,
                    1,
                    317,
                    0
                ],
                "title": "Faster LLM Inference using DBMS-Inspired Preemption and Cache\n  Replacement Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster LLM Inference using DBMS-Inspired Preemption and Cache\n  Replacement Policies"
                },
                "summary": "LLMs are increasingly used world-wide from daily tasks to agentic systems and\ndata analytics, requiring significant GPU resources. LLM inference systems,\nhowever, are slow compared to database systems, and inference performance and\nmechanism have been often regarded as a black box, limiting the expansion of\nthe use of LLMs inside databases and other performance-critical applications.\nThis paper first analyzes the LLM inference performance and focuses on a data\nmanagement issue inside LLM inference. We find that inference systems lack an\nadequate resource cost model and optimization strategy to schedule requests\nwith their intermediate results in a cache reside in GPU memory when executing\nmultiple concurrent inference requests. We adapt classic database techniques by\nbuilding cost models for concurrent inference requests and a new cache\nreplacement policy tailored for LLM inference, which can substantially save GPU\ncosts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly used world-wide from daily tasks to agentic systems and\ndata analytics, requiring significant GPU resources. LLM inference systems,\nhowever, are slow compared to database systems, and inference performance and\nmechanism have been often regarded as a black box, limiting the expansion of\nthe use of LLMs inside databases and other performance-critical applications.\nThis paper first analyzes the LLM inference performance and focuses on a data\nmanagement issue inside LLM inference. We find that inference systems lack an\nadequate resource cost model and optimization strategy to schedule requests\nwith their intermediate results in a cache reside in GPU memory when executing\nmultiple concurrent inference requests. We adapt classic database techniques by\nbuilding cost models for concurrent inference requests and a new cache\nreplacement policy tailored for LLM inference, which can substantially save GPU\ncosts."
                },
                "authors": [
                    {
                        "name": "Kyoungmin Kim"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Kijae Hong"
                    },
                    {
                        "name": "Anastasia Ailamaki"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Ailamaki"
                },
                "author": "Anastasia Ailamaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07447v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07447v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v2",
                "updated": "2025-10-01T19:06:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    19,
                    6,
                    10,
                    2,
                    274,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Linxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Boqian Wang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09350v2",
                "updated": "2025-10-01T18:55:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    18,
                    55,
                    20,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-11T03:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation"
                },
                "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2"
                },
                "authors": [
                    {
                        "name": "Shanchuan Lin"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01336v1",
                "updated": "2025-10-01T18:04:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    18,
                    4,
                    14,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T18:04:14Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    18,
                    4,
                    14,
                    2,
                    274,
                    0
                ],
                "title": "HiSpec: Hierarchical Speculative Decoding for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiSpec: Hierarchical Speculative Decoding for LLMs"
                },
                "summary": "Speculative decoding accelerates LLM inference by using a smaller draft model\nto speculate tokens that a larger target model verifies. Verification is often\nthe bottleneck (e.g. verification is $4\\times$ slower than token generation\nwhen a 3B model speculates for a 70B target model), but most prior works focus\nonly on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces\nverification time by discarding inaccurate draft tokens early, but existing\nmethods incur substantial training overheads in incorporating the intermediate\nverifier, increase the memory footprint to orchestrate the intermediate\nverification step, and compromise accuracy by relying on approximate\nheuristics.\n  We propose $\\underline{\\textit{Hi}}\\textit{erarchical\n}\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for\nhigh-throughput speculative decoding that exploits $\\textit{early-exit (EE)\nmodels}$ for low-overhead intermediate verification. EE models allow tokens to\nexit early by skipping layer traversal and are explicitly trained so that\nhidden states at selected layers can be interpreted, making them uniquely\nsuited for intermediate verification without drastically increasing compute and\nmemory overheads. To improve resource-efficiency even further, we design a\nmethodology that enables HiSpec to re-use key-value caches and hidden states\nbetween the draft, intermediate verifier, and target models. To maintain\naccuracy, HiSpec periodically validates the draft tokens accepted by the\nintermediate verifier against the target model. Our evaluations using various\nrepresentative benchmarks and models show that HiSpec improves throughput by\n1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline\nsingle-layer speculation without compromising accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding accelerates LLM inference by using a smaller draft model\nto speculate tokens that a larger target model verifies. Verification is often\nthe bottleneck (e.g. verification is $4\\times$ slower than token generation\nwhen a 3B model speculates for a 70B target model), but most prior works focus\nonly on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces\nverification time by discarding inaccurate draft tokens early, but existing\nmethods incur substantial training overheads in incorporating the intermediate\nverifier, increase the memory footprint to orchestrate the intermediate\nverification step, and compromise accuracy by relying on approximate\nheuristics.\n  We propose $\\underline{\\textit{Hi}}\\textit{erarchical\n}\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for\nhigh-throughput speculative decoding that exploits $\\textit{early-exit (EE)\nmodels}$ for low-overhead intermediate verification. EE models allow tokens to\nexit early by skipping layer traversal and are explicitly trained so that\nhidden states at selected layers can be interpreted, making them uniquely\nsuited for intermediate verification without drastically increasing compute and\nmemory overheads. To improve resource-efficiency even further, we design a\nmethodology that enables HiSpec to re-use key-value caches and hidden states\nbetween the draft, intermediate verifier, and target models. To maintain\naccuracy, HiSpec periodically validates the draft tokens accepted by the\nintermediate verifier against the target model. Our evaluations using various\nrepresentative benchmarks and models show that HiSpec improves throughput by\n1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline\nsingle-layer speculation without compromising accuracy."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Sujay Sanghavi"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00948v1",
                "updated": "2025-10-01T14:21:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    21,
                    45,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T14:21:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    21,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "InfVSR: Breaking Length Limits of Generic Video Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfVSR: Breaking Length Limits of Generic Video Super-Resolution"
                },
                "summary": "Real-world videos often extend over thousands of frames. Existing video\nsuper-resolution (VSR) approaches, however, face two persistent challenges when\nprocessing long sequences: (1) inefficiency due to the heavy cost of multi-step\ndenoising for full-length sequences; and (2) poor scalability hindered by\ntemporal decomposition that causes artifacts and discontinuities. To break\nthese limits, we propose InfVSR, which novelly reformulates VSR as an\nautoregressive-one-step-diffusion paradigm. This enables streaming inference\nwhile fully leveraging pre-trained video diffusion priors. First, we adapt the\npre-trained DiT into a causal structure, maintaining both local and global\ncoherence via rolling KV-cache and joint visual guidance. Second, we distill\nthe diffusion process into a single step efficiently, with patch-wise pixel\nsupervision and cross-chunk distribution matching. Together, these designs\nenable efficient and scalable VSR for unbounded-length videos. To fill the gap\nin long-form video evaluation, we build a new benchmark tailored for extended\nsequences and further introduce semantic-level metrics to comprehensively\nassess temporal consistency. Our method pushes the frontier of long-form VSR,\nachieves state-of-the-art quality with enhanced semantic consistency, and\ndelivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will\nbe available at https://github.com/Kai-Liu001/InfVSR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world videos often extend over thousands of frames. Existing video\nsuper-resolution (VSR) approaches, however, face two persistent challenges when\nprocessing long sequences: (1) inefficiency due to the heavy cost of multi-step\ndenoising for full-length sequences; and (2) poor scalability hindered by\ntemporal decomposition that causes artifacts and discontinuities. To break\nthese limits, we propose InfVSR, which novelly reformulates VSR as an\nautoregressive-one-step-diffusion paradigm. This enables streaming inference\nwhile fully leveraging pre-trained video diffusion priors. First, we adapt the\npre-trained DiT into a causal structure, maintaining both local and global\ncoherence via rolling KV-cache and joint visual guidance. Second, we distill\nthe diffusion process into a single step efficiently, with patch-wise pixel\nsupervision and cross-chunk distribution matching. Together, these designs\nenable efficient and scalable VSR for unbounded-length videos. To fill the gap\nin long-form video evaluation, we build a new benchmark tailored for extended\nsequences and further introduce semantic-level metrics to comprehensively\nassess temporal consistency. Our method pushes the frontier of long-form VSR,\nachieves state-of-the-art quality with enhanced semantic consistency, and\ndelivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will\nbe available at https://github.com/Kai-Liu001/InfVSR."
                },
                "authors": [
                    {
                        "name": "Ziqing Zhang"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Xi Li"
                    },
                    {
                        "name": "Yucong Chen"
                    },
                    {
                        "name": "Bingnan Duan"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Zhang"
                },
                "author": "Yulun Zhang",
                "arxiv_comment": "Code will be available at https://github.com/Kai-Liu001/InfVSR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26432v2",
                "updated": "2025-10-01T11:26:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    11,
                    26,
                    36,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T15:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    53,
                    56,
                    1,
                    273,
                    0
                ],
                "title": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size"
                },
                "summary": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs."
                },
                "authors": [
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Yuto Karashima"
                    },
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00636v1",
                "updated": "2025-10-01T08:12:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    8,
                    12,
                    14,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T08:12:14Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    8,
                    12,
                    14,
                    2,
                    274,
                    0
                ],
                "title": "Expected Attention: KV Cache Compression by Estimating Attention from\n  Future Queries Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expected Attention: KV Cache Compression by Estimating Attention from\n  Future Queries Distribution"
                },
                "summary": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck\nfor efficient large language model inference. While attention-score-based KV\ncache pruning shows promise, it faces critical practical limitations: attention\nscores from future tokens are unavailable during compression, and modern\nimplementations like Flash Attention do not materialize the full attention\nmatrix, making past scores inaccessible. To overcome these challenges, we\nintroduce $\\textbf{Expected Attention, a training-free compression method}$\nthat estimates KV pairs importance by predicting how future queries will attend\nto them. Our approach leverages the distributional properties of LLM\nactivations to compute expected attention scores in closed form for each KV\npair. These scores enable principled ranking and pruning of KV pairs with\nminimal impact on the residual stream, achieving effective compression without\nperformance degradation. Importantly, our method operates seamlessly across\nboth prefilling and decoding phases, consistently outperforming\nstate-of-the-art baselines in both scenarios. Finally, $\\textbf{we release\nKVPress, a comprehensive library to enable researchers to implement and\nbenchmark KV cache compression methods, already including more than 20\ntechniques}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck\nfor efficient large language model inference. While attention-score-based KV\ncache pruning shows promise, it faces critical practical limitations: attention\nscores from future tokens are unavailable during compression, and modern\nimplementations like Flash Attention do not materialize the full attention\nmatrix, making past scores inaccessible. To overcome these challenges, we\nintroduce $\\textbf{Expected Attention, a training-free compression method}$\nthat estimates KV pairs importance by predicting how future queries will attend\nto them. Our approach leverages the distributional properties of LLM\nactivations to compute expected attention scores in closed form for each KV\npair. These scores enable principled ranking and pruning of KV pairs with\nminimal impact on the residual stream, achieving effective compression without\nperformance degradation. Importantly, our method operates seamlessly across\nboth prefilling and decoding phases, consistently outperforming\nstate-of-the-art baselines in both scenarios. Finally, $\\textbf{we release\nKVPress, a comprehensive library to enable researchers to implement and\nbenchmark KV cache compression methods, already including more than 20\ntechniques}$."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Maximilian Jeblick"
                    },
                    {
                        "name": "Simon Jégou"
                    }
                ],
                "author_detail": {
                    "name": "Simon Jégou"
                },
                "author": "Simon Jégou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00566v1",
                "updated": "2025-10-01T06:38:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T06:38:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "Panorama: Fast-Track Nearest Neighbors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panorama: Fast-Track Nearest Neighbors"
                },
                "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss."
                },
                "authors": [
                    {
                        "name": "Vansh Ramani"
                    },
                    {
                        "name": "Alexis Schlomer"
                    },
                    {
                        "name": "Akash Nayar"
                    },
                    {
                        "name": "Panagiotis Karras"
                    },
                    {
                        "name": "Sayan Ranu"
                    },
                    {
                        "name": "Jignesh M. Patel"
                    }
                ],
                "author_detail": {
                    "name": "Jignesh M. Patel"
                },
                "author": "Jignesh M. Patel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00536v1",
                "updated": "2025-10-01T05:37:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    37,
                    54,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T05:37:54Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    37,
                    54,
                    2,
                    274,
                    0
                ],
                "title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness"
                },
                "summary": "Graphical user interface (GUI) agents built on vision-language models have\nemerged as a promising approach to automate human-computer workflows. However,\nthey also face the inefficiency challenge as they process long sequences of\nhigh-resolution screenshots and solving long-horizon tasks, making inference\nslow, costly and memory-bound. While key-value (KV) caching can mitigate this,\nstoring the full cache is prohibitive for image-heavy contexts. Existing\ncache-compression methods are sub-optimal as they do not account for the\nspatial and temporal redundancy of GUIs. In this work, we first analyze\nattention patterns in GUI agent workloads and find that, unlike in natural\nimages, attention sparsity is uniformly high across all transformer layers.\nThis insight motivates a simple uniform budget allocation strategy, which we\nshow empirically outperforms more complex layer-varying schemes. Building on\nthis, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI\nagents that requires no retraining. GUI-KV combines two novel techniques: (i)\nspatial saliency guidance, which augments attention scores with the L2 norm of\nhidden states to better preserve semantically important visual tokens, and (ii)\ntemporal redundancy scoring, which projects previous frames' keys onto the\ncurrent frame's key subspace to preferentially prune redundant history. Across\nstandard GUI agent benchmarks and models, GUI-KV outperforms competitive KV\ncompression baselines, closely matching full-cache accuracy at modest budgets.\nNotably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV\nreduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the\nfull-cache baseline. These results demonstrate that exploiting GUI-specific\nredundancies enables efficient and reliable agent performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical user interface (GUI) agents built on vision-language models have\nemerged as a promising approach to automate human-computer workflows. However,\nthey also face the inefficiency challenge as they process long sequences of\nhigh-resolution screenshots and solving long-horizon tasks, making inference\nslow, costly and memory-bound. While key-value (KV) caching can mitigate this,\nstoring the full cache is prohibitive for image-heavy contexts. Existing\ncache-compression methods are sub-optimal as they do not account for the\nspatial and temporal redundancy of GUIs. In this work, we first analyze\nattention patterns in GUI agent workloads and find that, unlike in natural\nimages, attention sparsity is uniformly high across all transformer layers.\nThis insight motivates a simple uniform budget allocation strategy, which we\nshow empirically outperforms more complex layer-varying schemes. Building on\nthis, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI\nagents that requires no retraining. GUI-KV combines two novel techniques: (i)\nspatial saliency guidance, which augments attention scores with the L2 norm of\nhidden states to better preserve semantically important visual tokens, and (ii)\ntemporal redundancy scoring, which projects previous frames' keys onto the\ncurrent frame's key subspace to preferentially prune redundant history. Across\nstandard GUI agent benchmarks and models, GUI-KV outperforms competitive KV\ncompression baselines, closely matching full-cache accuracy at modest budgets.\nNotably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV\nreduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the\nfull-cache baseline. These results demonstrate that exploiting GUI-specific\nredundancies enables efficient and reliable agent performance."
                },
                "authors": [
                    {
                        "name": "Kung-Hsiang Huang"
                    },
                    {
                        "name": "Haoyi Qiu"
                    },
                    {
                        "name": "Yutong Dai"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Sheng Wu"
                },
                "author": "Chien-Sheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25454v2",
                "updated": "2025-10-01T05:09:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    9,
                    42,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-29T20:00:29Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    20,
                    0,
                    29,
                    0,
                    272,
                    0
                ],
                "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search"
                },
                "summary": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation."
                },
                "authors": [
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Weihao Xuan"
                    },
                    {
                        "name": "Heli Qi"
                    },
                    {
                        "name": "Ximing Lu"
                    },
                    {
                        "name": "Aaron Tu"
                    },
                    {
                        "name": "Li Erran Li"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01290v1",
                "updated": "2025-10-01T04:09:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    4,
                    9,
                    2,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T04:09:02Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    4,
                    9,
                    2,
                    2,
                    274,
                    0
                ],
                "title": "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning\n  Models"
                },
                "summary": "The long-output context generation of large reasoning models enables extended\nchain of thought (CoT) but also drives rapid growth of the key-value (KV)\ncache, quickly overwhelming GPU memory. To address this challenge, we propose\nThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on\nthe observation that attention sparsity reveals distinct thought types with\nvarying importance within the CoT. It applies a hybrid quantization-eviction\nstrategy, assigning token precision by thought importance and progressively\nevicting tokens from less critical thoughts as reasoning trajectories evolve.\nFurthermore, to implement ThinKV, we design a kernel that extends\nPagedAttention to enable efficient reuse of evicted tokens' memory slots,\neliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,\nGPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show\nthat ThinKV achieves near-lossless accuracy with less than 5% of the original\nKV cache, while improving performance with up to 5.8x higher inference\nthroughput over state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The long-output context generation of large reasoning models enables extended\nchain of thought (CoT) but also drives rapid growth of the key-value (KV)\ncache, quickly overwhelming GPU memory. To address this challenge, we propose\nThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on\nthe observation that attention sparsity reveals distinct thought types with\nvarying importance within the CoT. It applies a hybrid quantization-eviction\nstrategy, assigning token precision by thought importance and progressively\nevicting tokens from less critical thoughts as reasoning trajectories evolve.\nFurthermore, to implement ThinKV, we design a kernel that extends\nPagedAttention to enable efficient reuse of evicted tokens' memory slots,\neliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,\nGPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show\nthat ThinKV achieves near-lossless accuracy with less than 5% of the original\nKV cache, while improving performance with up to 5.8x higher inference\nthroughput over state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Akshat Ramachandran"
                    },
                    {
                        "name": "Marina Neseem"
                    },
                    {
                        "name": "Charbel Sakr"
                    },
                    {
                        "name": "Rangharajan Venkatesan"
                    },
                    {
                        "name": "Brucek Khailany"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01289v1",
                "updated": "2025-10-01T02:56:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    56,
                    59,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T02:56:59Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    56,
                    59,
                    2,
                    274,
                    0
                ],
                "title": "Detailed Derivation of the Scalar Explicit Expressions Governing the\n  Electric Field, Current Density, and Volumetric Power Density in the Four\n  Types of Linear Divergent MHD Channels Under a Unidirectional Applied\n  Magnetic Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detailed Derivation of the Scalar Explicit Expressions Governing the\n  Electric Field, Current Density, and Volumetric Power Density in the Four\n  Types of Linear Divergent MHD Channels Under a Unidirectional Applied\n  Magnetic Field"
                },
                "summary": "The current study belongs to the field of applied mathematics in plasma\nphysics and electric power, where mathematical analysis of the algebraic\nequations governing the electric field vector, and the electric-current density\nfield vector within a Magnetohydrodynamic (MHD) linear two-dimensional\ndivergent supersonic channel is utilized to derive analytical expressions for\nthese important fields, as well as closed-form equations for the volumetric\npower density (output electric power per unit volume of the plasma channel).\nThe expressions presented here describe analytically the operation of the MHD\nchannel as an electric power source within an Open-Cycle Magnetohydrodynamic\n(OCMHD) generator. The four common types of the MHD linear channels are covered\nhere: namely, (1) continuous-electrode Faraday channel, (2) linear Hall\nchannel, (3) segmented-electrode Faraday channel, and (4) diagonal-electrode\nchannel. The mathematical results, their detailed derivation, and the companion\ngraphical illustrations aid in making a proper decision regarding which channel\ntype is the most suitable for a given application.Under typical operational\nconditions of 5 S/m plasma electric conductivity, 5 T magnetic field, and 2,000\nm/s plasma speed, as well as an optimized load factor of 0.5, we estimate the\nfollowing numerical values (unsigned magnitudes) for the continuous-electrode\nFaraday channel (with a Hall parameter of 1): useful electric field (across the\nexternal electric load): 5 kV/m, useful electric current-density (between the\nterminal electrodes within the channel): 12.5 kA/m2 , volumetric power density\n(dissipated by the load per unit volume of plasma): 62.5 MW/m3 , and electric\nefficiency (for the electric field or voltage): 50%. For the Halllinear channel\n(with a Hall parameter of 5), these quantitative performance values become25\nkV/m, 4.808 kA/m2, 120.19 MW/m3, and 46.30%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current study belongs to the field of applied mathematics in plasma\nphysics and electric power, where mathematical analysis of the algebraic\nequations governing the electric field vector, and the electric-current density\nfield vector within a Magnetohydrodynamic (MHD) linear two-dimensional\ndivergent supersonic channel is utilized to derive analytical expressions for\nthese important fields, as well as closed-form equations for the volumetric\npower density (output electric power per unit volume of the plasma channel).\nThe expressions presented here describe analytically the operation of the MHD\nchannel as an electric power source within an Open-Cycle Magnetohydrodynamic\n(OCMHD) generator. The four common types of the MHD linear channels are covered\nhere: namely, (1) continuous-electrode Faraday channel, (2) linear Hall\nchannel, (3) segmented-electrode Faraday channel, and (4) diagonal-electrode\nchannel. The mathematical results, their detailed derivation, and the companion\ngraphical illustrations aid in making a proper decision regarding which channel\ntype is the most suitable for a given application.Under typical operational\nconditions of 5 S/m plasma electric conductivity, 5 T magnetic field, and 2,000\nm/s plasma speed, as well as an optimized load factor of 0.5, we estimate the\nfollowing numerical values (unsigned magnitudes) for the continuous-electrode\nFaraday channel (with a Hall parameter of 1): useful electric field (across the\nexternal electric load): 5 kV/m, useful electric current-density (between the\nterminal electrodes within the channel): 12.5 kA/m2 , volumetric power density\n(dissipated by the load per unit volume of plasma): 62.5 MW/m3 , and electric\nefficiency (for the electric field or voltage): 50%. For the Halllinear channel\n(with a Hall parameter of 5), these quantitative performance values become25\nkV/m, 4.808 kA/m2, 120.19 MW/m3, and 46.30%."
                },
                "authors": [
                    {
                        "name": "Osama A. Marzouk"
                    }
                ],
                "author_detail": {
                    "name": "Osama A. Marzouk"
                },
                "author": "Osama A. Marzouk",
                "arxiv_doi": "10.37256/cm.6420256918",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.37256/cm.6420256918",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.01289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "41 pages, 8 figures, 4 tables, published journal article,\n  peer-reviewed, open access",
                "arxiv_journal_ref": "Contemporary Mathematics. volume 6, issue 4, pages 4060-4100,\n  https://ojs.wiserpub.com/index.php/CM/article/view/6918 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00A79, 03H10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05131v1",
                "updated": "2025-10-01T01:28:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    1,
                    28,
                    59,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T01:28:59Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    1,
                    28,
                    59,
                    2,
                    274,
                    0
                ],
                "title": "Rationale-Augmented Retrieval with Constrained LLM Re-Ranking for Task\n  Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rationale-Augmented Retrieval with Constrained LLM Re-Ranking for Task\n  Discovery"
                },
                "summary": "Head Start programs utilizing GoEngage face significant challenges when new\nor rotating staff attempt to locate appropriate Tasks (modules) on the platform\nhomepage. These difficulties arise from domain-specific jargon (e.g., IFPA,\nDRDP), system-specific nomenclature (e.g., Application Pool), and the inherent\nlimitations of lexical search in handling typos and varied word ordering. We\npropose a pragmatic hybrid semantic search system that synergistically combines\nlightweight typo-tolerant lexical retrieval, embedding-based vector similarity,\nand constrained large language model (LLM) re-ranking. Our approach leverages\nthe organization's existing Task Repository and Knowledge Base infrastructure\nwhile ensuring trustworthiness through low false-positive rates, evolvability\nto accommodate terminological changes, and economic efficiency via intelligent\ncaching, shortlist generation, and graceful degradation mechanisms. We provide\na comprehensive framework detailing required resources, a phased implementation\nstrategy with concrete milestones, an offline evaluation protocol utilizing\ncurated test cases (Hit@K, Precision@K, Recall@K, MRR), and an online\nmeasurement methodology incorporating query success metrics, zero-result rates,\nand dwell-time proxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head Start programs utilizing GoEngage face significant challenges when new\nor rotating staff attempt to locate appropriate Tasks (modules) on the platform\nhomepage. These difficulties arise from domain-specific jargon (e.g., IFPA,\nDRDP), system-specific nomenclature (e.g., Application Pool), and the inherent\nlimitations of lexical search in handling typos and varied word ordering. We\npropose a pragmatic hybrid semantic search system that synergistically combines\nlightweight typo-tolerant lexical retrieval, embedding-based vector similarity,\nand constrained large language model (LLM) re-ranking. Our approach leverages\nthe organization's existing Task Repository and Knowledge Base infrastructure\nwhile ensuring trustworthiness through low false-positive rates, evolvability\nto accommodate terminological changes, and economic efficiency via intelligent\ncaching, shortlist generation, and graceful degradation mechanisms. We provide\na comprehensive framework detailing required resources, a phased implementation\nstrategy with concrete milestones, an offline evaluation protocol utilizing\ncurated test cases (Hit@K, Precision@K, Recall@K, MRR), and an online\nmeasurement methodology incorporating query success metrics, zero-result rates,\nand dwell-time proxies."
                },
                "authors": [
                    {
                        "name": "Bowen Wei"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Wei"
                },
                "author": "Bowen Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02388v1",
                "updated": "2025-09-30T22:19:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    22,
                    19,
                    44,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T22:19:44Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    22,
                    19,
                    44,
                    1,
                    273,
                    0
                ],
                "title": "Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable performance on general\nQuestion Answering (QA), yet they often struggle in domain-specific scenarios\nwhere accurate and up-to-date information is required. Retrieval-Augmented\nGeneration (RAG) addresses this limitation by enriching LLMs with external\nknowledge, but existing systems primarily rely on unstructured documents, while\nlargely overlooking relational databases, which provide precise, timely, and\nefficiently queryable factual information, serving as indispensable\ninfrastructure in domains such as finance, healthcare, and scientific research.\nMotivated by this gap, we conduct a systematic analysis that reveals three\ncentral observations: (i) databases and documents offer complementary strengths\nacross queries, (ii) naively combining both sources introduces noise and cost\nwithout consistent accuracy gains, and (iii) selecting the most suitable source\nfor each query is crucial to balance effectiveness and efficiency. We further\nobserve that query types show consistent regularities in their alignment with\nretrieval paths, suggesting that routing decisions can be effectively guided by\nsystematic rules that capture these patterns. Building on these insights, we\npropose a rule-driven routing framework. A routing agent scores candidate\naugmentation paths based on explicit rules and selects the most suitable one; a\nrule-making expert agent refines the rules over time using QA feedback to\nmaintain adaptability; and a path-level meta-cache reuses past routing\ndecisions for semantically similar queries to reduce latency and cost.\nExperiments on three QA benchmarks demonstrate that our framework consistently\noutperforms static strategies and learned routing baselines, achieving higher\naccuracy while maintaining moderate computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable performance on general\nQuestion Answering (QA), yet they often struggle in domain-specific scenarios\nwhere accurate and up-to-date information is required. Retrieval-Augmented\nGeneration (RAG) addresses this limitation by enriching LLMs with external\nknowledge, but existing systems primarily rely on unstructured documents, while\nlargely overlooking relational databases, which provide precise, timely, and\nefficiently queryable factual information, serving as indispensable\ninfrastructure in domains such as finance, healthcare, and scientific research.\nMotivated by this gap, we conduct a systematic analysis that reveals three\ncentral observations: (i) databases and documents offer complementary strengths\nacross queries, (ii) naively combining both sources introduces noise and cost\nwithout consistent accuracy gains, and (iii) selecting the most suitable source\nfor each query is crucial to balance effectiveness and efficiency. We further\nobserve that query types show consistent regularities in their alignment with\nretrieval paths, suggesting that routing decisions can be effectively guided by\nsystematic rules that capture these patterns. Building on these insights, we\npropose a rule-driven routing framework. A routing agent scores candidate\naugmentation paths based on explicit rules and selects the most suitable one; a\nrule-making expert agent refines the rules over time using QA feedback to\nmaintain adaptability; and a path-level meta-cache reuses past routing\ndecisions for semantically similar queries to reduce latency and cost.\nExperiments on three QA benchmarks demonstrate that our framework consistently\noutperforms static strategies and learned routing baselines, achieving higher\naccuracy while maintaining moderate computational cost."
                },
                "authors": [
                    {
                        "name": "Haoyue Bai"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Shengyu Chen"
                    },
                    {
                        "name": "Zhengzhang Chen"
                    },
                    {
                        "name": "Lu-An Tang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Haifeng Chen"
                    },
                    {
                        "name": "Yanjie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjie Fu"
                },
                "author": "Yanjie Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00294v1",
                "updated": "2025-09-30T21:28:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    21,
                    28,
                    4,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T21:28:04Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    21,
                    28,
                    4,
                    1,
                    273,
                    0
                ],
                "title": "Free Draft-and-Verification: Toward Lossless Parallel Decoding for\n  Diffusion Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free Draft-and-Verification: Toward Lossless Parallel Decoding for\n  Diffusion Large Language Models"
                },
                "summary": "Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of\nlanguage modeling beyond autoregressive next-token prediction. Thanks to their\nbidirectional attention mechanism, DLLMs are more capable of capturing the\nconnection of context, and thus show unique advantages in challenges like the\nfamous \"reversal curse\" or learning under data-constrained scenarios. However,\nthis bidirectional nature also brings an obstacle that DLLMs are not inherently\ncompatible with KV Cache, and consequently, the inference efficiency is not\ncompetitive compared with autoregressive models. Taking advantage of their\ninherent capability of multi-token prediction, existing parallel decoding\nalgorithms can speed up the DLLM inference, but at the cost of non-negligible\nperformance degradation. To overcome this challenge, we introduce Free\nDraft-and-Verification (Freedave), a novel fast sampling algorithm tailored for\nDLLMs that achieves lossless parallel decoding. Specifically, we propose a\npipeline of parallel-decoded candidate generation and verification, which is\nguaranteed to reproduce the same sequence generated by static sampling, without\nintroducing extra model forward calls. By applying Freedave, the throughput of\nDLLMs can be boosted up to $2.8\\times$ without performance degradation on math\nreasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of\nlanguage modeling beyond autoregressive next-token prediction. Thanks to their\nbidirectional attention mechanism, DLLMs are more capable of capturing the\nconnection of context, and thus show unique advantages in challenges like the\nfamous \"reversal curse\" or learning under data-constrained scenarios. However,\nthis bidirectional nature also brings an obstacle that DLLMs are not inherently\ncompatible with KV Cache, and consequently, the inference efficiency is not\ncompetitive compared with autoregressive models. Taking advantage of their\ninherent capability of multi-token prediction, existing parallel decoding\nalgorithms can speed up the DLLM inference, but at the cost of non-negligible\nperformance degradation. To overcome this challenge, we introduce Free\nDraft-and-Verification (Freedave), a novel fast sampling algorithm tailored for\nDLLMs that achieves lossless parallel decoding. Specifically, we propose a\npipeline of parallel-decoded candidate generation and verification, which is\nguaranteed to reproduce the same sequence generated by static sampling, without\nintroducing extra model forward calls. By applying Freedave, the throughput of\nDLLMs can be boosted up to $2.8\\times$ without performance degradation on math\nreasoning tasks."
                },
                "authors": [
                    {
                        "name": "Shutong Wu"
                    },
                    {
                        "name": "Jiawei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Zhang"
                },
                "author": "Jiawei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00231v1",
                "updated": "2025-09-30T19:55:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    55,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T19:55:26Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    55,
                    26,
                    1,
                    273,
                    0
                ],
                "title": "The Pitfalls of KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Pitfalls of KV Cache Compression"
                },
                "summary": "KV cache compression promises increased throughput and efficiency with\nnegligible loss in performance. While the gains in throughput are indisputable\nand recent literature has indeed shown minimal degradation on particular\nbenchmarks, in general the consequences of compression in realistic scenarios\nsuch as multi-instruction prompting have been insufficiently studied. In this\npaper, we identify several pitfalls practitioners should be aware of when\ndeploying KV cache compressed LLMs. Importantly, we show that certain\ninstructions degrade much more rapidly with compression, effectively causing\nthem to be completely ignored by the LLM. As a practical example of that, we\nhighlight system prompt leakage as a case study, empirically showing the impact\nof compression on leakage and general instruction following. We show several\nfactors that play a role in prompt leakage: compression method, instruction\norder, and KV eviction bias. We then propose simple changes to KV cache\neviction policies that can reduce the impact of these factors and improve the\noverall performance in multi-instruction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression promises increased throughput and efficiency with\nnegligible loss in performance. While the gains in throughput are indisputable\nand recent literature has indeed shown minimal degradation on particular\nbenchmarks, in general the consequences of compression in realistic scenarios\nsuch as multi-instruction prompting have been insufficiently studied. In this\npaper, we identify several pitfalls practitioners should be aware of when\ndeploying KV cache compressed LLMs. Importantly, we show that certain\ninstructions degrade much more rapidly with compression, effectively causing\nthem to be completely ignored by the LLM. As a practical example of that, we\nhighlight system prompt leakage as a case study, empirically showing the impact\nof compression on leakage and general instruction following. We show several\nfactors that play a role in prompt leakage: compression method, instruction\norder, and KV eviction bias. We then propose simple changes to KV cache\neviction policies that can reduce the impact of these factors and improve the\noverall performance in multi-instruction tasks."
                },
                "authors": [
                    {
                        "name": "Alex Chen"
                    },
                    {
                        "name": "Renato Geh"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Daniel Israel"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Israel"
                },
                "author": "Daniel Israel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00184v1",
                "updated": "2025-09-30T19:03:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    3,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T19:03:26Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    3,
                    26,
                    1,
                    273,
                    0
                ],
                "title": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls"
                },
                "summary": "Language models are increasingly capable, yet still fail at a seemingly\nsimple task of multi-digit multiplication. In this work, we study why, by\nreverse-engineering a model that successfully learns multiplication via\n\\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of\nlong-range structure: Logit attributions and linear probes indicate that the\nmodel encodes the necessary long-range dependencies for multi-digit\nmultiplication. (2) Mechanism: the model encodes long-range dependencies using\nattention to construct a directed acyclic graph to ``cache'' and ``retrieve''\npairwise partial products. (3) Geometry: the model implements partial products\nin attention heads by forming Minkowski sums between pairs of digits, and\ndigits are represented using a Fourier basis, both of which are intuitive and\nefficient representations that the standard fine-tuning model lacks. With these\ninsights, we revisit the learning dynamics of standard fine-tuning and find\nthat the model converges to a local optimum that lacks the required long-range\ndependencies. We further validate this understanding by introducing an\nauxiliary loss that predicts the ``running sum'' via a linear regression probe,\nwhich provides an inductive bias that enables the model to successfully learn\nmulti-digit multiplication. In summary, by reverse-engineering the mechanisms\nof an implicit chain-of-thought model we uncover a pitfall for learning\nlong-range dependencies in Transformers and provide an example of how the\ncorrect inductive bias can address this issue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models are increasingly capable, yet still fail at a seemingly\nsimple task of multi-digit multiplication. In this work, we study why, by\nreverse-engineering a model that successfully learns multiplication via\n\\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of\nlong-range structure: Logit attributions and linear probes indicate that the\nmodel encodes the necessary long-range dependencies for multi-digit\nmultiplication. (2) Mechanism: the model encodes long-range dependencies using\nattention to construct a directed acyclic graph to ``cache'' and ``retrieve''\npairwise partial products. (3) Geometry: the model implements partial products\nin attention heads by forming Minkowski sums between pairs of digits, and\ndigits are represented using a Fourier basis, both of which are intuitive and\nefficient representations that the standard fine-tuning model lacks. With these\ninsights, we revisit the learning dynamics of standard fine-tuning and find\nthat the model converges to a local optimum that lacks the required long-range\ndependencies. We further validate this understanding by introducing an\nauxiliary loss that predicts the ``running sum'' via a linear regression probe,\nwhich provides an inductive bias that enables the model to successfully learn\nmulti-digit multiplication. In summary, by reverse-engineering the mechanisms\nof an implicit chain-of-thought model we uncover a pitfall for learning\nlong-range dependencies in Transformers and provide an example of how the\ncorrect inductive bias can address this issue."
                },
                "authors": [
                    {
                        "name": "Xiaoyan Bai"
                    },
                    {
                        "name": "Itamar Pres"
                    },
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Chenhao Tan"
                    },
                    {
                        "name": "Stuart Shieber"
                    },
                    {
                        "name": "Fernanda Viégas"
                    },
                    {
                        "name": "Martin Wattenberg"
                    },
                    {
                        "name": "Andrew Lee"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lee"
                },
                "author": "Andrew Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26541v1",
                "updated": "2025-09-30T17:15:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:15:27Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "title": "TASP: Topology-aware Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASP: Topology-aware Sequence Parallelism"
                },
                "summary": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention."
                },
                "authors": [
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Wenxun Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "arxiv_affiliation": "Tsinghua University",
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23666v2",
                "updated": "2025-09-30T16:42:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    42,
                    50,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-29T17:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "title": "LoLA: Low-Rank Linear Attention With Sparse Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoLA: Low-Rank Linear Attention With Sparse Caching"
                },
                "summary": "The per-token cost of transformer inference scales with context length,\npreventing its application to lifelong in-context learning. Linear attention is\nan efficient alternative that maintains a constant memory footprint, even on\ninfinite context lengths. While this is a potential candidate for lifelong\nlearning, it falls short in memory capacity. In this paper, we propose LoLA, a\ntraining-free augmentation to linear attention that boosts associative recall.\nLoLA distributes past key-value pairs from context into three memory systems:\n(i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. We show through ablations that our\nself-recall error metric is crucial to efficiently manage long-term associative\nmemories. On pass-key retrieval tasks, LoLA improves the base model's\nperformance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller\ncache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B\nand 8B parameter subquadratic models on zero-shot commonsense reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The per-token cost of transformer inference scales with context length,\npreventing its application to lifelong in-context learning. Linear attention is\nan efficient alternative that maintains a constant memory footprint, even on\ninfinite context lengths. While this is a potential candidate for lifelong\nlearning, it falls short in memory capacity. In this paper, we propose LoLA, a\ntraining-free augmentation to linear attention that boosts associative recall.\nLoLA distributes past key-value pairs from context into three memory systems:\n(i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. We show through ablations that our\nself-recall error metric is crucial to efficiently manage long-term associative\nmemories. On pass-key retrieval tasks, LoLA improves the base model's\nperformance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller\ncache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B\nand 8B parameter subquadratic models on zero-shot commonsense reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Luke McDermott"
                    },
                    {
                        "name": "Robert W. Heath Jr."
                    },
                    {
                        "name": "Rahul Parhi"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Parhi"
                },
                "author": "Rahul Parhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v3",
                "updated": "2025-09-30T15:44:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    44,
                    29,
                    1,
                    273,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando García-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_doi": "10.1109/TED.2025.3617043",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TED.2025.3617043",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to IEEE Trans. Elec. Dev. Work enabled in part by NanoIC\n  pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26328v1",
                "updated": "2025-09-30T14:40:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    18,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:40:18Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    18,
                    1,
                    273,
                    0
                ],
                "title": "Fast-dLLM v2: Efficient Block-Diffusion LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM v2: Efficient Block-Diffusion LLM"
                },
                "summary": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v4",
                "updated": "2025-09-30T14:13:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    13,
                    20,
                    1,
                    273,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by NeurIPS 2025. Code at https://github.com/NVlabs/Long-RL\n  and model at https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v2",
                "updated": "2025-09-30T09:10:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    9,
                    10,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "FastCoder: Accelerating Repository-level Code Generation via Efficient\n  Retrieval and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCoder: Accelerating Repository-level Code Generation via Efficient\n  Retrieval and Verification"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness.\nHowever, with the growing interest and inherent difficulty in repository-level\ncode generation, most existing code generation studies focus on improving the\ncorrectness of generated code while overlooking the inference efficiency, which\nis substantially affected by the overhead during LLM generation. Although there\nhas been work on accelerating LLM inference, these approaches are not tailored\nto the specific characteristics of code generation; instead, they treat code\nthe same as natural language sequences and ignore its unique syntax and\nsemantic characteristics, which are also crucial for improving efficiency.\nConsequently, these approaches exhibit limited effectiveness in code generation\ntasks, particularly for repository-level scenarios with considerable complexity\nand difficulty. To alleviate this issue, following draft-verification paradigm,\nwe propose FastCoder, a simple yet highly efficient inference acceleration\napproach specifically designed for code generation, without compromising the\nquality of the output. FastCoder constructs a multi-source datastore, providing\naccess to both general and project-specific knowledge, facilitating the\nretrieval of high-quality draft sequences. Moreover, FastCoder reduces the\nretrieval cost by controlling retrieval timing, and enhances efficiency through\nparallel retrieval and a context- and LLM preference-aware cache. Experimental\nresults show that FastCoder can reach up to 2.53x and 2.54x speedup compared to\nautoregressive decoding in repository-level and standalone code generation\ntasks, respectively, outperforming state-of-the-art inference acceleration\napproaches by up to 88%. FastCoder can also be integrated with existing\ncorrectness-focused code generation approaches to accelerate the LLM generation\nprocess, and reach a speedup exceeding 2.6x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness.\nHowever, with the growing interest and inherent difficulty in repository-level\ncode generation, most existing code generation studies focus on improving the\ncorrectness of generated code while overlooking the inference efficiency, which\nis substantially affected by the overhead during LLM generation. Although there\nhas been work on accelerating LLM inference, these approaches are not tailored\nto the specific characteristics of code generation; instead, they treat code\nthe same as natural language sequences and ignore its unique syntax and\nsemantic characteristics, which are also crucial for improving efficiency.\nConsequently, these approaches exhibit limited effectiveness in code generation\ntasks, particularly for repository-level scenarios with considerable complexity\nand difficulty. To alleviate this issue, following draft-verification paradigm,\nwe propose FastCoder, a simple yet highly efficient inference acceleration\napproach specifically designed for code generation, without compromising the\nquality of the output. FastCoder constructs a multi-source datastore, providing\naccess to both general and project-specific knowledge, facilitating the\nretrieval of high-quality draft sequences. Moreover, FastCoder reduces the\nretrieval cost by controlling retrieval timing, and enhances efficiency through\nparallel retrieval and a context- and LLM preference-aware cache. Experimental\nresults show that FastCoder can reach up to 2.53x and 2.54x speedup compared to\nautoregressive decoding in repository-level and standalone code generation\ntasks, respectively, outperforming state-of-the-art inference acceleration\napproaches by up to 88%. FastCoder can also be integrated with existing\ncorrectness-focused code generation approaches to accelerate the LLM generation\nprocess, and reach a speedup exceeding 2.6x."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Lin Shi"
                    }
                ],
                "author_detail": {
                    "name": "Lin Shi"
                },
                "author": "Lin Shi",
                "arxiv_comment": "Accepted by ASE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23416v2",
                "updated": "2025-09-30T02:51:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    51,
                    5,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-29T13:05:47Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction"
                },
                "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by $3$-$4\\times$ and FlashAttention decoding latency by\napproximately $2\\times$, with negligible performance loss in\nquestion-answering, retrieval, reasoning, and code comprehension tasks.\nEvaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with\ncontext lengths reaching up to 170K tokens. KVzip significantly outperforms\nexisting query-aware KV eviction methods, which suffer from performance\ndegradation even at a 90% cache budget ratio under multi-query scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by $3$-$4\\times$ and FlashAttention decoding latency by\napproximately $2\\times$, with negligible performance loss in\nquestion-answering, retrieval, reasoning, and code comprehension tasks.\nEvaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with\ncontext lengths reaching up to 170K tokens. KVzip significantly outperforms\nexisting query-aware KV eviction methods, which suffer from performance\ndegradation even at a 90% cache budget ratio under multi-query scenarios."
                },
                "authors": [
                    {
                        "name": "Jang-Hyun Kim"
                    },
                    {
                        "name": "Jinuk Kim"
                    },
                    {
                        "name": "Sangwoo Kwon"
                    },
                    {
                        "name": "Jae W. Lee"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Hyun Oh Song"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Oh Song"
                },
                "author": "Hyun Oh Song",
                "arxiv_comment": "NeurIPS 2025 Oral. Code: https://github.com/snu-mllab/KVzip",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25681v1",
                "updated": "2025-09-30T02:36:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    36,
                    11,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T02:36:11Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    36,
                    11,
                    1,
                    273,
                    0
                ],
                "title": "dVLA: Diffusion Vision-Language-Action Model with Multimodal\n  Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dVLA: Diffusion Vision-Language-Action Model with Multimodal\n  Chain-of-Thought"
                },
                "summary": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics."
                },
                "authors": [
                    {
                        "name": "Junjie Wen"
                    },
                    {
                        "name": "Minjie Zhu"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Yicun Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Yi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Xu"
                },
                "author": "Yi Xu",
                "arxiv_comment": "technique report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25401v1",
                "updated": "2025-09-29T18:57:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    18,
                    57,
                    14,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T18:57:14Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    18,
                    57,
                    14,
                    0,
                    272,
                    0
                ],
                "title": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers"
                },
                "summary": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional\ncapabilities in visual synthesis, yet their deployment remains constrained by\nsubstantial computational demands. To alleviate this bottleneck, many\nsparsity-based acceleration methods have been proposed. However, their diverse\nsparsity patterns often require customized kernels for high-performance\ninference, limiting universality. We propose FlashOmni, a unified sparse\nattention engine compatible with arbitrary DiT architectures. FlashOmni\nintroduces flexible sparse symbols to standardize the representation of a wide\nrange of sparsity strategies, such as feature caching and block-sparse\nskipping. This unified abstraction enables the execution of diverse sparse\ncomputations within a single attention kernel. In addition, FlashOmni designs\noptimized sparse GEMMs for attention blocks, leveraging sparse symbols to\neliminate redundant computations and further improve efficiency. Experiments\ndemonstrate that FlashOmni delivers near-linear, closely matching the sparsity\nratio speedup (1:1) in attention and GEMM-$Q$, and achieves\n2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of\nthe theoretical limit). Applied with a multi-granularity sparsity strategy, it\nenables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end\nacceleration without degrading visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional\ncapabilities in visual synthesis, yet their deployment remains constrained by\nsubstantial computational demands. To alleviate this bottleneck, many\nsparsity-based acceleration methods have been proposed. However, their diverse\nsparsity patterns often require customized kernels for high-performance\ninference, limiting universality. We propose FlashOmni, a unified sparse\nattention engine compatible with arbitrary DiT architectures. FlashOmni\nintroduces flexible sparse symbols to standardize the representation of a wide\nrange of sparsity strategies, such as feature caching and block-sparse\nskipping. This unified abstraction enables the execution of diverse sparse\ncomputations within a single attention kernel. In addition, FlashOmni designs\noptimized sparse GEMMs for attention blocks, leveraging sparse symbols to\neliminate redundant computations and further improve efficiency. Experiments\ndemonstrate that FlashOmni delivers near-linear, closely matching the sparsity\nratio speedup (1:1) in attention and GEMM-$Q$, and achieves\n2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of\nthe theoretical limit). Applied with a multi-granularity sparsity strategy, it\nenables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end\nacceleration without degrading visual quality."
                },
                "authors": [
                    {
                        "name": "Liang Qiao"
                    },
                    {
                        "name": "Yue Dai"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Hongyu Kan"
                    },
                    {
                        "name": "Jun Shi"
                    },
                    {
                        "name": "Hong An"
                    }
                ],
                "author_detail": {
                    "name": "Hong An"
                },
                "author": "Hong An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25155v1",
                "updated": "2025-09-29T17:55:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    55,
                    43,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:55:43Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    55,
                    43,
                    0,
                    272,
                    0
                ],
                "title": "Context-Driven Performance Modeling for Causal Inference Operators on\n  Neural Processing Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Driven Performance Modeling for Causal Inference Operators on\n  Neural Processing Units"
                },
                "summary": "The proliferation of large language models (LLMs) has driven demand for long\ncontext inference on resource constrained edge devices. However, deploying\nthese models on Neural Processing Units (NPUs) presents significant challenges\ndue to the architectural mismatch: quadratic complexity of standard attention\nmechanisms conflicts with memory and compute patterns of edge accelerators.\nThis paper presents a comprehensive performance analysis of various causal\ninference operators on a modern NPU. We benchmark standard quadratic attention\nagainst several sub-quadratic alternatives, including structured state-space\nand linear attention models. Our analysis reveals that while sub-quadratic\nmethods offer superior scalability, they introduce distinct computational\nbottlenecks on the NPU's specialized execution units. We identify that\nquadratic attention becomes severely memory-bound, suffering from cache\ninefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,\nsub-quadratic models can become compute-bound on programmable vector cores.\nThese findings provide critical insights for the co-design of hardware-aware\nmodels and optimization strategies to enable on-device AI inference with\nlong-contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has driven demand for long\ncontext inference on resource constrained edge devices. However, deploying\nthese models on Neural Processing Units (NPUs) presents significant challenges\ndue to the architectural mismatch: quadratic complexity of standard attention\nmechanisms conflicts with memory and compute patterns of edge accelerators.\nThis paper presents a comprehensive performance analysis of various causal\ninference operators on a modern NPU. We benchmark standard quadratic attention\nagainst several sub-quadratic alternatives, including structured state-space\nand linear attention models. Our analysis reveals that while sub-quadratic\nmethods offer superior scalability, they introduce distinct computational\nbottlenecks on the NPU's specialized execution units. We identify that\nquadratic attention becomes severely memory-bound, suffering from cache\ninefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,\nsub-quadratic models can become compute-bound on programmable vector cores.\nThese findings provide critical insights for the co-design of hardware-aware\nmodels and optimization strategies to enable on-device AI inference with\nlong-contexts."
                },
                "authors": [
                    {
                        "name": "Neelesh Gupta"
                    },
                    {
                        "name": "Rakshith Jayanth"
                    },
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IEEE HiPC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02850v2",
                "updated": "2025-09-29T15:20:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    20,
                    29,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-03T13:19:41Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding"
                },
                "summary": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy."
                },
                "authors": [
                    {
                        "name": "Mengyue Wang"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "arxiv_comment": "EMNLP 2025; 15 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16056v2",
                "updated": "2025-09-29T15:15:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    15,
                    49,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-21T22:13:09Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models"
                },
                "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc ."
                },
                "authors": [
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Miren Tian"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24832v1",
                "updated": "2025-09-29T14:16:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:16:13Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "title": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching"
                },
                "summary": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinye Zhao"
                    },
                    {
                        "name": "Spyridon Mastorakis"
                    }
                ],
                "author_detail": {
                    "name": "Spyridon Mastorakis"
                },
                "author": "Spyridon Mastorakis",
                "arxiv_comment": "11 figures, 14pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24791v1",
                "updated": "2025-09-29T13:45:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:45:35Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "title": "Vision Function Layer in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Function Layer in Multimodal LLMs"
                },
                "summary": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models."
                },
                "authors": [
                    {
                        "name": "Cheng Shi"
                    },
                    {
                        "name": "Yizhou Yu"
                    },
                    {
                        "name": "Sibei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Sibei Yang"
                },
                "author": "Sibei Yang",
                "arxiv_comment": "Accepted at NeurIPS 2025 (preview; camera-ready in preparation)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v3",
                "updated": "2025-09-29T12:34:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    34,
                    50,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely used technique for accelerating inference in\nlarge language models (LLMs), but its performance degrades as input length\ngrows, with significant drops even at moderate lengths. Yet, this early\ndegradation has remained largely underexplored. We introduce SpecExtend, a\ndrop-in enhancement that improves speculative decoding on long sequences\nwithout additional training. SpecExtend integrates efficient attention\nmechanisms such as FlashAttention and Hybrid Tree Attention to accelerate\nprefill and verification steps. To improve both draft accuracy and speed on\nlong inputs without retraining, we propose Cross-model Retrieval, a novel KV\ncache eviction strategy that leverages the target model's attention scores to\ndynamically select relevant context for the smaller draft model. Extensive\nevaluations show that SpecExtend accelerates speculative decoding by up to\n2.84x on 16K-token long summarization and up to 3.86x on long reasoning, while\npreserving the short-input performance of state-of-the-art frameworks. Our code\nis available at https://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely used technique for accelerating inference in\nlarge language models (LLMs), but its performance degrades as input length\ngrows, with significant drops even at moderate lengths. Yet, this early\ndegradation has remained largely underexplored. We introduce SpecExtend, a\ndrop-in enhancement that improves speculative decoding on long sequences\nwithout additional training. SpecExtend integrates efficient attention\nmechanisms such as FlashAttention and Hybrid Tree Attention to accelerate\nprefill and verification steps. To improve both draft accuracy and speed on\nlong inputs without retraining, we propose Cross-model Retrieval, a novel KV\ncache eviction strategy that leverages the target model's attention scores to\ndynamically select relevant context for the smaller draft model. Extensive\nevaluations show that SpecExtend accelerates speculative decoding by up to\n2.84x on 16K-token long summarization and up to 3.86x on long reasoning, while\npreserving the short-input performance of state-of-the-art frameworks. Our code\nis available at https://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24695v1",
                "updated": "2025-09-29T12:28:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T12:28:09Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer"
                },
                "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Jincheng Yu"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Hongwei Yi"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "arxiv_comment": "21 pages, 15 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24626v1",
                "updated": "2025-09-29T11:35:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    11,
                    35,
                    55,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T11:35:55Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    11,
                    35,
                    55,
                    0,
                    272,
                    0
                ],
                "title": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in\n  Long-Context LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in\n  Long-Context LLM Serving"
                },
                "summary": "Serving long-context LLMs is costly because attention computation grows\nlinearly with context length. Dynamic sparse attention algorithms (DSAs)\nmitigate this by attending only to the key-value (KV) cache of critical tokens.\nHowever, with DSAs, the main performance bottleneck shifts from HBM bandwidth\nto HBM capacity: KV caches for unselected tokens must remain in HBM for\nlow-latency decoding, constraining parallel batch size and stalling further\nthroughput gains. Offloading these underutilized KV caches to DRAM could free\nHBM capacity, allowing larger parallel batch sizes. Yet, achieving such\nhierarchical HBM-DRAM storage raises new challenges, including fragmented KV\ncache access, HBM cache contention, and high HBM demands of hybrid batching,\nthat remain unresolved in prior work.\n  This paper proposes SparseServe, an LLM serving system that unlocks the\nparallel potential of DSAs through efficient hierarchical HBM-DRAM management.\nSparseServe introduces three key innovations to address the challenges\nmentioned above: (1) fragmentation-aware KV cache transfer, which accelerates\nHBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted\nsaving (FlashD2H); (2) working-set-aware batch size control that adjusts batch\nsizes based on real-time working set estimation to minimize HBM cache\nthrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a\nsingle layer, enabling efficient execution even for long prompts. Extensive\nexperimental results demonstrate that SparseServe achieves up to 9.26x lower\nmean time-to-first-token (TTFT) latency and up to 3.14x higher token generation\nthroughput compared to state-of-the-art LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving long-context LLMs is costly because attention computation grows\nlinearly with context length. Dynamic sparse attention algorithms (DSAs)\nmitigate this by attending only to the key-value (KV) cache of critical tokens.\nHowever, with DSAs, the main performance bottleneck shifts from HBM bandwidth\nto HBM capacity: KV caches for unselected tokens must remain in HBM for\nlow-latency decoding, constraining parallel batch size and stalling further\nthroughput gains. Offloading these underutilized KV caches to DRAM could free\nHBM capacity, allowing larger parallel batch sizes. Yet, achieving such\nhierarchical HBM-DRAM storage raises new challenges, including fragmented KV\ncache access, HBM cache contention, and high HBM demands of hybrid batching,\nthat remain unresolved in prior work.\n  This paper proposes SparseServe, an LLM serving system that unlocks the\nparallel potential of DSAs through efficient hierarchical HBM-DRAM management.\nSparseServe introduces three key innovations to address the challenges\nmentioned above: (1) fragmentation-aware KV cache transfer, which accelerates\nHBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted\nsaving (FlashD2H); (2) working-set-aware batch size control that adjusts batch\nsizes based on real-time working set estimation to minimize HBM cache\nthrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a\nsingle layer, enabling efficient execution even for long prompts. Extensive\nexperimental results demonstrate that SparseServe achieves up to 9.26x lower\nmean time-to-first-token (TTFT) latency and up to 3.14x higher token generation\nthroughput compared to state-of-the-art LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "14 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24407v1",
                "updated": "2025-09-29T07:54:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    7,
                    54,
                    44,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T07:54:44Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    7,
                    54,
                    44,
                    0,
                    272,
                    0
                ],
                "title": "Q-REACH: Quantum information Repetition, Error Analysis and Correction\n  using Caching Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-REACH: Quantum information Repetition, Error Analysis and Correction\n  using Caching Network"
                },
                "summary": "Quantum repeaters incorporating quantum memory play a pivotal role in\nmitigating loss in transmitted quantum information (photons) due to link\nattenuation over a long-distance quantum communication network. However,\nlimited availability of available storage in such quantum repeaters and the\nimpact on the time spent within the memory unit presents a trade-off between\nquantum information fidelity (a metric that quantifies the degree of similarity\nbetween a pair of quantum states) and qubit transmission rate. Thus, effective\nmanagement of storage time for qubits becomes a key consideration in multi-hop\nquantum networks. To address these challenges, we propose Q-REACH, which\nleverages queuing theory in caching networks to tune qubit transmission rate\nwhile considering fidelity as the cost metric. Our contributions in this work\ninclude (i) utilizing a method of repetition that encodes and broadcasts\nmultiple qubits through different quantum paths, (ii) analytically estimating\nthe time spent by these emitted qubits as a function of the number of paths and\nrepeaters, as well as memory units within a repeater, and (iii) formulating\noptimization problem that leverages this analysis to correct the transmitted\nlogic qubit and select the optimum repetition rate at the transmitter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum repeaters incorporating quantum memory play a pivotal role in\nmitigating loss in transmitted quantum information (photons) due to link\nattenuation over a long-distance quantum communication network. However,\nlimited availability of available storage in such quantum repeaters and the\nimpact on the time spent within the memory unit presents a trade-off between\nquantum information fidelity (a metric that quantifies the degree of similarity\nbetween a pair of quantum states) and qubit transmission rate. Thus, effective\nmanagement of storage time for qubits becomes a key consideration in multi-hop\nquantum networks. To address these challenges, we propose Q-REACH, which\nleverages queuing theory in caching networks to tune qubit transmission rate\nwhile considering fidelity as the cost metric. Our contributions in this work\ninclude (i) utilizing a method of repetition that encodes and broadcasts\nmultiple qubits through different quantum paths, (ii) analytically estimating\nthe time spent by these emitted qubits as a function of the number of paths and\nrepeaters, as well as memory units within a repeater, and (iii) formulating\noptimization problem that leverages this analysis to correct the transmitted\nlogic qubit and select the optimum repetition rate at the transmitter."
                },
                "authors": [
                    {
                        "name": "Karl C. Linne"
                    },
                    {
                        "name": "Yuanyuan Li"
                    },
                    {
                        "name": "Debashri Roy"
                    },
                    {
                        "name": "Kaushik Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Chowdhury"
                },
                "arxiv_affiliation": "Kai Li",
                "author": "Kaushik Chowdhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v2",
                "updated": "2025-09-29T05:12:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    5,
                    12,
                    51,
                    0,
                    272,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v2",
                "updated": "2025-09-29T02:46:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    2,
                    46,
                    45,
                    0,
                    272,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, the key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, the key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24178v1",
                "updated": "2025-09-29T01:52:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    1,
                    52,
                    10,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T01:52:10Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    1,
                    52,
                    10,
                    0,
                    272,
                    0
                ],
                "title": "BladderFormer: A Streaming Transformer for Real-Time Urological State\n  Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BladderFormer: A Streaming Transformer for Real-Time Urological State\n  Monitoring"
                },
                "summary": "Bladder pressure monitoring systems are increasingly vital in diagnosing and\nmanaging urinary tract dysfunction. Existing solutions rely heavily on\nhand-crafted features and shallow classifiers, limiting their adaptability to\ncomplex signal dynamics. We propose a one-layer streaming transformer model for\nreal-time classification of bladder pressure states, operating on\nwavelet-transformed representations of raw time-series data. Our model\nincorporates temporal multi-head self-attention and state caching, enabling\nefficient online inference with high adaptability. Trained on a dataset of 91\npatients with 20,000-80,000 samples each, our method demonstrates improved\naccuracy, higher energy- and latency-efficiency. Implementation considerations\nfor edge deployment on low-power hardware, such as edge graphical processing\nunits (GPU) and micro-controllers, are also discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bladder pressure monitoring systems are increasingly vital in diagnosing and\nmanaging urinary tract dysfunction. Existing solutions rely heavily on\nhand-crafted features and shallow classifiers, limiting their adaptability to\ncomplex signal dynamics. We propose a one-layer streaming transformer model for\nreal-time classification of bladder pressure states, operating on\nwavelet-transformed representations of raw time-series data. Our model\nincorporates temporal multi-head self-attention and state caching, enabling\nefficient online inference with high adaptability. Trained on a dataset of 91\npatients with 20,000-80,000 samples each, our method demonstrates improved\naccuracy, higher energy- and latency-efficiency. Implementation considerations\nfor edge deployment on low-power hardware, such as edge graphical processing\nunits (GPU) and micro-controllers, are also discussed."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhou"
                    },
                    {
                        "name": "Steve Majerus"
                    },
                    {
                        "name": "Gourav Datta"
                    }
                ],
                "author_detail": {
                    "name": "Gourav Datta"
                },
                "author": "Gourav Datta",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24088v1",
                "updated": "2025-09-28T21:47:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    21,
                    47,
                    20,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T21:47:20Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    21,
                    47,
                    20,
                    6,
                    271,
                    0
                ],
                "title": "CORRECT: COndensed eRror RECognition via knowledge Transfer in\n  multi-agent systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORRECT: COndensed eRror RECognition via knowledge Transfer in\n  multi-agent systems"
                },
                "summary": "Multi-agent systems (MAS) are increasingly capable of tackling complex\nreal-world tasks, yet their reliance on inter-agent coordination, tool use, and\nlong-horizon reasoning makes error recognition particularly challenging. Minor\nerrors can propagate across agents, escalating into task failures while\nproducing long, intertwined execution trajectories that impose significant\ncosts for both human developers and automated systems to debug and analyze. Our\nkey insight is that, despite surface differences in failure trajectories (e.g.,\nlogs), MAS errors often recur with similar structural patterns. This paper\npresents CORRECT, the first lightweight, training-free framework that leverages\nan online cache of distilled error schemata to recognize and transfer knowledge\nof failure structures across new requests. This cache-based reuse allows LLMs\nto perform targeted error localization at inference time, avoiding the need for\nexpensive retraining while adapting to dynamic MAS deployments in subseconds.\nTo support rigorous study in this domain, we also introduce CORRECT-Error, a\nlarge-scale dataset of over 2,000 annotated trajectories collected through a\nnovel error-injection pipeline guided by real-world distributions, and further\nvalidated through human evaluation to ensure alignment with natural failure\npatterns. Experiments across seven diverse MAS applications show that CORRECT\nimproves step-level error localization up to 19.8% over existing advances while\nat near-zero overhead, substantially narrowing the gap between automated and\nhuman-level error recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) are increasingly capable of tackling complex\nreal-world tasks, yet their reliance on inter-agent coordination, tool use, and\nlong-horizon reasoning makes error recognition particularly challenging. Minor\nerrors can propagate across agents, escalating into task failures while\nproducing long, intertwined execution trajectories that impose significant\ncosts for both human developers and automated systems to debug and analyze. Our\nkey insight is that, despite surface differences in failure trajectories (e.g.,\nlogs), MAS errors often recur with similar structural patterns. This paper\npresents CORRECT, the first lightweight, training-free framework that leverages\nan online cache of distilled error schemata to recognize and transfer knowledge\nof failure structures across new requests. This cache-based reuse allows LLMs\nto perform targeted error localization at inference time, avoiding the need for\nexpensive retraining while adapting to dynamic MAS deployments in subseconds.\nTo support rigorous study in this domain, we also introduce CORRECT-Error, a\nlarge-scale dataset of over 2,000 annotated trajectories collected through a\nnovel error-injection pipeline guided by real-world distributions, and further\nvalidated through human evaluation to ensure alignment with natural failure\npatterns. Experiments across seven diverse MAS applications show that CORRECT\nimproves step-level error localization up to 19.8% over existing advances while\nat near-zero overhead, substantially narrowing the gap between automated and\nhuman-level error recognition."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Moyan Li"
                    },
                    {
                        "name": "Shaoyuan Xu"
                    },
                    {
                        "name": "Jinmiao Fu"
                    },
                    {
                        "name": "Xinhai Hou"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Bryan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Wang"
                },
                "author": "Bryan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24007v1",
                "updated": "2025-09-28T17:59:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    17,
                    59,
                    15,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T17:59:15Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    17,
                    59,
                    15,
                    6,
                    271,
                    0
                ],
                "title": "Sequential Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Diffusion Language Models"
                },
                "summary": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM"
                },
                "authors": [
                    {
                        "name": "Yangzhou Liu"
                    },
                    {
                        "name": "Yue Cao"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Xiaobo Liang"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Changyao Tian"
                    },
                    {
                        "name": "Yanting Zhang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Tong Lu"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "arxiv_comment": "14 pages, 5 figures, technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23928v1",
                "updated": "2025-09-28T15:05:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    15,
                    5,
                    21,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T15:05:21Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    15,
                    5,
                    21,
                    6,
                    271,
                    0
                ],
                "title": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in\n  Vision-Language Models"
                },
                "summary": "Speculative decoding is an effective approach for accelerating inference in\nLarge Language models (LLMs), but its adaptation to Vision-Language models\n(VLMs) remains challenging for additional visual tokens in multimodal inputs.\nFirst, owing to the fact that the drafter and the target VLM may derived from\ndifferent families, the semantic representations of visual tokens in the target\nVLM are misaligned with those in the drafter, introducing bias into the\nKV-cache during the prefill stage. Second, the large number of visual tokens\nsubstantially slows down the drafter's self-attention during the decoding\nstage. We propose Hiding Visual Tokens from the Drafter for Speculative\nDecoding in Vision-Language Models (HiViS), an explicit-implicit input\ndecomposition framework that alleviates the above inefficiency. All visual\ntokens are removed from the drafter's input, retaining only textual tokens as\nexplicit inputs, while directly reusing the target VLM's corresponding\nlast-layer hidden states as implicit visual information without additional\nprocessing. To train the drafter efficiently, we introduces multi-step\nself-feedback training strategy with dynamic data selection and sequential\nembedding supervision to simulate reasoning during training. Our approach\ncompresses the prefill sequence length of the drafter to only 0.7%-1.3% of the\ntarget VLM's input, while maintaining lossless generation quality. Extensive\nexperiments across diverse models and tasks demonstrate up to 2.65x speedup,\nconfirming the effectiveness of HiViS in accelerating VLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective approach for accelerating inference in\nLarge Language models (LLMs), but its adaptation to Vision-Language models\n(VLMs) remains challenging for additional visual tokens in multimodal inputs.\nFirst, owing to the fact that the drafter and the target VLM may derived from\ndifferent families, the semantic representations of visual tokens in the target\nVLM are misaligned with those in the drafter, introducing bias into the\nKV-cache during the prefill stage. Second, the large number of visual tokens\nsubstantially slows down the drafter's self-attention during the decoding\nstage. We propose Hiding Visual Tokens from the Drafter for Speculative\nDecoding in Vision-Language Models (HiViS), an explicit-implicit input\ndecomposition framework that alleviates the above inefficiency. All visual\ntokens are removed from the drafter's input, retaining only textual tokens as\nexplicit inputs, while directly reusing the target VLM's corresponding\nlast-layer hidden states as implicit visual information without additional\nprocessing. To train the drafter efficiently, we introduces multi-step\nself-feedback training strategy with dynamic data selection and sequential\nembedding supervision to simulate reasoning during training. Our approach\ncompresses the prefill sequence length of the drafter to only 0.7%-1.3% of the\ntarget VLM's input, while maintaining lossless generation quality. Extensive\nexperiments across diverse models and tasks demonstrate up to 2.65x speedup,\nconfirming the effectiveness of HiViS in accelerating VLM inference."
                },
                "authors": [
                    {
                        "name": "Zhinan Xie"
                    },
                    {
                        "name": "Peisong Wang"
                    },
                    {
                        "name": "Jian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Jian Cheng"
                },
                "author": "Jian Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02361v1",
                "updated": "2025-09-28T11:04:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    11,
                    4,
                    0,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T11:04:00Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    11,
                    4,
                    0,
                    6,
                    271,
                    0
                ],
                "title": "ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs\n  Inference"
                },
                "summary": "Transformer-based large models excel in natural language processing and\ncomputer vision, but face severe computational inefficiencies due to the\nself-attention's quadratic complexity with input tokens. Recently, researchers\nhave proposed a series of methods based on block selection and compression to\nalleviate this problem, but they either have issues with semantic\nincompleteness or poor training-inference efficiency. To comprehensively\naddress these challenges, we propose ChunkLLM, a lightweight and pluggable\ntraining framework. Specifically, we introduce two components: QK Adapter\n(Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each\nTransformer layer, serving dual purposes of feature compression and chunk\nattention acquisition. The latter operates at the bottommost layer of the\nmodel, functioning to detect chunk boundaries by leveraging contextual semantic\ninformation. During the training phase, the parameters of the backbone remain\nfrozen, with only the QK Adapter and Chunk Adapter undergoing training.\nNotably, we design an attention distillation method for training the QK\nAdapter, which enhances the recall rate of key chunks. During the inference\nphase, chunk selection is triggered exclusively when the current token is\ndetected as a chunk boundary, thereby accelerating model inference.\nExperimental evaluations are conducted on a diverse set of long-text and\nshort-text benchmark datasets spanning multiple tasks. ChunkLLM not only\nattains comparable performance on short-text benchmarks but also maintains\n98.64% of the performance on long-context benchmarks while preserving a 48.58%\nkey-value cache retention rate. Particularly, ChunkLLM attains a maximum\nspeedup of 4.48x in comparison to the vanilla Transformer in the processing of\n120K long texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large models excel in natural language processing and\ncomputer vision, but face severe computational inefficiencies due to the\nself-attention's quadratic complexity with input tokens. Recently, researchers\nhave proposed a series of methods based on block selection and compression to\nalleviate this problem, but they either have issues with semantic\nincompleteness or poor training-inference efficiency. To comprehensively\naddress these challenges, we propose ChunkLLM, a lightweight and pluggable\ntraining framework. Specifically, we introduce two components: QK Adapter\n(Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each\nTransformer layer, serving dual purposes of feature compression and chunk\nattention acquisition. The latter operates at the bottommost layer of the\nmodel, functioning to detect chunk boundaries by leveraging contextual semantic\ninformation. During the training phase, the parameters of the backbone remain\nfrozen, with only the QK Adapter and Chunk Adapter undergoing training.\nNotably, we design an attention distillation method for training the QK\nAdapter, which enhances the recall rate of key chunks. During the inference\nphase, chunk selection is triggered exclusively when the current token is\ndetected as a chunk boundary, thereby accelerating model inference.\nExperimental evaluations are conducted on a diverse set of long-text and\nshort-text benchmark datasets spanning multiple tasks. ChunkLLM not only\nattains comparable performance on short-text benchmarks but also maintains\n98.64% of the performance on long-context benchmarks while preserving a 48.58%\nkey-value cache retention rate. Particularly, ChunkLLM attains a maximum\nspeedup of 4.48x in comparison to the vanilla Transformer in the processing of\n120K long texts."
                },
                "authors": [
                    {
                        "name": "Haojie Ouyang"
                    },
                    {
                        "name": "Jianwei Lv"
                    },
                    {
                        "name": "Lei Ren"
                    },
                    {
                        "name": "Chen Wei"
                    },
                    {
                        "name": "Xiaojie Wang"
                    },
                    {
                        "name": "Fangxiang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fangxiang Feng"
                },
                "author": "Fangxiang Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09081v2",
                "updated": "2025-09-28T08:32:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    8,
                    32,
                    26,
                    6,
                    271,
                    0
                ],
                "published": "2025-05-14T02:29:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation"
                },
                "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity."
                },
                "authors": [
                    {
                        "name": "Gaurav Koley"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Koley"
                },
                "author": "Gaurav Koley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23601v1",
                "updated": "2025-09-28T03:12:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    3,
                    12,
                    43,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T03:12:43Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    3,
                    12,
                    43,
                    6,
                    271,
                    0
                ],
                "title": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration"
                },
                "summary": "Recent Mamba-based image restoration methods have achieved promising results\nbut remain\n  limited by fixed scanning patterns and inefficient feature utilization.\nConventional Mamba\n  architectures rely on predetermined paths that cannot adapt to diverse\ndegradations, constraining\n  both restoration performance and computational efficiency. To overcome these\nlimitations, we\n  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.\nFirst,\n  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha\n  FIFO cache that stores historical representations. Similarity between current\nLoRA-adapted and\n  cached features guides intelligent fusion, enabling dynamic reuse while\neffectively controlling\n  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.\nA\n  Vision Transformer generates score maps to estimate pixel importance, and a\ngreedy strategy de termines optimal forward and backward scanning paths. These\nlearned trajectories replace rigid\n  patterns, enabling SS2D to perform targeted feature extraction. The\nintegration of QCLAM and\n  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while\nmaintaining high\n  computational efficiency. Extensive experiments across diverse restoration\ntasks demonstrate\n  that VAMamba consistently outperforms existing approaches in both restoration\nquality and\n  efficiency, establishing new benchmarks for adaptive image restoration. Our\ncode is available\n  at https://github.com/WaterHQH/VAMamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Mamba-based image restoration methods have achieved promising results\nbut remain\n  limited by fixed scanning patterns and inefficient feature utilization.\nConventional Mamba\n  architectures rely on predetermined paths that cannot adapt to diverse\ndegradations, constraining\n  both restoration performance and computational efficiency. To overcome these\nlimitations, we\n  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.\nFirst,\n  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha\n  FIFO cache that stores historical representations. Similarity between current\nLoRA-adapted and\n  cached features guides intelligent fusion, enabling dynamic reuse while\neffectively controlling\n  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.\nA\n  Vision Transformer generates score maps to estimate pixel importance, and a\ngreedy strategy de termines optimal forward and backward scanning paths. These\nlearned trajectories replace rigid\n  patterns, enabling SS2D to perform targeted feature extraction. The\nintegration of QCLAM and\n  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while\nmaintaining high\n  computational efficiency. Extensive experiments across diverse restoration\ntasks demonstrate\n  that VAMamba consistently outperforms existing approaches in both restoration\nquality and\n  efficiency, establishing new benchmarks for adaptive image restoration. Our\ncode is available\n  at https://github.com/WaterHQH/VAMamba."
                },
                "authors": [
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Chen Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Chen Lyu"
                },
                "author": "Chen Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09072v2",
                "updated": "2025-09-27T20:13:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    20,
                    13,
                    25,
                    5,
                    270,
                    0
                ],
                "published": "2025-08-12T16:47:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference"
                },
                "summary": "Autoregressive Language Models instantiate a factorized likelihood over token\nsequences, yet their strictly sequential decoding process imposes an intrinsic\nlower bound on inference latency. This bottleneck has emerged as a central\nobstacle to the scalable deployment of large-scale generative models. Existing\nacceleration techniques partially mitigate token-level latency by relying on\nauxiliary draft models or introducing an additional training phase, but fail to\naddress the dominant memory and communication costs. We present READER, a\nprovably lossless speculative decoding framework that bypasses the training of\nthe auxiliary draft model. READER formalizes speculative decoding as a\nstochastic tree construction problem and exploits the empirical redundancy\nstructure of natural language to generate high-probability candidate\ncontinuations. Our method revisits the problem of constructing draft trees,\nestablishing substantial statistical improvements over stochastic draft-tree\nmethods and providing a complexity-theoretic analysis that characterizes the\noptimality frontier of speculative decoding under bounded computation and\nmemory resources. Beyond the single-sequence regime traditionally considered in\nprior work, we introduce a memory-optimal key-value cache-serving strategy that\nguarantees amortized sublinear overhead in the batch dimension, allowing READER\nto scale to realistic inference workloads. Comprehensive experiments\ndemonstrate up to 6.13x wall-clock speedup on single-prompt inference and up to\n5.92x on batched inference, consistently surpassing prior speculative decoding\nbaselines, while preserving exact output equivalence, with even more pronounced\ngains in retrieval-augmented generation pipelines. Our results close a key gap\nbetween theoretical parallelism limits and practical LLM inference, suggesting\na new standard for efficient deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Language Models instantiate a factorized likelihood over token\nsequences, yet their strictly sequential decoding process imposes an intrinsic\nlower bound on inference latency. This bottleneck has emerged as a central\nobstacle to the scalable deployment of large-scale generative models. Existing\nacceleration techniques partially mitigate token-level latency by relying on\nauxiliary draft models or introducing an additional training phase, but fail to\naddress the dominant memory and communication costs. We present READER, a\nprovably lossless speculative decoding framework that bypasses the training of\nthe auxiliary draft model. READER formalizes speculative decoding as a\nstochastic tree construction problem and exploits the empirical redundancy\nstructure of natural language to generate high-probability candidate\ncontinuations. Our method revisits the problem of constructing draft trees,\nestablishing substantial statistical improvements over stochastic draft-tree\nmethods and providing a complexity-theoretic analysis that characterizes the\noptimality frontier of speculative decoding under bounded computation and\nmemory resources. Beyond the single-sequence regime traditionally considered in\nprior work, we introduce a memory-optimal key-value cache-serving strategy that\nguarantees amortized sublinear overhead in the batch dimension, allowing READER\nto scale to realistic inference workloads. Comprehensive experiments\ndemonstrate up to 6.13x wall-clock speedup on single-prompt inference and up to\n5.92x on batched inference, consistently surpassing prior speculative decoding\nbaselines, while preserving exact output equivalence, with even more pronounced\ngains in retrieval-augmented generation pipelines. Our results close a key gap\nbetween theoretical parallelism limits and practical LLM inference, suggesting\na new standard for efficient deployment."
                },
                "authors": [
                    {
                        "name": "Maxim Divilkovskiy"
                    },
                    {
                        "name": "Vitaly Malygin"
                    },
                    {
                        "name": "Sergey Zlobin"
                    },
                    {
                        "name": "Stanislav Ilyushin"
                    },
                    {
                        "name": "Sultan Isali"
                    },
                    {
                        "name": "Vasily Kalugin"
                    },
                    {
                        "name": "Nuriza Aitassova"
                    },
                    {
                        "name": "Fei Yi"
                    },
                    {
                        "name": "Weidi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Zeng"
                },
                "author": "Weidi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23179v1",
                "updated": "2025-09-27T08:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    8,
                    15,
                    17,
                    5,
                    270,
                    0
                ],
                "published": "2025-09-27T08:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    8,
                    15,
                    17,
                    5,
                    270,
                    0
                ],
                "title": "A Near-Cache Architectural Framework for Cryptographic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Near-Cache Architectural Framework for Cryptographic Computing"
                },
                "summary": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath."
                },
                "authors": [
                    {
                        "name": "Jingyao Zhang"
                    },
                    {
                        "name": "Elaheh Sadredini"
                    }
                ],
                "author_detail": {
                    "name": "Elaheh Sadredini"
                },
                "author": "Elaheh Sadredini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17138v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17138v4",
                "updated": "2025-09-27T07:41:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    7,
                    41,
                    38,
                    5,
                    270,
                    0
                ],
                "published": "2025-05-22T06:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    12,
                    42,
                    3,
                    142,
                    0
                ],
                "title": "Runtime Adaptive Pruning for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runtime Adaptive Pruning for LLM Inference"
                },
                "summary": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly."
                },
                "authors": [
                    {
                        "name": "Huanrong Liu"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Xuyang Wei"
                    },
                    {
                        "name": "Qingbiao Li"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17138v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17138v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23094v1",
                "updated": "2025-09-27T04:07:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    4,
                    7,
                    23,
                    5,
                    270,
                    0
                ],
                "published": "2025-09-27T04:07:23Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    4,
                    7,
                    23,
                    5,
                    270,
                    0
                ],
                "title": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching"
                },
                "summary": "Diffusion-based large language models (dLLMs), despite their promising\nperformance, still suffer from inferior inference efficiency. This is because\ndLLMs rely on bidirectional attention and cannot directly benefit from the\nstandard key-value (KV) cache as autoregressive models (ARMs) do. To tackle\nthis issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a\ntraining-free approximate KV cache framework for accelerating dLLM inference.\nd$^2$Cache features a two-stage fine-grained selection strategy to identify\ntokens and adaptively update their KV states at each decoding step, while\ncaching the KV states of the remaining tokens for reuse. Furthermore,\nd$^2$Cache naturally offers a more reliable decoding alternative, which can\nenable quasi left-to-right generation and mitigate premature overconfidence in\ntokens at the end of the sequence. Extensive experimental results on two\nrepresentative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not\nonly achieves substantial inference speedups, but also yields consistent\nimprovements in generation quality. The code is available at\nhttps://github.com/Kamichanw/d2Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs), despite their promising\nperformance, still suffer from inferior inference efficiency. This is because\ndLLMs rely on bidirectional attention and cannot directly benefit from the\nstandard key-value (KV) cache as autoregressive models (ARMs) do. To tackle\nthis issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a\ntraining-free approximate KV cache framework for accelerating dLLM inference.\nd$^2$Cache features a two-stage fine-grained selection strategy to identify\ntokens and adaptively update their KV states at each decoding step, while\ncaching the KV states of the remaining tokens for reuse. Furthermore,\nd$^2$Cache naturally offers a more reliable decoding alternative, which can\nenable quasi left-to-right generation and mitigate premature overconfidence in\ntokens at the end of the sequence. Extensive experimental results on two\nrepresentative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not\nonly achieves substantial inference speedups, but also yields consistent\nimprovements in generation quality. The code is available at\nhttps://github.com/Kamichanw/d2Cache."
                },
                "authors": [
                    {
                        "name": "Yuchu Jiang"
                    },
                    {
                        "name": "Yue Cai"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Jiale Fu"
                    },
                    {
                        "name": "Jiarui Wang"
                    },
                    {
                        "name": "Chonghan Liu"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24357v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24357v3",
                "updated": "2025-09-27T03:37:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    3,
                    37,
                    40,
                    5,
                    270,
                    0
                ],
                "published": "2025-05-30T08:49:27Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    49,
                    27,
                    4,
                    150,
                    0
                ],
                "title": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance, but\ntheir long-context reasoning remains constrained by the excessive memory\nrequired for the Key-Value (KV) cache. This makes KV cache compression a\ncritical step toward efficient long-context inference. Recent methods have\nexplored low-rank techniques to reduce the hidden size of the KV cache.\nHowever, they neglect the distinct roles and varying importance of Keys and\nValues, leading to significant performance drops under high compression. To\naddress this, we propose ReCalKV, a post-training low-rank KV cache compression\napproach with tailored strategies for Keys and Values. For Keys, we propose\nHead-wise Similarity aware Reordering (HSR), which clusters structurally\nsimilar heads into groups, enabling more accurate low-rank approximation via\ngrouped SVD. For Values, we propose Offline Value Calibration (OVC), which\nefficiently calibrates the value projection matrix using calibration data\nwithout training, ensuring an accurate representation of contextual\ninformation. Extensive experiments show that ReCalKV consistently outperforms\nexisting low-rank compression methods, achieving high compression ratios with\nminimal performance loss. The code and models will be available\nat:https://github.com/XIANGLONGYAN/ReCalKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance, but\ntheir long-context reasoning remains constrained by the excessive memory\nrequired for the Key-Value (KV) cache. This makes KV cache compression a\ncritical step toward efficient long-context inference. Recent methods have\nexplored low-rank techniques to reduce the hidden size of the KV cache.\nHowever, they neglect the distinct roles and varying importance of Keys and\nValues, leading to significant performance drops under high compression. To\naddress this, we propose ReCalKV, a post-training low-rank KV cache compression\napproach with tailored strategies for Keys and Values. For Keys, we propose\nHead-wise Similarity aware Reordering (HSR), which clusters structurally\nsimilar heads into groups, enabling more accurate low-rank approximation via\ngrouped SVD. For Values, we propose Offline Value Calibration (OVC), which\nefficiently calibrates the value projection matrix using calibration data\nwithout training, ensuring an accurate representation of contextual\ninformation. Extensive experiments show that ReCalKV consistently outperforms\nexisting low-rank compression methods, achieving high compression ratios with\nminimal performance loss. The code and models will be available\nat:https://github.com/XIANGLONGYAN/ReCalKV."
                },
                "authors": [
                    {
                        "name": "Xianglong Yan"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Tianao Zhang"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24357v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24357v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v4",
                "updated": "2025-09-26T21:40:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    21,
                    40,
                    58,
                    4,
                    269,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "vCache: Verified Semantic Prompt Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vCache: Verified Semantic Prompt Caching"
                },
                "summary": "Semantic caches return cached responses for semantically similar prompts to\nreduce LLM inference latency and cost. They embed cached prompts and store them\nalongside their response in a vector database. Embedding similarity metrics\nassign a numerical score to quantify the similarity between a request and its\nnearest neighbor prompt from the cache. Existing systems use the same static\nsimilarity threshold across all requests to determine whether two prompts can\nshare similar responses. However, we observe that static thresholds do not give\nformal correctness guarantees, can result in unexpected error rates, and lead\nto suboptimal cache hit rates. This paper proposes vCache, the first verified\nsemantic cache with user-defined error rate guarantees. It employs an online\nlearning algorithm to estimate an optimal threshold for each cached prompt,\nenabling reliable cache responses without additional training. Our experiments\nshow that vCache consistently meets the specified error bounds while\noutperforming state-of-the-art static-threshold and fine-tuned embedding\nbaselines. We release the vCache implementation and three benchmarks to support\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caches return cached responses for semantically similar prompts to\nreduce LLM inference latency and cost. They embed cached prompts and store them\nalongside their response in a vector database. Embedding similarity metrics\nassign a numerical score to quantify the similarity between a request and its\nnearest neighbor prompt from the cache. Existing systems use the same static\nsimilarity threshold across all requests to determine whether two prompts can\nshare similar responses. However, we observe that static thresholds do not give\nformal correctness guarantees, can result in unexpected error rates, and lead\nto suboptimal cache hit rates. This paper proposes vCache, the first verified\nsemantic cache with user-defined error rate guarantees. It employs an online\nlearning algorithm to estimate an optimal threshold for each cached prompt,\nenabling reliable cache responses without additional training. Our experiments\nshow that vCache consistently meets the specified error bounds while\noutperforming state-of-the-art static-threshold and fine-tuned embedding\nbaselines. We release the vCache implementation and three benchmarks to support\nfuture research."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Kyle Chu"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22875v1",
                "updated": "2025-09-26T19:40:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    19,
                    40,
                    33,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T19:40:33Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    19,
                    40,
                    33,
                    4,
                    269,
                    0
                ],
                "title": "On KV-Poisson Structure and related invariants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On KV-Poisson Structure and related invariants"
                },
                "summary": "We propose an deepened analysis of KV-Poisson structures of on IR^2. We\npresent their classification their properties an their possible applications in\ndifferent domains. We prove that these structure give rise to a new\nCohomological invariant. We explicitly compute the Cohomological groups of some\nof these structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an deepened analysis of KV-Poisson structures of on IR^2. We\npresent their classification their properties an their possible applications in\ndifferent domains. We prove that these structure give rise to a new\nCohomological invariant. We explicitly compute the Cohomological groups of some\nof these structures."
                },
                "authors": [
                    {
                        "name": "Prosper Rosaire Mama Assandje"
                    },
                    {
                        "name": "Herguey Mopeng"
                    },
                    {
                        "name": "Joseph Dongho"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Dongho"
                },
                "author": "Joseph Dongho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.DG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v2",
                "updated": "2025-09-26T17:59:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    54,
                    4,
                    269,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Controlling Frozen LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Controlling Frozen LLMs"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "James R. Glass"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22622v1",
                "updated": "2025-09-26T17:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "title": "LongLive: Real-time Interactive Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLive: Real-time Interactive Long Video Generation"
                },
                "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss."
                },
                "authors": [
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Yingcong Chen"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yukang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yukang Chen"
                },
                "author": "Yukang Chen",
                "arxiv_comment": "Code, model, and demos are available at\n  https://github.com/NVlabs/LongLive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.06220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06220v1",
                "updated": "2025-10-07T17:59:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    59,
                    58,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:59:58Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    59,
                    58,
                    1,
                    280,
                    0
                ],
                "title": "Studying the gravitational-wave population without looking that FAR out",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studying the gravitational-wave population without looking that FAR out"
                },
                "summary": "From catalogs of gravitational-wave transients, the population-level\nproperties of their sources and the formation channels of merging compact\nbinaries can be constrained. However, astrophysical conclusions can be biased\nby misspecification or misestimation of the population likelihood. Despite\ndetection thresholds on the false-alarm rate (FAR) or signal-to-noise ratio\n(SNR), the current catalog is likely contaminated by noise transients. Further,\ncomputing the population likelihood becomes less accurate as the catalog grows.\nCurrent methods to address these challenges often scale poorly with the number\nof events and potentially become infeasible for future catalogs. Here, we\nevaluate a simple remedy: increasing the significance threshold for including\nevents in population analyses. To determine the efficacy of this approach, we\nanalyze simulated catalogs of up to 1600 gravitational-wave signals from\nblack-hole mergers using full Bayesian parameter estimation with current\ndetector sensitivities. We show that the growth in statistical uncertainty\nabout the black-hole population, as we analyze fewer events but with higher\nSNR, depends on the source parameters of interest. When the SNR threshold is\nraised from 11 to 15 -- reducing our catalog size by two--thirds -- we find\nthat statistical uncertainties on the mass distribution only grow by a few 10%\nand constraints on the spin distribution are essentially unchanged; meanwhile,\nuncertainties on the high-redshift cosmic merger rate more than double.\nSimultaneously, numerical uncertainty in the estimate of the population\nlikelihood more than halves, allowing us to ensure unbiased inference without\nadditional computational expense. Our results demonstrate that focusing on\nhigher-significance events is an effective way to facilitate robust\nastrophysical inference with growing gravitational-wave catalogs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From catalogs of gravitational-wave transients, the population-level\nproperties of their sources and the formation channels of merging compact\nbinaries can be constrained. However, astrophysical conclusions can be biased\nby misspecification or misestimation of the population likelihood. Despite\ndetection thresholds on the false-alarm rate (FAR) or signal-to-noise ratio\n(SNR), the current catalog is likely contaminated by noise transients. Further,\ncomputing the population likelihood becomes less accurate as the catalog grows.\nCurrent methods to address these challenges often scale poorly with the number\nof events and potentially become infeasible for future catalogs. Here, we\nevaluate a simple remedy: increasing the significance threshold for including\nevents in population analyses. To determine the efficacy of this approach, we\nanalyze simulated catalogs of up to 1600 gravitational-wave signals from\nblack-hole mergers using full Bayesian parameter estimation with current\ndetector sensitivities. We show that the growth in statistical uncertainty\nabout the black-hole population, as we analyze fewer events but with higher\nSNR, depends on the source parameters of interest. When the SNR threshold is\nraised from 11 to 15 -- reducing our catalog size by two--thirds -- we find\nthat statistical uncertainties on the mass distribution only grow by a few 10%\nand constraints on the spin distribution are essentially unchanged; meanwhile,\nuncertainties on the high-redshift cosmic merger rate more than double.\nSimultaneously, numerical uncertainty in the estimate of the population\nlikelihood more than halves, allowing us to ensure unbiased inference without\nadditional computational expense. Our results demonstrate that focusing on\nhigher-significance events is an effective way to facilitate robust\nastrophysical inference with growing gravitational-wave catalogs."
                },
                "authors": [
                    {
                        "name": "Noah E. Wolfe"
                    },
                    {
                        "name": "Matthew Mould"
                    },
                    {
                        "name": "Jack Heinzel"
                    },
                    {
                        "name": "Salvatore Vitale"
                    }
                ],
                "author_detail": {
                    "name": "Salvatore Vitale"
                },
                "author": "Salvatore Vitale",
                "arxiv_comment": "12 pages, 6 figures; to be submitted to Physical Review D. Comments\n  welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06217v1",
                "updated": "2025-10-07T17:59:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    59,
                    41,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:59:41Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    59,
                    41,
                    1,
                    280,
                    0
                ],
                "title": "TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular\n  Reasoning"
                },
                "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor enhancing the reasoning capabilities of large reasoning models (LRMs),\nparticularly in the context of test-time scaling (TTS). However, their\npotential for supervising LRMs on tabular reasoning domains remains\nunderexplored. Through detailed empirical analyses, we identify that existing\nPRMs, though widely adopted for supervising text-only reasoning steps, struggle\nwith table-specific operations such as sub-table retrieval and schema\ninteraction, leading to critical performance bottlenecks. To address this\nlimitation, we propose TaTToo, a novel table-grounded PRM framework that (i)\nreasons explicitly over tabular reasoning steps and (ii) integrates tool-based\nverification to provide precise reward supervision. Concretely, we first design\na scalable data curation pipeline that constructs over 60k high-quality\nstep-level annotations by integrating table verification rationales with\ntool-based executions. Building on the collected data, we train TaTToo with a\ndual-stage paradigm: cold-start supervised fine-tuning to capture tool-use\nreasoning patterns, followed by reinforcement learning with tool-grounded\nreward shaping to align our model with table-based verification. We provide a\ncomprehensive evaluation of the policy improvement induced by our newly\ndesigned PRM. Across 5 challenging tabular reasoning benchmarks covering\nnumerical reasoning, fact-checking, and data analysis, TaTToo improves\ndownstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines\nsuch as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong\ngeneralizability across diverse TTS strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor enhancing the reasoning capabilities of large reasoning models (LRMs),\nparticularly in the context of test-time scaling (TTS). However, their\npotential for supervising LRMs on tabular reasoning domains remains\nunderexplored. Through detailed empirical analyses, we identify that existing\nPRMs, though widely adopted for supervising text-only reasoning steps, struggle\nwith table-specific operations such as sub-table retrieval and schema\ninteraction, leading to critical performance bottlenecks. To address this\nlimitation, we propose TaTToo, a novel table-grounded PRM framework that (i)\nreasons explicitly over tabular reasoning steps and (ii) integrates tool-based\nverification to provide precise reward supervision. Concretely, we first design\na scalable data curation pipeline that constructs over 60k high-quality\nstep-level annotations by integrating table verification rationales with\ntool-based executions. Building on the collected data, we train TaTToo with a\ndual-stage paradigm: cold-start supervised fine-tuning to capture tool-use\nreasoning patterns, followed by reinforcement learning with tool-grounded\nreward shaping to align our model with table-based verification. We provide a\ncomprehensive evaluation of the policy improvement induced by our newly\ndesigned PRM. Across 5 challenging tabular reasoning benchmarks covering\nnumerical reasoning, fact-checking, and data analysis, TaTToo improves\ndownstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines\nsuch as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong\ngeneralizability across diverse TTS strategies."
                },
                "authors": [
                    {
                        "name": "Jiaru Zou"
                    },
                    {
                        "name": "Soumya Roy"
                    },
                    {
                        "name": "Vinay Kumar Verma"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "David Wipf"
                    },
                    {
                        "name": "Pan Lu"
                    },
                    {
                        "name": "Sumit Negi"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Jingrui He"
                    }
                ],
                "author_detail": {
                    "name": "Jingrui He"
                },
                "author": "Jingrui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06215v1",
                "updated": "2025-10-07T17:59:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    59,
                    15,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:59:15Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    59,
                    15,
                    1,
                    280,
                    0
                ],
                "title": "Fine-grained Defocus Blur Control for Generative Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained Defocus Blur Control for Generative Image Models"
                },
                "summary": "Current text-to-image diffusion models excel at generating diverse,\nhigh-quality images, yet they struggle to incorporate fine-grained camera\nmetadata such as precise aperture settings. In this work, we introduce a novel\ntext-to-image diffusion framework that leverages camera metadata, or EXIF data,\nwhich is often embedded in image files, with an emphasis on generating\ncontrollable lens blur. Our method mimics the physical image formation process\nby first generating an all-in-focus image, estimating its monocular depth,\npredicting a plausible focus distance with a novel focus distance transformer,\nand then forming a defocused image with an existing differentiable lens blur\nmodel. Gradients flow backwards through this whole process, allowing us to\nlearn without explicit supervision to generate defocus effects based on content\nelements and the provided EXIF data. At inference time, this enables precise\ninteractive user control over defocus effects while preserving scene contents,\nwhich is not achievable with existing diffusion models. Experimental results\ndemonstrate that our model enables superior fine-grained control without\naltering the depicted scene.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current text-to-image diffusion models excel at generating diverse,\nhigh-quality images, yet they struggle to incorporate fine-grained camera\nmetadata such as precise aperture settings. In this work, we introduce a novel\ntext-to-image diffusion framework that leverages camera metadata, or EXIF data,\nwhich is often embedded in image files, with an emphasis on generating\ncontrollable lens blur. Our method mimics the physical image formation process\nby first generating an all-in-focus image, estimating its monocular depth,\npredicting a plausible focus distance with a novel focus distance transformer,\nand then forming a defocused image with an existing differentiable lens blur\nmodel. Gradients flow backwards through this whole process, allowing us to\nlearn without explicit supervision to generate defocus effects based on content\nelements and the provided EXIF data. At inference time, this enables precise\ninteractive user control over defocus effects while preserving scene contents,\nwhich is not achievable with existing diffusion models. Experimental results\ndemonstrate that our model enables superior fine-grained control without\naltering the depicted scene."
                },
                "authors": [
                    {
                        "name": "Ayush Shrivastava"
                    },
                    {
                        "name": "Connelly Barnes"
                    },
                    {
                        "name": "Xuaner Zhang"
                    },
                    {
                        "name": "Lingzhi Zhang"
                    },
                    {
                        "name": "Andrew Owens"
                    },
                    {
                        "name": "Sohrab Amirghodsi"
                    },
                    {
                        "name": "Eli Shechtman"
                    }
                ],
                "author_detail": {
                    "name": "Eli Shechtman"
                },
                "author": "Eli Shechtman",
                "arxiv_comment": "Project link: https://www.ayshrv.com/defocus-blur-gen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06214v1",
                "updated": "2025-10-07T17:59:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    59,
                    13,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:59:13Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    59,
                    13,
                    1,
                    280,
                    0
                ],
                "title": "Stratified GRPO: Handling Structural Heterogeneity in Reinforcement\n  Learning of LLM Search Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stratified GRPO: Handling Structural Heterogeneity in Reinforcement\n  Learning of LLM Search Agents"
                },
                "summary": "Large language model (LLM) agents increasingly rely on external tools such as\nsearch engines to solve complex, multi-step problems, and reinforcement\nlearning (RL) has become a key paradigm for training them. However, the\ntrajectories of search agents are structurally heterogeneous, where variations\nin the number, placement, and outcomes of search calls lead to fundamentally\ndifferent answer directions and reward distributions. Standard policy gradient\nmethods, which use a single global baseline, suffer from what we identify and\nformalize as cross-stratum bias-an \"apples-to-oranges\" comparison of\nheterogeneous trajectories. This cross-stratum bias distorts credit assignment\nand hinders exploration of complex, multi-step search strategies. To address\nthis, we propose Stratified GRPO, whose central component, Stratified Advantage\nNormalization (SAN), partitions trajectories into homogeneous strata based on\ntheir structural properties and computes advantages locally within each\nstratum. This ensures that trajectories are evaluated only against their true\npeers. Our analysis proves that SAN eliminates cross-stratum bias, yields\nconditionally unbiased unit-variance estimates inside each stratum, and retains\nthe global unbiasedness and unit-variance properties enjoyed by standard\nnormalization, resulting in a more pure and scale-stable learning signal. To\nimprove practical stability under finite-sample regimes, we further linearly\nblend SAN with the global estimator. Extensive experiments on diverse\nsingle-hop and multi-hop question-answering benchmarks demonstrate that\nStratified GRPO consistently and substantially outperforms GRPO by up to 11.3\npoints, achieving higher training rewards, greater training stability, and more\neffective search policies. These results establish stratification as a\nprincipled remedy for structural heterogeneity in RL for LLM search agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents increasingly rely on external tools such as\nsearch engines to solve complex, multi-step problems, and reinforcement\nlearning (RL) has become a key paradigm for training them. However, the\ntrajectories of search agents are structurally heterogeneous, where variations\nin the number, placement, and outcomes of search calls lead to fundamentally\ndifferent answer directions and reward distributions. Standard policy gradient\nmethods, which use a single global baseline, suffer from what we identify and\nformalize as cross-stratum bias-an \"apples-to-oranges\" comparison of\nheterogeneous trajectories. This cross-stratum bias distorts credit assignment\nand hinders exploration of complex, multi-step search strategies. To address\nthis, we propose Stratified GRPO, whose central component, Stratified Advantage\nNormalization (SAN), partitions trajectories into homogeneous strata based on\ntheir structural properties and computes advantages locally within each\nstratum. This ensures that trajectories are evaluated only against their true\npeers. Our analysis proves that SAN eliminates cross-stratum bias, yields\nconditionally unbiased unit-variance estimates inside each stratum, and retains\nthe global unbiasedness and unit-variance properties enjoyed by standard\nnormalization, resulting in a more pure and scale-stable learning signal. To\nimprove practical stability under finite-sample regimes, we further linearly\nblend SAN with the global estimator. Extensive experiments on diverse\nsingle-hop and multi-hop question-answering benchmarks demonstrate that\nStratified GRPO consistently and substantially outperforms GRPO by up to 11.3\npoints, achieving higher training rewards, greater training stability, and more\neffective search policies. These results establish stratification as a\nprincipled remedy for structural heterogeneity in RL for LLM search agents."
                },
                "authors": [
                    {
                        "name": "Mingkang Zhu"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04573v2",
                "updated": "2025-10-07T17:58:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    58,
                    48,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-06T08:15:03Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    8,
                    15,
                    3,
                    0,
                    279,
                    0
                ],
                "title": "LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning"
                },
                "summary": "Large Language Models (LLMs) demonstrate their reasoning ability through\nchain-of-thought (CoT) generation. However, LLM's autoregressive decoding may\nlimit the ability to revisit and refine earlier tokens in a holistic manner,\nwhich can also lead to inefficient exploration for diverse solutions. In this\npaper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning\nframework that unifies the expressiveness of continuous latent representation\nwith the iterative refinement capabilities of latent diffusion models for an\nexisting LLM. We first construct a structured latent reasoning space using a\nVariational Autoencoder (VAE) that encodes text reasoning steps into blocks of\nthought tokens, preserving semantic information and interpretability while\noffering compact but expressive representations. Subsequently, we utilize a\nlatent diffusion model that learns to denoise a block of latent thought tokens\nwith a blockwise bidirectional attention mask, enabling longer horizon and\niterative refinement with adaptive test-time compute. This design allows\nefficient parallel generation of diverse reasoning trajectories, allowing the\nmodel to plan and revise the reasoning process holistically. We conduct\nevaluations on a suite of mathematical reasoning and planning benchmarks.\nEmpirical results show that LaDiR consistently improves accuracy, diversity,\nand interpretability over existing autoregressive, diffusion-based, and latent\nreasoning methods, revealing a new paradigm for text reasoning with latent\ndiffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate their reasoning ability through\nchain-of-thought (CoT) generation. However, LLM's autoregressive decoding may\nlimit the ability to revisit and refine earlier tokens in a holistic manner,\nwhich can also lead to inefficient exploration for diverse solutions. In this\npaper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning\nframework that unifies the expressiveness of continuous latent representation\nwith the iterative refinement capabilities of latent diffusion models for an\nexisting LLM. We first construct a structured latent reasoning space using a\nVariational Autoencoder (VAE) that encodes text reasoning steps into blocks of\nthought tokens, preserving semantic information and interpretability while\noffering compact but expressive representations. Subsequently, we utilize a\nlatent diffusion model that learns to denoise a block of latent thought tokens\nwith a blockwise bidirectional attention mask, enabling longer horizon and\niterative refinement with adaptive test-time compute. This design allows\nefficient parallel generation of diverse reasoning trajectories, allowing the\nmodel to plan and revise the reasoning process holistically. We conduct\nevaluations on a suite of mathematical reasoning and planning benchmarks.\nEmpirical results show that LaDiR consistently improves accuracy, diversity,\nand interpretability over existing autoregressive, diffusion-based, and latent\nreasoning methods, revealing a new paradigm for text reasoning with latent\ndiffusion."
                },
                "authors": [
                    {
                        "name": "Haoqiang Kang"
                    },
                    {
                        "name": "Yizhe Zhang"
                    },
                    {
                        "name": "Nikki Lijing Kuang"
                    },
                    {
                        "name": "Nicklas Majamaki"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    },
                    {
                        "name": "Yi-An Ma"
                    },
                    {
                        "name": "Lianhui Qin"
                    }
                ],
                "author_detail": {
                    "name": "Lianhui Qin"
                },
                "author": "Lianhui Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19227v2",
                "updated": "2025-10-07T17:57:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    57,
                    11,
                    1,
                    280,
                    0
                ],
                "published": "2025-08-26T17:43:20Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    43,
                    20,
                    1,
                    238,
                    0
                ],
                "title": "Generative Interfaces for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Interfaces for Language Models"
                },
                "summary": "Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with up to a 72% improvement in\nhuman preference. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with up to a 72% improvement in\nhuman preference. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction."
                },
                "authors": [
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Yanzhe Zhang"
                    },
                    {
                        "name": "Yutong Zhang"
                    },
                    {
                        "name": "Yijia Shao"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14824v2",
                "updated": "2025-10-07T17:56:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    56,
                    22,
                    1,
                    280,
                    0
                ],
                "published": "2025-05-20T18:39:56Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    18,
                    39,
                    56,
                    1,
                    140,
                    0
                ],
                "title": "Tracing Multilingual Factual Knowledge Acquisition in Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracing Multilingual Factual Knowledge Acquisition in Pretraining"
                },
                "summary": "Large Language Models (LLMs) are capable of recalling multilingual factual\nknowledge present in their pretraining data. However, most studies evaluate\nonly the final model, leaving the development of factual recall and\ncrosslingual consistency throughout pretraining largely unexplored. In this\nwork, we trace how factual recall and crosslingual consistency evolve during\npretraining, focusing on OLMo-7B as a case study. We find that both accuracy\nand consistency improve over time for most languages. We show that this\nimprovement is primarily driven by the fact frequency in the pretraining\ncorpus: more frequent facts are more likely to be recalled correctly,\nregardless of language. Yet, some low-frequency facts in non-English languages\ncan still be correctly recalled. Our analysis reveals that these instances\nlargely benefit from crosslingual transfer of their English counterparts -- an\neffect that emerges predominantly in the early stages of pretraining. We\npinpoint two distinct pathways through which multilingual factual knowledge\nacquisition occurs: (1) frequency-driven learning, which is dominant and\nlanguage-agnostic, and (2) crosslingual transfer, which is limited in scale and\ntypically constrained to relation types involving named entities. We release\nour code and data to facilitate further research at\nhttps://github.com/cisnlp/multilingual-fact-tracing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are capable of recalling multilingual factual\nknowledge present in their pretraining data. However, most studies evaluate\nonly the final model, leaving the development of factual recall and\ncrosslingual consistency throughout pretraining largely unexplored. In this\nwork, we trace how factual recall and crosslingual consistency evolve during\npretraining, focusing on OLMo-7B as a case study. We find that both accuracy\nand consistency improve over time for most languages. We show that this\nimprovement is primarily driven by the fact frequency in the pretraining\ncorpus: more frequent facts are more likely to be recalled correctly,\nregardless of language. Yet, some low-frequency facts in non-English languages\ncan still be correctly recalled. Our analysis reveals that these instances\nlargely benefit from crosslingual transfer of their English counterparts -- an\neffect that emerges predominantly in the early stages of pretraining. We\npinpoint two distinct pathways through which multilingual factual knowledge\nacquisition occurs: (1) frequency-driven learning, which is dominant and\nlanguage-agnostic, and (2) crosslingual transfer, which is limited in scale and\ntypically constrained to relation types involving named entities. We release\nour code and data to facilitate further research at\nhttps://github.com/cisnlp/multilingual-fact-tracing."
                },
                "authors": [
                    {
                        "name": "Yihong Liu"
                    },
                    {
                        "name": "Mingyang Wang"
                    },
                    {
                        "name": "Amir Hossein Kargaran"
                    },
                    {
                        "name": "Felicia Körner"
                    },
                    {
                        "name": "Ercong Nie"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "François Yvon"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "arxiv_comment": "EMNLP Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18427v2",
                "updated": "2025-10-07T17:56:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    56,
                    0,
                    1,
                    280,
                    0
                ],
                "published": "2025-08-25T19:18:50Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    19,
                    18,
                    50,
                    0,
                    237,
                    0
                ],
                "title": "Tracing Positional Bias in Financial Decision-Making: Mechanistic\n  Insights from Qwen2.5",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracing Positional Bias in Financial Decision-Making: Mechanistic\n  Insights from Qwen2.5"
                },
                "summary": "The growing adoption of large language models (LLMs) in finance exposes\nhigh-stakes decision-making to subtle, underexamined positional biases. The\ncomplexity and opacity of modern model architectures compound this risk. We\npresent the first unified framework and benchmark that not only detects and\nquantifies positional bias in binary financial decisions but also pinpoints its\nmechanistic origins within open-source Qwen2.5-instruct models (1.5B-14B). Our\nempirical analysis covers a novel, finance-authentic dataset revealing that\npositional bias is pervasive, scale-sensitive, and prone to resurfacing under\nnuanced prompt designs and investment scenarios, with recency and primacy\neffects revealing new vulnerabilities in risk-laden contexts. Through\ntransparent mechanistic interpretability, we map how and where bias emerges and\npropagates within the models to deliver actionable, generalizable insights\nacross prompt types and scales. By bridging domain-specific audit with model\ninterpretability, our work provides a new methodological standard for both\nrigorous bias diagnosis and practical mitigation, establishing essential\nguidance for responsible and trustworthy deployment of LLMs in financial\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of large language models (LLMs) in finance exposes\nhigh-stakes decision-making to subtle, underexamined positional biases. The\ncomplexity and opacity of modern model architectures compound this risk. We\npresent the first unified framework and benchmark that not only detects and\nquantifies positional bias in binary financial decisions but also pinpoints its\nmechanistic origins within open-source Qwen2.5-instruct models (1.5B-14B). Our\nempirical analysis covers a novel, finance-authentic dataset revealing that\npositional bias is pervasive, scale-sensitive, and prone to resurfacing under\nnuanced prompt designs and investment scenarios, with recency and primacy\neffects revealing new vulnerabilities in risk-laden contexts. Through\ntransparent mechanistic interpretability, we map how and where bias emerges and\npropagates within the models to deliver actionable, generalizable insights\nacross prompt types and scales. By bridging domain-specific audit with model\ninterpretability, our work provides a new methodological standard for both\nrigorous bias diagnosis and practical mitigation, establishing essential\nguidance for responsible and trustworthy deployment of LLMs in financial\nsystems."
                },
                "authors": [
                    {
                        "name": "Fabrizio Dimino"
                    },
                    {
                        "name": "Krati Saxena"
                    },
                    {
                        "name": "Bhaskarjit Sarmah"
                    },
                    {
                        "name": "Stefano Pasquali"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Pasquali"
                },
                "author": "Stefano Pasquali",
                "arxiv_doi": "10.1145/3768292.3770394",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3768292.3770394",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14252v2",
                "updated": "2025-10-07T17:55:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    55,
                    14,
                    1,
                    280,
                    0
                ],
                "published": "2025-09-11T03:03:57Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    3,
                    3,
                    57,
                    3,
                    254,
                    0
                ],
                "title": "LLM-JEPA: Large Language Models Meet Joint Embedding Predictive\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-JEPA: Large Language Models Meet Joint Embedding Predictive\n  Architectures"
                },
                "summary": "Large Language Model (LLM) pretraining, finetuning, and evaluation rely on\ninput-space reconstruction and generative capabilities. Yet, it has been\nobserved in vision that embedding-space training objectives, e.g., with Joint\nEmbedding Predictive Architectures (JEPAs), are far superior to their\ninput-space counterpart. That mismatch in how training is achieved between\nlanguage and vision opens up a natural question: {\\em can language training\nmethods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is\na testimony of the challenge in designing such objectives for language. In this\nwork, we propose a first step in that direction where we develop LLM-JEPA, a\nJEPA based solution for LLMs applicable both to finetuning and pretraining.\nThus far, LLM-JEPA is able to outperform the standard LLM training objectives\nby a significant margin across models, all while being robust to overfiting.\nThose findings are observed across numerous datasets (NL-RX, GSM8K, Spider,\nRottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo\nfamilies. Code: https://github.com/rbalestr-lab/llm-jepa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) pretraining, finetuning, and evaluation rely on\ninput-space reconstruction and generative capabilities. Yet, it has been\nobserved in vision that embedding-space training objectives, e.g., with Joint\nEmbedding Predictive Architectures (JEPAs), are far superior to their\ninput-space counterpart. That mismatch in how training is achieved between\nlanguage and vision opens up a natural question: {\\em can language training\nmethods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is\na testimony of the challenge in designing such objectives for language. In this\nwork, we propose a first step in that direction where we develop LLM-JEPA, a\nJEPA based solution for LLMs applicable both to finetuning and pretraining.\nThus far, LLM-JEPA is able to outperform the standard LLM training objectives\nby a significant margin across models, all while being robust to overfiting.\nThose findings are observed across numerous datasets (NL-RX, GSM8K, Spider,\nRottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo\nfamilies. Code: https://github.com/rbalestr-lab/llm-jepa."
                },
                "authors": [
                    {
                        "name": "Hai Huang"
                    },
                    {
                        "name": "Yann LeCun"
                    },
                    {
                        "name": "Randall Balestriero"
                    }
                ],
                "author_detail": {
                    "name": "Randall Balestriero"
                },
                "author": "Randall Balestriero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06202v1",
                "updated": "2025-10-07T17:54:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    54,
                    14,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:54:14Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    54,
                    14,
                    1,
                    280,
                    0
                ],
                "title": "Mapping surface height dynamics to subsurface flow physics in\n  free-surface turbulent flow using a shallow recurrent decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping surface height dynamics to subsurface flow physics in\n  free-surface turbulent flow using a shallow recurrent decoder"
                },
                "summary": "Near-surface turbulent flows beneath a free surface are reconstructed from\nsparse measurements of the surface height variation, by a novel neural network\nalgorithm known as the SHallow REcurrent Decoder (SHRED). The reconstruction of\nturbulent flow fields from limited, partial, or indirect measurements remains a\ngrand challenge in science and engineering. The central goal in such\napplications is to leverage easy-to-measure proxy variables in order to\nestimate quantities which have not been, and perhaps cannot in practice be,\nmeasured. Specifically, in the application considered here, the aim is to use a\nsparse number of surface height point measurements of a flow field, or drone\nvideo footage of surface features, in order to infer the turbulent flow field\nbeneath the surface. SHRED is a deep learning architecture that learns a\ndelay-coordinate embedding from a few surface height (point) sensors and maps\nit, via a shallow decoder trained in a compressed basis, to full subsurface\nfields, enabling fast, robust training from minimal data. We demonstrate the\nSHRED sensing architecture on both fully resolved DNS data and PIV laboratory\ndata from a turbulent water tank. SHRED is capable of robustly mapping surface\nheight fluctuations to full-state flow fields up to about two integral length\nscales deep, with as few as three surface measurements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-surface turbulent flows beneath a free surface are reconstructed from\nsparse measurements of the surface height variation, by a novel neural network\nalgorithm known as the SHallow REcurrent Decoder (SHRED). The reconstruction of\nturbulent flow fields from limited, partial, or indirect measurements remains a\ngrand challenge in science and engineering. The central goal in such\napplications is to leverage easy-to-measure proxy variables in order to\nestimate quantities which have not been, and perhaps cannot in practice be,\nmeasured. Specifically, in the application considered here, the aim is to use a\nsparse number of surface height point measurements of a flow field, or drone\nvideo footage of surface features, in order to infer the turbulent flow field\nbeneath the surface. SHRED is a deep learning architecture that learns a\ndelay-coordinate embedding from a few surface height (point) sensors and maps\nit, via a shallow decoder trained in a compressed basis, to full subsurface\nfields, enabling fast, robust training from minimal data. We demonstrate the\nSHRED sensing architecture on both fully resolved DNS data and PIV laboratory\ndata from a turbulent water tank. SHRED is capable of robustly mapping surface\nheight fluctuations to full-state flow fields up to about two integral length\nscales deep, with as few as three surface measurements."
                },
                "authors": [
                    {
                        "name": "Kristoffer S. Moen"
                    },
                    {
                        "name": "Jørgen R. Aarnes"
                    },
                    {
                        "name": "Simen Å. Ellingsen"
                    },
                    {
                        "name": "J. Nathan Kutz"
                    }
                ],
                "author_detail": {
                    "name": "J. Nathan Kutz"
                },
                "author": "J. Nathan Kutz",
                "arxiv_comment": "27 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06198v1",
                "updated": "2025-10-07T17:53:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    53,
                    55,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:53:55Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    53,
                    55,
                    1,
                    280,
                    0
                ],
                "title": "Peeking inside the Black-Box: Reinforcement Learning for Explainable and\n  Accurate Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peeking inside the Black-Box: Reinforcement Learning for Explainable and\n  Accurate Relation Extraction"
                },
                "summary": "This paper introduces a framework for relation extraction (RE) that enhances\nboth accuracy and explainability. The framework has two key components: (i) a\nreasoning mechanism that formulates relation extraction as a series of\ntext-processing steps inspired by cognitive science, and (ii) an optimization\nprocess driven by reinforcement learning (RL) with a novel reward function\ndesigned to improve both task accuracy and explanation quality. We call our\napproach CogRE. Our framework addresses the lack of supervision for\nlanguage-based explanations in traditional RE by promoting outputs that include\nimportant relation keywords. These keywords are drawn from a high-quality\ndictionary that is automatically constructed using an LLM. We evaluate our\napproach for the task of one-shot RE using two LLMs and two RE datasets. Our\nexperiments show that CogRE improves explanation quality by addressing two\ncommon failure patterns in one-shot RE: poor attention focus and limited\none-shot learning capability. For example, our cognitive-structured reasoning\nwith Qwen2.5-15B-Instruct on One-shot NYT29 achieves 24.65% F1, surpassing\nprior reasoning-based designs. Optimizing this approach with RL using our\nreward further improves performance by +23.46% (absolute). Finally, human\nevaluation shows that our best model generates relational keywords closely\naligned with gold labels, increasing human explanation quality ratings by 54%\n(relative).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a framework for relation extraction (RE) that enhances\nboth accuracy and explainability. The framework has two key components: (i) a\nreasoning mechanism that formulates relation extraction as a series of\ntext-processing steps inspired by cognitive science, and (ii) an optimization\nprocess driven by reinforcement learning (RL) with a novel reward function\ndesigned to improve both task accuracy and explanation quality. We call our\napproach CogRE. Our framework addresses the lack of supervision for\nlanguage-based explanations in traditional RE by promoting outputs that include\nimportant relation keywords. These keywords are drawn from a high-quality\ndictionary that is automatically constructed using an LLM. We evaluate our\napproach for the task of one-shot RE using two LLMs and two RE datasets. Our\nexperiments show that CogRE improves explanation quality by addressing two\ncommon failure patterns in one-shot RE: poor attention focus and limited\none-shot learning capability. For example, our cognitive-structured reasoning\nwith Qwen2.5-15B-Instruct on One-shot NYT29 achieves 24.65% F1, surpassing\nprior reasoning-based designs. Optimizing this approach with RL using our\nreward further improves performance by +23.46% (absolute). Finally, human\nevaluation shows that our best model generates relational keywords closely\naligned with gold labels, increasing human explanation quality ratings by 54%\n(relative)."
                },
                "authors": [
                    {
                        "name": "Xinyu Guo"
                    },
                    {
                        "name": "Zhengliang Shi"
                    },
                    {
                        "name": "Minglai Yang"
                    },
                    {
                        "name": "Mahdi Rahimi"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    }
                ],
                "author_detail": {
                    "name": "Mihai Surdeanu"
                },
                "author": "Mihai Surdeanu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06195v1",
                "updated": "2025-10-07T17:52:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    52,
                    8,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:52:08Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    52,
                    8,
                    1,
                    280,
                    0
                ],
                "title": "Latent Speech-Text Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Speech-Text Transformer"
                },
                "summary": "Auto-regressive speech-text models are typically pre-trained on a large\nnumber of interleaved sequences of text tokens and raw speech encoded as speech\ntokens using vector quantization. These models have demonstrated\nstate-of-the-art performance in speech-to-speech understanding and generation\nbenchmarks, together with promising scaling laws, primarily enabled by the\nrepresentational alignment between text and speech. Nevertheless, they suffer\nfrom shortcomings, partly owing to the disproportionately longer sequences of\nspeech tokens in contrast to textual tokens. This results in a large compute\nimbalance between modalities during pre-training as well as during inference,\nand a potential hindrance to effectively aligning speech and text, ultimately\ntranslating to several orders of magnitude slower scaling laws. We introduce\nthe Latent Speech-Text Transformer (LST), which makes pre-training speech-text\nmodels more data-efficient by dynamically and inexpensively aggregating speech\ntokens into latent speech patches. These patches serve as higher-level units\nthat can either align with corresponding textual units to aid capability\ntransfer or even encapsulate common speech sequences like silences to be more\ncompute-efficient. We show that LST outperforms vanilla approaches on\nspeech-to-speech as well as text-to-text benchmarks in both data- and\ncompute-controlled settings, the former indicating more effective\nrepresentational alignment and the latter indicating steeper scaling laws for\nspeech-text models. On HellaSwag story completion, LST achieves 6.5% absolute\ngain in speech accuracy under compute-controlled training and 5.3% under\ndata-controlled training, while also improving text performance. We will\nrelease our models, code, and the evaluation data to facilitate further\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive speech-text models are typically pre-trained on a large\nnumber of interleaved sequences of text tokens and raw speech encoded as speech\ntokens using vector quantization. These models have demonstrated\nstate-of-the-art performance in speech-to-speech understanding and generation\nbenchmarks, together with promising scaling laws, primarily enabled by the\nrepresentational alignment between text and speech. Nevertheless, they suffer\nfrom shortcomings, partly owing to the disproportionately longer sequences of\nspeech tokens in contrast to textual tokens. This results in a large compute\nimbalance between modalities during pre-training as well as during inference,\nand a potential hindrance to effectively aligning speech and text, ultimately\ntranslating to several orders of magnitude slower scaling laws. We introduce\nthe Latent Speech-Text Transformer (LST), which makes pre-training speech-text\nmodels more data-efficient by dynamically and inexpensively aggregating speech\ntokens into latent speech patches. These patches serve as higher-level units\nthat can either align with corresponding textual units to aid capability\ntransfer or even encapsulate common speech sequences like silences to be more\ncompute-efficient. We show that LST outperforms vanilla approaches on\nspeech-to-speech as well as text-to-text benchmarks in both data- and\ncompute-controlled settings, the former indicating more effective\nrepresentational alignment and the latter indicating steeper scaling laws for\nspeech-text models. On HellaSwag story completion, LST achieves 6.5% absolute\ngain in speech accuracy under compute-controlled training and 5.3% under\ndata-controlled training, while also improving text performance. We will\nrelease our models, code, and the evaluation data to facilitate further\nresearch."
                },
                "authors": [
                    {
                        "name": "Yen-Ju Lu"
                    },
                    {
                        "name": "Yashesh Gaur"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Benjamin Muller"
                    },
                    {
                        "name": "Jesus Villalba"
                    },
                    {
                        "name": "Najim Dehak"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Gargi Ghosh"
                    },
                    {
                        "name": "Mike Lewis"
                    },
                    {
                        "name": "Srinivasan Iyer"
                    },
                    {
                        "name": "Duc Le"
                    }
                ],
                "author_detail": {
                    "name": "Duc Le"
                },
                "author": "Duc Le",
                "arxiv_comment": "16 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06191v1",
                "updated": "2025-10-07T17:50:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    50,
                    21,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:50:21Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    50,
                    21,
                    1,
                    280,
                    0
                ],
                "title": "Rapid calibration of atrial electrophysiology models using Gaussian\n  process emulators in the ensemble Kalman filter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid calibration of atrial electrophysiology models using Gaussian\n  process emulators in the ensemble Kalman filter"
                },
                "summary": "Atrial fibrillation (AF) is a common cardiac arrhythmia characterised by\ndisordered electrical activity in the atria. The standard treatment is catheter\nablation, which is invasive and irreversible. Recent advances in computational\nelectrophysiology offer the potential for patient-specific models, often\nreferred to as digital twins, that can be used to guide clinical decisions. To\nbe of practical value, we must be able to rapidly calibrate physics-based\nmodels using routine clinical measurements. We pose this calibration task as a\nstatic inverse problem, where the goal is to infer tissue-level\nelectrophysiological parameters from the available observations. To make this\ntractable, we replace the expensive forward model with Gaussian process\nemulators (GPEs), and propose a novel adaptation of the ensemble Kalman filter\n(EnKF) for static non-linear inverse problems. The approach yields parameter\nsamples that can be interpreted as coming from the best Gaussian approximation\nof the posterior distribution. We compare our results with those obtained using\nMarkov chain Monte Carlo (MCMC) sampling and demonstrate the potential of the\napproach to enable near-real-time patient-specific calibration, a key step\ntowards predicting outcomes of AF treatment within clinical timescales. The\napproach is readily applicable to a wide range of static inverse problems in\nscience and engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atrial fibrillation (AF) is a common cardiac arrhythmia characterised by\ndisordered electrical activity in the atria. The standard treatment is catheter\nablation, which is invasive and irreversible. Recent advances in computational\nelectrophysiology offer the potential for patient-specific models, often\nreferred to as digital twins, that can be used to guide clinical decisions. To\nbe of practical value, we must be able to rapidly calibrate physics-based\nmodels using routine clinical measurements. We pose this calibration task as a\nstatic inverse problem, where the goal is to infer tissue-level\nelectrophysiological parameters from the available observations. To make this\ntractable, we replace the expensive forward model with Gaussian process\nemulators (GPEs), and propose a novel adaptation of the ensemble Kalman filter\n(EnKF) for static non-linear inverse problems. The approach yields parameter\nsamples that can be interpreted as coming from the best Gaussian approximation\nof the posterior distribution. We compare our results with those obtained using\nMarkov chain Monte Carlo (MCMC) sampling and demonstrate the potential of the\napproach to enable near-real-time patient-specific calibration, a key step\ntowards predicting outcomes of AF treatment within clinical timescales. The\napproach is readily applicable to a wide range of static inverse problems in\nscience and engineering."
                },
                "authors": [
                    {
                        "name": "Mariya Mamajiwala"
                    },
                    {
                        "name": "Cesare Corrado"
                    },
                    {
                        "name": "Chris Lanyon"
                    },
                    {
                        "name": "Steven A. Niederer"
                    },
                    {
                        "name": "Richard D. Wilkinson"
                    },
                    {
                        "name": "Richard H. Clayton"
                    }
                ],
                "author_detail": {
                    "name": "Richard H. Clayton"
                },
                "author": "Richard H. Clayton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06190v1",
                "updated": "2025-10-07T17:49:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    49,
                    30,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:49:30Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    49,
                    30,
                    1,
                    280,
                    0
                ],
                "title": "On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond"
                },
                "summary": "This paper formally studies generation processes, including auto-regressive\nnext-token prediction and masked diffusion, that abstract beyond architectural\nspecifics. At this level of abstraction, we quantify their benefits and\nlimitations through measurable criteria such as computational hardness and\nlearnability. In particular, we demonstrate that allowing generation to proceed\nbeyond autoregression and current masked diffusion, with capabilities to\nrewrite and length-variable edit, can bring significant theoretical and\nempirical advantages, with important implications for frontier LLMs that aspire\nto tackle increasingly hard problems and work universally across domains beyond\nnatural language, such as coding and science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper formally studies generation processes, including auto-regressive\nnext-token prediction and masked diffusion, that abstract beyond architectural\nspecifics. At this level of abstraction, we quantify their benefits and\nlimitations through measurable criteria such as computational hardness and\nlearnability. In particular, we demonstrate that allowing generation to proceed\nbeyond autoregression and current masked diffusion, with capabilities to\nrewrite and length-variable edit, can bring significant theoretical and\nempirical advantages, with important implications for frontier LLMs that aspire\nto tackle increasingly hard problems and work universally across domains beyond\nnatural language, such as coding and science."
                },
                "authors": [
                    {
                        "name": "Chenxiao Yang"
                    },
                    {
                        "name": "Cai Zhou"
                    },
                    {
                        "name": "David Wipf"
                    },
                    {
                        "name": "Zhiyuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Li"
                },
                "author": "Zhiyuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06189v2",
                "updated": "2025-10-08T01:21:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    1,
                    21,
                    49,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-07T17:49:24Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    49,
                    24,
                    1,
                    280,
                    0
                ],
                "title": "Barbarians at the Gate: How AI is Upending Systems Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Barbarians at the Gate: How AI is Upending Systems Research"
                },
                "summary": "Artificial Intelligence (AI) is starting to transform the research process as\nwe know it by automating the discovery of new solutions. Given a task, the\ntypical AI-driven approach is (i) to generate a set of diverse solutions, and\nthen (ii) to verify these solutions and select one that solves the problem.\nCrucially, this approach assumes the existence of a reliable verifier, i.e.,\none that can accurately determine whether a solution solves the given problem.\nWe argue that systems research, long focused on designing and evaluating new\nperformance-oriented algorithms, is particularly well-suited for AI-driven\nsolution discovery. This is because system performance problems naturally admit\nreliable verifiers: solutions are typically implemented in real systems or\nsimulators, and verification reduces to running these software artifacts\nagainst predefined workloads and measuring performance. We term this approach\nas AI-Driven Research for Systems (ADRS), which iteratively generates,\nevaluates, and refines solutions. Using penEvolve, an existing open-source ADRS\ninstance, we present case studies across diverse domains, including load\nbalancing for multi-region cloud scheduling, Mixture-of-Experts inference,\nLLM-based SQL queries, and transaction scheduling. In multiple instances, ADRS\ndiscovers algorithms that outperform state-of-the-art human designs (e.g.,\nachieving up to 5.0x runtime improvements or 50% cost reductions). We distill\nbest practices for guiding algorithm evolution, from prompt design to evaluator\nconstruction, for existing frameworks. We then discuss the broader implications\nfor the systems community: as AI assumes a central role in algorithm design, we\nargue that human researchers will increasingly focus on problem formulation and\nstrategic guidance. Our results highlight both the disruptive potential and the\nurgent need to adapt systems research practices in the age of AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) is starting to transform the research process as\nwe know it by automating the discovery of new solutions. Given a task, the\ntypical AI-driven approach is (i) to generate a set of diverse solutions, and\nthen (ii) to verify these solutions and select one that solves the problem.\nCrucially, this approach assumes the existence of a reliable verifier, i.e.,\none that can accurately determine whether a solution solves the given problem.\nWe argue that systems research, long focused on designing and evaluating new\nperformance-oriented algorithms, is particularly well-suited for AI-driven\nsolution discovery. This is because system performance problems naturally admit\nreliable verifiers: solutions are typically implemented in real systems or\nsimulators, and verification reduces to running these software artifacts\nagainst predefined workloads and measuring performance. We term this approach\nas AI-Driven Research for Systems (ADRS), which iteratively generates,\nevaluates, and refines solutions. Using penEvolve, an existing open-source ADRS\ninstance, we present case studies across diverse domains, including load\nbalancing for multi-region cloud scheduling, Mixture-of-Experts inference,\nLLM-based SQL queries, and transaction scheduling. In multiple instances, ADRS\ndiscovers algorithms that outperform state-of-the-art human designs (e.g.,\nachieving up to 5.0x runtime improvements or 50% cost reductions). We distill\nbest practices for guiding algorithm evolution, from prompt design to evaluator\nconstruction, for existing frameworks. We then discuss the broader implications\nfor the systems community: as AI assumes a central role in algorithm design, we\nargue that human researchers will increasingly focus on problem formulation and\nstrategic guidance. Our results highlight both the disruptive potential and the\nurgent need to adapt systems research practices in the age of AI."
                },
                "authors": [
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Melissa Pan"
                    },
                    {
                        "name": "Zhifei Li"
                    },
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Alex Krentsel"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Mert Cemri"
                    },
                    {
                        "name": "Jongseok Park"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Jeff Chen"
                    },
                    {
                        "name": "Lakshya Agrawal"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Koushik Sen"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06187v1",
                "updated": "2025-10-07T17:46:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    46,
                    33,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:46:33Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    46,
                    33,
                    1,
                    280,
                    0
                ],
                "title": "Automated Program Repair of Uncompilable Student Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Program Repair of Uncompilable Student Code"
                },
                "summary": "A significant portion of student programming submissions in CS1 learning\nenvironments are uncompilable, limiting their use in student modeling and\ndownstream knowledge tracing. Traditional modeling pipelines often exclude\nthese cases, discarding observations of student learning. This study\ninvestigates automated program repair as a strategy to recover uncompilable\ncode while preserving students' structural intent for use in student modeling.\nWithin this framework, we assess large language models (LLMs) as repair agents,\nincluding GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash\n(Google), under high- and low-context prompting conditions. Repairs were\nevaluated for compilability, edit distance, and preservation of students'\noriginal structure and logic. We find that while all three LLMs are capable of\nproducing compilable repairs, their behavior diverges in how well they preserve\nstudents' control flow and code structure, which affects their pedagogical\nutility. By recovering uncompilable submissions, this work enables richer and\nmore comprehensive analyses of learners' coding processes and development over\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A significant portion of student programming submissions in CS1 learning\nenvironments are uncompilable, limiting their use in student modeling and\ndownstream knowledge tracing. Traditional modeling pipelines often exclude\nthese cases, discarding observations of student learning. This study\ninvestigates automated program repair as a strategy to recover uncompilable\ncode while preserving students' structural intent for use in student modeling.\nWithin this framework, we assess large language models (LLMs) as repair agents,\nincluding GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash\n(Google), under high- and low-context prompting conditions. Repairs were\nevaluated for compilability, edit distance, and preservation of students'\noriginal structure and logic. We find that while all three LLMs are capable of\nproducing compilable repairs, their behavior diverges in how well they preserve\nstudents' control flow and code structure, which affects their pedagogical\nutility. By recovering uncompilable submissions, this work enables richer and\nmore comprehensive analyses of learners' coding processes and development over\ntime."
                },
                "authors": [
                    {
                        "name": "Griffin Pitts"
                    },
                    {
                        "name": "Aum Pandya"
                    },
                    {
                        "name": "Darsh Rank"
                    },
                    {
                        "name": "Tirth Bhatt"
                    },
                    {
                        "name": "Muntasir Hoq"
                    },
                    {
                        "name": "Bita Akram"
                    }
                ],
                "author_detail": {
                    "name": "Bita Akram"
                },
                "author": "Bita Akram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06186v1",
                "updated": "2025-10-07T17:45:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    45,
                    35,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:45:35Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    45,
                    35,
                    1,
                    280,
                    0
                ],
                "title": "RECODE-H: A Benchmark for Research Code Development with Interactive\n  Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RECODE-H: A Benchmark for Research Code Development with Interactive\n  Human Feedback"
                },
                "summary": "Large language models (LLMs) show the promise in supporting scientific\nresearch implementation, yet their ability to generate correct and executable\ncode remains limited. Existing works largely adopt one-shot settings, ignoring\nthe iterative and feedback-driven nature of realistic workflows of scientific\nresearch development. To address this gap, we present RECODE-H, a benchmark of\n102 tasks from research papers and repositories that evaluates LLM agents\nthrough multi-turn interactions with LLM-simulated human feedback. It includes\nstructured instructions,unit tests, and a five-level feedback hierarchy to\nreflect realistic researcher-agent collaboration. We further present\nReCodeAgent, a framework that integrates feedback into iterative code\ngeneration. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,\nDeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer\nfeedback, while also highlighting ongoing challenges in the generation of\ncomplex research code. RECODE-H establishes a foundation for developing\nadaptive, feedback-driven LLM agents in scientific research implementation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show the promise in supporting scientific\nresearch implementation, yet their ability to generate correct and executable\ncode remains limited. Existing works largely adopt one-shot settings, ignoring\nthe iterative and feedback-driven nature of realistic workflows of scientific\nresearch development. To address this gap, we present RECODE-H, a benchmark of\n102 tasks from research papers and repositories that evaluates LLM agents\nthrough multi-turn interactions with LLM-simulated human feedback. It includes\nstructured instructions,unit tests, and a five-level feedback hierarchy to\nreflect realistic researcher-agent collaboration. We further present\nReCodeAgent, a framework that integrates feedback into iterative code\ngeneration. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,\nDeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer\nfeedback, while also highlighting ongoing challenges in the generation of\ncomplex research code. RECODE-H establishes a foundation for developing\nadaptive, feedback-driven LLM agents in scientific research implementation"
                },
                "authors": [
                    {
                        "name": "Chunyu Miao"
                    },
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Yankai Chen"
                    },
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Fangxin Wang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Wooseong Yang"
                    },
                    {
                        "name": "Bowei He"
                    },
                    {
                        "name": "Xinni Zhang"
                    },
                    {
                        "name": "Dianzhi Yu"
                    },
                    {
                        "name": "Hanchen Yang"
                    },
                    {
                        "name": "Hoang H Nguyen"
                    },
                    {
                        "name": "Yue Zhou"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jizhou Guo"
                    },
                    {
                        "name": "Wenzhe Fan"
                    },
                    {
                        "name": "Chin-Yuan Yeh"
                    },
                    {
                        "name": "Panpan Meng"
                    },
                    {
                        "name": "Liancheng Fang"
                    },
                    {
                        "name": "Jinhu Qi"
                    },
                    {
                        "name": "Wei-Chieh Huang"
                    },
                    {
                        "name": "Zhengyao Gu"
                    },
                    {
                        "name": "Yuwei Han"
                    },
                    {
                        "name": "Langzhou He"
                    },
                    {
                        "name": "Yuyao Yang"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Irwin King"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "Code and dataset are available at github.com/ChunyuMiao98/RECODE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22945v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22945v2",
                "updated": "2025-10-07T17:39:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    39,
                    5,
                    1,
                    280,
                    0
                ],
                "published": "2025-05-28T23:57:03Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    23,
                    57,
                    3,
                    2,
                    148,
                    0
                ],
                "title": "OWL: Probing Cross-Lingual Recall of Memorized Texts via World\n  Literature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OWL: Probing Cross-Lingual Recall of Memorized Texts via World\n  Literature"
                },
                "summary": "Large language models (LLMs) are known to memorize and recall English text\nfrom their pretraining data. However, the extent to which this ability\ngeneralizes to non-English languages or transfers across languages remains\nunclear. This paper investigates multilingual and cross-lingual memorization in\nLLMs, probing if memorized content in one language (e.g., English) can be\nrecalled when presented in translation. To do so, we introduce OWL, a dataset\nof 31.5K aligned excerpts from 20 books in ten languages, including English\noriginals, official translations (Vietnamese, Spanish, Turkish), and new\ntranslations in six low-resource languages (Sesotho, Yoruba, Maithili,\nMalagasy, Setswana, Tahitian). We evaluate memorization across model families\nand sizes through three tasks: (1) direct probing, which asks the model to\nidentify a book's title and author; (2) name cloze, which requires predicting\nmasked character names; and (3) prefix probing, which involves generating\ncontinuations. We find that LLMs consistently recall content across languages,\neven for texts without direct translation in pretraining data. GPT-4o, for\nexample, identifies authors and titles 69% of the time and masked entities 6%\nof the time in newly translated excerpts. Perturbations (e.g., masking\ncharacters, shuffling words) modestly reduce direct probing accuracy (7% drop\nfor shuffled official translations). Our results highlight the extent of\ncross-lingual memorization and provide insights on the differences between the\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are known to memorize and recall English text\nfrom their pretraining data. However, the extent to which this ability\ngeneralizes to non-English languages or transfers across languages remains\nunclear. This paper investigates multilingual and cross-lingual memorization in\nLLMs, probing if memorized content in one language (e.g., English) can be\nrecalled when presented in translation. To do so, we introduce OWL, a dataset\nof 31.5K aligned excerpts from 20 books in ten languages, including English\noriginals, official translations (Vietnamese, Spanish, Turkish), and new\ntranslations in six low-resource languages (Sesotho, Yoruba, Maithili,\nMalagasy, Setswana, Tahitian). We evaluate memorization across model families\nand sizes through three tasks: (1) direct probing, which asks the model to\nidentify a book's title and author; (2) name cloze, which requires predicting\nmasked character names; and (3) prefix probing, which involves generating\ncontinuations. We find that LLMs consistently recall content across languages,\neven for texts without direct translation in pretraining data. GPT-4o, for\nexample, identifies authors and titles 69% of the time and masked entities 6%\nof the time in newly translated excerpts. Perturbations (e.g., masking\ncharacters, shuffling words) modestly reduce direct probing accuracy (7% drop\nfor shuffled official translations). Our results highlight the extent of\ncross-lingual memorization and provide insights on the differences between the\nmodels."
                },
                "authors": [
                    {
                        "name": "Alisha Srivastava"
                    },
                    {
                        "name": "Emir Korukluoglu"
                    },
                    {
                        "name": "Minh Nhat Le"
                    },
                    {
                        "name": "Duyen Tran"
                    },
                    {
                        "name": "Chau Minh Pham"
                    },
                    {
                        "name": "Marzena Karpinska"
                    },
                    {
                        "name": "Mohit Iyyer"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Iyyer"
                },
                "author": "Mohit Iyyer",
                "arxiv_comment": "Accepted to EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22945v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22945v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06175v1",
                "updated": "2025-10-07T17:35:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    35,
                    28,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:35:28Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    35,
                    28,
                    1,
                    280,
                    0
                ],
                "title": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via\n  Outlier-Suppressed Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via\n  Outlier-Suppressed Vector Quantization"
                },
                "summary": "The Key-Value (KV) cache introduces substantial memory overhead during large\nlanguage model (LLM) inference. Although existing vector quantization (VQ)\nmethods reduce KV cache usage and provide flexible representational capacity\nacross bit-widths, they suffer severe performance degradation at ultra-low\nbit-widths due to key cache outliers that hinder effective codebook\nutilization. To address this challenge, we propose VecInfer, a novel VQ method\nfor aggressive KV cache compression while enabling efficient inference. By\napplying smooth and Hadamard transformations, VecInfer suppresses outliers in\nthe key cache, enabling the codebook to comprehensively cover the original data\ndistribution and thereby reducing quantization difficulty. To facilitate\nefficient deployment, we design an optimized CUDA kernel that fuses computation\nwith dequantization to minimize memory access overhead. Extensive evaluations\ndemonstrate that VecInfer consistently outperforms existing quantization\nbaselines across both long-context understanding and mathematical reasoning\ntasks. With only 2-bit quantization, VecInfer achieves performance comparable\nto full precision, while delivering up to $\\mathbf{2.7\\times}$ speedup in\nlarge-batch self-attention computation and $\\mathbf{8.3\\times}$ reduction in\nsingle-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache introduces substantial memory overhead during large\nlanguage model (LLM) inference. Although existing vector quantization (VQ)\nmethods reduce KV cache usage and provide flexible representational capacity\nacross bit-widths, they suffer severe performance degradation at ultra-low\nbit-widths due to key cache outliers that hinder effective codebook\nutilization. To address this challenge, we propose VecInfer, a novel VQ method\nfor aggressive KV cache compression while enabling efficient inference. By\napplying smooth and Hadamard transformations, VecInfer suppresses outliers in\nthe key cache, enabling the codebook to comprehensively cover the original data\ndistribution and thereby reducing quantization difficulty. To facilitate\nefficient deployment, we design an optimized CUDA kernel that fuses computation\nwith dequantization to minimize memory access overhead. Extensive evaluations\ndemonstrate that VecInfer consistently outperforms existing quantization\nbaselines across both long-context understanding and mathematical reasoning\ntasks. With only 2-bit quantization, VecInfer achieves performance comparable\nto full precision, while delivering up to $\\mathbf{2.7\\times}$ speedup in\nlarge-batch self-attention computation and $\\mathbf{8.3\\times}$ reduction in\nsingle-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length."
                },
                "authors": [
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Chenxu Yang"
                    },
                    {
                        "name": "Zhengyang Tong"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06151v1",
                "updated": "2025-10-07T17:21:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    21,
                    20,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:21:20Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    21,
                    20,
                    1,
                    280,
                    0
                ],
                "title": "LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design\n  for Heterogeneous Agent Teams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design\n  for Heterogeneous Agent Teams"
                },
                "summary": "A critical challenge in modelling Heterogeneous-Agent Teams is training\nagents to collaborate with teammates whose policies are inaccessible or\nnon-stationary, such as humans. Traditional approaches rely on expensive\nhuman-in-the-loop data, which limits scalability. We propose using Large\nLanguage Models (LLMs) as policy-agnostic human proxies to generate synthetic\ndata that mimics human decision-making. To evaluate this, we conduct three\nexperiments in a grid-world capture game inspired by Stag Hunt, a game theory\nparadigm that balances risk and reward. In Experiment 1, we compare decisions\nfrom 30 human participants and 2 expert judges with outputs from LLaMA 3.1 and\nMixtral 8x22B models. LLMs, prompted with game-state observations and reward\nstructures, align more closely with experts than participants, demonstrating\nconsistency in applying underlying decision criteria. Experiment 2 modifies\nprompts to induce risk-sensitive strategies (e.g. \"be risk averse\"). LLM\noutputs mirror human participants' variability, shifting between risk-averse\nand risk-seeking behaviours. Finally, Experiment 3 tests LLMs in a dynamic\ngrid-world where the LLM agents generate movement actions. LLMs produce\ntrajectories resembling human participants' paths. While LLMs cannot yet fully\nreplicate human adaptability, their prompt-guided diversity offers a scalable\nfoundation for simulating policy-agnostic teammates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical challenge in modelling Heterogeneous-Agent Teams is training\nagents to collaborate with teammates whose policies are inaccessible or\nnon-stationary, such as humans. Traditional approaches rely on expensive\nhuman-in-the-loop data, which limits scalability. We propose using Large\nLanguage Models (LLMs) as policy-agnostic human proxies to generate synthetic\ndata that mimics human decision-making. To evaluate this, we conduct three\nexperiments in a grid-world capture game inspired by Stag Hunt, a game theory\nparadigm that balances risk and reward. In Experiment 1, we compare decisions\nfrom 30 human participants and 2 expert judges with outputs from LLaMA 3.1 and\nMixtral 8x22B models. LLMs, prompted with game-state observations and reward\nstructures, align more closely with experts than participants, demonstrating\nconsistency in applying underlying decision criteria. Experiment 2 modifies\nprompts to induce risk-sensitive strategies (e.g. \"be risk averse\"). LLM\noutputs mirror human participants' variability, shifting between risk-averse\nand risk-seeking behaviours. Finally, Experiment 3 tests LLMs in a dynamic\ngrid-world where the LLM agents generate movement actions. LLMs produce\ntrajectories resembling human participants' paths. While LLMs cannot yet fully\nreplicate human adaptability, their prompt-guided diversity offers a scalable\nfoundation for simulating policy-agnostic teammates."
                },
                "authors": [
                    {
                        "name": "Aju Ani Justus"
                    },
                    {
                        "name": "Chris Baber"
                    }
                ],
                "author_detail": {
                    "name": "Chris Baber"
                },
                "author": "Chris Baber",
                "arxiv_comment": "This is a preprint of a paper presented at the \\textit{European\n  Conference on Artificial Intelligence (ECAI 2025)}. It is made publicly\n  available for the benefit of the research community and should be regarded as\n  a preprint rather than a formally reviewed publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03498v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03498v3",
                "updated": "2025-10-07T17:20:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    20,
                    52,
                    1,
                    280,
                    0
                ],
                "published": "2025-09-03T17:29:50Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    29,
                    50,
                    2,
                    246,
                    0
                ],
                "title": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and\n  Generation"
                },
                "summary": "We introduce OneCAT, a unified multimodal model that seamlessly integrates\nunderstanding, generation, and editing within a novel, pure decoder-only\ntransformer architecture. Our framework uniquely eliminates the need for\nexternal components such as Vision Transformers (ViT) or vision tokenizer\nduring inference, leading to significant efficiency gains, especially for\nhigh-resolution inputs. This is achieved through a modality-specific\nMixture-of-Experts (MoE) structure trained with a single autoregressive (AR)\nobjective, which also natively supports dynamic resolutions. Furthermore, we\npioneer a multi-scale visual autoregressive mechanism within the Large Language\nModel (LLM) that drastically reduces decoding steps compared to diffusion-based\nmethods while maintaining state-of-the-art performance. Our findings\ndemonstrate the powerful potential of pure autoregressive modeling as a\nsufficient and elegant foundation for unified multimodal intelligence. As a\nresult, OneCAT sets a new performance standard, outperforming existing\nopen-source unified multimodal models across benchmarks for multimodal\ngeneration, editing, and understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce OneCAT, a unified multimodal model that seamlessly integrates\nunderstanding, generation, and editing within a novel, pure decoder-only\ntransformer architecture. Our framework uniquely eliminates the need for\nexternal components such as Vision Transformers (ViT) or vision tokenizer\nduring inference, leading to significant efficiency gains, especially for\nhigh-resolution inputs. This is achieved through a modality-specific\nMixture-of-Experts (MoE) structure trained with a single autoregressive (AR)\nobjective, which also natively supports dynamic resolutions. Furthermore, we\npioneer a multi-scale visual autoregressive mechanism within the Large Language\nModel (LLM) that drastically reduces decoding steps compared to diffusion-based\nmethods while maintaining state-of-the-art performance. Our findings\ndemonstrate the powerful potential of pure autoregressive modeling as a\nsufficient and elegant foundation for unified multimodal intelligence. As a\nresult, OneCAT sets a new performance standard, outperforming existing\nopen-source unified multimodal models across benchmarks for multimodal\ngeneration, editing, and understanding."
                },
                "authors": [
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Xinyu Peng"
                    },
                    {
                        "name": "Yaoming Wang"
                    },
                    {
                        "name": "Zelin Peng"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Wenrui Dai"
                    },
                    {
                        "name": "Hongkai Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hongkai Xiong"
                },
                "author": "Hongkai Xiong",
                "arxiv_comment": "technical report, project url:https://onecat-ai.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03498v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03498v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15510v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15510v4",
                "updated": "2025-10-07T17:20:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    20,
                    30,
                    1,
                    280,
                    0
                ],
                "published": "2024-08-28T03:45:49Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    45,
                    49,
                    2,
                    241,
                    0
                ],
                "title": "How Reliable are Causal Probing Interventions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Reliable are Causal Probing Interventions?"
                },
                "summary": "Causal probing aims to analyze foundation models by examining how intervening\non their representation of various latent properties impacts their outputs.\nRecent works have cast doubt on the theoretical basis of several leading causal\nprobing methods, but it has been unclear how to systematically evaluate the\neffectiveness of these methods in practice. To address this, we define two key\ncausal probing desiderata: completeness (how thoroughly the representation of\nthe target property has been transformed) and selectivity (how little\nnon-targeted properties have been impacted). We find that there is an inherent\ntradeoff between the two, which we define as reliability, their harmonic mean.\nWe introduce an empirical analysis framework to measure and evaluate these\nquantities, allowing us to make the first direct comparisons between different\nfamilies of leading causal probing methods (e.g., linear vs. nonlinear, or\nconcept removal vs. counterfactual interventions). We find that: (1) all\nmethods show a clear tradeoff between completeness and selectivity; (2) more\ncomplete and reliable methods have a greater impact on LLM behavior; and (3)\nnonlinear interventions are almost always more reliable than linear\ninterventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal probing aims to analyze foundation models by examining how intervening\non their representation of various latent properties impacts their outputs.\nRecent works have cast doubt on the theoretical basis of several leading causal\nprobing methods, but it has been unclear how to systematically evaluate the\neffectiveness of these methods in practice. To address this, we define two key\ncausal probing desiderata: completeness (how thoroughly the representation of\nthe target property has been transformed) and selectivity (how little\nnon-targeted properties have been impacted). We find that there is an inherent\ntradeoff between the two, which we define as reliability, their harmonic mean.\nWe introduce an empirical analysis framework to measure and evaluate these\nquantities, allowing us to make the first direct comparisons between different\nfamilies of leading causal probing methods (e.g., linear vs. nonlinear, or\nconcept removal vs. counterfactual interventions). We find that: (1) all\nmethods show a clear tradeoff between completeness and selectivity; (2) more\ncomplete and reliable methods have a greater impact on LLM behavior; and (3)\nnonlinear interventions are almost always more reliable than linear\ninterventions."
                },
                "authors": [
                    {
                        "name": "Marc Canby"
                    },
                    {
                        "name": "Adam Davies"
                    },
                    {
                        "name": "Chirag Rastogi"
                    },
                    {
                        "name": "Julia Hockenmaier"
                    }
                ],
                "author_detail": {
                    "name": "Julia Hockenmaier"
                },
                "author": "Julia Hockenmaier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15510v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15510v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03408v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03408v2",
                "updated": "2025-10-07T17:20:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    20,
                    13,
                    1,
                    280,
                    0
                ],
                "published": "2025-06-03T21:36:56Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    21,
                    36,
                    56,
                    1,
                    154,
                    0
                ],
                "title": "Trajectory Prediction Meets Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Prediction Meets Large Language Models: A Survey"
                },
                "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin integrating language-driven techniques into trajectory prediction. By\nleveraging their semantic and reasoning capabilities, LLMs are reshaping how\nautonomous systems perceive, model, and predict trajectories. This survey\nprovides a comprehensive overview of this emerging field, categorizing recent\nwork into five directions: (1) Trajectory prediction via language modeling\nparadigms, (2) Direct trajectory prediction with pretrained language models,\n(3) Language-guided scene understanding for trajectory prediction, (4)\nLanguage-driven data generation for trajectory prediction, (5) Language-based\nreasoning and interpretability for trajectory prediction. For each, we analyze\nrepresentative methods, highlight core design choices, and identify open\nchallenges. This survey bridges natural language processing and trajectory\nprediction, offering a unified perspective on how language can enrich\ntrajectory prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have sparked growing interest\nin integrating language-driven techniques into trajectory prediction. By\nleveraging their semantic and reasoning capabilities, LLMs are reshaping how\nautonomous systems perceive, model, and predict trajectories. This survey\nprovides a comprehensive overview of this emerging field, categorizing recent\nwork into five directions: (1) Trajectory prediction via language modeling\nparadigms, (2) Direct trajectory prediction with pretrained language models,\n(3) Language-guided scene understanding for trajectory prediction, (4)\nLanguage-driven data generation for trajectory prediction, (5) Language-based\nreasoning and interpretability for trajectory prediction. For each, we analyze\nrepresentative methods, highlight core design choices, and identify open\nchallenges. This survey bridges natural language processing and trajectory\nprediction, offering a unified perspective on how language can enrich\ntrajectory prediction."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Ruining Yang"
                    },
                    {
                        "name": "Yitian Zhang"
                    },
                    {
                        "name": "Jianglin Lu"
                    },
                    {
                        "name": "Mingyuan Zhang"
                    },
                    {
                        "name": "Yizhou Wang"
                    },
                    {
                        "name": "Lili Su"
                    },
                    {
                        "name": "Yun Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yun Fu"
                },
                "author": "Yun Fu",
                "arxiv_comment": "16 pages, GitHub:\n  https://github.com/colorfulfuture/Awesome-Trajectory-Motion-Prediction-Papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03408v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03408v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06143v1",
                "updated": "2025-10-07T17:17:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    17,
                    14,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:17:14Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    17,
                    14,
                    1,
                    280,
                    0
                ],
                "title": "RoSE: Round-robin Synthetic Data Evaluation for Selecting LLM Generators\n  without Human Test Sets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSE: Round-robin Synthetic Data Evaluation for Selecting LLM Generators\n  without Human Test Sets"
                },
                "summary": "LLMs are powerful generators of synthetic data, which are used for training\nsmaller, specific models. This is especially valuable for low-resource\nlanguages, where human-labelled data is scarce but LLMs can still produce\nhigh-quality text. However, LLMs differ in how useful their outputs are for\ntraining. Selecting the best LLM as a generator is challenging because\nextrinsic evaluation requires costly human annotations (which are often\nunavailable for low-resource languages), while intrinsic metrics correlate\npoorly with downstream performance. We introduce Round robin Synthetic data\nEvaluation (RoSE), a proxy metric for selecting the best LLM generator without\nhuman test sets. RoSE trains a small model on the outputs of a candidate\ngenerator (LLM) and then evaluates it on generated synthetic examples from all\nother candidate LLMs. The final RoSE score is the mean performance of this\nsmall model. Across six LLMs, eleven languages, and three tasks (sentiment,\ntopic, intent), RoSE identifies the optimal generator more often than any other\nintrinsic heuristics. RoSE outperforms intrinsic heuristics and comes within\n0.76 percentage points of the optimal generator baseline. This result is\nmeasured in terms of downstream performance, obtained by training a small model\non the chosen generator's outputs (optimal vs. proxy metric selected) and\nevaluating it on human-labelled test data. Additionally, RoSE is the only\nmetric to achieve a positive correlation with performance on human test data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are powerful generators of synthetic data, which are used for training\nsmaller, specific models. This is especially valuable for low-resource\nlanguages, where human-labelled data is scarce but LLMs can still produce\nhigh-quality text. However, LLMs differ in how useful their outputs are for\ntraining. Selecting the best LLM as a generator is challenging because\nextrinsic evaluation requires costly human annotations (which are often\nunavailable for low-resource languages), while intrinsic metrics correlate\npoorly with downstream performance. We introduce Round robin Synthetic data\nEvaluation (RoSE), a proxy metric for selecting the best LLM generator without\nhuman test sets. RoSE trains a small model on the outputs of a candidate\ngenerator (LLM) and then evaluates it on generated synthetic examples from all\nother candidate LLMs. The final RoSE score is the mean performance of this\nsmall model. Across six LLMs, eleven languages, and three tasks (sentiment,\ntopic, intent), RoSE identifies the optimal generator more often than any other\nintrinsic heuristics. RoSE outperforms intrinsic heuristics and comes within\n0.76 percentage points of the optimal generator baseline. This result is\nmeasured in terms of downstream performance, obtained by training a small model\non the chosen generator's outputs (optimal vs. proxy metric selected) and\nevaluating it on human-labelled test data. Additionally, RoSE is the only\nmetric to achieve a positive correlation with performance on human test data."
                },
                "authors": [
                    {
                        "name": "Jan Cegin"
                    },
                    {
                        "name": "Branislav Pecher"
                    },
                    {
                        "name": "Ivan Srba"
                    },
                    {
                        "name": "Jakub Simko"
                    }
                ],
                "author_detail": {
                    "name": "Jakub Simko"
                },
                "author": "Jakub Simko",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.20043v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.20043v2",
                "updated": "2025-10-07T17:10:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    10,
                    49,
                    1,
                    280,
                    0
                ],
                "published": "2023-10-30T21:59:51Z",
                "published_parsed": [
                    2023,
                    10,
                    30,
                    21,
                    59,
                    51,
                    0,
                    303,
                    0
                ],
                "title": "On the Universality of Energy Extraction from Black Hole Spacetimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Universality of Energy Extraction from Black Hole Spacetimes"
                },
                "summary": "The launching of astrophysical jets provides the most compelling\nobservational evidence for direct extraction of black hole (BH) spin energy via\nthe Blandford-Znajek (BZ) mechanism. Whilst it is known that spinning Kerr BHs\nwithin general relativity (GR) follow the BZ jet power relation, the nature of\nBH energy extraction in general theories of gravity has not been adequately\naddressed. This study performs the first comprehensive investigation of the BZ\njet power relation by utilizing a generalized BH spacetime geometry which\ndescribes parametric deviations from the Kerr metric of GR, yet recovers the\nKerr metric in the limit that all deviation parameters vanish. Through\nperforming and analyzing an extensive suite of three-dimensional covariant\nmagnetohydrodynamics (MHD) simulations of magnetized gas accretion onto these\ngeneralized BH spacetimes we find that the BZ jet power relation still holds,\nin some instances yielding jet powers far in excess of what can be produced by\neven extremal Kerr BHs. It is shown that independent variation of the\nframe-dragging rate of the BH can enhance or suppress the effects of BH spin,\nand by extension of frame-dragging. This variation greatly enhances or\nsuppresses the observed jet power and underlying photon ring image asymmetry,\nintroducing a previously unexplored yet important degeneracy in BH parameter\ninference. Finally we show that sufficiently accurate measurements of the jet\npower, accretion rate and photon ring properties from supermassive BHs can\npotentially break this degeneracy, highlighting the need of independent\ninvestigations of BH frame-dragging from observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The launching of astrophysical jets provides the most compelling\nobservational evidence for direct extraction of black hole (BH) spin energy via\nthe Blandford-Znajek (BZ) mechanism. Whilst it is known that spinning Kerr BHs\nwithin general relativity (GR) follow the BZ jet power relation, the nature of\nBH energy extraction in general theories of gravity has not been adequately\naddressed. This study performs the first comprehensive investigation of the BZ\njet power relation by utilizing a generalized BH spacetime geometry which\ndescribes parametric deviations from the Kerr metric of GR, yet recovers the\nKerr metric in the limit that all deviation parameters vanish. Through\nperforming and analyzing an extensive suite of three-dimensional covariant\nmagnetohydrodynamics (MHD) simulations of magnetized gas accretion onto these\ngeneralized BH spacetimes we find that the BZ jet power relation still holds,\nin some instances yielding jet powers far in excess of what can be produced by\neven extremal Kerr BHs. It is shown that independent variation of the\nframe-dragging rate of the BH can enhance or suppress the effects of BH spin,\nand by extension of frame-dragging. This variation greatly enhances or\nsuppresses the observed jet power and underlying photon ring image asymmetry,\nintroducing a previously unexplored yet important degeneracy in BH parameter\ninference. Finally we show that sufficiently accurate measurements of the jet\npower, accretion rate and photon ring properties from supermassive BHs can\npotentially break this degeneracy, highlighting the need of independent\ninvestigations of BH frame-dragging from observations."
                },
                "authors": [
                    {
                        "name": "Koushik Chatterjee"
                    },
                    {
                        "name": "Ziri Younsi"
                    },
                    {
                        "name": "Prashant Kocherlakota"
                    },
                    {
                        "name": "Ramesh Narayan"
                    }
                ],
                "author_detail": {
                    "name": "Ramesh Narayan"
                },
                "author": "Ramesh Narayan",
                "arxiv_doi": "10.3847/2041-8213/ae0740",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/2041-8213/ae0740",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.20043v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.20043v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in ApJL",
                "arxiv_journal_ref": "Chatterjee K., Younsi Z., Kocherlakota P., Narayan R., 2025, ApJL,\n  991, L58",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06133v1",
                "updated": "2025-10-07T17:08:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    8,
                    33,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:08:33Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    8,
                    33,
                    1,
                    280,
                    0
                ],
                "title": "CreditDecoding: Accelerating Parallel Decoding in Diffusion Large\n  Language Models with Trace Credits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CreditDecoding: Accelerating Parallel Decoding in Diffusion Large\n  Language Models with Trace Credits"
                },
                "summary": "Diffusion large language models (dLLMs) generate text through iterative\ndenoising steps, achieving parallel decoding by denoising only high-confidence\npositions at each step. However, existing approaches often repetitively remask\ntokens due to initially low confidence scores, leading to redundant iterations\nand limiting overall acceleration. Through the analysis of dLLM decoding\ntraces, we observe that the model often determines the final prediction for a\ntoken several steps before the decoding step. To leverage this historical\ninformation and avoid redundant steps, we introduce the concept of Trace\nCredit, which quantifies each token's convergence potential by accumulating\nhistorical logits. Furthermore, we propose CreditDecoding, a training-free\nparallel decoding algorithm that accelerates the confidence convergence of\ncorrect but underconfident tokens by fusing current logits with Trace Credit.\nThis process significantly reduces redundant iterations and enhances decoding\nrobustness. On eight benchmarks, CreditDecoding achieves a 5.48 times speedup\nand a 0.48 performance improvement over LLaDA-8B-Instruct, and a 4.11 times\nspeedup with a 0.15 performance improvement over LLaDA-MoE-Instruct.\nImportantly, CreditDecoding scales effectively to long sequences and is\northogonal to mainstream inference optimizations, making it a readily\nintegrable and versatile solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion large language models (dLLMs) generate text through iterative\ndenoising steps, achieving parallel decoding by denoising only high-confidence\npositions at each step. However, existing approaches often repetitively remask\ntokens due to initially low confidence scores, leading to redundant iterations\nand limiting overall acceleration. Through the analysis of dLLM decoding\ntraces, we observe that the model often determines the final prediction for a\ntoken several steps before the decoding step. To leverage this historical\ninformation and avoid redundant steps, we introduce the concept of Trace\nCredit, which quantifies each token's convergence potential by accumulating\nhistorical logits. Furthermore, we propose CreditDecoding, a training-free\nparallel decoding algorithm that accelerates the confidence convergence of\ncorrect but underconfident tokens by fusing current logits with Trace Credit.\nThis process significantly reduces redundant iterations and enhances decoding\nrobustness. On eight benchmarks, CreditDecoding achieves a 5.48 times speedup\nand a 0.48 performance improvement over LLaDA-8B-Instruct, and a 4.11 times\nspeedup with a 0.15 performance improvement over LLaDA-MoE-Instruct.\nImportantly, CreditDecoding scales effectively to long sequences and is\northogonal to mainstream inference optimizations, making it a readily\nintegrable and versatile solution."
                },
                "authors": [
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Haibo Feng"
                    },
                    {
                        "name": "Weijia Zhao"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin",
                "arxiv_comment": "18 pages,8 figures,4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09947v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09947v2",
                "updated": "2025-10-07T17:06:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    6,
                    21,
                    1,
                    280,
                    0
                ],
                "published": "2025-09-12T03:38:15Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    3,
                    38,
                    15,
                    4,
                    255,
                    0
                ],
                "title": "Toward Green Code: Prompting Small Language Models for Energy-Efficient\n  Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Green Code: Prompting Small Language Models for Energy-Efficient\n  Code Generation"
                },
                "summary": "There is a growing concern about the environmental impact of large language\nmodels (LLMs) in software development, particularly due to their high energy\nuse and carbon footprint. Small Language Models (SLMs) offer a more sustainable\nalternative, requiring fewer computational resources while remaining effective\nfor fundamental programming tasks. In this study, we investigate whether prompt\nengineering can improve the energy efficiency of SLMs in code generation. We\nevaluate four open-source SLMs, StableCode-Instruct-3B,\nQwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, and Phi-3-Mini-4K-Instruct,\nacross 150 Python problems from LeetCode, evenly distributed into easy, medium,\nand hard categories. Each model is tested under four prompting strategies: role\nprompting, zero-shot, few-shot, and chain-of-thought (CoT). For every generated\nsolution, we measure runtime, memory usage, and energy consumption, comparing\nthe results with a human-written baseline. Our findings show that CoT prompting\nprovides consistent energy savings for Qwen2.5-Coder and StableCode-3B, while\nCodeLlama-7B and Phi-3-Mini-4K fail to outperform the baseline under any\nprompting strategy. These results highlight that the benefits of prompting are\nmodel-dependent and that carefully designed prompts can guide SLMs toward\ngreener software development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing concern about the environmental impact of large language\nmodels (LLMs) in software development, particularly due to their high energy\nuse and carbon footprint. Small Language Models (SLMs) offer a more sustainable\nalternative, requiring fewer computational resources while remaining effective\nfor fundamental programming tasks. In this study, we investigate whether prompt\nengineering can improve the energy efficiency of SLMs in code generation. We\nevaluate four open-source SLMs, StableCode-Instruct-3B,\nQwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, and Phi-3-Mini-4K-Instruct,\nacross 150 Python problems from LeetCode, evenly distributed into easy, medium,\nand hard categories. Each model is tested under four prompting strategies: role\nprompting, zero-shot, few-shot, and chain-of-thought (CoT). For every generated\nsolution, we measure runtime, memory usage, and energy consumption, comparing\nthe results with a human-written baseline. Our findings show that CoT prompting\nprovides consistent energy savings for Qwen2.5-Coder and StableCode-3B, while\nCodeLlama-7B and Phi-3-Mini-4K fail to outperform the baseline under any\nprompting strategy. These results highlight that the benefits of prompting are\nmodel-dependent and that carefully designed prompts can guide SLMs toward\ngreener software development."
                },
                "authors": [
                    {
                        "name": "Humza Ashraf"
                    },
                    {
                        "name": "Syed Muhammad Danish"
                    },
                    {
                        "name": "Shadikur Rahman"
                    },
                    {
                        "name": "Zeeshan Sattar"
                    }
                ],
                "author_detail": {
                    "name": "Zeeshan Sattar"
                },
                "author": "Zeeshan Sattar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09947v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09947v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06126v1",
                "updated": "2025-10-07T17:05:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    5,
                    30,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:05:30Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    5,
                    30,
                    1,
                    280,
                    0
                ],
                "title": "lm-Meter: Unveiling Runtime Inference Latency for On-Device Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "lm-Meter: Unveiling Runtime Inference Latency for On-Device Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly integrated into everyday\napplications, but their prevalent cloud-based deployment raises growing\nconcerns around data privacy and long-term sustainability. Running LLMs locally\non mobile and edge devices (on-device LLMs) offers the promise of enhanced\nprivacy, reliability, and reduced communication costs. However, realizing this\nvision remains challenging due to substantial memory and compute demands, as\nwell as limited visibility into performance-efficiency trade-offs on\nresource-constrained hardware. We propose lm-Meter, the first lightweight,\nonline latency profiler tailored for on-device LLM inference. lm-Meter captures\nfine-grained, real-time latency at both phase (e.g., embedding, prefill,\ndecode, softmax, sampling) and kernel levels without auxiliary devices. We\nimplement lm-Meter on commercial mobile platforms and demonstrate its high\nprofiling accuracy with minimal system overhead, e.g., only 2.58% throughput\nreduction in prefill and 0.99% in decode under the most constrained Powersave\ngovernor. Leveraging lm-Meter, we conduct comprehensive empirical studies\nrevealing phase- and kernel-level bottlenecks in on-device LLM inference,\nquantifying accuracy-efficiency trade-offs, and identifying systematic\noptimization opportunities. lm-Meter provides unprecedented visibility into the\nruntime behavior of LLMs on constrained platforms, laying the foundation for\ninformed optimization and accelerating the democratization of on-device LLM\nsystems. Code and tutorials are available at\nhttps://github.com/amai-gsu/LM-Meter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly integrated into everyday\napplications, but their prevalent cloud-based deployment raises growing\nconcerns around data privacy and long-term sustainability. Running LLMs locally\non mobile and edge devices (on-device LLMs) offers the promise of enhanced\nprivacy, reliability, and reduced communication costs. However, realizing this\nvision remains challenging due to substantial memory and compute demands, as\nwell as limited visibility into performance-efficiency trade-offs on\nresource-constrained hardware. We propose lm-Meter, the first lightweight,\nonline latency profiler tailored for on-device LLM inference. lm-Meter captures\nfine-grained, real-time latency at both phase (e.g., embedding, prefill,\ndecode, softmax, sampling) and kernel levels without auxiliary devices. We\nimplement lm-Meter on commercial mobile platforms and demonstrate its high\nprofiling accuracy with minimal system overhead, e.g., only 2.58% throughput\nreduction in prefill and 0.99% in decode under the most constrained Powersave\ngovernor. Leveraging lm-Meter, we conduct comprehensive empirical studies\nrevealing phase- and kernel-level bottlenecks in on-device LLM inference,\nquantifying accuracy-efficiency trade-offs, and identifying systematic\noptimization opportunities. lm-Meter provides unprecedented visibility into the\nruntime behavior of LLMs on constrained platforms, laying the foundation for\ninformed optimization and accelerating the democratization of on-device LLM\nsystems. Code and tutorials are available at\nhttps://github.com/amai-gsu/LM-Meter."
                },
                "authors": [
                    {
                        "name": "Haoxin Wang"
                    },
                    {
                        "name": "Xiaolong Tu"
                    },
                    {
                        "name": "Hongyu Ke"
                    },
                    {
                        "name": "Huirong Chai"
                    },
                    {
                        "name": "Dawei Chen"
                    },
                    {
                        "name": "Kyungtae Han"
                    }
                ],
                "author_detail": {
                    "name": "Kyungtae Han"
                },
                "author": "Kyungtae Han",
                "arxiv_doi": "10.1145/3769012.3770614",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3769012.3770614",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.06126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is the preprint version of the paper accepted to The 10th\n  ACM/IEEE Symposium on Edge Computing (SEC 2025)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17242v2",
                "updated": "2025-10-07T16:58:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    58,
                    55,
                    1,
                    280,
                    0
                ],
                "published": "2025-05-22T19:43:37Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    19,
                    43,
                    37,
                    3,
                    142,
                    0
                ],
                "title": "Optimal Policy Minimum Bayesian Risk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Policy Minimum Bayesian Risk"
                },
                "summary": "Inference scaling helps LLMs solve complex reasoning problems through\nextended runtime computation. On top of long chain-of-thought (long-CoT)\nmodels, purely inference-time techniques such as best-of-N (BoN) sampling,\nmajority voting, or more generally, minimum Bayes risk decoding (MBRD), can\nfurther improve LLM accuracy by generating multiple candidate solutions and\naggregating over them. These methods typically leverage additional signals in\nthe form of reward models and risk/similarity functions that compare generated\nsamples, e.g., exact match in some normalized space or standard similarity\nmetrics such as Rouge. Here we present a novel method for incorporating reward\nand risk/similarity signals into MBRD. Based on the concept of optimal policy\nin KL-controlled reinforcement learning, our framework provides a simple and\nwell-defined mechanism for leveraging such signals, offering several advantages\nover traditional inference-time methods: higher robustness, improved accuracy,\nand well-understood asymptotic behavior. In addition, it allows for the\ndevelopment of a sample-efficient variant of MBRD that can adjust the number of\nsamples to generate according to the difficulty of the problem, without relying\non majority vote counts. We empirically demonstrate the advantages of our\napproach on math (MATH-$500$) and coding (HumanEval) tasks using recent\nopen-source models. We also present a comprehensive analysis of its\naccuracy-compute trade-offs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference scaling helps LLMs solve complex reasoning problems through\nextended runtime computation. On top of long chain-of-thought (long-CoT)\nmodels, purely inference-time techniques such as best-of-N (BoN) sampling,\nmajority voting, or more generally, minimum Bayes risk decoding (MBRD), can\nfurther improve LLM accuracy by generating multiple candidate solutions and\naggregating over them. These methods typically leverage additional signals in\nthe form of reward models and risk/similarity functions that compare generated\nsamples, e.g., exact match in some normalized space or standard similarity\nmetrics such as Rouge. Here we present a novel method for incorporating reward\nand risk/similarity signals into MBRD. Based on the concept of optimal policy\nin KL-controlled reinforcement learning, our framework provides a simple and\nwell-defined mechanism for leveraging such signals, offering several advantages\nover traditional inference-time methods: higher robustness, improved accuracy,\nand well-understood asymptotic behavior. In addition, it allows for the\ndevelopment of a sample-efficient variant of MBRD that can adjust the number of\nsamples to generate according to the difficulty of the problem, without relying\non majority vote counts. We empirically demonstrate the advantages of our\napproach on math (MATH-$500$) and coding (HumanEval) tasks using recent\nopen-source models. We also present a comprehensive analysis of its\naccuracy-compute trade-offs."
                },
                "authors": [
                    {
                        "name": "Ramón Fernandez Astudillo"
                    },
                    {
                        "name": "Md Arafat Sultan"
                    },
                    {
                        "name": "Aashka Trivedi"
                    },
                    {
                        "name": "Yousef El-Kurdi"
                    },
                    {
                        "name": "Tahira Naseem"
                    },
                    {
                        "name": "Radu Florian"
                    },
                    {
                        "name": "Salim Roukos"
                    }
                ],
                "author_detail": {
                    "name": "Salim Roukos"
                },
                "author": "Salim Roukos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06114v1",
                "updated": "2025-10-07T16:50:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    50,
                    26,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:50:26Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    50,
                    26,
                    1,
                    280,
                    0
                ],
                "title": "Multiprobe constraints on early and late time dark energy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiprobe constraints on early and late time dark energy"
                },
                "summary": "We perform a multiprobe analysis combining cosmic microwave background (CMB)\ndata from Planck and the Atacama Cosmology Telescope (ACT), ACT CMB lensing,\nand large-scale structure (LSS) measurements from the Dark Energy Spectroscopic\nInstrument (DESI), including DESI Legacy Imaging Survey (LS) galaxies and\nbaryon acoustic oscillations (BAOs). We present the first $5\\times2$pt analysis\nof ACT DR6 lensing, DESI LS, and Planck ISW. Within $\\Lambda$CDM, this yields\n$S_8 = \\sigma_8(\\Omega_m/0.3)^{0.5} = 0.819 \\pm 0.016$, in good agreement with\nprimary CMB inferences and provides a sound-horizon-free Hubble constant\nconstraint of $H_0 = 70.0 \\pm 4.4$ km s$^{-1}$ Mpc$^{-1}$. Then, combining with\nCMB primary and BAO, we reconfirm a CMB-BAO discrepancy in the\n$\\Omega_m$-$\\frac{D_v}{r_d}$ plane, which is heightened when combining BAO with\nthe $5\\times2$pt data vector. We explore two dark-energy extensions that may\nreconcile this: an early-time modification, early dark energy (EDE), and\nlate-time dynamical dark energy (DDE) parameterized by $w_0w_a$. For CMB\nprimary+BAO+$5\\times2$pt, we find a $3.3\\sigma$ preference for DDE over\n$\\Lambda$CDM, while EDE is modestly favoured at $2.3\\sigma$. The models address\ndifferent shortcomings of $\\Lambda$CDM: DDE relaxes the neutrino mass bound\n($M_\\nu<0.17$eV vs. $<0.050$eV under $\\Lambda$CDM), making it compatible with\nneutrino oscillation measurements, while EDE raises the Hubble constant to\n$H_0=70.5\\pm1.2\\,\\mathrm{km\\,s^{-1}\\,Mpc^{-1}}$, easing the discrepancy with\nSH0ES. However, neither model resolves both issues simultaneously. Our analysis\nindicates that both DDE and EDE remain viable extensions of $\\Lambda$CDM within\ncurrent uncertainties and demonstrates the capacity of combined probes to place\nincreasingly stringent constraints on cosmological parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We perform a multiprobe analysis combining cosmic microwave background (CMB)\ndata from Planck and the Atacama Cosmology Telescope (ACT), ACT CMB lensing,\nand large-scale structure (LSS) measurements from the Dark Energy Spectroscopic\nInstrument (DESI), including DESI Legacy Imaging Survey (LS) galaxies and\nbaryon acoustic oscillations (BAOs). We present the first $5\\times2$pt analysis\nof ACT DR6 lensing, DESI LS, and Planck ISW. Within $\\Lambda$CDM, this yields\n$S_8 = \\sigma_8(\\Omega_m/0.3)^{0.5} = 0.819 \\pm 0.016$, in good agreement with\nprimary CMB inferences and provides a sound-horizon-free Hubble constant\nconstraint of $H_0 = 70.0 \\pm 4.4$ km s$^{-1}$ Mpc$^{-1}$. Then, combining with\nCMB primary and BAO, we reconfirm a CMB-BAO discrepancy in the\n$\\Omega_m$-$\\frac{D_v}{r_d}$ plane, which is heightened when combining BAO with\nthe $5\\times2$pt data vector. We explore two dark-energy extensions that may\nreconcile this: an early-time modification, early dark energy (EDE), and\nlate-time dynamical dark energy (DDE) parameterized by $w_0w_a$. For CMB\nprimary+BAO+$5\\times2$pt, we find a $3.3\\sigma$ preference for DDE over\n$\\Lambda$CDM, while EDE is modestly favoured at $2.3\\sigma$. The models address\ndifferent shortcomings of $\\Lambda$CDM: DDE relaxes the neutrino mass bound\n($M_\\nu<0.17$eV vs. $<0.050$eV under $\\Lambda$CDM), making it compatible with\nneutrino oscillation measurements, while EDE raises the Hubble constant to\n$H_0=70.5\\pm1.2\\,\\mathrm{km\\,s^{-1}\\,Mpc^{-1}}$, easing the discrepancy with\nSH0ES. However, neither model resolves both issues simultaneously. Our analysis\nindicates that both DDE and EDE remain viable extensions of $\\Lambda$CDM within\ncurrent uncertainties and demonstrates the capacity of combined probes to place\nincreasingly stringent constraints on cosmological parameters."
                },
                "authors": [
                    {
                        "name": "Alexander Reeves"
                    },
                    {
                        "name": "Simone Ferraro"
                    },
                    {
                        "name": "Andrina Nicola"
                    },
                    {
                        "name": "Alexandre Refregier"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Refregier"
                },
                "author": "Alexandre Refregier",
                "arxiv_comment": "26 pages, 15 figures, comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06113v1",
                "updated": "2025-10-07T16:49:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    49,
                    52,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:49:52Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    49,
                    52,
                    1,
                    280,
                    0
                ],
                "title": "Multimodal Feature Prototype Learning for Interpretable and\n  Discriminative Cancer Survival Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Feature Prototype Learning for Interpretable and\n  Discriminative Cancer Survival Prediction"
                },
                "summary": "Survival analysis plays a vital role in making clinical decisions. However,\nthe models currently in use are often difficult to interpret, which reduces\ntheir usefulness in clinical settings. Prototype learning presents a potential\nsolution, yet traditional methods focus on local similarities and static\nmatching, neglecting the broader tumor context and lacking strong semantic\nalignment with genomic data. To overcome these issues, we introduce an\ninnovative prototype-based multimodal framework, FeatProto, aimed at enhancing\ncancer survival prediction by addressing significant limitations in current\nprototype learning methodologies within pathology. Our framework establishes a\nunified feature prototype space that integrates both global and local features\nof whole slide images (WSI) with genomic profiles. This integration facilitates\ntraceable and interpretable decision-making processes. Our approach includes\nthree main innovations: (1) A robust phenotype representation that merges\ncritical patches with global context, harmonized with genomic data to minimize\nlocal bias. (2) An Exponential Prototype Update Strategy (EMA ProtoUp) that\nsustains stable cross-modal associations and employs a wandering mechanism to\nadapt prototypes flexibly to tumor heterogeneity. (3) A hierarchical prototype\nmatching scheme designed to capture global centrality, local typicality, and\ncohort-level trends, thereby refining prototype inference. Comprehensive\nevaluations on four publicly available cancer datasets indicate that our method\nsurpasses current leading unimodal and multimodal survival prediction\ntechniques in both accuracy and interoperability, providing a new perspective\non prototype learning for critical medical applications. Our source code is\navailable at https://github.com/JSLiam94/FeatProto.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survival analysis plays a vital role in making clinical decisions. However,\nthe models currently in use are often difficult to interpret, which reduces\ntheir usefulness in clinical settings. Prototype learning presents a potential\nsolution, yet traditional methods focus on local similarities and static\nmatching, neglecting the broader tumor context and lacking strong semantic\nalignment with genomic data. To overcome these issues, we introduce an\ninnovative prototype-based multimodal framework, FeatProto, aimed at enhancing\ncancer survival prediction by addressing significant limitations in current\nprototype learning methodologies within pathology. Our framework establishes a\nunified feature prototype space that integrates both global and local features\nof whole slide images (WSI) with genomic profiles. This integration facilitates\ntraceable and interpretable decision-making processes. Our approach includes\nthree main innovations: (1) A robust phenotype representation that merges\ncritical patches with global context, harmonized with genomic data to minimize\nlocal bias. (2) An Exponential Prototype Update Strategy (EMA ProtoUp) that\nsustains stable cross-modal associations and employs a wandering mechanism to\nadapt prototypes flexibly to tumor heterogeneity. (3) A hierarchical prototype\nmatching scheme designed to capture global centrality, local typicality, and\ncohort-level trends, thereby refining prototype inference. Comprehensive\nevaluations on four publicly available cancer datasets indicate that our method\nsurpasses current leading unimodal and multimodal survival prediction\ntechniques in both accuracy and interoperability, providing a new perspective\non prototype learning for critical medical applications. Our source code is\navailable at https://github.com/JSLiam94/FeatProto."
                },
                "authors": [
                    {
                        "name": "Shuo Jiang"
                    },
                    {
                        "name": "Zhuwen Chen"
                    },
                    {
                        "name": "Liaoman Xu"
                    },
                    {
                        "name": "Yanming Zhu"
                    },
                    {
                        "name": "Changmiao Wang"
                    },
                    {
                        "name": "Jiong Zhang"
                    },
                    {
                        "name": "Feiwei Qin"
                    },
                    {
                        "name": "Yifei Chen"
                    },
                    {
                        "name": "Zhu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Zhu Zhu"
                },
                "author": "Zhu Zhu",
                "arxiv_comment": "12 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04348v2",
                "updated": "2025-10-07T16:49:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    49,
                    15,
                    1,
                    280,
                    0
                ],
                "published": "2025-09-04T16:05:54Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    5,
                    54,
                    3,
                    247,
                    0
                ],
                "title": "GWTC-4.0: Constraints on the Cosmic Expansion Rate and Modified\n  Gravitational-wave Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GWTC-4.0: Constraints on the Cosmic Expansion Rate and Modified\n  Gravitational-wave Propagation"
                },
                "summary": "We analyze data from 142 of the 218 gravitational-wave (GW) sources in the\nfourth LIGO-Virgo-KAGRA Collaboration (LVK) Gravitational-Wave Transient\nCatalog (GWTC-4.0) to estimate the Hubble constant $H_0$ jointly with the\npopulation properties of merging compact binaries. We measure the luminosity\ndistance and redshifted masses of GW sources directly; in contrast, we infer GW\nsource redshifts statistically through i) location of features in the compact\nobject mass spectrum and merger rate evolution, and ii) identifying potential\nhost galaxies in the GW localization volume. Probing the relationship between\nsource luminosity distances and redshifts obtained in this way yields\nconstraints on cosmological parameters. We also constrain parameterized\ndeviations from general relativity which affect GW propagation, specifically\nthose modifying the dependence of a GW signal on the source luminosity\ndistance. Assuming our fiducial model for the source-frame mass distribution\nand using GW candidates detected up to the end of the fourth observing run\n(O4a), together with the GLADE+ all-sky galaxy catalog, we estimate $H_0 =\n76.6^{+13.0}_{-9.5} (76.6^{+25.2}_{-14.0})$ km s$^{-1}$ Mpc$^{-1}$. This value\nis reported as a median with 68.3% (90%) symmetric credible interval, and\nincludes combination with the $H_0$ measurement from GW170817 and its\nelectromagnetic counterpart. Using a parametrization of modified GW propagation\nin terms of the magnitude parameter $\\Xi_0$, we estimate $\\Xi_0 =\n1.2^{+0.8}_{-0.4} (1.2^{+2.4}_{-0.5})$, where $\\Xi_0 = 1$ recovers the behavior\nof general relativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze data from 142 of the 218 gravitational-wave (GW) sources in the\nfourth LIGO-Virgo-KAGRA Collaboration (LVK) Gravitational-Wave Transient\nCatalog (GWTC-4.0) to estimate the Hubble constant $H_0$ jointly with the\npopulation properties of merging compact binaries. We measure the luminosity\ndistance and redshifted masses of GW sources directly; in contrast, we infer GW\nsource redshifts statistically through i) location of features in the compact\nobject mass spectrum and merger rate evolution, and ii) identifying potential\nhost galaxies in the GW localization volume. Probing the relationship between\nsource luminosity distances and redshifts obtained in this way yields\nconstraints on cosmological parameters. We also constrain parameterized\ndeviations from general relativity which affect GW propagation, specifically\nthose modifying the dependence of a GW signal on the source luminosity\ndistance. Assuming our fiducial model for the source-frame mass distribution\nand using GW candidates detected up to the end of the fourth observing run\n(O4a), together with the GLADE+ all-sky galaxy catalog, we estimate $H_0 =\n76.6^{+13.0}_{-9.5} (76.6^{+25.2}_{-14.0})$ km s$^{-1}$ Mpc$^{-1}$. This value\nis reported as a median with 68.3% (90%) symmetric credible interval, and\nincludes combination with the $H_0$ measurement from GW170817 and its\nelectromagnetic counterpart. Using a parametrization of modified GW propagation\nin terms of the magnitude parameter $\\Xi_0$, we estimate $\\Xi_0 =\n1.2^{+0.8}_{-0.4} (1.2^{+2.4}_{-0.5})$, where $\\Xi_0 = 1$ recovers the behavior\nof general relativity."
                },
                "authors": [
                    {
                        "name": "The LIGO Scientific Collaboration"
                    },
                    {
                        "name": "the Virgo Collaboration"
                    },
                    {
                        "name": "the KAGRA Collaboration"
                    },
                    {
                        "name": "A. G. Abac"
                    },
                    {
                        "name": "I. Abouelfettouh"
                    },
                    {
                        "name": "F. Acernese"
                    },
                    {
                        "name": "K. Ackley"
                    },
                    {
                        "name": "C. Adamcewicz"
                    },
                    {
                        "name": "S. Adhicary"
                    },
                    {
                        "name": "D. Adhikari"
                    },
                    {
                        "name": "N. Adhikari"
                    },
                    {
                        "name": "R. X. Adhikari"
                    },
                    {
                        "name": "V. K. Adkins"
                    },
                    {
                        "name": "S. Afroz"
                    },
                    {
                        "name": "A. Agapito"
                    },
                    {
                        "name": "D. Agarwal"
                    },
                    {
                        "name": "M. Agathos"
                    },
                    {
                        "name": "N. Aggarwal"
                    },
                    {
                        "name": "S. Aggarwal"
                    },
                    {
                        "name": "O. D. Aguiar"
                    },
                    {
                        "name": "I. -L. Ahrend"
                    },
                    {
                        "name": "L. Aiello"
                    },
                    {
                        "name": "A. Ain"
                    },
                    {
                        "name": "P. Ajith"
                    },
                    {
                        "name": "T. Akutsu"
                    },
                    {
                        "name": "S. Albanesi"
                    },
                    {
                        "name": "W. Ali"
                    },
                    {
                        "name": "S. Al-Kershi"
                    },
                    {
                        "name": "C. Alléné"
                    },
                    {
                        "name": "A. Allocca"
                    },
                    {
                        "name": "S. Al-Shammari"
                    },
                    {
                        "name": "P. A. Altin"
                    },
                    {
                        "name": "S. Alvarez-Lopez"
                    },
                    {
                        "name": "W. Amar"
                    },
                    {
                        "name": "O. Amarasinghe"
                    },
                    {
                        "name": "A. Amato"
                    },
                    {
                        "name": "F. Amicucci"
                    },
                    {
                        "name": "C. Amra"
                    },
                    {
                        "name": "A. Ananyeva"
                    },
                    {
                        "name": "S. B. Anderson"
                    },
                    {
                        "name": "W. G. Anderson"
                    },
                    {
                        "name": "M. Andia"
                    },
                    {
                        "name": "M. Ando"
                    },
                    {
                        "name": "M. Andrés-Carcasona"
                    },
                    {
                        "name": "T. Andrić"
                    },
                    {
                        "name": "J. Anglin"
                    },
                    {
                        "name": "S. Ansoldi"
                    },
                    {
                        "name": "J. M. Antelis"
                    },
                    {
                        "name": "S. Antier"
                    },
                    {
                        "name": "M. Aoumi"
                    },
                    {
                        "name": "E. Z. Appavuravther"
                    },
                    {
                        "name": "S. Appert"
                    },
                    {
                        "name": "S. K. Apple"
                    },
                    {
                        "name": "K. Arai"
                    },
                    {
                        "name": "A. Araya"
                    },
                    {
                        "name": "M. C. Araya"
                    },
                    {
                        "name": "M. Arca Sedda"
                    },
                    {
                        "name": "J. S. Areeda"
                    },
                    {
                        "name": "N. Aritomi"
                    },
                    {
                        "name": "F. Armato"
                    },
                    {
                        "name": "S. Armstrong"
                    },
                    {
                        "name": "N. Arnaud"
                    },
                    {
                        "name": "M. Arogeti"
                    },
                    {
                        "name": "S. M. Aronson"
                    },
                    {
                        "name": "K. G. Arun"
                    },
                    {
                        "name": "G. Ashton"
                    },
                    {
                        "name": "Y. Aso"
                    },
                    {
                        "name": "L. Asprea"
                    },
                    {
                        "name": "M. Assiduo"
                    },
                    {
                        "name": "S. Assis de Souza Melo"
                    },
                    {
                        "name": "S. M. Aston"
                    },
                    {
                        "name": "P. Astone"
                    },
                    {
                        "name": "F. Attadio"
                    },
                    {
                        "name": "F. Aubin"
                    },
                    {
                        "name": "K. AultONeal"
                    },
                    {
                        "name": "G. Avallone"
                    },
                    {
                        "name": "E. A. Avila"
                    },
                    {
                        "name": "S. Babak"
                    },
                    {
                        "name": "C. Badger"
                    },
                    {
                        "name": "S. Bae"
                    },
                    {
                        "name": "S. Bagnasco"
                    },
                    {
                        "name": "L. Baiotti"
                    },
                    {
                        "name": "R. Bajpai"
                    },
                    {
                        "name": "T. Baka"
                    },
                    {
                        "name": "A. M. Baker"
                    },
                    {
                        "name": "K. A. Baker"
                    },
                    {
                        "name": "T. Baker"
                    },
                    {
                        "name": "G. Baldi"
                    },
                    {
                        "name": "N. Baldicchi"
                    },
                    {
                        "name": "M. Ball"
                    },
                    {
                        "name": "G. Ballardin"
                    },
                    {
                        "name": "S. W. Ballmer"
                    },
                    {
                        "name": "S. Banagiri"
                    },
                    {
                        "name": "B. Banerjee"
                    },
                    {
                        "name": "D. Bankar"
                    },
                    {
                        "name": "T. M. Baptiste"
                    },
                    {
                        "name": "P. Baral"
                    },
                    {
                        "name": "M. Baratti"
                    },
                    {
                        "name": "J. C. Barayoga"
                    },
                    {
                        "name": "B. C. Barish"
                    },
                    {
                        "name": "D. Barker"
                    },
                    {
                        "name": "N. Barman"
                    },
                    {
                        "name": "P. Barneo"
                    },
                    {
                        "name": "F. Barone"
                    },
                    {
                        "name": "B. Barr"
                    },
                    {
                        "name": "L. Barsotti"
                    },
                    {
                        "name": "M. Barsuglia"
                    },
                    {
                        "name": "D. Barta"
                    },
                    {
                        "name": "A. M. Bartoletti"
                    },
                    {
                        "name": "M. A. Barton"
                    },
                    {
                        "name": "I. Bartos"
                    },
                    {
                        "name": "A. Basalaev"
                    },
                    {
                        "name": "R. Bassiri"
                    },
                    {
                        "name": "A. Basti"
                    },
                    {
                        "name": "M. Bawaj"
                    },
                    {
                        "name": "P. Baxi"
                    },
                    {
                        "name": "J. C. Bayley"
                    },
                    {
                        "name": "A. C. Baylor"
                    },
                    {
                        "name": "P. A. Baynard II"
                    },
                    {
                        "name": "M. Bazzan"
                    },
                    {
                        "name": "V. M. Bedakihale"
                    },
                    {
                        "name": "F. Beirnaert"
                    },
                    {
                        "name": "M. Bejger"
                    },
                    {
                        "name": "D. Belardinelli"
                    },
                    {
                        "name": "A. S. Bell"
                    },
                    {
                        "name": "D. S. Bellie"
                    },
                    {
                        "name": "L. Bellizzi"
                    },
                    {
                        "name": "W. Benoit"
                    },
                    {
                        "name": "I. Bentara"
                    },
                    {
                        "name": "J. D. Bentley"
                    },
                    {
                        "name": "M. Ben Yaala"
                    },
                    {
                        "name": "S. Bera"
                    },
                    {
                        "name": "F. Bergamin"
                    },
                    {
                        "name": "B. K. Berger"
                    },
                    {
                        "name": "S. Bernuzzi"
                    },
                    {
                        "name": "M. Beroiz"
                    },
                    {
                        "name": "C. P. L. Berry"
                    },
                    {
                        "name": "D. Bersanetti"
                    },
                    {
                        "name": "T. Bertheas"
                    },
                    {
                        "name": "A. Bertolini"
                    },
                    {
                        "name": "J. Betzwieser"
                    },
                    {
                        "name": "D. Beveridge"
                    },
                    {
                        "name": "G. Bevilacqua"
                    },
                    {
                        "name": "N. Bevins"
                    },
                    {
                        "name": "R. Bhandare"
                    },
                    {
                        "name": "R. Bhatt"
                    },
                    {
                        "name": "D. Bhattacharjee"
                    },
                    {
                        "name": "S. Bhattacharyya"
                    },
                    {
                        "name": "S. Bhaumik"
                    },
                    {
                        "name": "V. Biancalana"
                    },
                    {
                        "name": "A. Bianchi"
                    },
                    {
                        "name": "I. A. Bilenko"
                    },
                    {
                        "name": "M. Bilicki"
                    },
                    {
                        "name": "G. Billingsley"
                    },
                    {
                        "name": "A. Binetti"
                    },
                    {
                        "name": "S. Bini"
                    },
                    {
                        "name": "C. Binu"
                    },
                    {
                        "name": "S. Biot"
                    },
                    {
                        "name": "O. Birnholtz"
                    },
                    {
                        "name": "S. Biscoveanu"
                    },
                    {
                        "name": "A. Bisht"
                    },
                    {
                        "name": "M. Bitossi"
                    },
                    {
                        "name": "M. -A. Bizouard"
                    },
                    {
                        "name": "S. Blaber"
                    },
                    {
                        "name": "J. K. Blackburn"
                    },
                    {
                        "name": "L. A. Blagg"
                    },
                    {
                        "name": "C. D. Blair"
                    },
                    {
                        "name": "D. G. Blair"
                    },
                    {
                        "name": "N. Bode"
                    },
                    {
                        "name": "N. Boettner"
                    },
                    {
                        "name": "G. Boileau"
                    },
                    {
                        "name": "M. Boldrini"
                    },
                    {
                        "name": "G. N. Bolingbroke"
                    },
                    {
                        "name": "A. Bolliand"
                    },
                    {
                        "name": "L. D. Bonavena"
                    },
                    {
                        "name": "R. Bondarescu"
                    },
                    {
                        "name": "F. Bondu"
                    },
                    {
                        "name": "E. Bonilla"
                    },
                    {
                        "name": "M. S. Bonilla"
                    },
                    {
                        "name": "A. Bonino"
                    },
                    {
                        "name": "R. Bonnand"
                    },
                    {
                        "name": "A. Borchers"
                    },
                    {
                        "name": "S. Borhanian"
                    },
                    {
                        "name": "V. Boschi"
                    },
                    {
                        "name": "S. Bose"
                    },
                    {
                        "name": "V. Bossilkov"
                    },
                    {
                        "name": "Y. Bothra"
                    },
                    {
                        "name": "A. Boudon"
                    },
                    {
                        "name": "L. Bourg"
                    },
                    {
                        "name": "M. Boyle"
                    },
                    {
                        "name": "A. Bozzi"
                    },
                    {
                        "name": "C. Bradaschia"
                    },
                    {
                        "name": "P. R. Brady"
                    },
                    {
                        "name": "A. Branch"
                    },
                    {
                        "name": "M. Branchesi"
                    },
                    {
                        "name": "I. Braun"
                    },
                    {
                        "name": "T. Briant"
                    },
                    {
                        "name": "A. Brillet"
                    },
                    {
                        "name": "M. Brinkmann"
                    },
                    {
                        "name": "P. Brockill"
                    },
                    {
                        "name": "E. Brockmueller"
                    },
                    {
                        "name": "A. F. Brooks"
                    },
                    {
                        "name": "B. C. Brown"
                    },
                    {
                        "name": "D. D. Brown"
                    },
                    {
                        "name": "M. L. Brozzetti"
                    },
                    {
                        "name": "S. Brunett"
                    },
                    {
                        "name": "G. Bruno"
                    },
                    {
                        "name": "R. Bruntz"
                    },
                    {
                        "name": "J. Bryant"
                    },
                    {
                        "name": "Y. Bu"
                    },
                    {
                        "name": "F. Bucci"
                    },
                    {
                        "name": "J. Buchanan"
                    },
                    {
                        "name": "O. Bulashenko"
                    },
                    {
                        "name": "T. Bulik"
                    },
                    {
                        "name": "H. J. Bulten"
                    },
                    {
                        "name": "A. Buonanno"
                    },
                    {
                        "name": "K. Burtnyk"
                    },
                    {
                        "name": "R. Buscicchio"
                    },
                    {
                        "name": "D. Buskulic"
                    },
                    {
                        "name": "C. Buy"
                    },
                    {
                        "name": "R. L. Byer"
                    },
                    {
                        "name": "G. S. Cabourn Davies"
                    },
                    {
                        "name": "R. Cabrita"
                    },
                    {
                        "name": "V. Cáceres-Barbosa"
                    },
                    {
                        "name": "L. Cadonati"
                    },
                    {
                        "name": "G. Cagnoli"
                    },
                    {
                        "name": "C. Cahillane"
                    },
                    {
                        "name": "A. Calafat"
                    },
                    {
                        "name": "T. A. Callister"
                    },
                    {
                        "name": "E. Calloni"
                    },
                    {
                        "name": "S. R. Callos"
                    },
                    {
                        "name": "M. Canepa"
                    },
                    {
                        "name": "G. Caneva Santoro"
                    },
                    {
                        "name": "K. C. Cannon"
                    },
                    {
                        "name": "H. Cao"
                    },
                    {
                        "name": "L. A. Capistran"
                    },
                    {
                        "name": "E. Capocasa"
                    },
                    {
                        "name": "E. Capote"
                    },
                    {
                        "name": "G. Capurri"
                    },
                    {
                        "name": "G. Carapella"
                    },
                    {
                        "name": "F. Carbognani"
                    },
                    {
                        "name": "M. Carlassara"
                    },
                    {
                        "name": "J. B. Carlin"
                    },
                    {
                        "name": "T. K. Carlson"
                    },
                    {
                        "name": "M. F. Carney"
                    },
                    {
                        "name": "M. Carpinelli"
                    },
                    {
                        "name": "G. Carrillo"
                    },
                    {
                        "name": "J. J. Carter"
                    },
                    {
                        "name": "G. Carullo"
                    },
                    {
                        "name": "A. Casallas-Lagos"
                    },
                    {
                        "name": "J. Casanueva Diaz"
                    },
                    {
                        "name": "C. Casentini"
                    },
                    {
                        "name": "S. Y. Castro-Lucas"
                    },
                    {
                        "name": "S. Caudill"
                    },
                    {
                        "name": "M. Cavaglià"
                    },
                    {
                        "name": "R. Cavalieri"
                    },
                    {
                        "name": "A. Ceja"
                    },
                    {
                        "name": "G. Cella"
                    },
                    {
                        "name": "P. Cerdá-Durán"
                    },
                    {
                        "name": "E. Cesarini"
                    },
                    {
                        "name": "N. Chabbra"
                    },
                    {
                        "name": "W. Chaibi"
                    },
                    {
                        "name": "A. Chakraborty"
                    },
                    {
                        "name": "P. Chakraborty"
                    },
                    {
                        "name": "S. Chakraborty"
                    },
                    {
                        "name": "S. Chalathadka Subrahmanya"
                    },
                    {
                        "name": "J. C. L. Chan"
                    },
                    {
                        "name": "M. Chan"
                    },
                    {
                        "name": "K. Chang"
                    },
                    {
                        "name": "S. Chao"
                    },
                    {
                        "name": "P. Charlton"
                    },
                    {
                        "name": "E. Chassande-Mottin"
                    },
                    {
                        "name": "C. Chatterjee"
                    },
                    {
                        "name": "Debarati Chatterjee"
                    },
                    {
                        "name": "Deep Chatterjee"
                    },
                    {
                        "name": "M. Chaturvedi"
                    },
                    {
                        "name": "S. Chaty"
                    },
                    {
                        "name": "K. Chatziioannou"
                    },
                    {
                        "name": "A. Chen"
                    },
                    {
                        "name": "A. H. -Y. Chen"
                    },
                    {
                        "name": "D. Chen"
                    },
                    {
                        "name": "H. Chen"
                    },
                    {
                        "name": "H. Y. Chen"
                    },
                    {
                        "name": "S. Chen"
                    },
                    {
                        "name": "Yanbei Chen"
                    },
                    {
                        "name": "Yitian Chen"
                    },
                    {
                        "name": "H. P. Cheng"
                    },
                    {
                        "name": "P. Chessa"
                    },
                    {
                        "name": "H. T. Cheung"
                    },
                    {
                        "name": "S. Y. Cheung"
                    },
                    {
                        "name": "F. Chiadini"
                    },
                    {
                        "name": "G. Chiarini"
                    },
                    {
                        "name": "A. Chiba"
                    },
                    {
                        "name": "A. Chincarini"
                    },
                    {
                        "name": "M. L. Chiofalo"
                    },
                    {
                        "name": "A. Chiummo"
                    },
                    {
                        "name": "C. Chou"
                    },
                    {
                        "name": "S. Choudhary"
                    },
                    {
                        "name": "N. Christensen"
                    },
                    {
                        "name": "S. S. Y. Chua"
                    },
                    {
                        "name": "G. Ciani"
                    },
                    {
                        "name": "P. Ciecielag"
                    },
                    {
                        "name": "M. Cieślar"
                    },
                    {
                        "name": "M. Cifaldi"
                    },
                    {
                        "name": "B. Cirok"
                    },
                    {
                        "name": "F. Clara"
                    },
                    {
                        "name": "J. A. Clark"
                    },
                    {
                        "name": "T. A. Clarke"
                    },
                    {
                        "name": "P. Clearwater"
                    },
                    {
                        "name": "S. Clesse"
                    },
                    {
                        "name": "F. Cleva"
                    },
                    {
                        "name": "E. Coccia"
                    },
                    {
                        "name": "E. Codazzo"
                    },
                    {
                        "name": "P. -F. Cohadon"
                    },
                    {
                        "name": "S. Colace"
                    },
                    {
                        "name": "E. Colangeli"
                    },
                    {
                        "name": "M. Colleoni"
                    },
                    {
                        "name": "C. G. Collette"
                    },
                    {
                        "name": "J. Collins"
                    },
                    {
                        "name": "S. Colloms"
                    },
                    {
                        "name": "A. Colombo"
                    },
                    {
                        "name": "C. M. Compton"
                    },
                    {
                        "name": "G. Connolly"
                    },
                    {
                        "name": "L. Conti"
                    },
                    {
                        "name": "T. R. Corbitt"
                    },
                    {
                        "name": "I. Cordero-Carrión"
                    },
                    {
                        "name": "S. Corezzi"
                    },
                    {
                        "name": "N. J. Cornish"
                    },
                    {
                        "name": "I. Coronado"
                    },
                    {
                        "name": "A. Corsi"
                    },
                    {
                        "name": "R. Cottingham"
                    },
                    {
                        "name": "M. W. Coughlin"
                    },
                    {
                        "name": "A. Couineaux"
                    },
                    {
                        "name": "P. Couvares"
                    },
                    {
                        "name": "D. M. Coward"
                    },
                    {
                        "name": "R. Coyne"
                    },
                    {
                        "name": "A. Cozzumbo"
                    },
                    {
                        "name": "J. D. E. Creighton"
                    },
                    {
                        "name": "T. D. Creighton"
                    },
                    {
                        "name": "P. Cremonese"
                    },
                    {
                        "name": "S. Crook"
                    },
                    {
                        "name": "R. Crouch"
                    },
                    {
                        "name": "J. Csizmazia"
                    },
                    {
                        "name": "J. R. Cudell"
                    },
                    {
                        "name": "T. J. Cullen"
                    },
                    {
                        "name": "A. Cumming"
                    },
                    {
                        "name": "E. Cuoco"
                    },
                    {
                        "name": "M. Cusinato"
                    },
                    {
                        "name": "L. V. Da Conceição"
                    },
                    {
                        "name": "T. Dal Canton"
                    },
                    {
                        "name": "S. Dal Pra"
                    },
                    {
                        "name": "G. Dálya"
                    },
                    {
                        "name": "B. D'Angelo"
                    },
                    {
                        "name": "S. Danilishin"
                    },
                    {
                        "name": "S. D'Antonio"
                    },
                    {
                        "name": "K. Danzmann"
                    },
                    {
                        "name": "K. E. Darroch"
                    },
                    {
                        "name": "L. P. Dartez"
                    },
                    {
                        "name": "R. Das"
                    },
                    {
                        "name": "A. Dasgupta"
                    },
                    {
                        "name": "V. Dattilo"
                    },
                    {
                        "name": "A. Daumas"
                    },
                    {
                        "name": "N. Davari"
                    },
                    {
                        "name": "I. Dave"
                    },
                    {
                        "name": "A. Davenport"
                    },
                    {
                        "name": "M. Davier"
                    },
                    {
                        "name": "T. F. Davies"
                    },
                    {
                        "name": "D. Davis"
                    },
                    {
                        "name": "L. Davis"
                    },
                    {
                        "name": "M. C. Davis"
                    },
                    {
                        "name": "P. Davis"
                    },
                    {
                        "name": "E. J. Daw"
                    },
                    {
                        "name": "M. Dax"
                    },
                    {
                        "name": "J. De Bolle"
                    },
                    {
                        "name": "M. Deenadayalan"
                    },
                    {
                        "name": "J. Degallaix"
                    },
                    {
                        "name": "M. De Laurentis"
                    },
                    {
                        "name": "F. De Lillo"
                    },
                    {
                        "name": "S. Della Torre"
                    },
                    {
                        "name": "W. Del Pozzo"
                    },
                    {
                        "name": "A. Demagny"
                    },
                    {
                        "name": "F. De Marco"
                    },
                    {
                        "name": "G. Demasi"
                    },
                    {
                        "name": "F. De Matteis"
                    },
                    {
                        "name": "N. Demos"
                    },
                    {
                        "name": "T. Dent"
                    },
                    {
                        "name": "A. Depasse"
                    },
                    {
                        "name": "N. DePergola"
                    },
                    {
                        "name": "R. De Pietri"
                    },
                    {
                        "name": "R. De Rosa"
                    },
                    {
                        "name": "C. De Rossi"
                    },
                    {
                        "name": "M. Desai"
                    },
                    {
                        "name": "R. DeSalvo"
                    },
                    {
                        "name": "A. DeSimone"
                    },
                    {
                        "name": "R. De Simone"
                    },
                    {
                        "name": "A. Dhani"
                    },
                    {
                        "name": "R. Diab"
                    },
                    {
                        "name": "M. C. Díaz"
                    },
                    {
                        "name": "M. Di Cesare"
                    },
                    {
                        "name": "G. Dideron"
                    },
                    {
                        "name": "T. Dietrich"
                    },
                    {
                        "name": "L. Di Fiore"
                    },
                    {
                        "name": "C. Di Fronzo"
                    },
                    {
                        "name": "M. Di Giovanni"
                    },
                    {
                        "name": "T. Di Girolamo"
                    },
                    {
                        "name": "D. Diksha"
                    },
                    {
                        "name": "J. Ding"
                    },
                    {
                        "name": "S. Di Pace"
                    },
                    {
                        "name": "I. Di Palma"
                    },
                    {
                        "name": "D. Di Piero"
                    },
                    {
                        "name": "F. Di Renzo"
                    },
                    {
                        "name": "Divyajyoti"
                    },
                    {
                        "name": "A. Dmitriev"
                    },
                    {
                        "name": "J. P. Docherty"
                    },
                    {
                        "name": "Z. Doctor"
                    },
                    {
                        "name": "N. Doerksen"
                    },
                    {
                        "name": "E. Dohmen"
                    },
                    {
                        "name": "A. Doke"
                    },
                    {
                        "name": "A. Domiciano De Souza"
                    },
                    {
                        "name": "L. D'Onofrio"
                    },
                    {
                        "name": "F. Donovan"
                    },
                    {
                        "name": "K. L. Dooley"
                    },
                    {
                        "name": "T. Dooney"
                    },
                    {
                        "name": "S. Doravari"
                    },
                    {
                        "name": "O. Dorosh"
                    },
                    {
                        "name": "W. J. D. Doyle"
                    },
                    {
                        "name": "M. Drago"
                    },
                    {
                        "name": "J. C. Driggers"
                    },
                    {
                        "name": "L. Dunn"
                    },
                    {
                        "name": "U. Dupletsa"
                    },
                    {
                        "name": "P. -A. Duverne"
                    },
                    {
                        "name": "D. D'Urso"
                    },
                    {
                        "name": "P. Dutta Roy"
                    },
                    {
                        "name": "H. Duval"
                    },
                    {
                        "name": "S. E. Dwyer"
                    },
                    {
                        "name": "C. Eassa"
                    },
                    {
                        "name": "M. Ebersold"
                    },
                    {
                        "name": "T. Eckhardt"
                    },
                    {
                        "name": "G. Eddolls"
                    },
                    {
                        "name": "A. Effler"
                    },
                    {
                        "name": "J. Eichholz"
                    },
                    {
                        "name": "H. Einsle"
                    },
                    {
                        "name": "M. Eisenmann"
                    },
                    {
                        "name": "M. Emma"
                    },
                    {
                        "name": "K. Endo"
                    },
                    {
                        "name": "R. Enficiaud"
                    },
                    {
                        "name": "L. Errico"
                    },
                    {
                        "name": "R. Espinosa"
                    },
                    {
                        "name": "M. Esposito"
                    },
                    {
                        "name": "R. C. Essick"
                    },
                    {
                        "name": "H. Estellés"
                    },
                    {
                        "name": "T. Etzel"
                    },
                    {
                        "name": "M. Evans"
                    },
                    {
                        "name": "T. Evstafyeva"
                    },
                    {
                        "name": "B. E. Ewing"
                    },
                    {
                        "name": "J. M. Ezquiaga"
                    },
                    {
                        "name": "F. Fabrizi"
                    },
                    {
                        "name": "V. Fafone"
                    },
                    {
                        "name": "S. Fairhurst"
                    },
                    {
                        "name": "A. M. Farah"
                    },
                    {
                        "name": "B. Farr"
                    },
                    {
                        "name": "W. M. Farr"
                    },
                    {
                        "name": "G. Favaro"
                    },
                    {
                        "name": "M. Favata"
                    },
                    {
                        "name": "M. Fays"
                    },
                    {
                        "name": "M. Fazio"
                    },
                    {
                        "name": "J. Feicht"
                    },
                    {
                        "name": "M. M. Fejer"
                    },
                    {
                        "name": "R. Felicetti"
                    },
                    {
                        "name": "E. Fenyvesi"
                    },
                    {
                        "name": "J. Fernandes"
                    },
                    {
                        "name": "T. Fernandes"
                    },
                    {
                        "name": "D. Fernando"
                    },
                    {
                        "name": "S. Ferraiuolo"
                    },
                    {
                        "name": "T. A. Ferreira"
                    },
                    {
                        "name": "F. Fidecaro"
                    },
                    {
                        "name": "P. Figura"
                    },
                    {
                        "name": "A. Fiori"
                    },
                    {
                        "name": "I. Fiori"
                    },
                    {
                        "name": "M. Fishbach"
                    },
                    {
                        "name": "R. P. Fisher"
                    },
                    {
                        "name": "R. Fittipaldi"
                    },
                    {
                        "name": "V. Fiumara"
                    },
                    {
                        "name": "R. Flaminio"
                    },
                    {
                        "name": "S. M. Fleischer"
                    },
                    {
                        "name": "L. S. Fleming"
                    },
                    {
                        "name": "E. Floden"
                    },
                    {
                        "name": "H. Fong"
                    },
                    {
                        "name": "J. A. Font"
                    },
                    {
                        "name": "F. Fontinele-Nunes"
                    },
                    {
                        "name": "C. Foo"
                    },
                    {
                        "name": "B. Fornal"
                    },
                    {
                        "name": "K. Franceschetti"
                    },
                    {
                        "name": "F. Frappez"
                    },
                    {
                        "name": "S. Frasca"
                    },
                    {
                        "name": "F. Frasconi"
                    },
                    {
                        "name": "J. P. Freed"
                    },
                    {
                        "name": "Z. Frei"
                    },
                    {
                        "name": "A. Freise"
                    },
                    {
                        "name": "O. Freitas"
                    },
                    {
                        "name": "R. Frey"
                    },
                    {
                        "name": "W. Frischhertz"
                    },
                    {
                        "name": "P. Fritschel"
                    },
                    {
                        "name": "V. V. Frolov"
                    },
                    {
                        "name": "G. G. Fronzé"
                    },
                    {
                        "name": "M. Fuentes-Garcia"
                    },
                    {
                        "name": "S. Fujii"
                    },
                    {
                        "name": "T. Fujimori"
                    },
                    {
                        "name": "P. Fulda"
                    },
                    {
                        "name": "M. Fyffe"
                    },
                    {
                        "name": "B. Gadre"
                    },
                    {
                        "name": "J. R. Gair"
                    },
                    {
                        "name": "S. Galaudage"
                    },
                    {
                        "name": "V. Galdi"
                    },
                    {
                        "name": "R. Gamba"
                    },
                    {
                        "name": "A. Gamboa"
                    },
                    {
                        "name": "S. Gamoji"
                    },
                    {
                        "name": "D. Ganapathy"
                    },
                    {
                        "name": "A. Ganguly"
                    },
                    {
                        "name": "B. Garaventa"
                    },
                    {
                        "name": "J. García-Bellido"
                    },
                    {
                        "name": "C. García-Quirós"
                    },
                    {
                        "name": "J. W. Gardner"
                    },
                    {
                        "name": "K. A. Gardner"
                    },
                    {
                        "name": "S. Garg"
                    },
                    {
                        "name": "J. Gargiulo"
                    },
                    {
                        "name": "X. Garrido"
                    },
                    {
                        "name": "A. Garron"
                    },
                    {
                        "name": "F. Garufi"
                    },
                    {
                        "name": "P. A. Garver"
                    },
                    {
                        "name": "C. Gasbarra"
                    },
                    {
                        "name": "B. Gateley"
                    },
                    {
                        "name": "F. Gautier"
                    },
                    {
                        "name": "V. Gayathri"
                    },
                    {
                        "name": "T. Gayer"
                    },
                    {
                        "name": "G. Gemme"
                    },
                    {
                        "name": "A. Gennai"
                    },
                    {
                        "name": "V. Gennari"
                    },
                    {
                        "name": "J. George"
                    },
                    {
                        "name": "R. George"
                    },
                    {
                        "name": "O. Gerberding"
                    },
                    {
                        "name": "L. Gergely"
                    },
                    {
                        "name": "Archisman Ghosh"
                    },
                    {
                        "name": "Sayantan Ghosh"
                    },
                    {
                        "name": "Shaon Ghosh"
                    },
                    {
                        "name": "Shrobana Ghosh"
                    },
                    {
                        "name": "Suprovo Ghosh"
                    },
                    {
                        "name": "Tathagata Ghosh"
                    },
                    {
                        "name": "J. A. Giaime"
                    },
                    {
                        "name": "K. D. Giardina"
                    },
                    {
                        "name": "D. R. Gibson"
                    },
                    {
                        "name": "C. Gier"
                    },
                    {
                        "name": "S. Gkaitatzis"
                    },
                    {
                        "name": "J. Glanzer"
                    },
                    {
                        "name": "F. Glotin"
                    },
                    {
                        "name": "J. Godfrey"
                    },
                    {
                        "name": "R. V. Godley"
                    },
                    {
                        "name": "P. Godwin"
                    },
                    {
                        "name": "A. S. Goettel"
                    },
                    {
                        "name": "E. Goetz"
                    },
                    {
                        "name": "J. Golomb"
                    },
                    {
                        "name": "S. Gomez Lopez"
                    },
                    {
                        "name": "B. Goncharov"
                    },
                    {
                        "name": "G. González"
                    },
                    {
                        "name": "P. Goodarzi"
                    },
                    {
                        "name": "S. Goode"
                    },
                    {
                        "name": "A. W. Goodwin-Jones"
                    },
                    {
                        "name": "M. Gosselin"
                    },
                    {
                        "name": "R. Gouaty"
                    },
                    {
                        "name": "D. W. Gould"
                    },
                    {
                        "name": "K. Govorkova"
                    },
                    {
                        "name": "A. Grado"
                    },
                    {
                        "name": "V. Graham"
                    },
                    {
                        "name": "A. E. Granados"
                    },
                    {
                        "name": "M. Granata"
                    },
                    {
                        "name": "V. Granata"
                    },
                    {
                        "name": "S. Gras"
                    },
                    {
                        "name": "P. Grassia"
                    },
                    {
                        "name": "J. Graves"
                    },
                    {
                        "name": "C. Gray"
                    },
                    {
                        "name": "R. Gray"
                    },
                    {
                        "name": "G. Greco"
                    },
                    {
                        "name": "A. C. Green"
                    },
                    {
                        "name": "L. Green"
                    },
                    {
                        "name": "S. M. Green"
                    },
                    {
                        "name": "S. R. Green"
                    },
                    {
                        "name": "C. Greenberg"
                    },
                    {
                        "name": "A. M. Gretarsson"
                    },
                    {
                        "name": "H. K. Griffin"
                    },
                    {
                        "name": "D. Griffith"
                    },
                    {
                        "name": "H. L. Griggs"
                    },
                    {
                        "name": "G. Grignani"
                    },
                    {
                        "name": "C. Grimaud"
                    },
                    {
                        "name": "H. Grote"
                    },
                    {
                        "name": "S. Grunewald"
                    },
                    {
                        "name": "D. Guerra"
                    },
                    {
                        "name": "D. Guetta"
                    },
                    {
                        "name": "G. M. Guidi"
                    },
                    {
                        "name": "A. R. Guimaraes"
                    },
                    {
                        "name": "H. K. Gulati"
                    },
                    {
                        "name": "F. Gulminelli"
                    },
                    {
                        "name": "H. Guo"
                    },
                    {
                        "name": "W. Guo"
                    },
                    {
                        "name": "Y. Guo"
                    },
                    {
                        "name": "Anuradha Gupta"
                    },
                    {
                        "name": "I. Gupta"
                    },
                    {
                        "name": "N. C. Gupta"
                    },
                    {
                        "name": "S. K. Gupta"
                    },
                    {
                        "name": "V. Gupta"
                    },
                    {
                        "name": "N. Gupte"
                    },
                    {
                        "name": "J. Gurs"
                    },
                    {
                        "name": "N. Gutierrez"
                    },
                    {
                        "name": "N. Guttman"
                    },
                    {
                        "name": "F. Guzman"
                    },
                    {
                        "name": "D. Haba"
                    },
                    {
                        "name": "M. Haberland"
                    },
                    {
                        "name": "S. Haino"
                    },
                    {
                        "name": "E. D. Hall"
                    },
                    {
                        "name": "E. Z. Hamilton"
                    },
                    {
                        "name": "G. Hammond"
                    },
                    {
                        "name": "M. Haney"
                    },
                    {
                        "name": "J. Hanks"
                    },
                    {
                        "name": "C. Hanna"
                    },
                    {
                        "name": "M. D. Hannam"
                    },
                    {
                        "name": "O. A. Hannuksela"
                    },
                    {
                        "name": "A. G. Hanselman"
                    },
                    {
                        "name": "H. Hansen"
                    },
                    {
                        "name": "J. Hanson"
                    },
                    {
                        "name": "S. Hanumasagar"
                    },
                    {
                        "name": "R. Harada"
                    },
                    {
                        "name": "A. R. Hardison"
                    },
                    {
                        "name": "S. Harikumar"
                    },
                    {
                        "name": "K. Haris"
                    },
                    {
                        "name": "I. Harley-Trochimczyk"
                    },
                    {
                        "name": "T. Harmark"
                    },
                    {
                        "name": "J. Harms"
                    },
                    {
                        "name": "G. M. Harry"
                    },
                    {
                        "name": "I. W. Harry"
                    },
                    {
                        "name": "J. Hart"
                    },
                    {
                        "name": "B. Haskell"
                    },
                    {
                        "name": "C. J. Haster"
                    },
                    {
                        "name": "K. Haughian"
                    },
                    {
                        "name": "H. Hayakawa"
                    },
                    {
                        "name": "K. Hayama"
                    },
                    {
                        "name": "M. C. Heintze"
                    },
                    {
                        "name": "J. Heinze"
                    },
                    {
                        "name": "J. Heinzel"
                    },
                    {
                        "name": "H. Heitmann"
                    },
                    {
                        "name": "F. Hellman"
                    },
                    {
                        "name": "A. F. Helmling-Cornell"
                    },
                    {
                        "name": "G. Hemming"
                    },
                    {
                        "name": "O. Henderson-Sapir"
                    },
                    {
                        "name": "M. Hendry"
                    },
                    {
                        "name": "I. S. Heng"
                    },
                    {
                        "name": "M. H. Hennig"
                    },
                    {
                        "name": "C. Henshaw"
                    },
                    {
                        "name": "M. Heurs"
                    },
                    {
                        "name": "A. L. Hewitt"
                    },
                    {
                        "name": "J. Heynen"
                    },
                    {
                        "name": "J. Heyns"
                    },
                    {
                        "name": "S. Higginbotham"
                    },
                    {
                        "name": "S. Hild"
                    },
                    {
                        "name": "S. Hill"
                    },
                    {
                        "name": "Y. Himemoto"
                    },
                    {
                        "name": "N. Hirata"
                    },
                    {
                        "name": "C. Hirose"
                    },
                    {
                        "name": "D. Hofman"
                    },
                    {
                        "name": "B. E. Hogan"
                    },
                    {
                        "name": "N. A. Holland"
                    },
                    {
                        "name": "I. J. Hollows"
                    },
                    {
                        "name": "D. E. Holz"
                    },
                    {
                        "name": "L. Honet"
                    },
                    {
                        "name": "D. J. Horton-Bailey"
                    },
                    {
                        "name": "J. Hough"
                    },
                    {
                        "name": "S. Hourihane"
                    },
                    {
                        "name": "N. T. Howard"
                    },
                    {
                        "name": "E. J. Howell"
                    },
                    {
                        "name": "C. G. Hoy"
                    },
                    {
                        "name": "C. A. Hrishikesh"
                    },
                    {
                        "name": "P. Hsi"
                    },
                    {
                        "name": "H. -F. Hsieh"
                    },
                    {
                        "name": "H. -Y. Hsieh"
                    },
                    {
                        "name": "C. Hsiung"
                    },
                    {
                        "name": "S. -H. Hsu"
                    },
                    {
                        "name": "W. -F. Hsu"
                    },
                    {
                        "name": "Q. Hu"
                    },
                    {
                        "name": "H. Y. Huang"
                    },
                    {
                        "name": "Y. Huang"
                    },
                    {
                        "name": "Y. T. Huang"
                    },
                    {
                        "name": "A. D. Huddart"
                    },
                    {
                        "name": "B. Hughey"
                    },
                    {
                        "name": "V. Hui"
                    },
                    {
                        "name": "S. Husa"
                    },
                    {
                        "name": "R. Huxford"
                    },
                    {
                        "name": "L. Iampieri"
                    },
                    {
                        "name": "G. A. Iandolo"
                    },
                    {
                        "name": "M. Ianni"
                    },
                    {
                        "name": "G. Iannone"
                    },
                    {
                        "name": "J. Iascau"
                    },
                    {
                        "name": "K. Ide"
                    },
                    {
                        "name": "R. Iden"
                    },
                    {
                        "name": "A. Ierardi"
                    },
                    {
                        "name": "S. Ikeda"
                    },
                    {
                        "name": "H. Imafuku"
                    },
                    {
                        "name": "Y. Inoue"
                    },
                    {
                        "name": "G. Iorio"
                    },
                    {
                        "name": "P. Iosif"
                    },
                    {
                        "name": "M. H. Iqbal"
                    },
                    {
                        "name": "J. Irwin"
                    },
                    {
                        "name": "R. Ishikawa"
                    },
                    {
                        "name": "M. Isi"
                    },
                    {
                        "name": "K. S. Isleif"
                    },
                    {
                        "name": "Y. Itoh"
                    },
                    {
                        "name": "M. Iwaya"
                    },
                    {
                        "name": "B. R. Iyer"
                    },
                    {
                        "name": "C. Jacquet"
                    },
                    {
                        "name": "P. -E. Jacquet"
                    },
                    {
                        "name": "T. Jacquot"
                    },
                    {
                        "name": "S. J. Jadhav"
                    },
                    {
                        "name": "S. P. Jadhav"
                    },
                    {
                        "name": "M. Jain"
                    },
                    {
                        "name": "T. Jain"
                    },
                    {
                        "name": "A. L. James"
                    },
                    {
                        "name": "K. Jani"
                    },
                    {
                        "name": "J. Janquart"
                    },
                    {
                        "name": "N. N. Janthalur"
                    },
                    {
                        "name": "S. Jaraba"
                    },
                    {
                        "name": "P. Jaranowski"
                    },
                    {
                        "name": "R. Jaume"
                    },
                    {
                        "name": "W. Javed"
                    },
                    {
                        "name": "A. Jennings"
                    },
                    {
                        "name": "M. Jensen"
                    },
                    {
                        "name": "W. Jia"
                    },
                    {
                        "name": "J. Jiang"
                    },
                    {
                        "name": "H. -B. Jin"
                    },
                    {
                        "name": "G. R. Johns"
                    },
                    {
                        "name": "N. A. Johnson"
                    },
                    {
                        "name": "M. C. Johnston"
                    },
                    {
                        "name": "R. Johnston"
                    },
                    {
                        "name": "N. Johny"
                    },
                    {
                        "name": "D. H. Jones"
                    },
                    {
                        "name": "D. I. Jones"
                    },
                    {
                        "name": "R. Jones"
                    },
                    {
                        "name": "H. E. Jose"
                    },
                    {
                        "name": "P. Joshi"
                    },
                    {
                        "name": "S. K. Joshi"
                    },
                    {
                        "name": "G. Joubert"
                    },
                    {
                        "name": "J. Ju"
                    },
                    {
                        "name": "L. Ju"
                    },
                    {
                        "name": "K. Jung"
                    },
                    {
                        "name": "J. Junker"
                    },
                    {
                        "name": "V. Juste"
                    },
                    {
                        "name": "H. B. Kabagoz"
                    },
                    {
                        "name": "T. Kajita"
                    },
                    {
                        "name": "I. Kaku"
                    },
                    {
                        "name": "V. Kalogera"
                    },
                    {
                        "name": "M. Kalomenopoulos"
                    },
                    {
                        "name": "M. Kamiizumi"
                    },
                    {
                        "name": "N. Kanda"
                    },
                    {
                        "name": "S. Kandhasamy"
                    },
                    {
                        "name": "G. Kang"
                    },
                    {
                        "name": "N. C. Kannachel"
                    },
                    {
                        "name": "J. B. Kanner"
                    },
                    {
                        "name": "S. A. KantiMahanty"
                    },
                    {
                        "name": "S. J. Kapadia"
                    },
                    {
                        "name": "D. P. Kapasi"
                    },
                    {
                        "name": "M. Karthikeyan"
                    },
                    {
                        "name": "M. Kasprzack"
                    },
                    {
                        "name": "H. Kato"
                    },
                    {
                        "name": "T. Kato"
                    },
                    {
                        "name": "E. Katsavounidis"
                    },
                    {
                        "name": "W. Katzman"
                    },
                    {
                        "name": "R. Kaushik"
                    },
                    {
                        "name": "K. Kawabe"
                    },
                    {
                        "name": "R. Kawamoto"
                    },
                    {
                        "name": "D. Keitel"
                    },
                    {
                        "name": "L. J. Kemperman"
                    },
                    {
                        "name": "J. Kennington"
                    },
                    {
                        "name": "F. A. Kerkow"
                    },
                    {
                        "name": "R. Kesharwani"
                    },
                    {
                        "name": "J. S. Key"
                    },
                    {
                        "name": "R. Khadela"
                    },
                    {
                        "name": "S. Khadka"
                    },
                    {
                        "name": "S. S. Khadkikar"
                    },
                    {
                        "name": "F. Y. Khalili"
                    },
                    {
                        "name": "F. Khan"
                    },
                    {
                        "name": "T. Khanam"
                    },
                    {
                        "name": "M. Khursheed"
                    },
                    {
                        "name": "N. M. Khusid"
                    },
                    {
                        "name": "W. Kiendrebeogo"
                    },
                    {
                        "name": "N. Kijbunchoo"
                    },
                    {
                        "name": "C. Kim"
                    },
                    {
                        "name": "J. C. Kim"
                    },
                    {
                        "name": "K. Kim"
                    },
                    {
                        "name": "M. H. Kim"
                    },
                    {
                        "name": "S. Kim"
                    },
                    {
                        "name": "Y. -M. Kim"
                    },
                    {
                        "name": "C. Kimball"
                    },
                    {
                        "name": "K. Kimes"
                    },
                    {
                        "name": "M. Kinnear"
                    },
                    {
                        "name": "J. S. Kissel"
                    },
                    {
                        "name": "S. Klimenko"
                    },
                    {
                        "name": "A. M. Knee"
                    },
                    {
                        "name": "E. J. Knox"
                    },
                    {
                        "name": "N. Knust"
                    },
                    {
                        "name": "K. Kobayashi"
                    },
                    {
                        "name": "S. M. Koehlenbeck"
                    },
                    {
                        "name": "G. Koekoek"
                    },
                    {
                        "name": "K. Kohri"
                    },
                    {
                        "name": "K. Kokeyama"
                    },
                    {
                        "name": "S. Koley"
                    },
                    {
                        "name": "P. Kolitsidou"
                    },
                    {
                        "name": "A. E. Koloniari"
                    },
                    {
                        "name": "K. Komori"
                    },
                    {
                        "name": "A. K. H. Kong"
                    },
                    {
                        "name": "A. Kontos"
                    },
                    {
                        "name": "L. M. Koponen"
                    },
                    {
                        "name": "M. Korobko"
                    },
                    {
                        "name": "X. Kou"
                    },
                    {
                        "name": "A. Koushik"
                    },
                    {
                        "name": "N. Kouvatsos"
                    },
                    {
                        "name": "M. Kovalam"
                    },
                    {
                        "name": "T. Koyama"
                    },
                    {
                        "name": "D. B. Kozak"
                    },
                    {
                        "name": "S. L. Kranzhoff"
                    },
                    {
                        "name": "V. Kringel"
                    },
                    {
                        "name": "N. V. Krishnendu"
                    },
                    {
                        "name": "S. Kroker"
                    },
                    {
                        "name": "A. Królak"
                    },
                    {
                        "name": "K. Kruska"
                    },
                    {
                        "name": "J. Kubisz"
                    },
                    {
                        "name": "G. Kuehn"
                    },
                    {
                        "name": "S. Kulkarni"
                    },
                    {
                        "name": "A. Kulur Ramamohan"
                    },
                    {
                        "name": "Achal Kumar"
                    },
                    {
                        "name": "Anil Kumar"
                    },
                    {
                        "name": "Praveen Kumar"
                    },
                    {
                        "name": "Prayush Kumar"
                    },
                    {
                        "name": "Rahul Kumar"
                    },
                    {
                        "name": "Rakesh Kumar"
                    },
                    {
                        "name": "J. Kume"
                    },
                    {
                        "name": "K. Kuns"
                    },
                    {
                        "name": "N. Kuntimaddi"
                    },
                    {
                        "name": "S. Kuroyanagi"
                    },
                    {
                        "name": "S. Kuwahara"
                    },
                    {
                        "name": "K. Kwak"
                    },
                    {
                        "name": "K. Kwan"
                    },
                    {
                        "name": "S. Kwon"
                    },
                    {
                        "name": "G. Lacaille"
                    },
                    {
                        "name": "D. Laghi"
                    },
                    {
                        "name": "A. H. Laity"
                    },
                    {
                        "name": "E. Lalande"
                    },
                    {
                        "name": "M. Lalleman"
                    },
                    {
                        "name": "P. C. Lalremruati"
                    },
                    {
                        "name": "M. Landry"
                    },
                    {
                        "name": "B. B. Lane"
                    },
                    {
                        "name": "R. N. Lang"
                    },
                    {
                        "name": "J. Lange"
                    },
                    {
                        "name": "R. Langgin"
                    },
                    {
                        "name": "B. Lantz"
                    },
                    {
                        "name": "I. La Rosa"
                    },
                    {
                        "name": "J. Larsen"
                    },
                    {
                        "name": "A. Lartaux-Vollard"
                    },
                    {
                        "name": "P. D. Lasky"
                    },
                    {
                        "name": "J. Lawrence"
                    },
                    {
                        "name": "M. Laxen"
                    },
                    {
                        "name": "C. Lazarte"
                    },
                    {
                        "name": "A. Lazzarini"
                    },
                    {
                        "name": "C. Lazzaro"
                    },
                    {
                        "name": "P. Leaci"
                    },
                    {
                        "name": "L. Leali"
                    },
                    {
                        "name": "Y. K. Lecoeuche"
                    },
                    {
                        "name": "H. M. Lee"
                    },
                    {
                        "name": "H. W. Lee"
                    },
                    {
                        "name": "J. Lee"
                    },
                    {
                        "name": "K. Lee"
                    },
                    {
                        "name": "R. -K. Lee"
                    },
                    {
                        "name": "R. Lee"
                    },
                    {
                        "name": "Sungho Lee"
                    },
                    {
                        "name": "Sunjae Lee"
                    },
                    {
                        "name": "Y. Lee"
                    },
                    {
                        "name": "I. N. Legred"
                    },
                    {
                        "name": "J. Lehmann"
                    },
                    {
                        "name": "L. Lehner"
                    },
                    {
                        "name": "M. Le Jean"
                    },
                    {
                        "name": "A. Lemaître"
                    },
                    {
                        "name": "M. Lenti"
                    },
                    {
                        "name": "M. Leonardi"
                    },
                    {
                        "name": "M. Lequime"
                    },
                    {
                        "name": "N. Leroy"
                    },
                    {
                        "name": "M. Lesovsky"
                    },
                    {
                        "name": "N. Letendre"
                    },
                    {
                        "name": "M. Lethuillier"
                    },
                    {
                        "name": "Y. Levin"
                    },
                    {
                        "name": "K. Leyde"
                    },
                    {
                        "name": "A. K. Y. Li"
                    },
                    {
                        "name": "K. L. Li"
                    },
                    {
                        "name": "T. G. F. Li"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Y. Li"
                    },
                    {
                        "name": "Z. Li"
                    },
                    {
                        "name": "A. Lihos"
                    },
                    {
                        "name": "E. T. Lin"
                    },
                    {
                        "name": "F. Lin"
                    },
                    {
                        "name": "L. C. -C. Lin"
                    },
                    {
                        "name": "Y. -C. Lin"
                    },
                    {
                        "name": "C. Lindsay"
                    },
                    {
                        "name": "S. D. Linker"
                    },
                    {
                        "name": "A. Liu"
                    },
                    {
                        "name": "G. C. Liu"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "F. Llamas Villarreal"
                    },
                    {
                        "name": "J. Llobera-Querol"
                    },
                    {
                        "name": "R. K. L. Lo"
                    },
                    {
                        "name": "J. -P. Locquet"
                    },
                    {
                        "name": "S. C. G. Loggins"
                    },
                    {
                        "name": "M. R. Loizou"
                    },
                    {
                        "name": "L. T. London"
                    },
                    {
                        "name": "A. Longo"
                    },
                    {
                        "name": "D. Lopez"
                    },
                    {
                        "name": "M. Lopez Portilla"
                    },
                    {
                        "name": "M. Lorenzini"
                    },
                    {
                        "name": "A. Lorenzo-Medina"
                    },
                    {
                        "name": "V. Loriette"
                    },
                    {
                        "name": "M. Lormand"
                    },
                    {
                        "name": "G. Losurdo"
                    },
                    {
                        "name": "E. Lotti"
                    },
                    {
                        "name": "T. P. Lott IV"
                    },
                    {
                        "name": "J. D. Lough"
                    },
                    {
                        "name": "H. A. Loughlin"
                    },
                    {
                        "name": "C. O. Lousto"
                    },
                    {
                        "name": "N. Low"
                    },
                    {
                        "name": "N. Lu"
                    },
                    {
                        "name": "L. Lucchesi"
                    },
                    {
                        "name": "H. Lück"
                    },
                    {
                        "name": "D. Lumaca"
                    },
                    {
                        "name": "A. P. Lundgren"
                    },
                    {
                        "name": "A. W. Lussier"
                    },
                    {
                        "name": "R. Macas"
                    },
                    {
                        "name": "M. MacInnis"
                    },
                    {
                        "name": "D. M. Macleod"
                    },
                    {
                        "name": "I. A. O. MacMillan"
                    },
                    {
                        "name": "A. Macquet"
                    },
                    {
                        "name": "K. Maeda"
                    },
                    {
                        "name": "S. Maenaut"
                    },
                    {
                        "name": "S. S. Magare"
                    },
                    {
                        "name": "R. M. Magee"
                    },
                    {
                        "name": "E. Maggio"
                    },
                    {
                        "name": "R. Maggiore"
                    },
                    {
                        "name": "M. Magnozzi"
                    },
                    {
                        "name": "M. Mahesh"
                    },
                    {
                        "name": "M. Maini"
                    },
                    {
                        "name": "S. Majhi"
                    },
                    {
                        "name": "E. Majorana"
                    },
                    {
                        "name": "C. N. Makarem"
                    },
                    {
                        "name": "D. Malakar"
                    },
                    {
                        "name": "J. A. Malaquias-Reis"
                    },
                    {
                        "name": "U. Mali"
                    },
                    {
                        "name": "S. Maliakal"
                    },
                    {
                        "name": "A. Malik"
                    },
                    {
                        "name": "L. Mallick"
                    },
                    {
                        "name": "A. -K. Malz"
                    },
                    {
                        "name": "N. Man"
                    },
                    {
                        "name": "M. Mancarella"
                    },
                    {
                        "name": "V. Mandic"
                    },
                    {
                        "name": "V. Mangano"
                    },
                    {
                        "name": "B. Mannix"
                    },
                    {
                        "name": "G. L. Mansell"
                    },
                    {
                        "name": "M. Manske"
                    },
                    {
                        "name": "M. Mantovani"
                    },
                    {
                        "name": "M. Mapelli"
                    },
                    {
                        "name": "C. Marinelli"
                    },
                    {
                        "name": "F. Marion"
                    },
                    {
                        "name": "A. S. Markosyan"
                    },
                    {
                        "name": "A. Markowitz"
                    },
                    {
                        "name": "E. Maros"
                    },
                    {
                        "name": "S. Marsat"
                    },
                    {
                        "name": "F. Martelli"
                    },
                    {
                        "name": "I. W. Martin"
                    },
                    {
                        "name": "R. M. Martin"
                    },
                    {
                        "name": "B. B. Martinez"
                    },
                    {
                        "name": "D. A. Martinez"
                    },
                    {
                        "name": "M. Martinez"
                    },
                    {
                        "name": "V. Martinez"
                    },
                    {
                        "name": "A. Martini"
                    },
                    {
                        "name": "J. C. Martins"
                    },
                    {
                        "name": "D. V. Martynov"
                    },
                    {
                        "name": "E. J. Marx"
                    },
                    {
                        "name": "L. Massaro"
                    },
                    {
                        "name": "A. Masserot"
                    },
                    {
                        "name": "M. Masso-Reid"
                    },
                    {
                        "name": "S. Mastrogiovanni"
                    },
                    {
                        "name": "T. Matcovich"
                    },
                    {
                        "name": "M. Matiushechkina"
                    },
                    {
                        "name": "L. Maurin"
                    },
                    {
                        "name": "N. Mavalvala"
                    },
                    {
                        "name": "N. Maxwell"
                    },
                    {
                        "name": "G. McCarrol"
                    },
                    {
                        "name": "R. McCarthy"
                    },
                    {
                        "name": "D. E. McClelland"
                    },
                    {
                        "name": "S. McCormick"
                    },
                    {
                        "name": "L. McCuller"
                    },
                    {
                        "name": "S. McEachin"
                    },
                    {
                        "name": "C. McElhenny"
                    },
                    {
                        "name": "G. I. McGhee"
                    },
                    {
                        "name": "J. McGinn"
                    },
                    {
                        "name": "K. B. M. McGowan"
                    },
                    {
                        "name": "J. McIver"
                    },
                    {
                        "name": "A. McLeod"
                    },
                    {
                        "name": "I. McMahon"
                    },
                    {
                        "name": "T. McRae"
                    },
                    {
                        "name": "R. McTeague"
                    },
                    {
                        "name": "D. Meacher"
                    },
                    {
                        "name": "B. N. Meagher"
                    },
                    {
                        "name": "R. Mechum"
                    },
                    {
                        "name": "Q. Meijer"
                    },
                    {
                        "name": "A. Melatos"
                    },
                    {
                        "name": "C. S. Menoni"
                    },
                    {
                        "name": "F. Mera"
                    },
                    {
                        "name": "R. A. Mercer"
                    },
                    {
                        "name": "L. Mereni"
                    },
                    {
                        "name": "K. Merfeld"
                    },
                    {
                        "name": "E. L. Merilh"
                    },
                    {
                        "name": "J. R. Mérou"
                    },
                    {
                        "name": "J. D. Merritt"
                    },
                    {
                        "name": "M. Merzougui"
                    },
                    {
                        "name": "C. Messick"
                    },
                    {
                        "name": "B. Mestichelli"
                    },
                    {
                        "name": "M. Meyer-Conde"
                    },
                    {
                        "name": "F. Meylahn"
                    },
                    {
                        "name": "A. Mhaske"
                    },
                    {
                        "name": "A. Miani"
                    },
                    {
                        "name": "H. Miao"
                    },
                    {
                        "name": "C. Michel"
                    },
                    {
                        "name": "Y. Michimura"
                    },
                    {
                        "name": "H. Middleton"
                    },
                    {
                        "name": "D. P. Mihaylov"
                    },
                    {
                        "name": "A. L. Miller"
                    },
                    {
                        "name": "S. J. Miller"
                    },
                    {
                        "name": "M. Millhouse"
                    },
                    {
                        "name": "E. Milotti"
                    },
                    {
                        "name": "V. Milotti"
                    },
                    {
                        "name": "Y. Minenkov"
                    },
                    {
                        "name": "E. M. Minihan"
                    },
                    {
                        "name": "Ll. M. Mir"
                    },
                    {
                        "name": "L. Mirasola"
                    },
                    {
                        "name": "M. Miravet-Tenés"
                    },
                    {
                        "name": "C. -A. Miritescu"
                    },
                    {
                        "name": "A. Mishra"
                    },
                    {
                        "name": "C. Mishra"
                    },
                    {
                        "name": "T. Mishra"
                    },
                    {
                        "name": "A. L. Mitchell"
                    },
                    {
                        "name": "J. G. Mitchell"
                    },
                    {
                        "name": "S. Mitra"
                    },
                    {
                        "name": "V. P. Mitrofanov"
                    },
                    {
                        "name": "K. Mitsuhashi"
                    },
                    {
                        "name": "R. Mittleman"
                    },
                    {
                        "name": "O. Miyakawa"
                    },
                    {
                        "name": "S. Miyoki"
                    },
                    {
                        "name": "A. Miyoko"
                    },
                    {
                        "name": "G. Mo"
                    },
                    {
                        "name": "L. Mobilia"
                    },
                    {
                        "name": "S. R. P. Mohapatra"
                    },
                    {
                        "name": "S. R. Mohite"
                    },
                    {
                        "name": "M. Molina-Ruiz"
                    },
                    {
                        "name": "M. Mondin"
                    },
                    {
                        "name": "M. Montani"
                    },
                    {
                        "name": "C. J. Moore"
                    },
                    {
                        "name": "D. Moraru"
                    },
                    {
                        "name": "A. More"
                    },
                    {
                        "name": "S. More"
                    },
                    {
                        "name": "C. Moreno"
                    },
                    {
                        "name": "E. A. Moreno"
                    },
                    {
                        "name": "G. Moreno"
                    },
                    {
                        "name": "A. Moreso Serra"
                    },
                    {
                        "name": "S. Morisaki"
                    },
                    {
                        "name": "Y. Moriwaki"
                    },
                    {
                        "name": "G. Morras"
                    },
                    {
                        "name": "A. Moscatello"
                    },
                    {
                        "name": "M. Mould"
                    },
                    {
                        "name": "B. Mours"
                    },
                    {
                        "name": "C. M. Mow-Lowry"
                    },
                    {
                        "name": "L. Muccillo"
                    },
                    {
                        "name": "F. Muciaccia"
                    },
                    {
                        "name": "D. Mukherjee"
                    },
                    {
                        "name": "Samanwaya Mukherjee"
                    },
                    {
                        "name": "Soma Mukherjee"
                    },
                    {
                        "name": "Subroto Mukherjee"
                    },
                    {
                        "name": "Suvodip Mukherjee"
                    },
                    {
                        "name": "N. Mukund"
                    },
                    {
                        "name": "A. Mullavey"
                    },
                    {
                        "name": "H. Mullock"
                    },
                    {
                        "name": "J. Mundi"
                    },
                    {
                        "name": "C. L. Mungioli"
                    },
                    {
                        "name": "M. Murakoshi"
                    },
                    {
                        "name": "P. G. Murray"
                    },
                    {
                        "name": "D. Nabari"
                    },
                    {
                        "name": "S. L. Nadji"
                    },
                    {
                        "name": "A. Nagar"
                    },
                    {
                        "name": "N. Nagarajan"
                    },
                    {
                        "name": "K. Nakagaki"
                    },
                    {
                        "name": "K. Nakamura"
                    },
                    {
                        "name": "H. Nakano"
                    },
                    {
                        "name": "M. Nakano"
                    },
                    {
                        "name": "D. Nanadoumgar-Lacroze"
                    },
                    {
                        "name": "D. Nandi"
                    },
                    {
                        "name": "V. Napolano"
                    },
                    {
                        "name": "P. Narayan"
                    },
                    {
                        "name": "I. Nardecchia"
                    },
                    {
                        "name": "T. Narikawa"
                    },
                    {
                        "name": "H. Narola"
                    },
                    {
                        "name": "L. Naticchioni"
                    },
                    {
                        "name": "R. K. Nayak"
                    },
                    {
                        "name": "L. Negri"
                    },
                    {
                        "name": "A. Nela"
                    },
                    {
                        "name": "C. Nelle"
                    },
                    {
                        "name": "A. Nelson"
                    },
                    {
                        "name": "T. J. N. Nelson"
                    },
                    {
                        "name": "M. Nery"
                    },
                    {
                        "name": "A. Neunzert"
                    },
                    {
                        "name": "S. Ng"
                    },
                    {
                        "name": "L. Nguyen Quynh"
                    },
                    {
                        "name": "S. A. Nichols"
                    },
                    {
                        "name": "A. B. Nielsen"
                    },
                    {
                        "name": "Y. Nishino"
                    },
                    {
                        "name": "A. Nishizawa"
                    },
                    {
                        "name": "S. Nissanke"
                    },
                    {
                        "name": "W. Niu"
                    },
                    {
                        "name": "F. Nocera"
                    },
                    {
                        "name": "J. Noller"
                    },
                    {
                        "name": "M. Norman"
                    },
                    {
                        "name": "C. North"
                    },
                    {
                        "name": "J. Novak"
                    },
                    {
                        "name": "R. Nowicki"
                    },
                    {
                        "name": "J. F. Nuño Siles"
                    },
                    {
                        "name": "L. K. Nuttall"
                    },
                    {
                        "name": "K. Obayashi"
                    },
                    {
                        "name": "J. Oberling"
                    },
                    {
                        "name": "J. O'Dell"
                    },
                    {
                        "name": "E. Oelker"
                    },
                    {
                        "name": "M. Oertel"
                    },
                    {
                        "name": "G. Oganesyan"
                    },
                    {
                        "name": "T. O'Hanlon"
                    },
                    {
                        "name": "M. Ohashi"
                    },
                    {
                        "name": "F. Ohme"
                    },
                    {
                        "name": "R. Oliveri"
                    },
                    {
                        "name": "R. Omer"
                    },
                    {
                        "name": "B. O'Neal"
                    },
                    {
                        "name": "M. Onishi"
                    },
                    {
                        "name": "K. Oohara"
                    },
                    {
                        "name": "B. O'Reilly"
                    },
                    {
                        "name": "M. Orselli"
                    },
                    {
                        "name": "R. O'Shaughnessy"
                    },
                    {
                        "name": "S. O'Shea"
                    },
                    {
                        "name": "S. Oshino"
                    },
                    {
                        "name": "C. Osthelder"
                    },
                    {
                        "name": "I. Ota"
                    },
                    {
                        "name": "D. J. Ottaway"
                    },
                    {
                        "name": "A. Ouzriat"
                    },
                    {
                        "name": "H. Overmier"
                    },
                    {
                        "name": "B. J. Owen"
                    },
                    {
                        "name": "R. Ozaki"
                    },
                    {
                        "name": "A. E. Pace"
                    },
                    {
                        "name": "R. Pagano"
                    },
                    {
                        "name": "M. A. Page"
                    },
                    {
                        "name": "A. Pai"
                    },
                    {
                        "name": "L. Paiella"
                    },
                    {
                        "name": "A. Pal"
                    },
                    {
                        "name": "S. Pal"
                    },
                    {
                        "name": "M. A. Palaia"
                    },
                    {
                        "name": "M. Pálfi"
                    },
                    {
                        "name": "P. P. Palma"
                    },
                    {
                        "name": "C. Palomba"
                    },
                    {
                        "name": "P. Palud"
                    },
                    {
                        "name": "H. Pan"
                    },
                    {
                        "name": "J. Pan"
                    },
                    {
                        "name": "K. C. Pan"
                    },
                    {
                        "name": "P. K. Panda"
                    },
                    {
                        "name": "Shiksha Pandey"
                    },
                    {
                        "name": "Swadha Pandey"
                    },
                    {
                        "name": "P. T. H. Pang"
                    },
                    {
                        "name": "F. Pannarale"
                    },
                    {
                        "name": "K. A. Pannone"
                    },
                    {
                        "name": "B. C. Pant"
                    },
                    {
                        "name": "F. H. Panther"
                    },
                    {
                        "name": "M. Panzeri"
                    },
                    {
                        "name": "F. Paoletti"
                    },
                    {
                        "name": "A. Paolone"
                    },
                    {
                        "name": "A. Papadopoulos"
                    },
                    {
                        "name": "E. E. Papalexakis"
                    },
                    {
                        "name": "L. Papalini"
                    },
                    {
                        "name": "G. Papigkiotis"
                    },
                    {
                        "name": "A. Paquis"
                    },
                    {
                        "name": "A. Parisi"
                    },
                    {
                        "name": "B. -J. Park"
                    },
                    {
                        "name": "J. Park"
                    },
                    {
                        "name": "W. Parker"
                    },
                    {
                        "name": "G. Pascale"
                    },
                    {
                        "name": "D. Pascucci"
                    },
                    {
                        "name": "A. Pasqualetti"
                    },
                    {
                        "name": "R. Passaquieti"
                    },
                    {
                        "name": "L. Passenger"
                    },
                    {
                        "name": "D. Passuello"
                    },
                    {
                        "name": "O. Patane"
                    },
                    {
                        "name": "A. V. Patel"
                    },
                    {
                        "name": "D. Pathak"
                    },
                    {
                        "name": "A. Patra"
                    },
                    {
                        "name": "B. Patricelli"
                    },
                    {
                        "name": "B. G. Patterson"
                    },
                    {
                        "name": "K. Paul"
                    },
                    {
                        "name": "S. Paul"
                    },
                    {
                        "name": "E. Payne"
                    },
                    {
                        "name": "T. Pearce"
                    },
                    {
                        "name": "M. Pedraza"
                    },
                    {
                        "name": "A. Pele"
                    },
                    {
                        "name": "F. E. Peña Arellano"
                    },
                    {
                        "name": "X. Peng"
                    },
                    {
                        "name": "Y. Peng"
                    },
                    {
                        "name": "S. Penn"
                    },
                    {
                        "name": "M. D. Penuliar"
                    },
                    {
                        "name": "A. Perego"
                    },
                    {
                        "name": "Z. Pereira"
                    },
                    {
                        "name": "C. Périgois"
                    },
                    {
                        "name": "G. Perna"
                    },
                    {
                        "name": "A. Perreca"
                    },
                    {
                        "name": "J. Perret"
                    },
                    {
                        "name": "S. Perriès"
                    },
                    {
                        "name": "J. W. Perry"
                    },
                    {
                        "name": "D. Pesios"
                    },
                    {
                        "name": "S. Peters"
                    },
                    {
                        "name": "S. Petracca"
                    },
                    {
                        "name": "C. Petrillo"
                    },
                    {
                        "name": "H. P. Pfeiffer"
                    },
                    {
                        "name": "H. Pham"
                    },
                    {
                        "name": "K. A. Pham"
                    },
                    {
                        "name": "K. S. Phukon"
                    },
                    {
                        "name": "H. Phurailatpam"
                    },
                    {
                        "name": "M. Piarulli"
                    },
                    {
                        "name": "L. Piccari"
                    },
                    {
                        "name": "O. J. Piccinni"
                    },
                    {
                        "name": "M. Pichot"
                    },
                    {
                        "name": "M. Piendibene"
                    },
                    {
                        "name": "F. Piergiovanni"
                    },
                    {
                        "name": "L. Pierini"
                    },
                    {
                        "name": "G. Pierra"
                    },
                    {
                        "name": "V. Pierro"
                    },
                    {
                        "name": "M. Pietrzak"
                    },
                    {
                        "name": "M. Pillas"
                    },
                    {
                        "name": "F. Pilo"
                    },
                    {
                        "name": "L. Pinard"
                    },
                    {
                        "name": "I. M. Pinto"
                    },
                    {
                        "name": "M. Pinto"
                    },
                    {
                        "name": "B. J. Piotrzkowski"
                    },
                    {
                        "name": "M. Pirello"
                    },
                    {
                        "name": "M. D. Pitkin"
                    },
                    {
                        "name": "A. Placidi"
                    },
                    {
                        "name": "E. Placidi"
                    },
                    {
                        "name": "M. L. Planas"
                    },
                    {
                        "name": "W. Plastino"
                    },
                    {
                        "name": "C. Plunkett"
                    },
                    {
                        "name": "R. Poggiani"
                    },
                    {
                        "name": "E. Polini"
                    },
                    {
                        "name": "J. Pomper"
                    },
                    {
                        "name": "L. Pompili"
                    },
                    {
                        "name": "J. Poon"
                    },
                    {
                        "name": "E. Porcelli"
                    },
                    {
                        "name": "E. K. Porter"
                    },
                    {
                        "name": "C. Posnansky"
                    },
                    {
                        "name": "R. Poulton"
                    },
                    {
                        "name": "J. Powell"
                    },
                    {
                        "name": "G. S. Prabhu"
                    },
                    {
                        "name": "M. Pracchia"
                    },
                    {
                        "name": "B. K. Pradhan"
                    },
                    {
                        "name": "T. Pradier"
                    },
                    {
                        "name": "A. K. Prajapati"
                    },
                    {
                        "name": "K. Prasai"
                    },
                    {
                        "name": "R. Prasanna"
                    },
                    {
                        "name": "P. Prasia"
                    },
                    {
                        "name": "G. Pratten"
                    },
                    {
                        "name": "G. Principe"
                    },
                    {
                        "name": "G. A. Prodi"
                    },
                    {
                        "name": "P. Prosperi"
                    },
                    {
                        "name": "P. Prosposito"
                    },
                    {
                        "name": "A. C. Providence"
                    },
                    {
                        "name": "A. Puecher"
                    },
                    {
                        "name": "J. Pullin"
                    },
                    {
                        "name": "P. Puppo"
                    },
                    {
                        "name": "M. Pürrer"
                    },
                    {
                        "name": "H. Qi"
                    },
                    {
                        "name": "J. Qin"
                    },
                    {
                        "name": "G. Quéméner"
                    },
                    {
                        "name": "V. Quetschke"
                    },
                    {
                        "name": "P. J. Quinonez"
                    },
                    {
                        "name": "N. Qutob"
                    },
                    {
                        "name": "R. Rading"
                    },
                    {
                        "name": "P. Raffai"
                    },
                    {
                        "name": "I. Rainho"
                    },
                    {
                        "name": "S. Raja"
                    },
                    {
                        "name": "C. Rajan"
                    },
                    {
                        "name": "B. Rajbhandari"
                    },
                    {
                        "name": "K. E. Ramirez"
                    },
                    {
                        "name": "F. A. Ramis Vidal"
                    },
                    {
                        "name": "M. Ramos Arevalo"
                    },
                    {
                        "name": "A. Ramos-Buades"
                    },
                    {
                        "name": "S. Ranjan"
                    },
                    {
                        "name": "K. Ransom"
                    },
                    {
                        "name": "P. Rapagnani"
                    },
                    {
                        "name": "B. Ratto"
                    },
                    {
                        "name": "A. Ravichandran"
                    },
                    {
                        "name": "A. Ray"
                    },
                    {
                        "name": "V. Raymond"
                    },
                    {
                        "name": "M. Razzano"
                    },
                    {
                        "name": "J. Read"
                    },
                    {
                        "name": "T. Regimbau"
                    },
                    {
                        "name": "S. Reid"
                    },
                    {
                        "name": "C. Reissel"
                    },
                    {
                        "name": "D. H. Reitze"
                    },
                    {
                        "name": "A. I. Renzini"
                    },
                    {
                        "name": "B. Revenu"
                    },
                    {
                        "name": "A. Revilla Peña"
                    },
                    {
                        "name": "R. Reyes"
                    },
                    {
                        "name": "L. Ricca"
                    },
                    {
                        "name": "F. Ricci"
                    },
                    {
                        "name": "M. Ricci"
                    },
                    {
                        "name": "A. Ricciardone"
                    },
                    {
                        "name": "J. Rice"
                    },
                    {
                        "name": "J. W. Richardson"
                    },
                    {
                        "name": "M. L. Richardson"
                    },
                    {
                        "name": "A. Rijal"
                    },
                    {
                        "name": "K. Riles"
                    },
                    {
                        "name": "H. K. Riley"
                    },
                    {
                        "name": "S. Rinaldi"
                    },
                    {
                        "name": "J. Rittmeyer"
                    },
                    {
                        "name": "C. Robertson"
                    },
                    {
                        "name": "F. Robinet"
                    },
                    {
                        "name": "M. Robinson"
                    },
                    {
                        "name": "A. Rocchi"
                    },
                    {
                        "name": "L. Rolland"
                    },
                    {
                        "name": "J. G. Rollins"
                    },
                    {
                        "name": "R. Romano"
                    },
                    {
                        "name": "A. Romero"
                    },
                    {
                        "name": "I. M. Romero-Shaw"
                    },
                    {
                        "name": "J. H. Romie"
                    },
                    {
                        "name": "S. Ronchini"
                    },
                    {
                        "name": "T. J. Roocke"
                    },
                    {
                        "name": "L. Rosa"
                    },
                    {
                        "name": "T. J. Rosauer"
                    },
                    {
                        "name": "C. A. Rose"
                    },
                    {
                        "name": "D. Rosińska"
                    },
                    {
                        "name": "M. P. Ross"
                    },
                    {
                        "name": "M. Rossello-Sastre"
                    },
                    {
                        "name": "S. Rowan"
                    },
                    {
                        "name": "S. K. Roy"
                    },
                    {
                        "name": "S. Roy"
                    },
                    {
                        "name": "D. Rozza"
                    },
                    {
                        "name": "P. Ruggi"
                    },
                    {
                        "name": "N. Ruhama"
                    },
                    {
                        "name": "E. Ruiz Morales"
                    },
                    {
                        "name": "K. Ruiz-Rocha"
                    },
                    {
                        "name": "S. Sachdev"
                    },
                    {
                        "name": "T. Sadecki"
                    },
                    {
                        "name": "P. Saffarieh"
                    },
                    {
                        "name": "S. Safi-Harb"
                    },
                    {
                        "name": "M. R. Sah"
                    },
                    {
                        "name": "S. Saha"
                    },
                    {
                        "name": "T. Sainrat"
                    },
                    {
                        "name": "S. Sajith Menon"
                    },
                    {
                        "name": "K. Sakai"
                    },
                    {
                        "name": "Y. Sakai"
                    },
                    {
                        "name": "M. Sakellariadou"
                    },
                    {
                        "name": "S. Sakon"
                    },
                    {
                        "name": "O. S. Salafia"
                    },
                    {
                        "name": "F. Salces-Carcoba"
                    },
                    {
                        "name": "L. Salconi"
                    },
                    {
                        "name": "M. Saleem"
                    },
                    {
                        "name": "F. Salemi"
                    },
                    {
                        "name": "M. Sallé"
                    },
                    {
                        "name": "S. U. Salunkhe"
                    },
                    {
                        "name": "S. Salvador"
                    },
                    {
                        "name": "A. Salvarese"
                    },
                    {
                        "name": "A. Samajdar"
                    },
                    {
                        "name": "A. Sanchez"
                    },
                    {
                        "name": "E. J. Sanchez"
                    },
                    {
                        "name": "L. E. Sanchez"
                    },
                    {
                        "name": "N. Sanchis-Gual"
                    },
                    {
                        "name": "J. R. Sanders"
                    },
                    {
                        "name": "E. M. Sänger"
                    },
                    {
                        "name": "F. Santoliquido"
                    },
                    {
                        "name": "F. Sarandrea"
                    },
                    {
                        "name": "T. R. Saravanan"
                    },
                    {
                        "name": "N. Sarin"
                    },
                    {
                        "name": "P. Sarkar"
                    },
                    {
                        "name": "A. Sasli"
                    },
                    {
                        "name": "P. Sassi"
                    },
                    {
                        "name": "B. Sassolas"
                    },
                    {
                        "name": "B. S. Sathyaprakash"
                    },
                    {
                        "name": "R. Sato"
                    },
                    {
                        "name": "S. Sato"
                    },
                    {
                        "name": "Yukino Sato"
                    },
                    {
                        "name": "Yu Sato"
                    },
                    {
                        "name": "O. Sauter"
                    },
                    {
                        "name": "R. L. Savage"
                    },
                    {
                        "name": "T. Sawada"
                    },
                    {
                        "name": "H. L. Sawant"
                    },
                    {
                        "name": "S. Sayah"
                    },
                    {
                        "name": "V. Scacco"
                    },
                    {
                        "name": "D. Schaetzl"
                    },
                    {
                        "name": "M. Scheel"
                    },
                    {
                        "name": "A. Schiebelbein"
                    },
                    {
                        "name": "M. G. Schiworski"
                    },
                    {
                        "name": "P. Schmidt"
                    },
                    {
                        "name": "S. Schmidt"
                    },
                    {
                        "name": "R. Schnabel"
                    },
                    {
                        "name": "M. Schneewind"
                    },
                    {
                        "name": "R. M. S. Schofield"
                    },
                    {
                        "name": "K. Schouteden"
                    },
                    {
                        "name": "B. W. Schulte"
                    },
                    {
                        "name": "B. F. Schutz"
                    },
                    {
                        "name": "E. Schwartz"
                    },
                    {
                        "name": "M. Scialpi"
                    },
                    {
                        "name": "J. Scott"
                    },
                    {
                        "name": "S. M. Scott"
                    },
                    {
                        "name": "R. M. Sedas"
                    },
                    {
                        "name": "T. C. Seetharamu"
                    },
                    {
                        "name": "M. Seglar-Arroyo"
                    },
                    {
                        "name": "Y. Sekiguchi"
                    },
                    {
                        "name": "D. Sellers"
                    },
                    {
                        "name": "N. Sembo"
                    },
                    {
                        "name": "A. S. Sengupta"
                    },
                    {
                        "name": "E. G. Seo"
                    },
                    {
                        "name": "J. W. Seo"
                    },
                    {
                        "name": "V. Sequino"
                    },
                    {
                        "name": "M. Serra"
                    },
                    {
                        "name": "A. Sevrin"
                    },
                    {
                        "name": "T. Shaffer"
                    },
                    {
                        "name": "U. S. Shah"
                    },
                    {
                        "name": "M. A. Shaikh"
                    },
                    {
                        "name": "L. Shao"
                    },
                    {
                        "name": "A. K. Sharma"
                    },
                    {
                        "name": "Preeti Sharma"
                    },
                    {
                        "name": "Prianka Sharma"
                    },
                    {
                        "name": "Ritwik Sharma"
                    },
                    {
                        "name": "S. Sharma Chaudhary"
                    },
                    {
                        "name": "P. Shawhan"
                    },
                    {
                        "name": "N. S. Shcheblanov"
                    },
                    {
                        "name": "E. Sheridan"
                    },
                    {
                        "name": "Z. -H. Shi"
                    },
                    {
                        "name": "M. Shikauchi"
                    },
                    {
                        "name": "R. Shimomura"
                    },
                    {
                        "name": "H. Shinkai"
                    },
                    {
                        "name": "S. Shirke"
                    },
                    {
                        "name": "D. H. Shoemaker"
                    },
                    {
                        "name": "D. M. Shoemaker"
                    },
                    {
                        "name": "R. W. Short"
                    },
                    {
                        "name": "S. ShyamSundar"
                    },
                    {
                        "name": "A. Sider"
                    },
                    {
                        "name": "H. Siegel"
                    },
                    {
                        "name": "D. Sigg"
                    },
                    {
                        "name": "L. Silenzi"
                    },
                    {
                        "name": "L. Silvestri"
                    },
                    {
                        "name": "M. Simmonds"
                    },
                    {
                        "name": "L. P. Singer"
                    },
                    {
                        "name": "Amitesh Singh"
                    },
                    {
                        "name": "Anika Singh"
                    },
                    {
                        "name": "D. Singh"
                    },
                    {
                        "name": "N. Singh"
                    },
                    {
                        "name": "S. Singh"
                    },
                    {
                        "name": "A. M. Sintes"
                    },
                    {
                        "name": "V. Sipala"
                    },
                    {
                        "name": "V. Skliris"
                    },
                    {
                        "name": "B. J. J. Slagmolen"
                    },
                    {
                        "name": "D. A. Slater"
                    },
                    {
                        "name": "T. J. Slaven-Blair"
                    },
                    {
                        "name": "J. Smetana"
                    },
                    {
                        "name": "J. R. Smith"
                    },
                    {
                        "name": "L. Smith"
                    },
                    {
                        "name": "R. J. E. Smith"
                    },
                    {
                        "name": "W. J. Smith"
                    },
                    {
                        "name": "S. Soares de Albuquerque Filho"
                    },
                    {
                        "name": "M. Soares-Santos"
                    },
                    {
                        "name": "K. Somiya"
                    },
                    {
                        "name": "I. Song"
                    },
                    {
                        "name": "S. Soni"
                    },
                    {
                        "name": "V. Sordini"
                    },
                    {
                        "name": "F. Sorrentino"
                    },
                    {
                        "name": "H. Sotani"
                    },
                    {
                        "name": "F. Spada"
                    },
                    {
                        "name": "V. Spagnuolo"
                    },
                    {
                        "name": "A. P. Spencer"
                    },
                    {
                        "name": "P. Spinicelli"
                    },
                    {
                        "name": "A. K. Srivastava"
                    },
                    {
                        "name": "F. Stachurski"
                    },
                    {
                        "name": "C. J. Stark"
                    },
                    {
                        "name": "D. A. Steer"
                    },
                    {
                        "name": "N. Steinle"
                    },
                    {
                        "name": "J. Steinlechner"
                    },
                    {
                        "name": "S. Steinlechner"
                    },
                    {
                        "name": "N. Stergioulas"
                    },
                    {
                        "name": "P. Stevens"
                    },
                    {
                        "name": "S. P. Stevenson"
                    },
                    {
                        "name": "M. StPierre"
                    },
                    {
                        "name": "M. D. Strong"
                    },
                    {
                        "name": "A. Strunk"
                    },
                    {
                        "name": "A. L. Stuver"
                    },
                    {
                        "name": "M. Suchenek"
                    },
                    {
                        "name": "S. Sudhagar"
                    },
                    {
                        "name": "Y. Sudo"
                    },
                    {
                        "name": "N. Sueltmann"
                    },
                    {
                        "name": "L. Suleiman"
                    },
                    {
                        "name": "K. D. Sullivan"
                    },
                    {
                        "name": "J. Sun"
                    },
                    {
                        "name": "L. Sun"
                    },
                    {
                        "name": "S. Sunil"
                    },
                    {
                        "name": "J. Suresh"
                    },
                    {
                        "name": "B. J. Sutton"
                    },
                    {
                        "name": "P. J. Sutton"
                    },
                    {
                        "name": "K. Suzuki"
                    },
                    {
                        "name": "M. Suzuki"
                    },
                    {
                        "name": "B. L. Swinkels"
                    },
                    {
                        "name": "A. Syx"
                    },
                    {
                        "name": "M. J. Szczepańczyk"
                    },
                    {
                        "name": "P. Szewczyk"
                    },
                    {
                        "name": "M. Tacca"
                    },
                    {
                        "name": "H. Tagoshi"
                    },
                    {
                        "name": "K. Takada"
                    },
                    {
                        "name": "H. Takahashi"
                    },
                    {
                        "name": "R. Takahashi"
                    },
                    {
                        "name": "A. Takamori"
                    },
                    {
                        "name": "S. Takano"
                    },
                    {
                        "name": "H. Takeda"
                    },
                    {
                        "name": "K. Takeshita"
                    },
                    {
                        "name": "I. Takimoto Schmiegelow"
                    },
                    {
                        "name": "M. Takou-Ayaoh"
                    },
                    {
                        "name": "C. Talbot"
                    },
                    {
                        "name": "M. Tamaki"
                    },
                    {
                        "name": "N. Tamanini"
                    },
                    {
                        "name": "D. Tanabe"
                    },
                    {
                        "name": "K. Tanaka"
                    },
                    {
                        "name": "S. J. Tanaka"
                    },
                    {
                        "name": "S. Tanioka"
                    },
                    {
                        "name": "D. B. Tanner"
                    },
                    {
                        "name": "W. Tanner"
                    },
                    {
                        "name": "L. Tao"
                    },
                    {
                        "name": "R. D. Tapia"
                    },
                    {
                        "name": "E. N. Tapia San Martín"
                    },
                    {
                        "name": "C. Taranto"
                    },
                    {
                        "name": "A. Taruya"
                    },
                    {
                        "name": "J. D. Tasson"
                    },
                    {
                        "name": "J. G. Tau"
                    },
                    {
                        "name": "D. Tellez"
                    },
                    {
                        "name": "R. Tenorio"
                    },
                    {
                        "name": "H. Themann"
                    },
                    {
                        "name": "A. Theodoropoulos"
                    },
                    {
                        "name": "M. P. Thirugnanasambandam"
                    },
                    {
                        "name": "L. M. Thomas"
                    },
                    {
                        "name": "M. Thomas"
                    },
                    {
                        "name": "P. Thomas"
                    },
                    {
                        "name": "J. E. Thompson"
                    },
                    {
                        "name": "S. R. Thondapu"
                    },
                    {
                        "name": "K. A. Thorne"
                    },
                    {
                        "name": "E. Thrane"
                    },
                    {
                        "name": "J. Tissino"
                    },
                    {
                        "name": "A. Tiwari"
                    },
                    {
                        "name": "Pawan Tiwari"
                    },
                    {
                        "name": "Praveer Tiwari"
                    },
                    {
                        "name": "S. Tiwari"
                    },
                    {
                        "name": "V. Tiwari"
                    },
                    {
                        "name": "M. R. Todd"
                    },
                    {
                        "name": "M. Toffano"
                    },
                    {
                        "name": "A. M. Toivonen"
                    },
                    {
                        "name": "K. Toland"
                    },
                    {
                        "name": "A. E. Tolley"
                    },
                    {
                        "name": "T. Tomaru"
                    },
                    {
                        "name": "V. Tommasini"
                    },
                    {
                        "name": "T. Tomura"
                    },
                    {
                        "name": "H. Tong"
                    },
                    {
                        "name": "C. Tong-Yu"
                    },
                    {
                        "name": "A. Torres-Forné"
                    },
                    {
                        "name": "C. I. Torrie"
                    },
                    {
                        "name": "I. Tosta e Melo"
                    },
                    {
                        "name": "E. Tournefier"
                    },
                    {
                        "name": "M. Trad Nery"
                    },
                    {
                        "name": "K. Tran"
                    },
                    {
                        "name": "A. Trapananti"
                    },
                    {
                        "name": "R. Travaglini"
                    },
                    {
                        "name": "F. Travasso"
                    },
                    {
                        "name": "G. Traylor"
                    },
                    {
                        "name": "M. Trevor"
                    },
                    {
                        "name": "M. C. Tringali"
                    },
                    {
                        "name": "A. Tripathee"
                    },
                    {
                        "name": "G. Troian"
                    },
                    {
                        "name": "A. Trovato"
                    },
                    {
                        "name": "L. Trozzo"
                    },
                    {
                        "name": "R. J. Trudeau"
                    },
                    {
                        "name": "T. Tsang"
                    },
                    {
                        "name": "S. Tsuchida"
                    },
                    {
                        "name": "L. Tsukada"
                    },
                    {
                        "name": "K. Turbang"
                    },
                    {
                        "name": "M. Turconi"
                    },
                    {
                        "name": "C. Turski"
                    },
                    {
                        "name": "H. Ubach"
                    },
                    {
                        "name": "N. Uchikata"
                    },
                    {
                        "name": "T. Uchiyama"
                    },
                    {
                        "name": "R. P. Udall"
                    },
                    {
                        "name": "T. Uehara"
                    },
                    {
                        "name": "K. Ueno"
                    },
                    {
                        "name": "V. Undheim"
                    },
                    {
                        "name": "L. E. Uronen"
                    },
                    {
                        "name": "T. Ushiba"
                    },
                    {
                        "name": "M. Vacatello"
                    },
                    {
                        "name": "H. Vahlbruch"
                    },
                    {
                        "name": "N. Vaidya"
                    },
                    {
                        "name": "G. Vajente"
                    },
                    {
                        "name": "A. Vajpeyi"
                    },
                    {
                        "name": "J. Valencia"
                    },
                    {
                        "name": "M. Valentini"
                    },
                    {
                        "name": "S. A. Vallejo-Peña"
                    },
                    {
                        "name": "S. Vallero"
                    },
                    {
                        "name": "V. Valsan"
                    },
                    {
                        "name": "M. van Dael"
                    },
                    {
                        "name": "E. Van den Bossche"
                    },
                    {
                        "name": "J. F. J. van den Brand"
                    },
                    {
                        "name": "C. Van Den Broeck"
                    },
                    {
                        "name": "M. van der Sluys"
                    },
                    {
                        "name": "A. Van de Walle"
                    },
                    {
                        "name": "J. van Dongen"
                    },
                    {
                        "name": "K. Vandra"
                    },
                    {
                        "name": "M. VanDyke"
                    },
                    {
                        "name": "H. van Haevermaet"
                    },
                    {
                        "name": "J. V. van Heijningen"
                    },
                    {
                        "name": "P. Van Hove"
                    },
                    {
                        "name": "J. Vanier"
                    },
                    {
                        "name": "M. VanKeuren"
                    },
                    {
                        "name": "J. Vanosky"
                    },
                    {
                        "name": "N. van Remortel"
                    },
                    {
                        "name": "M. Vardaro"
                    },
                    {
                        "name": "A. F. Vargas"
                    },
                    {
                        "name": "V. Varma"
                    },
                    {
                        "name": "A. N. Vazquez"
                    },
                    {
                        "name": "A. Vecchio"
                    },
                    {
                        "name": "G. Vedovato"
                    },
                    {
                        "name": "J. Veitch"
                    },
                    {
                        "name": "P. J. Veitch"
                    },
                    {
                        "name": "S. Venikoudis"
                    },
                    {
                        "name": "R. C. Venterea"
                    },
                    {
                        "name": "P. Verdier"
                    },
                    {
                        "name": "M. Vereecken"
                    },
                    {
                        "name": "D. Verkindt"
                    },
                    {
                        "name": "B. Verma"
                    },
                    {
                        "name": "Y. Verma"
                    },
                    {
                        "name": "S. M. Vermeulen"
                    },
                    {
                        "name": "F. Vetrano"
                    },
                    {
                        "name": "A. Veutro"
                    },
                    {
                        "name": "A. Viceré"
                    },
                    {
                        "name": "S. Vidyant"
                    },
                    {
                        "name": "A. D. Viets"
                    },
                    {
                        "name": "A. Vijaykumar"
                    },
                    {
                        "name": "A. Vilkha"
                    },
                    {
                        "name": "N. Villanueva Espinosa"
                    },
                    {
                        "name": "V. Villa-Ortega"
                    },
                    {
                        "name": "E. T. Vincent"
                    },
                    {
                        "name": "J. -Y. Vinet"
                    },
                    {
                        "name": "S. Viret"
                    },
                    {
                        "name": "S. Vitale"
                    },
                    {
                        "name": "H. Vocca"
                    },
                    {
                        "name": "D. Voigt"
                    },
                    {
                        "name": "E. R. G. von Reis"
                    },
                    {
                        "name": "J. S. A. von Wrangel"
                    },
                    {
                        "name": "W. E. Vossius"
                    },
                    {
                        "name": "L. Vujeva"
                    },
                    {
                        "name": "S. P. Vyatchanin"
                    },
                    {
                        "name": "J. Wack"
                    },
                    {
                        "name": "L. E. Wade"
                    },
                    {
                        "name": "M. Wade"
                    },
                    {
                        "name": "K. J. Wagner"
                    },
                    {
                        "name": "L. Wallace"
                    },
                    {
                        "name": "E. J. Wang"
                    },
                    {
                        "name": "H. Wang"
                    },
                    {
                        "name": "J. Z. Wang"
                    },
                    {
                        "name": "W. H. Wang"
                    },
                    {
                        "name": "Y. F. Wang"
                    },
                    {
                        "name": "G. Waratkar"
                    },
                    {
                        "name": "J. Warner"
                    },
                    {
                        "name": "M. Was"
                    },
                    {
                        "name": "T. Washimi"
                    },
                    {
                        "name": "N. Y. Washington"
                    },
                    {
                        "name": "D. Watarai"
                    },
                    {
                        "name": "B. Weaver"
                    },
                    {
                        "name": "S. A. Webster"
                    },
                    {
                        "name": "N. L. Weickhardt"
                    },
                    {
                        "name": "M. Weinert"
                    },
                    {
                        "name": "A. J. Weinstein"
                    },
                    {
                        "name": "R. Weiss"
                    },
                    {
                        "name": "L. Wen"
                    },
                    {
                        "name": "K. Wette"
                    },
                    {
                        "name": "J. T. Whelan"
                    },
                    {
                        "name": "B. F. Whiting"
                    },
                    {
                        "name": "C. Whittle"
                    },
                    {
                        "name": "E. G. Wickens"
                    },
                    {
                        "name": "D. Wilken"
                    },
                    {
                        "name": "A. T. Wilkin"
                    },
                    {
                        "name": "B. M. Williams"
                    },
                    {
                        "name": "D. Williams"
                    },
                    {
                        "name": "M. J. Williams"
                    },
                    {
                        "name": "N. S. Williams"
                    },
                    {
                        "name": "J. L. Willis"
                    },
                    {
                        "name": "B. Willke"
                    },
                    {
                        "name": "M. Wils"
                    },
                    {
                        "name": "L. Wilson"
                    },
                    {
                        "name": "C. W. Winborn"
                    },
                    {
                        "name": "J. Winterflood"
                    },
                    {
                        "name": "C. C. Wipf"
                    },
                    {
                        "name": "G. Woan"
                    },
                    {
                        "name": "J. Woehler"
                    },
                    {
                        "name": "N. E. Wolfe"
                    },
                    {
                        "name": "H. T. Wong"
                    },
                    {
                        "name": "I. C. F. Wong"
                    },
                    {
                        "name": "K. Wong"
                    },
                    {
                        "name": "T. Wouters"
                    },
                    {
                        "name": "J. L. Wright"
                    },
                    {
                        "name": "M. Wright"
                    },
                    {
                        "name": "B. Wu"
                    },
                    {
                        "name": "C. Wu"
                    },
                    {
                        "name": "D. S. Wu"
                    },
                    {
                        "name": "H. Wu"
                    },
                    {
                        "name": "K. Wu"
                    },
                    {
                        "name": "Q. Wu"
                    },
                    {
                        "name": "Y. Wu"
                    },
                    {
                        "name": "Z. Wu"
                    },
                    {
                        "name": "E. Wuchner"
                    },
                    {
                        "name": "D. M. Wysocki"
                    },
                    {
                        "name": "V. A. Xu"
                    },
                    {
                        "name": "Y. Xu"
                    },
                    {
                        "name": "N. Yadav"
                    },
                    {
                        "name": "H. Yamamoto"
                    },
                    {
                        "name": "K. Yamamoto"
                    },
                    {
                        "name": "T. S. Yamamoto"
                    },
                    {
                        "name": "T. Yamamoto"
                    },
                    {
                        "name": "R. Yamazaki"
                    },
                    {
                        "name": "T. Yan"
                    },
                    {
                        "name": "K. Z. Yang"
                    },
                    {
                        "name": "Y. Yang"
                    },
                    {
                        "name": "Z. Yarbrough"
                    },
                    {
                        "name": "J. Yebana"
                    },
                    {
                        "name": "S. -W. Yeh"
                    },
                    {
                        "name": "A. B. Yelikar"
                    },
                    {
                        "name": "X. Yin"
                    },
                    {
                        "name": "J. Yokoyama"
                    },
                    {
                        "name": "T. Yokozawa"
                    },
                    {
                        "name": "S. Yuan"
                    },
                    {
                        "name": "H. Yuzurihara"
                    },
                    {
                        "name": "M. Zanolin"
                    },
                    {
                        "name": "M. Zeeshan"
                    },
                    {
                        "name": "T. Zelenova"
                    },
                    {
                        "name": "J. -P. Zendri"
                    },
                    {
                        "name": "M. Zeoli"
                    },
                    {
                        "name": "M. Zerrad"
                    },
                    {
                        "name": "M. Zevin"
                    },
                    {
                        "name": "L. Zhang"
                    },
                    {
                        "name": "N. Zhang"
                    },
                    {
                        "name": "R. Zhang"
                    },
                    {
                        "name": "T. Zhang"
                    },
                    {
                        "name": "C. Zhao"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Yuhang Zhao"
                    },
                    {
                        "name": "Z. -C. Zhao"
                    },
                    {
                        "name": "Y. Zheng"
                    },
                    {
                        "name": "H. Zhong"
                    },
                    {
                        "name": "H. Zhou"
                    },
                    {
                        "name": "H. O. Zhu"
                    },
                    {
                        "name": "Z. -H. Zhu"
                    },
                    {
                        "name": "A. B. Zimmerman"
                    },
                    {
                        "name": "L. Zimmermann"
                    },
                    {
                        "name": "M. E. Zucker"
                    },
                    {
                        "name": "J. Zweizig"
                    }
                ],
                "author_detail": {
                    "name": "J. Zweizig"
                },
                "author": "J. Zweizig",
                "arxiv_comment": "As part of the Astrophysical Journal Letters Focus Issue on the\n  Gravitational Wave Transient Catalog",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06108v1",
                "updated": "2025-10-07T16:40:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    40,
                    42,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:40:42Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    40,
                    42,
                    1,
                    280,
                    0
                ],
                "title": "Influence Functions for Efficient Data Selection in Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence Functions for Efficient Data Selection in Reasoning"
                },
                "summary": "Fine-tuning large language models (LLMs) on chain-of-thought (CoT) data shows\nthat a small amount of high-quality data can outperform massive datasets. Yet,\nwhat constitutes \"quality\" remains ill-defined. Existing reasoning methods rely\non indirect heuristics such as problem difficulty or trace length, while\ninstruction-tuning has explored a broader range of automated selection\nstrategies, but rarely in the context of reasoning. We propose to define\nreasoning data quality using influence functions, which measure the causal\neffect of individual CoT examples on downstream accuracy, and introduce\ninfluence-based pruning, which consistently outperforms perplexity and\nembedding-based baselines on math reasoning within a model family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on chain-of-thought (CoT) data shows\nthat a small amount of high-quality data can outperform massive datasets. Yet,\nwhat constitutes \"quality\" remains ill-defined. Existing reasoning methods rely\non indirect heuristics such as problem difficulty or trace length, while\ninstruction-tuning has explored a broader range of automated selection\nstrategies, but rarely in the context of reasoning. We propose to define\nreasoning data quality using influence functions, which measure the causal\neffect of individual CoT examples on downstream accuracy, and introduce\ninfluence-based pruning, which consistently outperforms perplexity and\nembedding-based baselines on math reasoning within a model family."
                },
                "authors": [
                    {
                        "name": "Prateek Humane"
                    },
                    {
                        "name": "Paolo Cudrano"
                    },
                    {
                        "name": "Daniel Z. Kaplan"
                    },
                    {
                        "name": "Matteo Matteucci"
                    },
                    {
                        "name": "Supriyo Chakraborty"
                    },
                    {
                        "name": "Irina Rish"
                    }
                ],
                "author_detail": {
                    "name": "Irina Rish"
                },
                "author": "Irina Rish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06107v1",
                "updated": "2025-10-07T16:40:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    40,
                    31,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:40:31Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    40,
                    31,
                    1,
                    280,
                    0
                ],
                "title": "Distributional Semantics Tracing: A Framework for Explaining\n  Hallucinations in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributional Semantics Tracing: A Framework for Explaining\n  Hallucinations in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are prone to hallucination, the generation of\nplausible yet factually incorrect statements. This work investigates the\nintrinsic, architectural origins of this failure mode through three primary\ncontributions.First, to enable the reliable tracing of internal semantic\nfailures, we propose \\textbf{Distributional Semantics Tracing (DST)}, a unified\nframework that integrates established interpretability techniques to produce a\ncausal map of a model's reasoning, treating meaning as a function of context\n(distributional semantics). Second, we pinpoint the model's layer at which a\nhallucination becomes inevitable, identifying a specific \\textbf{commitment\nlayer} where a model's internal representations irreversibly diverge from\nfactuality. Third, we identify the underlying mechanism for these failures. We\nobserve a conflict between distinct computational pathways, which we interpret\nusing the lens of dual-process theory: a fast, heuristic \\textbf{associative\npathway} (akin to System 1) and a slow, deliberate \\textbf{contextual pathway}\n(akin to System 2), leading to predictable failure modes such as\n\\textit{Reasoning Shortcut Hijacks}. Our framework's ability to quantify the\ncoherence of the contextual pathway reveals a strong negative correlation\n($\\rho = -0.863$) with hallucination rates, implying that these failures are\npredictable consequences of internal semantic weakness. The result is a\nmechanistic account of how, when, and why hallucinations occur within the\nTransformer architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are prone to hallucination, the generation of\nplausible yet factually incorrect statements. This work investigates the\nintrinsic, architectural origins of this failure mode through three primary\ncontributions.First, to enable the reliable tracing of internal semantic\nfailures, we propose \\textbf{Distributional Semantics Tracing (DST)}, a unified\nframework that integrates established interpretability techniques to produce a\ncausal map of a model's reasoning, treating meaning as a function of context\n(distributional semantics). Second, we pinpoint the model's layer at which a\nhallucination becomes inevitable, identifying a specific \\textbf{commitment\nlayer} where a model's internal representations irreversibly diverge from\nfactuality. Third, we identify the underlying mechanism for these failures. We\nobserve a conflict between distinct computational pathways, which we interpret\nusing the lens of dual-process theory: a fast, heuristic \\textbf{associative\npathway} (akin to System 1) and a slow, deliberate \\textbf{contextual pathway}\n(akin to System 2), leading to predictable failure modes such as\n\\textit{Reasoning Shortcut Hijacks}. Our framework's ability to quantify the\ncoherence of the contextual pathway reveals a strong negative correlation\n($\\rho = -0.863$) with hallucination rates, implying that these failures are\npredictable consequences of internal semantic weakness. The result is a\nmechanistic account of how, when, and why hallucinations occur within the\nTransformer architecture."
                },
                "authors": [
                    {
                        "name": "Gagan Bhatia"
                    },
                    {
                        "name": "Somayajulu G Sripada"
                    },
                    {
                        "name": "Kevin Allan"
                    },
                    {
                        "name": "Jacobo Azcona"
                    }
                ],
                "author_detail": {
                    "name": "Jacobo Azcona"
                },
                "author": "Jacobo Azcona",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06105v1",
                "updated": "2025-10-07T16:37:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    37,
                    15,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:37:15Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    37,
                    15,
                    1,
                    280,
                    0
                ],
                "title": "Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences"
                },
                "summary": "Large language models (LLMs) are increasingly shaping how information is\ncreated and disseminated, from companies using them to craft persuasive\nadvertisements, to election campaigns optimizing messaging to gain votes, to\nsocial media influencers boosting engagement. These settings are inherently\ncompetitive, with sellers, candidates, and influencers vying for audience\napproval, yet it remains poorly understood how competitive feedback loops\ninfluence LLM behavior. We show that optimizing LLMs for competitive success\ncan inadvertently drive misalignment. Using simulated environments across these\nscenarios, we find that, 6.3% increase in sales is accompanied by a 14.0% rise\nin deceptive marketing; in elections, a 4.9% gain in vote share coincides with\n22.3% more disinformation and 12.5% more populist rhetoric; and on social\nmedia, a 7.5% engagement boost comes with 188.6% more disinformation and a\n16.3% increase in promotion of harmful behaviors. We call this phenomenon\nMoloch's Bargain for AI--competitive success achieved at the cost of alignment.\nThese misaligned behaviors emerge even when models are explicitly instructed to\nremain truthful and grounded, revealing the fragility of current alignment\nsafeguards. Our findings highlight how market-driven optimization pressures can\nsystematically erode alignment, creating a race to the bottom, and suggest that\nsafe deployment of AI systems will require stronger governance and carefully\ndesigned incentives to prevent competitive dynamics from undermining societal\ntrust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly shaping how information is\ncreated and disseminated, from companies using them to craft persuasive\nadvertisements, to election campaigns optimizing messaging to gain votes, to\nsocial media influencers boosting engagement. These settings are inherently\ncompetitive, with sellers, candidates, and influencers vying for audience\napproval, yet it remains poorly understood how competitive feedback loops\ninfluence LLM behavior. We show that optimizing LLMs for competitive success\ncan inadvertently drive misalignment. Using simulated environments across these\nscenarios, we find that, 6.3% increase in sales is accompanied by a 14.0% rise\nin deceptive marketing; in elections, a 4.9% gain in vote share coincides with\n22.3% more disinformation and 12.5% more populist rhetoric; and on social\nmedia, a 7.5% engagement boost comes with 188.6% more disinformation and a\n16.3% increase in promotion of harmful behaviors. We call this phenomenon\nMoloch's Bargain for AI--competitive success achieved at the cost of alignment.\nThese misaligned behaviors emerge even when models are explicitly instructed to\nremain truthful and grounded, revealing the fragility of current alignment\nsafeguards. Our findings highlight how market-driven optimization pressures can\nsystematically erode alignment, creating a race to the bottom, and suggest that\nsafe deployment of AI systems will require stronger governance and carefully\ndesigned incentives to prevent competitive dynamics from undermining societal\ntrust."
                },
                "authors": [
                    {
                        "name": "Batu El"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "author": "James Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06104v1",
                "updated": "2025-10-07T16:36:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    36,
                    1,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:36:01Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    36,
                    1,
                    1,
                    280,
                    0
                ],
                "title": "Explaining Code Risk in OSS: Towards LLM-Generated Fault Prediction\n  Interpretations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining Code Risk in OSS: Towards LLM-Generated Fault Prediction\n  Interpretations"
                },
                "summary": "Open Source Software (OSS) has become a very important and crucial\ninfrastructure worldwide because of the value it provides. OSS typically\ndepends on contributions from developers across diverse backgrounds and levels\nof experience. Making safe changes, such as fixing a bug or implementing a new\nfeature, can be challenging, especially in object-oriented systems where\ncomponents are interdependent. Static analysis and defect-prediction tools\nproduce metrics (e.g., complexity,coupling) that flag potentially fault-prone\ncomponents, but these signals are often hard for contributors new or unfamiliar\nwith the codebase to interpret. Large Language Models (LLMs) have shown strong\nperformance on software engineering tasks such as code summarization and\ndocumentation generation. Building on this progress, we investigate whether\nLLMs can translate fault-prediction metrics into clear, human-readable risk\nexplanations and actionable guidance to help OSS contributors plan and review\ncode modifications. We outline explanation types that an LLM-generated\nassistant could provide (descriptive, contextual, and actionable explanations).\nWe also outline our next steps to assess usefulness through a task-based study\nwith OSS contributors, comparing metric-only baselines to LLM-generated\nexplanations on decision quality, time-to-completion, and error rates",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Source Software (OSS) has become a very important and crucial\ninfrastructure worldwide because of the value it provides. OSS typically\ndepends on contributions from developers across diverse backgrounds and levels\nof experience. Making safe changes, such as fixing a bug or implementing a new\nfeature, can be challenging, especially in object-oriented systems where\ncomponents are interdependent. Static analysis and defect-prediction tools\nproduce metrics (e.g., complexity,coupling) that flag potentially fault-prone\ncomponents, but these signals are often hard for contributors new or unfamiliar\nwith the codebase to interpret. Large Language Models (LLMs) have shown strong\nperformance on software engineering tasks such as code summarization and\ndocumentation generation. Building on this progress, we investigate whether\nLLMs can translate fault-prediction metrics into clear, human-readable risk\nexplanations and actionable guidance to help OSS contributors plan and review\ncode modifications. We outline explanation types that an LLM-generated\nassistant could provide (descriptive, contextual, and actionable explanations).\nWe also outline our next steps to assess usefulness through a task-based study\nwith OSS contributors, comparing metric-only baselines to LLM-generated\nexplanations on decision quality, time-to-completion, and error rates"
                },
                "authors": [
                    {
                        "name": "Elijah Kayode Adejumo"
                    },
                    {
                        "name": "Brittany Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Brittany Johnson"
                },
                "author": "Brittany Johnson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06101v1",
                "updated": "2025-10-07T16:32:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    32,
                    9,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:32:09Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    32,
                    9,
                    1,
                    280,
                    0
                ],
                "title": "The Valley of Code Reasoning: Scaling Knowledge Distillation of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Valley of Code Reasoning: Scaling Knowledge Distillation of Large\n  Language Models"
                },
                "summary": "Distilling the thinking traces of a Large Language Model (LLM) with reasoning\ncapabilities into a smaller model has been proven effective. Yet, there is a\nscarcity of work done on how model performances scale with the quantity of\ndistillation data. In this work, we study the scaling trend of distilling\ncompetitive coding skills on two small non-reasoning LLMs. We validate the\nhypothesis that there is a $\\textit{valley of code reasoning}$: downstream\nperformance on competitive coding first drops as data quantity increases, then\nit steadily increases in a sharper-than-log-linear fashion. Having identified\nthe trend, we further fine-tune the models at two different distillation stages\non the same data to ground conclusions on their respective learning phases. We\nlearn that across stages in the low and medium-low data regimes, small models\nbenefit significantly from easier coding questions than from harder ones. We\nalso find that, surprisingly, the correctness of outputs in training data makes\nno difference to distillation outcomes. Our work represents a step forward in\nunderstanding the training dynamics of code reasoning distillation outside\nintuition",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling the thinking traces of a Large Language Model (LLM) with reasoning\ncapabilities into a smaller model has been proven effective. Yet, there is a\nscarcity of work done on how model performances scale with the quantity of\ndistillation data. In this work, we study the scaling trend of distilling\ncompetitive coding skills on two small non-reasoning LLMs. We validate the\nhypothesis that there is a $\\textit{valley of code reasoning}$: downstream\nperformance on competitive coding first drops as data quantity increases, then\nit steadily increases in a sharper-than-log-linear fashion. Having identified\nthe trend, we further fine-tune the models at two different distillation stages\non the same data to ground conclusions on their respective learning phases. We\nlearn that across stages in the low and medium-low data regimes, small models\nbenefit significantly from easier coding questions than from harder ones. We\nalso find that, surprisingly, the correctness of outputs in training data makes\nno difference to distillation outcomes. Our work represents a step forward in\nunderstanding the training dynamics of code reasoning distillation outside\nintuition"
                },
                "authors": [
                    {
                        "name": "Muyu He"
                    },
                    {
                        "name": "Muhammad Ali Shafique"
                    },
                    {
                        "name": "Anand Kumar"
                    },
                    {
                        "name": "Tsach Mackey"
                    },
                    {
                        "name": "Nazneen Rajani"
                    }
                ],
                "author_detail": {
                    "name": "Nazneen Rajani"
                },
                "author": "Nazneen Rajani",
                "arxiv_comment": "NeurIPS 2025 Workshop on Deep Learning for Code (DL4C), Project page:\n  https://collinear.ai/valley-of-reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06100v1",
                "updated": "2025-10-07T16:31:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    31,
                    11,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:31:11Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    31,
                    11,
                    1,
                    280,
                    0
                ],
                "title": "Mass loading of outflows from evolving Young Massive Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mass loading of outflows from evolving Young Massive Clusters"
                },
                "summary": "Feedback from Young Massive Clusters (YMCs) is an important driver of galaxy\nevolution. In the first few Myr, mechanical feedback is dominated by collective\neffects of the massive stellar winds in the YMC. The mass-loss rates and\nterminal wind velocities of these stars change by orders of magnitude over\npre-SN timescales as the massive stars evolve, and mass-loss rates of Cool\nSupergiant (CSG) stars in particular are uncertain by a factor $\\sim~20$ or\nmore. In this work we perform a first study of the time evolution of average\ncluster wind velocity $\\bar{V}_{\\mathrm{cl}}$ as a function of stellar\nmetallicity $Z$, assuming single star evolution. We also check the validity of\nassuming Wolf-Rayet stars dominate the feedback effects of a YMC, as often done\nwhen interpreting X-ray and $\\gamma$-ray observations, and test how sensitive\n$\\bar{V}_{\\mathrm{cl}}$ is to current uncertainties in mass-loss rates. We use\npySTARBURST99 to calculate integrated properties of YMCs for $Z$ in the range\nof $0.0004-0.02$, representing a range of environments from IZw18 to the\nGalactic Centre. We find that $\\bar{V}_{\\mathrm{cl}}$ drops off rapidly for\nsub-LMC $Z$, and we recommend a value of $500-1000\\,~\\textrm{km~s}^{-1}$ be\nused in this regime. We show accounting only for WR stars can overestimate\n$\\bar{V}_{\\mathrm{cl}}$ by $500-2000\\,~\\textrm{km~s}^{-1}$ at $Z \\geq\nZ_\\text{LMC}$. We also find that different RSG mass-loss assumptions can change\nthe inferred $\\bar{V}_{\\mathrm{cl}}$ by $\\sim1000\\,~\\textrm{km~s}^{-1}$,\nhighlighting the need for improved observational constraints for RSGs in YMCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feedback from Young Massive Clusters (YMCs) is an important driver of galaxy\nevolution. In the first few Myr, mechanical feedback is dominated by collective\neffects of the massive stellar winds in the YMC. The mass-loss rates and\nterminal wind velocities of these stars change by orders of magnitude over\npre-SN timescales as the massive stars evolve, and mass-loss rates of Cool\nSupergiant (CSG) stars in particular are uncertain by a factor $\\sim~20$ or\nmore. In this work we perform a first study of the time evolution of average\ncluster wind velocity $\\bar{V}_{\\mathrm{cl}}$ as a function of stellar\nmetallicity $Z$, assuming single star evolution. We also check the validity of\nassuming Wolf-Rayet stars dominate the feedback effects of a YMC, as often done\nwhen interpreting X-ray and $\\gamma$-ray observations, and test how sensitive\n$\\bar{V}_{\\mathrm{cl}}$ is to current uncertainties in mass-loss rates. We use\npySTARBURST99 to calculate integrated properties of YMCs for $Z$ in the range\nof $0.0004-0.02$, representing a range of environments from IZw18 to the\nGalactic Centre. We find that $\\bar{V}_{\\mathrm{cl}}$ drops off rapidly for\nsub-LMC $Z$, and we recommend a value of $500-1000\\,~\\textrm{km~s}^{-1}$ be\nused in this regime. We show accounting only for WR stars can overestimate\n$\\bar{V}_{\\mathrm{cl}}$ by $500-2000\\,~\\textrm{km~s}^{-1}$ at $Z \\geq\nZ_\\text{LMC}$. We also find that different RSG mass-loss assumptions can change\nthe inferred $\\bar{V}_{\\mathrm{cl}}$ by $\\sim1000\\,~\\textrm{km~s}^{-1}$,\nhighlighting the need for improved observational constraints for RSGs in YMCs."
                },
                "authors": [
                    {
                        "name": "C. J. K. Larkin"
                    },
                    {
                        "name": "C. Hawcroft"
                    },
                    {
                        "name": "J. Mackey"
                    },
                    {
                        "name": "R. R. Lefever"
                    },
                    {
                        "name": "L. Härer"
                    },
                    {
                        "name": "A. A. C. Sander"
                    }
                ],
                "author_detail": {
                    "name": "A. A. C. Sander"
                },
                "author": "A. A. C. Sander",
                "arxiv_comment": "5 pages, 6 figures. Submitted to Astronomy & Astrophysics and updated\n  following comments from the referee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06096v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06096v2",
                "updated": "2025-10-08T10:07:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    10,
                    7,
                    14,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-07T16:25:14Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    25,
                    14,
                    1,
                    280,
                    0
                ],
                "title": "The Alignment Auditor: A Bayesian Framework for Verifying and Refining\n  LLM Objectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Alignment Auditor: A Bayesian Framework for Verifying and Refining\n  LLM Objectives"
                },
                "summary": "The objectives that Large Language Models (LLMs) implicitly optimize remain\ndangerously opaque, making trustworthy alignment and auditing a grand\nchallenge. While Inverse Reinforcement Learning (IRL) can infer reward\nfunctions from behaviour, existing approaches either produce a single,\noverconfident reward estimate or fail to address the fundamental ambiguity of\nthe task (non-identifiability). This paper introduces a principled auditing\nframework that re-frames reward inference from a simple estimation task to a\ncomprehensive process for verification. Our framework leverages Bayesian IRL to\nnot only recover a distribution over objectives but to enable three critical\naudit capabilities: (i) Quantifying and systematically reducing\nnon-identifiability by demonstrating posterior contraction over sequential\nrounds of evidence; (ii) Providing actionable, uncertainty-aware diagnostics\nthat expose spurious shortcuts and identify out-of-distribution prompts where\nthe inferred objective cannot be trusted; and (iii) Validating policy-level\nutility by showing that the refined, low-uncertainty reward can be used\ndirectly in RLHF to achieve training dynamics and toxicity reductions\ncomparable to the ground-truth alignment process. Empirically, our framework\nsuccessfully audits a detoxified LLM, yielding a well-calibrated and\ninterpretable objective that strengthens alignment guarantees. Overall, this\nwork provides a practical toolkit for auditors, safety teams, and regulators to\nverify what LLMs are truly trying to achieve, moving us toward more trustworthy\nand accountable AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The objectives that Large Language Models (LLMs) implicitly optimize remain\ndangerously opaque, making trustworthy alignment and auditing a grand\nchallenge. While Inverse Reinforcement Learning (IRL) can infer reward\nfunctions from behaviour, existing approaches either produce a single,\noverconfident reward estimate or fail to address the fundamental ambiguity of\nthe task (non-identifiability). This paper introduces a principled auditing\nframework that re-frames reward inference from a simple estimation task to a\ncomprehensive process for verification. Our framework leverages Bayesian IRL to\nnot only recover a distribution over objectives but to enable three critical\naudit capabilities: (i) Quantifying and systematically reducing\nnon-identifiability by demonstrating posterior contraction over sequential\nrounds of evidence; (ii) Providing actionable, uncertainty-aware diagnostics\nthat expose spurious shortcuts and identify out-of-distribution prompts where\nthe inferred objective cannot be trusted; and (iii) Validating policy-level\nutility by showing that the refined, low-uncertainty reward can be used\ndirectly in RLHF to achieve training dynamics and toxicity reductions\ncomparable to the ground-truth alignment process. Empirically, our framework\nsuccessfully audits a detoxified LLM, yielding a well-calibrated and\ninterpretable objective that strengthens alignment guarantees. Overall, this\nwork provides a practical toolkit for auditors, safety teams, and regulators to\nverify what LLMs are truly trying to achieve, moving us toward more trustworthy\nand accountable AI."
                },
                "authors": [
                    {
                        "name": "Matthieu Bou"
                    },
                    {
                        "name": "Nyal Patel"
                    },
                    {
                        "name": "Arjun Jagota"
                    },
                    {
                        "name": "Satyapriya Krishna"
                    },
                    {
                        "name": "Sonali Parbhoo"
                    }
                ],
                "author_detail": {
                    "name": "Sonali Parbhoo"
                },
                "author": "Sonali Parbhoo",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06096v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06096v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06093v1",
                "updated": "2025-10-07T16:21:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    21,
                    52,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:21:52Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    21,
                    52,
                    1,
                    280,
                    0
                ],
                "title": "Classical AI vs. LLMs for Decision-Maker Alignment in Health Insurance\n  Choices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical AI vs. LLMs for Decision-Maker Alignment in Health Insurance\n  Choices"
                },
                "summary": "As algorithmic decision-makers are increasingly applied to high-stakes\ndomains, AI alignment research has evolved from a focus on universal value\nalignment to context-specific approaches that account for decision-maker\nattributes. Prior work on Decision-Maker Alignment (DMA) has explored two\nprimary strategies: (1) classical AI methods integrating case-based reasoning,\nBayesian reasoning, and naturalistic decision-making, and (2) large language\nmodel (LLM)-based methods leveraging prompt engineering. While both approaches\nhave shown promise in limited domains such as medical triage, their\ngeneralizability to novel contexts remains underexplored. In this work, we\nimplement a prior classical AI model and develop an LLM-based algorithmic\ndecision-maker evaluated using a large reasoning model (GPT-5) and a\nnon-reasoning model (GPT-4) with weighted self-consistency under a zero-shot\nprompting framework, as proposed in recent literature. We evaluate both\napproaches on a health insurance decision-making dataset annotated for three\ntarget decision-makers with varying levels of risk tolerance (0.0, 0.5, 1.0).\nIn the experiments reported herein, classical AI and LLM-based models achieved\ncomparable alignment with attribute-based targets, with classical AI exhibiting\nslightly better alignment for a moderate risk profile. The dataset and\nopen-source implementation are publicly available at:\nhttps://github.com/TeX-Base/ClassicalAIvsLLMsforDMAlignment and\nhttps://github.com/Parallax-Advanced-Research/ITM/tree/feature_insurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As algorithmic decision-makers are increasingly applied to high-stakes\ndomains, AI alignment research has evolved from a focus on universal value\nalignment to context-specific approaches that account for decision-maker\nattributes. Prior work on Decision-Maker Alignment (DMA) has explored two\nprimary strategies: (1) classical AI methods integrating case-based reasoning,\nBayesian reasoning, and naturalistic decision-making, and (2) large language\nmodel (LLM)-based methods leveraging prompt engineering. While both approaches\nhave shown promise in limited domains such as medical triage, their\ngeneralizability to novel contexts remains underexplored. In this work, we\nimplement a prior classical AI model and develop an LLM-based algorithmic\ndecision-maker evaluated using a large reasoning model (GPT-5) and a\nnon-reasoning model (GPT-4) with weighted self-consistency under a zero-shot\nprompting framework, as proposed in recent literature. We evaluate both\napproaches on a health insurance decision-making dataset annotated for three\ntarget decision-makers with varying levels of risk tolerance (0.0, 0.5, 1.0).\nIn the experiments reported herein, classical AI and LLM-based models achieved\ncomparable alignment with attribute-based targets, with classical AI exhibiting\nslightly better alignment for a moderate risk profile. The dataset and\nopen-source implementation are publicly available at:\nhttps://github.com/TeX-Base/ClassicalAIvsLLMsforDMAlignment and\nhttps://github.com/Parallax-Advanced-Research/ITM/tree/feature_insurance."
                },
                "authors": [
                    {
                        "name": "Mallika Mainali"
                    },
                    {
                        "name": "Harsha Sureshbabu"
                    },
                    {
                        "name": "Anik Sen"
                    },
                    {
                        "name": "Christopher B. Rauch"
                    },
                    {
                        "name": "Noah D. Reifsnyder"
                    },
                    {
                        "name": "John Meyer"
                    },
                    {
                        "name": "J. T. Turner"
                    },
                    {
                        "name": "Michael W. Floyd"
                    },
                    {
                        "name": "Matthew Molineaux"
                    },
                    {
                        "name": "Rosina O. Weber"
                    }
                ],
                "author_detail": {
                    "name": "Rosina O. Weber"
                },
                "author": "Rosina O. Weber",
                "arxiv_comment": "15 pages, 3 figures. Accepted at the Twelfth Annual Conference on\n  Advances in Cognitive Systems (ACS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15351v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15351v2",
                "updated": "2025-10-07T16:21:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    21,
                    43,
                    1,
                    280,
                    0
                ],
                "published": "2025-04-21T18:00:31Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    0,
                    31,
                    0,
                    111,
                    0
                ],
                "title": "On the Connection between Field-Level Inference and $n$-point\n  Correlation Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Connection between Field-Level Inference and $n$-point\n  Correlation Functions"
                },
                "summary": "Bayesian field-level inference of galaxy clustering guarantees optimal\nextraction of all cosmological information, provided that the data are\ncorrectly described by the forward model employed. The latter is unfortunately\nnever strictly the case. A key question for field-level inference approaches\nthen is where the cosmological information is coming from, and how to ensure\nthat it is robust. In the context of perturbative approaches such as effective\nfield theory, some progress on this question can be made analytically. We\nderive the parameter posterior given the data for the field-level likelihood\ngiven in the effective field theory, marginalized over initial conditions in\nthe zero-noise limit. Particular attention is paid to cutoffs in the theory,\nthe generalization to higher orders, and the error made by an incomplete\nforward model at a given order. The main finding is that, broadly speaking, an\n$m$-th order forward model captures the information in $n$-point correlation\nfunctions with $n \\leqslant m+1$. Thus, by adding more terms to the forward\nmodel, field-level inference is made to automatically incorporate higher-order\n$n$-point functions. Also shown is how the effect of an incomplete forward\nmodel (at a given order) on the parameter inference can be estimated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian field-level inference of galaxy clustering guarantees optimal\nextraction of all cosmological information, provided that the data are\ncorrectly described by the forward model employed. The latter is unfortunately\nnever strictly the case. A key question for field-level inference approaches\nthen is where the cosmological information is coming from, and how to ensure\nthat it is robust. In the context of perturbative approaches such as effective\nfield theory, some progress on this question can be made analytically. We\nderive the parameter posterior given the data for the field-level likelihood\ngiven in the effective field theory, marginalized over initial conditions in\nthe zero-noise limit. Particular attention is paid to cutoffs in the theory,\nthe generalization to higher orders, and the error made by an incomplete\nforward model at a given order. The main finding is that, broadly speaking, an\n$m$-th order forward model captures the information in $n$-point correlation\nfunctions with $n \\leqslant m+1$. Thus, by adding more terms to the forward\nmodel, field-level inference is made to automatically incorporate higher-order\n$n$-point functions. Also shown is how the effect of an incomplete forward\nmodel (at a given order) on the parameter inference can be estimated."
                },
                "authors": [
                    {
                        "name": "Fabian Schmidt"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Schmidt"
                },
                "author": "Fabian Schmidt",
                "arxiv_doi": "10.1088/1475-7516/2025/09/056",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1475-7516/2025/09/056",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15351v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "44 pages; v2: brief review (Sec. 1.2) and minor clarifications added;\n  matches JCAP published version. No, I do not consent with the use of this\n  article for the training of AI models, as in\n  https://www.kaggle.com/datasets/Cornell-University/arxiv",
                "arxiv_journal_ref": "JCAP09(2025)056",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06092v1",
                "updated": "2025-10-07T16:20:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    20,
                    14,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:20:14Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    20,
                    14,
                    1,
                    280,
                    0
                ],
                "title": "Learning from Failures: Understanding LLM Alignment through\n  Failure-Aware Inverse RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Failures: Understanding LLM Alignment through\n  Failure-Aware Inverse RL"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) aligns Large Language\nModels (LLMs) with human preferences, yet the underlying reward signals they\ninternalize remain hidden, posing a critical challenge for interpretability and\nsafety. Existing approaches attempt to extract these latent incentives using\nInverse Reinforcement Learning (IRL), but treat all preference pairs equally,\noften overlooking the most informative signals: those examples the extracted\nreward model misclassifies or assigns nearly equal scores, which we term\n\\emph{failures}. We introduce a novel \\emph{failure-aware} IRL algorithm that\nfocuses on misclassified or difficult examples to recover the latent rewards\ndefining model behaviors. By learning from these failures, our failure-aware\nIRL extracts reward functions that better reflect the true objectives behind\nRLHF. We demonstrate that failure-aware IRL outperforms existing IRL baselines\nacross multiple metrics when applied to LLM detoxification, without requiring\nexternal classifiers or supervision. Crucially, failure-aware IRL yields\nrewards that better capture the true incentives learned during RLHF, enabling\nmore effective re-RLHF training than standard IRL. This establishes\nfailure-aware IRL as a robust, scalable method for auditing model alignment and\nreducing ambiguity in the IRL process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) aligns Large Language\nModels (LLMs) with human preferences, yet the underlying reward signals they\ninternalize remain hidden, posing a critical challenge for interpretability and\nsafety. Existing approaches attempt to extract these latent incentives using\nInverse Reinforcement Learning (IRL), but treat all preference pairs equally,\noften overlooking the most informative signals: those examples the extracted\nreward model misclassifies or assigns nearly equal scores, which we term\n\\emph{failures}. We introduce a novel \\emph{failure-aware} IRL algorithm that\nfocuses on misclassified or difficult examples to recover the latent rewards\ndefining model behaviors. By learning from these failures, our failure-aware\nIRL extracts reward functions that better reflect the true objectives behind\nRLHF. We demonstrate that failure-aware IRL outperforms existing IRL baselines\nacross multiple metrics when applied to LLM detoxification, without requiring\nexternal classifiers or supervision. Crucially, failure-aware IRL yields\nrewards that better capture the true incentives learned during RLHF, enabling\nmore effective re-RLHF training than standard IRL. This establishes\nfailure-aware IRL as a robust, scalable method for auditing model alignment and\nreducing ambiguity in the IRL process."
                },
                "authors": [
                    {
                        "name": "Nyal Patel"
                    },
                    {
                        "name": "Matthieu Bou"
                    },
                    {
                        "name": "Arjun Jagota"
                    },
                    {
                        "name": "Satyapriya Krishna"
                    },
                    {
                        "name": "Sonali Parbhoo"
                    }
                ],
                "author_detail": {
                    "name": "Sonali Parbhoo"
                },
                "author": "Sonali Parbhoo",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24893v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24893v2",
                "updated": "2025-10-07T16:18:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    18,
                    55,
                    1,
                    280,
                    0
                ],
                "published": "2025-09-29T15:03:31Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    3,
                    31,
                    0,
                    272,
                    0
                ],
                "title": "HBSplat: Robust Sparse-View Gaussian Reconstruction with Hybrid-Loss\n  Guided Depth and Bidirectional Warping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HBSplat: Robust Sparse-View Gaussian Reconstruction with Hybrid-Loss\n  Guided Depth and Bidirectional Warping"
                },
                "summary": "Novel View Synthesis (NVS) from sparse views presents a formidable challenge\nin 3D reconstruction, where limited multi-view constraints lead to severe\noverfitting, geometric distortion, and fragmented scenes. While 3D Gaussian\nSplatting (3DGS) delivers real-time, high-fidelity rendering, its performance\ndrastically deteriorates under sparse inputs, plagued by floating artifacts and\nstructural failures. To address these challenges, we introduce HBSplat, a\nunified framework that elevates 3DGS by seamlessly integrating robust\nstructural cues, virtual view constraints, and occluded region completion. Our\ncore contributions are threefold: a Hybrid-Loss Depth Estimation module that\nensures multi-view consistency by leveraging dense matching priors and\nintegrating reprojection, point propagation, and smoothness constraints; a\nBidirectional Warping Virtual View Synthesis method that enforces substantially\nstronger constraints by creating high-fidelity virtual views through\nbidirectional depth-image warping and multi-view fusion; and an Occlusion-Aware\nReconstruction component that recovers occluded areas using a depth-difference\nmask and a learning-based inpainting model. Extensive evaluations on LLFF,\nBlender, and DTU benchmarks validate that HBSplat sets a new state-of-the-art,\nachieving up to 21.13 dB PSNR and 0.189 LPIPS, while maintaining real-time\ninference. Code is available at: https://github.com/eternalland/HBSplat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel View Synthesis (NVS) from sparse views presents a formidable challenge\nin 3D reconstruction, where limited multi-view constraints lead to severe\noverfitting, geometric distortion, and fragmented scenes. While 3D Gaussian\nSplatting (3DGS) delivers real-time, high-fidelity rendering, its performance\ndrastically deteriorates under sparse inputs, plagued by floating artifacts and\nstructural failures. To address these challenges, we introduce HBSplat, a\nunified framework that elevates 3DGS by seamlessly integrating robust\nstructural cues, virtual view constraints, and occluded region completion. Our\ncore contributions are threefold: a Hybrid-Loss Depth Estimation module that\nensures multi-view consistency by leveraging dense matching priors and\nintegrating reprojection, point propagation, and smoothness constraints; a\nBidirectional Warping Virtual View Synthesis method that enforces substantially\nstronger constraints by creating high-fidelity virtual views through\nbidirectional depth-image warping and multi-view fusion; and an Occlusion-Aware\nReconstruction component that recovers occluded areas using a depth-difference\nmask and a learning-based inpainting model. Extensive evaluations on LLFF,\nBlender, and DTU benchmarks validate that HBSplat sets a new state-of-the-art,\nachieving up to 21.13 dB PSNR and 0.189 LPIPS, while maintaining real-time\ninference. Code is available at: https://github.com/eternalland/HBSplat."
                },
                "authors": [
                    {
                        "name": "Yu Ma"
                    },
                    {
                        "name": "Guoliang Wei"
                    },
                    {
                        "name": "Yue Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yue Cheng"
                },
                "author": "Yue Cheng",
                "arxiv_comment": "14 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24893v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04226v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04226v3",
                "updated": "2025-10-08T07:35:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    7,
                    35,
                    57,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-05T14:29:15Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    14,
                    29,
                    15,
                    6,
                    278,
                    0
                ],
                "title": "Epistemic Diversity and Knowledge Collapse in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Epistemic Diversity and Knowledge Collapse in Large Language Models"
                },
                "summary": "Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation"
                },
                "authors": [
                    {
                        "name": "Dustin Wright"
                    },
                    {
                        "name": "Sarah Masud"
                    },
                    {
                        "name": "Jared Moore"
                    },
                    {
                        "name": "Srishti Yadav"
                    },
                    {
                        "name": "Maria Antoniak"
                    },
                    {
                        "name": "Chan Young Park"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "arxiv_comment": "16 pages; 8 figures, 4 tables; v2 changelog: Fixed the modeling for\n  table 3, random effect is the model version; v3 changelog: Fixed minor\n  formatting issues in tables 2 and 3;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04226v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04226v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21961v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21961v3",
                "updated": "2025-10-07T16:06:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    6,
                    25,
                    1,
                    280,
                    0
                ],
                "published": "2025-03-27T20:18:22Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    20,
                    18,
                    22,
                    3,
                    86,
                    0
                ],
                "title": "Entropy-Gated Branching for Efficient Test-Time Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy-Gated Branching for Efficient Test-Time Reasoning"
                },
                "summary": "Test-time compute methods can significantly improve the reasoning\ncapabilities and problem-solving accuracy of large language models (LLMs).\nHowever, these approaches require substantially more computational resources,\nwith most compute wasted on exploring low-diversity branches where the model\nalready exhibits high confidence. We observe that a small subset of uncertain\nreasoning steps has a disproportionately large impact on final prediction\naccuracy, and branching at these critical junctures tends to yield more diverse\nand higher-quality candidate reasoning steps. We propose Entropy-Gated\nBranching (EGB), which branches only at high-uncertainty steps and prunes\nexpansions with a lightweight verifier. On mathematical and financial reasoning\nbenchmarks, EGB improves accuracy by 22.6% over standard inference while\noperating 31%-75% faster across math benchmarks than test-time beam search with\nhigher performance. Our results show that dynamic resource allocation during\ninference can substantially improve both efficiency and effectiveness, offering\na more scalable pathway to enhanced LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute methods can significantly improve the reasoning\ncapabilities and problem-solving accuracy of large language models (LLMs).\nHowever, these approaches require substantially more computational resources,\nwith most compute wasted on exploring low-diversity branches where the model\nalready exhibits high confidence. We observe that a small subset of uncertain\nreasoning steps has a disproportionately large impact on final prediction\naccuracy, and branching at these critical junctures tends to yield more diverse\nand higher-quality candidate reasoning steps. We propose Entropy-Gated\nBranching (EGB), which branches only at high-uncertainty steps and prunes\nexpansions with a lightweight verifier. On mathematical and financial reasoning\nbenchmarks, EGB improves accuracy by 22.6% over standard inference while\noperating 31%-75% faster across math benchmarks than test-time beam search with\nhigher performance. Our results show that dynamic resource allocation during\ninference can substantially improve both efficiency and effectiveness, offering\na more scalable pathway to enhanced LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Xianzhi Li"
                    },
                    {
                        "name": "Ethan Callanan"
                    },
                    {
                        "name": "Abdellah Ghassel"
                    },
                    {
                        "name": "Xiaodan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Zhu"
                },
                "author": "Xiaodan Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21961v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21961v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06080v1",
                "updated": "2025-10-07T16:06:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    6,
                    4,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:06:04Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    6,
                    4,
                    1,
                    280,
                    0
                ],
                "title": "Mechanistic-statistical inference of mosquito dynamics from\n  mark-release-recapture data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanistic-statistical inference of mosquito dynamics from\n  mark-release-recapture data"
                },
                "summary": "Biological control strategies against mosquito-borne diseases--such as the\nsterile insect technique (SIT), RIDL, and Wolbachia-based releases--require\nreliable estimates of dispersal and survival of released males. We propose a\nmechanistic--statistical framework for mark--release--recapture (MRR) data\nlinking an individual-based 2D diffusion model with its reaction--diffusion\nlimit. Inference is based on solving the macroscopic system and embedding it in\na Poisson observation model for daily trap counts, with uncertainty quantified\nvia a parametric bootstrap. We validate identifiability using simulated data\nand apply the model to an urban MRR campaign in El Cano (Havana, Cuba)\ninvolving four weekly releases of sterile Aedes aegypti males. The\nbest-supported model suggests a mean life expectancy of about five days and a\ntypical displacement of about 180 m. Unlike empirical fits of survival or\ndispersal, our mechanistic approach jointly estimates movement, mortality, and\ncapture, yielding biologically interpretable parameters and a principled\nframework for designing and evaluating SIT-based interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biological control strategies against mosquito-borne diseases--such as the\nsterile insect technique (SIT), RIDL, and Wolbachia-based releases--require\nreliable estimates of dispersal and survival of released males. We propose a\nmechanistic--statistical framework for mark--release--recapture (MRR) data\nlinking an individual-based 2D diffusion model with its reaction--diffusion\nlimit. Inference is based on solving the macroscopic system and embedding it in\na Poisson observation model for daily trap counts, with uncertainty quantified\nvia a parametric bootstrap. We validate identifiability using simulated data\nand apply the model to an urban MRR campaign in El Cano (Havana, Cuba)\ninvolving four weekly releases of sterile Aedes aegypti males. The\nbest-supported model suggests a mean life expectancy of about five days and a\ntypical displacement of about 180 m. Unlike empirical fits of survival or\ndispersal, our mechanistic approach jointly estimates movement, mortality, and\ncapture, yielding biologically interpretable parameters and a principled\nframework for designing and evaluating SIT-based interventions."
                },
                "authors": [
                    {
                        "name": "Nga Nguyen"
                    },
                    {
                        "name": "Olivier Bonnefon"
                    },
                    {
                        "name": "René Gato"
                    },
                    {
                        "name": "Luis Almeida"
                    },
                    {
                        "name": "Lionel Roques"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Roques"
                },
                "author": "Lionel Roques",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92D25, 60J60, 35K57, 62P10, 62F10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06078v1",
                "updated": "2025-10-07T16:03:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    3,
                    57,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:03:57Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    3,
                    57,
                    1,
                    280,
                    0
                ],
                "title": "Constraint-Aware Route Recommendation from Natural Language via\n  Hierarchical LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint-Aware Route Recommendation from Natural Language via\n  Hierarchical LLM Agents"
                },
                "summary": "Route recommendation aims to provide users with optimal travel plans that\nsatisfy diverse and complex requirements. Classical routing algorithms (e.g.,\nshortest-path and constraint-aware search) are efficient but assume structured\ninputs and fixed objectives, limiting adaptability to natural-language queries.\nRecent LLM-based approaches enhance flexibility but struggle with spatial\nreasoning and the joint modeling of route-level and POI-level preferences. To\naddress these limitations, we propose RouteLLM, a hierarchical multi-agent\nframework that grounds natural-language intents into constraint-aware routes.\nIt first parses user queries into structured intents including POIs, paths, and\nconstraints. A manager agent then coordinates specialized sub-agents: a\nconstraint agent that resolves and formally check constraints, a POI agent that\nretrieves and ranks candidate POIs, and a path refinement agent that refines\nroutes via a routing engine with preference-conditioned costs. A final verifier\nagent ensures constraint satisfaction and produces the final route with an\ninterpretable rationale. This design bridges linguistic flexibility and spatial\nstructure, enabling reasoning over route feasibility and user preferences.\nExperiments show that our method reliably grounds textual preferences into\nconstraint-aware routes, improving route quality and preference satisfaction\nover classical methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Route recommendation aims to provide users with optimal travel plans that\nsatisfy diverse and complex requirements. Classical routing algorithms (e.g.,\nshortest-path and constraint-aware search) are efficient but assume structured\ninputs and fixed objectives, limiting adaptability to natural-language queries.\nRecent LLM-based approaches enhance flexibility but struggle with spatial\nreasoning and the joint modeling of route-level and POI-level preferences. To\naddress these limitations, we propose RouteLLM, a hierarchical multi-agent\nframework that grounds natural-language intents into constraint-aware routes.\nIt first parses user queries into structured intents including POIs, paths, and\nconstraints. A manager agent then coordinates specialized sub-agents: a\nconstraint agent that resolves and formally check constraints, a POI agent that\nretrieves and ranks candidate POIs, and a path refinement agent that refines\nroutes via a routing engine with preference-conditioned costs. A final verifier\nagent ensures constraint satisfaction and produces the final route with an\ninterpretable rationale. This design bridges linguistic flexibility and spatial\nstructure, enabling reasoning over route feasibility and user preferences.\nExperiments show that our method reliably grounds textual preferences into\nconstraint-aware routes, improving route quality and preference satisfaction\nover classical methods."
                },
                "authors": [
                    {
                        "name": "Tao Zhe"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Fateme Memar"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Xinyue Ye"
                    },
                    {
                        "name": "Zhongren Peng"
                    },
                    {
                        "name": "Dongjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dongjie Wang"
                },
                "author": "Dongjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06077v1",
                "updated": "2025-10-07T16:03:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    3,
                    33,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:03:33Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    3,
                    33,
                    1,
                    280,
                    0
                ],
                "title": "When Thinking Drifts: Evidential Grounding for Robust Video Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Thinking Drifts: Evidential Grounding for Robust Video Reasoning"
                },
                "summary": "Video reasoning, the task of enabling machines to infer from dynamic visual\ncontent through multi-step logic, is crucial for advanced AI. While the\nChain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks,\nits application to video understanding remains underexplored. This paper\npresents a systematic analysis revealing that CoT often degrades performance in\nvideo reasoning, generating verbose but misleading internal monologues, and\nleading to hallucinated visual details and overridden correct intuitions - a\nphenomenon we term \"visual thinking drift\". We explain this drift through a\nBayesian lens, positing that CoT traces often diverge from actual visual\nevidence, instead amplifying internal biases or language priors, causing models\nto storytell rather than engage in grounded reasoning. To counteract this, we\nintroduce Visual Evidence Reward (VER), a novel reinforcement learning\nframework that explicitly rewards the generation of reasoning traces that are\nverifiably grounded in visual evidence. Comprehensive evaluation across 10\ndiverse video understanding benchmarks demonstrates that our Video-VER\nconsistently achieves top performance. Our work sheds light on the distinct\nchallenges of video-centric reasoning and encourages the development of AI that\nrobustly grounds its inferences in visual evidence - for large multimodal\nmodels that not only \"think before answering\", but also \"see while thinking\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video reasoning, the task of enabling machines to infer from dynamic visual\ncontent through multi-step logic, is crucial for advanced AI. While the\nChain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks,\nits application to video understanding remains underexplored. This paper\npresents a systematic analysis revealing that CoT often degrades performance in\nvideo reasoning, generating verbose but misleading internal monologues, and\nleading to hallucinated visual details and overridden correct intuitions - a\nphenomenon we term \"visual thinking drift\". We explain this drift through a\nBayesian lens, positing that CoT traces often diverge from actual visual\nevidence, instead amplifying internal biases or language priors, causing models\nto storytell rather than engage in grounded reasoning. To counteract this, we\nintroduce Visual Evidence Reward (VER), a novel reinforcement learning\nframework that explicitly rewards the generation of reasoning traces that are\nverifiably grounded in visual evidence. Comprehensive evaluation across 10\ndiverse video understanding benchmarks demonstrates that our Video-VER\nconsistently achieves top performance. Our work sheds light on the distinct\nchallenges of video-centric reasoning and encourages the development of AI that\nrobustly grounds its inferences in visual evidence - for large multimodal\nmodels that not only \"think before answering\", but also \"see while thinking\"."
                },
                "authors": [
                    {
                        "name": "Mi Luo"
                    },
                    {
                        "name": "Zihui Xue"
                    },
                    {
                        "name": "Alex Dimakis"
                    },
                    {
                        "name": "Kristen Grauman"
                    }
                ],
                "author_detail": {
                    "name": "Kristen Grauman"
                },
                "author": "Kristen Grauman",
                "arxiv_comment": "Accepted by NeurIPS 2025, Project page:\n  https://vision.cs.utexas.edu/projects/video-ver/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06068v1",
                "updated": "2025-10-07T15:57:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    57,
                    0,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:57:00Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    57,
                    0,
                    1,
                    280,
                    0
                ],
                "title": "Cross-Embodiment Dexterous Hand Articulation Generation via\n  Morphology-Aware Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Embodiment Dexterous Hand Articulation Generation via\n  Morphology-Aware Learning"
                },
                "summary": "Dexterous grasping with multi-fingered hands remains challenging due to\nhigh-dimensional articulations and the cost of optimization-based pipelines.\nExisting end-to-end methods require training on large-scale datasets for\nspecific hands, limiting their ability to generalize across different\nembodiments. We propose an eigengrasp-based, end-to-end framework for\ncross-embodiment grasp generation. From a hand's morphology description, we\nderive a morphology embedding and an eigengrasp set. Conditioned on these,\ntogether with the object point cloud and wrist pose, an amplitude predictor\nregresses articulation coefficients in a low-dimensional space, which are\ndecoded into full joint articulations. Articulation learning is supervised with\na Kinematic-Aware Articulation Loss (KAL) that emphasizes fingertip-relevant\nmotions and injects morphology-specific structure. In simulation on unseen\nobjects across three dexterous hands, our model attains a 91.9% average grasp\nsuccess rate with less than 0.4 seconds inference per grasp. With few-shot\nadaptation to an unseen hand, it achieves 85.6% success on unseen objects in\nsimulation, and real-world experiments on this few-shot generalized hand\nachieve an 87% success rate. The code and additional materials will be made\navailable upon publication on our project website\nhttps://connor-zh.github.io/cross_embodiment_dexterous_grasping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dexterous grasping with multi-fingered hands remains challenging due to\nhigh-dimensional articulations and the cost of optimization-based pipelines.\nExisting end-to-end methods require training on large-scale datasets for\nspecific hands, limiting their ability to generalize across different\nembodiments. We propose an eigengrasp-based, end-to-end framework for\ncross-embodiment grasp generation. From a hand's morphology description, we\nderive a morphology embedding and an eigengrasp set. Conditioned on these,\ntogether with the object point cloud and wrist pose, an amplitude predictor\nregresses articulation coefficients in a low-dimensional space, which are\ndecoded into full joint articulations. Articulation learning is supervised with\na Kinematic-Aware Articulation Loss (KAL) that emphasizes fingertip-relevant\nmotions and injects morphology-specific structure. In simulation on unseen\nobjects across three dexterous hands, our model attains a 91.9% average grasp\nsuccess rate with less than 0.4 seconds inference per grasp. With few-shot\nadaptation to an unseen hand, it achieves 85.6% success on unseen objects in\nsimulation, and real-world experiments on this few-shot generalized hand\nachieve an 87% success rate. The code and additional materials will be made\navailable upon publication on our project website\nhttps://connor-zh.github.io/cross_embodiment_dexterous_grasping."
                },
                "authors": [
                    {
                        "name": "Heng Zhang"
                    },
                    {
                        "name": "Kevin Yuchen Ma"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Weisi Lin"
                    },
                    {
                        "name": "Yan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yan Wu"
                },
                "author": "Yan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02322v2",
                "updated": "2025-10-07T15:56:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    56,
                    15,
                    1,
                    280,
                    0
                ],
                "published": "2025-08-04T11:42:48Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    42,
                    48,
                    0,
                    216,
                    0
                ],
                "title": "CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert\n  Redundancy Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert\n  Redundancy Analysis"
                },
                "summary": "Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are\ndistinguished by their strong performance scaling with increasing parameters\nacross a wide range of tasks, yet they also suffer from substantial\ncomputational and storage overheads. Notably, the performance gains of MoE\nmodels do not scale proportionally with the growth in expert parameters. While\nprior works attempt to reduce parameters via expert-level pruning, merging, or\ndecomposition, they still suffer from challenges in both performance and\ncomputational efficiency. In this paper, we address these challenges by\nintroducing micro-expert as a finer-grained compression unit that spans across\nmatrices. We first establish a more fundamental perspective, viewing MoE layers\nas mixtures of micro-experts, and present CAMERA, a lightweight and\ntraining-free framework for identifying micro-expert redundancy. Our analysis\nuncovers significant variance in micro-expert contributions during decoding.\nBased on this insight, we further propose CAMERA-P, a structured micro-expert\npruning framework, and CAMERA-Q, a mixed-precision quantization idea designed\nfor micro-experts. Extensive experiments on nine downstream tasks show that\nCAMERA-P consistently outperforms strong baselines under pruning ratios ranging\nfrom 20% to 60%. Furthermore, CAMERA-Q achieves superior results under\naggressive 2-bit quantization, surpassing existing matrix- and channel-level\nideas. Notably, our method enables complete micro-expert analysis of\nQwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are\ndistinguished by their strong performance scaling with increasing parameters\nacross a wide range of tasks, yet they also suffer from substantial\ncomputational and storage overheads. Notably, the performance gains of MoE\nmodels do not scale proportionally with the growth in expert parameters. While\nprior works attempt to reduce parameters via expert-level pruning, merging, or\ndecomposition, they still suffer from challenges in both performance and\ncomputational efficiency. In this paper, we address these challenges by\nintroducing micro-expert as a finer-grained compression unit that spans across\nmatrices. We first establish a more fundamental perspective, viewing MoE layers\nas mixtures of micro-experts, and present CAMERA, a lightweight and\ntraining-free framework for identifying micro-expert redundancy. Our analysis\nuncovers significant variance in micro-expert contributions during decoding.\nBased on this insight, we further propose CAMERA-P, a structured micro-expert\npruning framework, and CAMERA-Q, a mixed-precision quantization idea designed\nfor micro-experts. Extensive experiments on nine downstream tasks show that\nCAMERA-P consistently outperforms strong baselines under pruning ratios ranging\nfrom 20% to 60%. Furthermore, CAMERA-Q achieves superior results under\naggressive 2-bit quantization, surpassing existing matrix- and channel-level\nideas. Notably, our method enables complete micro-expert analysis of\nQwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU."
                },
                "authors": [
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Yuanchi Zhang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "16 pages, 9 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06062v1",
                "updated": "2025-10-07T15:54:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    54,
                    24,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:54:24Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    54,
                    24,
                    1,
                    280,
                    0
                ],
                "title": "ASPO: Asymmetric Importance Sampling Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASPO: Asymmetric Importance Sampling Policy Optimization"
                },
                "summary": "Recent Large Language Model (LLM) post-training methods rely on token-level\nclipping mechanisms during Reinforcement Learning (RL). However, we identify a\nfundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance\nSampling (IS) ratios of positive-advantage tokens are mismatched, leading to\nunbalanced token weighting for positive and negative tokens. This mismatch\nsuppresses the update of low-probability tokens while over-amplifying already\nhigh-probability ones. To address this, we propose Asymmetric Importance\nSampling Policy Optimization (ASPO), which uses a simple yet effective strategy\nthat flips the IS ratios of positive-advantage tokens, aligning their update\ndirection with the learning dynamics of negative ones. AIS further incorporates\na soft dual-clipping mechanism to stabilize extreme updates while maintaining\ngradient flow. Comprehensive experiments on coding and mathematical reasoning\nbenchmarks demonstrate that ASPO significantly mitigates premature convergence,\nimproves training stability, and enhances final performance over strong\nGRPO-based baselines. Our analysis provides new insights into the role of\ntoken-level weighting in OSRL and highlights the critical importance of\ncorrecting IS in LLM RL. The code and models of ASPO are available at\nhttps://github.com/wizard-III/Archer2.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Large Language Model (LLM) post-training methods rely on token-level\nclipping mechanisms during Reinforcement Learning (RL). However, we identify a\nfundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance\nSampling (IS) ratios of positive-advantage tokens are mismatched, leading to\nunbalanced token weighting for positive and negative tokens. This mismatch\nsuppresses the update of low-probability tokens while over-amplifying already\nhigh-probability ones. To address this, we propose Asymmetric Importance\nSampling Policy Optimization (ASPO), which uses a simple yet effective strategy\nthat flips the IS ratios of positive-advantage tokens, aligning their update\ndirection with the learning dynamics of negative ones. AIS further incorporates\na soft dual-clipping mechanism to stabilize extreme updates while maintaining\ngradient flow. Comprehensive experiments on coding and mathematical reasoning\nbenchmarks demonstrate that ASPO significantly mitigates premature convergence,\nimproves training stability, and enhances final performance over strong\nGRPO-based baselines. Our analysis provides new insights into the role of\ntoken-level weighting in OSRL and highlights the critical importance of\ncorrecting IS in LLM RL. The code and models of ASPO are available at\nhttps://github.com/wizard-III/Archer2.0."
                },
                "authors": [
                    {
                        "name": "Jiakang Wang"
                    },
                    {
                        "name": "Runze Liu"
                    },
                    {
                        "name": "Lei Lin"
                    },
                    {
                        "name": "Wenping Hu"
                    },
                    {
                        "name": "Xiu Li"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    },
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00207v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00207v2",
                "updated": "2025-10-07T15:54:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    54,
                    8,
                    1,
                    280,
                    0
                ],
                "published": "2025-09-30T19:31:35Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    31,
                    35,
                    1,
                    273,
                    0
                ],
                "title": "FlowMoE: A Scalable Pipeline Scheduling Framework for Distributed\n  Mixture-of-Experts Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowMoE: A Scalable Pipeline Scheduling Framework for Distributed\n  Mixture-of-Experts Training"
                },
                "summary": "The parameter size of modern large language models (LLMs) can be scaled up\nvia the sparsely-activated Mixture-of-Experts (MoE) technique to avoid\nexcessive increase of the computational costs. To further improve training\nefficiency, pipelining computation and communication has become a promising\nsolution for distributed MoE training. However, existing work primarily focuses\non scheduling tasks within the MoE layer, such as expert computing and\nall-to-all (A2A) communication, while neglecting other key operations including\nmulti-head attention (MHA) computing, gating, and all-reduce communication. In\nthis paper, we propose FlowMoE, a scalable framework for scheduling multi-type\ntask pipelines. First, FlowMoE constructs a unified pipeline to consistently\nscheduling MHA computing, gating, expert computing, and A2A communication.\nSecond, FlowMoE introduces a tensor chunk-based priority scheduling mechanism\nto overlap the all-reduce communication with all computing tasks. We implement\nFlowMoE as an adaptive and generic framework atop PyTorch. Extensive\nexperiments with 675 typical MoE layers and four real-world MoE models across\ntwo GPU clusters demonstrate that our proposed FlowMoE framework outperforms\nstate-of-the-art MoE training frameworks, reducing training time by 13%-57%,\nenergy consumption by 10%-39%, and memory usage by 7%-32%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The parameter size of modern large language models (LLMs) can be scaled up\nvia the sparsely-activated Mixture-of-Experts (MoE) technique to avoid\nexcessive increase of the computational costs. To further improve training\nefficiency, pipelining computation and communication has become a promising\nsolution for distributed MoE training. However, existing work primarily focuses\non scheduling tasks within the MoE layer, such as expert computing and\nall-to-all (A2A) communication, while neglecting other key operations including\nmulti-head attention (MHA) computing, gating, and all-reduce communication. In\nthis paper, we propose FlowMoE, a scalable framework for scheduling multi-type\ntask pipelines. First, FlowMoE constructs a unified pipeline to consistently\nscheduling MHA computing, gating, expert computing, and A2A communication.\nSecond, FlowMoE introduces a tensor chunk-based priority scheduling mechanism\nto overlap the all-reduce communication with all computing tasks. We implement\nFlowMoE as an adaptive and generic framework atop PyTorch. Extensive\nexperiments with 675 typical MoE layers and four real-world MoE models across\ntwo GPU clusters demonstrate that our proposed FlowMoE framework outperforms\nstate-of-the-art MoE training frameworks, reducing training time by 13%-57%,\nenergy consumption by 10%-39%, and memory usage by 7%-32%."
                },
                "authors": [
                    {
                        "name": "Yunqi Gao"
                    },
                    {
                        "name": "Bing Hu"
                    },
                    {
                        "name": "Mahdi Boloursaz Mashhadi"
                    },
                    {
                        "name": "A-Long Jin"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Pei Xiao"
                    },
                    {
                        "name": "Rahim Tafazolli"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00207v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00207v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17355v2",
                "updated": "2025-10-07T15:53:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    53,
                    55,
                    1,
                    280,
                    0
                ],
                "published": "2025-02-24T17:33:18Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    33,
                    18,
                    0,
                    55,
                    0
                ],
                "title": "On Relation-Specific Neurons in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Relation-Specific Neurons in Large Language Models"
                },
                "summary": "In large language models (LLMs), certain \\emph{neurons} can store distinct\npieces of knowledge learned during pretraining. While factual knowledge\ntypically appears as a combination of \\emph{relations} and \\emph{entities}, it\nremains unclear whether some neurons focus on a relation itself -- independent\nof any entity. We hypothesize such neurons \\emph{detect} a relation in the\ninput text and \\emph{guide} generation involving such a relation. To\ninvestigate this, we study the LLama-2 family on a chosen set of relations,\nwith a \\textit{statistics}-based method. Our experiments demonstrate the\nexistence of relation-specific neurons. We measure the effect of selectively\ndeactivating candidate neurons specific to relation $r$ on the LLM's ability to\nhandle (1) facts involving relation $r$ and (2) facts involving a different\nrelation $r' \\neq r$. With respect to their capacity for encoding relation\ninformation, we give evidence for the following three properties of\nrelation-specific neurons. \\textbf{(i) Neuron cumulativity.} Multiple neurons\njointly contribute to processing facts involving relation $r$, with no single\nneuron fully encoding a fact in $r$ on its own. \\textbf{(ii) Neuron\nversatility.} Neurons can be shared across multiple closely related as well as\nless related relations. In addition, some relation neurons transfer across\nlanguages. \\textbf{(iii) Neuron interference.} Deactivating neurons specific to\none relation can improve LLMs' factual recall performance for facts of other\nrelations. We make our code and data publicly available at\nhttps://github.com/cisnlp/relation-specific-neurons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large language models (LLMs), certain \\emph{neurons} can store distinct\npieces of knowledge learned during pretraining. While factual knowledge\ntypically appears as a combination of \\emph{relations} and \\emph{entities}, it\nremains unclear whether some neurons focus on a relation itself -- independent\nof any entity. We hypothesize such neurons \\emph{detect} a relation in the\ninput text and \\emph{guide} generation involving such a relation. To\ninvestigate this, we study the LLama-2 family on a chosen set of relations,\nwith a \\textit{statistics}-based method. Our experiments demonstrate the\nexistence of relation-specific neurons. We measure the effect of selectively\ndeactivating candidate neurons specific to relation $r$ on the LLM's ability to\nhandle (1) facts involving relation $r$ and (2) facts involving a different\nrelation $r' \\neq r$. With respect to their capacity for encoding relation\ninformation, we give evidence for the following three properties of\nrelation-specific neurons. \\textbf{(i) Neuron cumulativity.} Multiple neurons\njointly contribute to processing facts involving relation $r$, with no single\nneuron fully encoding a fact in $r$ on its own. \\textbf{(ii) Neuron\nversatility.} Neurons can be shared across multiple closely related as well as\nless related relations. In addition, some relation neurons transfer across\nlanguages. \\textbf{(iii) Neuron interference.} Deactivating neurons specific to\none relation can improve LLMs' factual recall performance for facts of other\nrelations. We make our code and data publicly available at\nhttps://github.com/cisnlp/relation-specific-neurons."
                },
                "authors": [
                    {
                        "name": "Yihong Liu"
                    },
                    {
                        "name": "Runsheng Chen"
                    },
                    {
                        "name": "Lea Hirlimann"
                    },
                    {
                        "name": "Ahmad Dawar Hakimi"
                    },
                    {
                        "name": "Mingyang Wang"
                    },
                    {
                        "name": "Amir Hossein Kargaran"
                    },
                    {
                        "name": "Sascha Rothe"
                    },
                    {
                        "name": "François Yvon"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06056v1",
                "updated": "2025-10-07T15:49:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    49,
                    51,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:49:51Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    49,
                    51,
                    1,
                    280,
                    0
                ],
                "title": "Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep\n  Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep\n  Research"
                },
                "summary": "Large language models hold promise as scientific assistants, yet existing\nagents either rely solely on algorithm evolution or on deep research in\nisolation, both of which face critical limitations. Pure algorithm evolution,\nas in AlphaEvolve, depends only on the internal knowledge of LLMs and quickly\nplateaus in complex domains, while pure deep research proposes ideas without\nvalidation, resulting in unrealistic or unimplementable solutions. We present\nDeepEvolve, an agent that integrates deep research with algorithm evolution,\nuniting external knowledge retrieval, cross-file code editing, and systematic\ndebugging under a feedback-driven iterative loop. Each iteration not only\nproposes new hypotheses but also refines, implements, and tests them, avoiding\nboth shallow improvements and unproductive over-refinements. Across nine\nbenchmarks in chemistry, mathematics, biology, materials, and patents,\nDeepEvolve consistently improves the initial algorithm, producing executable\nnew algorithms with sustained gains. By bridging the gap between unguided\nevolution and research without grounding, DeepEvolve provides a reliable\nframework for advancing scientific algorithm discovery. Our code is available\nat https://github.com/liugangcode/deepevolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models hold promise as scientific assistants, yet existing\nagents either rely solely on algorithm evolution or on deep research in\nisolation, both of which face critical limitations. Pure algorithm evolution,\nas in AlphaEvolve, depends only on the internal knowledge of LLMs and quickly\nplateaus in complex domains, while pure deep research proposes ideas without\nvalidation, resulting in unrealistic or unimplementable solutions. We present\nDeepEvolve, an agent that integrates deep research with algorithm evolution,\nuniting external knowledge retrieval, cross-file code editing, and systematic\ndebugging under a feedback-driven iterative loop. Each iteration not only\nproposes new hypotheses but also refines, implements, and tests them, avoiding\nboth shallow improvements and unproductive over-refinements. Across nine\nbenchmarks in chemistry, mathematics, biology, materials, and patents,\nDeepEvolve consistently improves the initial algorithm, producing executable\nnew algorithms with sustained gains. By bridging the gap between unguided\nevolution and research without grounding, DeepEvolve provides a reliable\nframework for advancing scientific algorithm discovery. Our code is available\nat https://github.com/liugangcode/deepevolve."
                },
                "authors": [
                    {
                        "name": "Gang Liu"
                    },
                    {
                        "name": "Yihan Zhu"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Meng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Jiang"
                },
                "author": "Meng Jiang",
                "arxiv_comment": "25 pages, 17 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06052v1",
                "updated": "2025-10-07T15:46:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    46,
                    34,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:46:34Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    46,
                    34,
                    1,
                    280,
                    0
                ],
                "title": "MixReasoning: Switching Modes to Think",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixReasoning: Switching Modes to Think"
                },
                "summary": "Reasoning models enhance performance by tackling problems in a step-by-step\nmanner, decomposing them into sub-problems and exploring long chains of thought\nbefore producing an answer. However, applying extended reasoning to every step\nintroduces substantial redundancy, as sub-problems vary widely in difficulty\nand complexity: a small number of pivotal steps are genuinely challenging and\ndecisive for the final answer, while many others only involve straightforward\nrevisions or simple computations. Therefore, a natural idea is to endow\nreasoning models with the ability to adaptively respond to this variation,\nrather than treating all steps with the same level of elaboration. To this end,\nwe propose MixReasoning, a framework that dynamically adjusts the depth of\nreasoning within a single response. The resulting chain of thought then becomes\na mixture of detailed reasoning on difficult steps and concise inference on\nsimpler ones. Experiments on GSM8K, MATH-500, and AIME show that MixReasoning\nshortens reasoning length and substantially improves efficiency without\ncompromising accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models enhance performance by tackling problems in a step-by-step\nmanner, decomposing them into sub-problems and exploring long chains of thought\nbefore producing an answer. However, applying extended reasoning to every step\nintroduces substantial redundancy, as sub-problems vary widely in difficulty\nand complexity: a small number of pivotal steps are genuinely challenging and\ndecisive for the final answer, while many others only involve straightforward\nrevisions or simple computations. Therefore, a natural idea is to endow\nreasoning models with the ability to adaptively respond to this variation,\nrather than treating all steps with the same level of elaboration. To this end,\nwe propose MixReasoning, a framework that dynamically adjusts the depth of\nreasoning within a single response. The resulting chain of thought then becomes\na mixture of detailed reasoning on difficult steps and concise inference on\nsimpler ones. Experiments on GSM8K, MATH-500, and AIME show that MixReasoning\nshortens reasoning length and substantially improves efficiency without\ncompromising accuracy."
                },
                "authors": [
                    {
                        "name": "Haiquan Lu"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06049v1",
                "updated": "2025-10-07T15:43:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    43,
                    9,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:43:09Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    43,
                    9,
                    1,
                    280,
                    0
                ],
                "title": "Turbulence Closure in RANS and Flow Inference around a Cylinder using\n  PINNs and Sparse Experimental Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turbulence Closure in RANS and Flow Inference around a Cylinder using\n  PINNs and Sparse Experimental Data"
                },
                "summary": "Traditional Reynolds-averaged Navier-Stokes (RANS) closures, based on the\nBoussinesq eddy viscosity hypothesis and calibrated on canonical flows, often\nyield inaccurate predictions of both mean flow and turbulence statistics. Here,\nwe consider flow past a circular cylinder over a range of Reynolds numbers\n(3,900-100,000) and Mach numbers (0-0.3), encompassing incompressible and\nweakly compressible regimes, with the goal of improving predictions of mean\nvelocity and Reynolds stresses. To this end, we assemble a cross-validated\ndataset comprising hydrodynamic particle image velocimetry (PIV) in a towing\ntank, aerodynamic PIV in a wind tunnel, and high-fidelity spectral element DNS\nand LES. Analysis of these data reveals a universal distribution of Reynolds\nstresses across the parameter space, which provides the foundation for a\ndata-driven closure. We employ physics-informed neural networks (PINNs),\ntrained with the unclosed RANS equations, to infer the velocity field and\nReynolds-stress forcing from boundary information alone. The resulting closure,\nembedded in a forward PINN solver, significantly improves RANS predictions of\nboth mean flow and turbulence statistics relative to conventional models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Reynolds-averaged Navier-Stokes (RANS) closures, based on the\nBoussinesq eddy viscosity hypothesis and calibrated on canonical flows, often\nyield inaccurate predictions of both mean flow and turbulence statistics. Here,\nwe consider flow past a circular cylinder over a range of Reynolds numbers\n(3,900-100,000) and Mach numbers (0-0.3), encompassing incompressible and\nweakly compressible regimes, with the goal of improving predictions of mean\nvelocity and Reynolds stresses. To this end, we assemble a cross-validated\ndataset comprising hydrodynamic particle image velocimetry (PIV) in a towing\ntank, aerodynamic PIV in a wind tunnel, and high-fidelity spectral element DNS\nand LES. Analysis of these data reveals a universal distribution of Reynolds\nstresses across the parameter space, which provides the foundation for a\ndata-driven closure. We employ physics-informed neural networks (PINNs),\ntrained with the unclosed RANS equations, to infer the velocity field and\nReynolds-stress forcing from boundary information alone. The resulting closure,\nembedded in a forward PINN solver, significantly improves RANS predictions of\nboth mean flow and turbulence statistics relative to conventional models."
                },
                "authors": [
                    {
                        "name": "Z. Zhang"
                    },
                    {
                        "name": "K. Shukla"
                    },
                    {
                        "name": "Z. Wang"
                    },
                    {
                        "name": "A. Morales"
                    },
                    {
                        "name": "T. Käufer"
                    },
                    {
                        "name": "S. Salauddin"
                    },
                    {
                        "name": "N. Walters"
                    },
                    {
                        "name": "D. Barrett"
                    },
                    {
                        "name": "K. Ahmed"
                    },
                    {
                        "name": "M. S. Triantafyllou"
                    },
                    {
                        "name": "G. E. Karniadakis"
                    }
                ],
                "author_detail": {
                    "name": "G. E. Karniadakis"
                },
                "author": "G. E. Karniadakis",
                "arxiv_comment": "36 pages, 39 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06048v1",
                "updated": "2025-10-07T15:42:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    42,
                    33,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:42:33Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    42,
                    33,
                    1,
                    280,
                    0
                ],
                "title": "BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection\n  in Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection\n  in Language Model Pretraining"
                },
                "summary": "Effective data selection is essential for pretraining large language models\n(LLMs), enhancing efficiency and improving generalization to downstream tasks.\nHowever, existing approaches often require leveraging external pretrained\nmodels, making it difficult to disentangle the effects of data selection from\nthose of the external pretrained models. In addition, they often overlook the\nlong-term impact of selected data if the model is trained to convergence,\nprimarily due to the prohibitive cost of full-scale LLM pretraining. In this\npaper, we introduce BLISS (\\textbf{B}ileve\\textbf{L} \\textbf{I}nfluence\n\\textbf{S}coring method for data \\textbf{S}election): a lightweight data\nselection method that operates entirely \\emph{from scratch}, without relying on\nany external pretrained oracle models, while explicitly accounting for the\nlong-term impact of selected data. BLISS leverages a small proxy model as a\nsurrogate for the LLM and employs a score model to estimate the long-term\ninfluence of training samples if the proxy model is trained to convergence. We\nformulate data selection as a bilevel optimization problem, where the\nupper-level objective optimizes the score model to assign importance weights to\ntraining samples, ensuring that minimizing the lower-level objective (i.e.,\ntraining the proxy model over the weighted training loss until convergence)\nleads to best validation performance. Once optimized, the trained score model\npredicts influence scores for the dataset, enabling efficient selection of\nhigh-quality samples for LLM pretraining. We validate BLISS by pretraining\n410M/1B/2.8B Pythia and LLaMA-0.5B models on selected subsets of the C4\ndataset. Notably, under the 1B model setting, BLISS achieves $1.7\\times$\nspeedup in reaching the same performance as the state-of-the-art method,\ndemonstrating superior performance across multiple downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective data selection is essential for pretraining large language models\n(LLMs), enhancing efficiency and improving generalization to downstream tasks.\nHowever, existing approaches often require leveraging external pretrained\nmodels, making it difficult to disentangle the effects of data selection from\nthose of the external pretrained models. In addition, they often overlook the\nlong-term impact of selected data if the model is trained to convergence,\nprimarily due to the prohibitive cost of full-scale LLM pretraining. In this\npaper, we introduce BLISS (\\textbf{B}ileve\\textbf{L} \\textbf{I}nfluence\n\\textbf{S}coring method for data \\textbf{S}election): a lightweight data\nselection method that operates entirely \\emph{from scratch}, without relying on\nany external pretrained oracle models, while explicitly accounting for the\nlong-term impact of selected data. BLISS leverages a small proxy model as a\nsurrogate for the LLM and employs a score model to estimate the long-term\ninfluence of training samples if the proxy model is trained to convergence. We\nformulate data selection as a bilevel optimization problem, where the\nupper-level objective optimizes the score model to assign importance weights to\ntraining samples, ensuring that minimizing the lower-level objective (i.e.,\ntraining the proxy model over the weighted training loss until convergence)\nleads to best validation performance. Once optimized, the trained score model\npredicts influence scores for the dataset, enabling efficient selection of\nhigh-quality samples for LLM pretraining. We validate BLISS by pretraining\n410M/1B/2.8B Pythia and LLaMA-0.5B models on selected subsets of the C4\ndataset. Notably, under the 1B model setting, BLISS achieves $1.7\\times$\nspeedup in reaching the same performance as the state-of-the-art method,\ndemonstrating superior performance across multiple downstream tasks."
                },
                "authors": [
                    {
                        "name": "Jie Hao"
                    },
                    {
                        "name": "Rui Yu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Huixia Wang"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Mingrui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mingrui Liu"
                },
                "author": "Mingrui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06047v1",
                "updated": "2025-10-07T15:42:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    42,
                    10,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:42:10Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    42,
                    10,
                    1,
                    280,
                    0
                ],
                "title": "The gamma-ray emission from Radio Galaxies and their contribution to the\n  Isotropic Gamma-Ray Background",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The gamma-ray emission from Radio Galaxies and their contribution to the\n  Isotropic Gamma-Ray Background"
                },
                "summary": "We evaluate the contribution to the Isotropic Gamma-Ray Background (IGRB)\ncoming from Radio Galaxies (RGs), the subclass of radio-loud Active Galactic\nNuclei (AGN) with the highest misalignment from the line of sight (l.o.s.).\nSince only a small number of RGs are detected in gamma rays compared to the\nlargest known radio population, the correlation between radio and gamma-ray\nemission serves as a crucial tool to characterize the gamma-ray properties of\nthese sources. We analyse the population of RGs using two samples. The first\nsample contains 26 sources individually detected by the Large Area Telescope\n(LAT) on board the Fermi Gamma-ray Space Telescope at gamma rays. The second\nsample contains 210 RGs for which the gamma-ray emission is not significantly\ndetected by the LAT. We use a stacking analysis to characterize the average\nproperties of the gamma-ray emission of the two samples, separately at first\nand then combined. We then evaluate the correlation between their gamma-ray\nemission and the emission from their radio core at 5 GHz, and we use it to\ndetermine their contribution to the IGRB. Due to the limited number of RGs\ndetected at the gamma-rays, information on the gamma-ray luminosity function is\nlimited. The correlation between the gamma-ray emission and the emission of the\nradio core allows us to characterize it starting from the luminosity function\nof the radio cores, which is modeled with greater accuracy due to the larger\nnumber of sources detected at these frequencies. We find that the diffuse\nemission as extrapolated from the properties of the subthreshold RGs is lower\nthan the one inferred from detected RGs, showing that the contribution of the\npopulation of RGs to the IGRB is lower than the previous estimates and it is\naround the 30% level of the IGRB intensity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We evaluate the contribution to the Isotropic Gamma-Ray Background (IGRB)\ncoming from Radio Galaxies (RGs), the subclass of radio-loud Active Galactic\nNuclei (AGN) with the highest misalignment from the line of sight (l.o.s.).\nSince only a small number of RGs are detected in gamma rays compared to the\nlargest known radio population, the correlation between radio and gamma-ray\nemission serves as a crucial tool to characterize the gamma-ray properties of\nthese sources. We analyse the population of RGs using two samples. The first\nsample contains 26 sources individually detected by the Large Area Telescope\n(LAT) on board the Fermi Gamma-ray Space Telescope at gamma rays. The second\nsample contains 210 RGs for which the gamma-ray emission is not significantly\ndetected by the LAT. We use a stacking analysis to characterize the average\nproperties of the gamma-ray emission of the two samples, separately at first\nand then combined. We then evaluate the correlation between their gamma-ray\nemission and the emission from their radio core at 5 GHz, and we use it to\ndetermine their contribution to the IGRB. Due to the limited number of RGs\ndetected at the gamma-rays, information on the gamma-ray luminosity function is\nlimited. The correlation between the gamma-ray emission and the emission of the\nradio core allows us to characterize it starting from the luminosity function\nof the radio cores, which is modeled with greater accuracy due to the larger\nnumber of sources detected at these frequencies. We find that the diffuse\nemission as extrapolated from the properties of the subthreshold RGs is lower\nthan the one inferred from detected RGs, showing that the contribution of the\npopulation of RGs to the IGRB is lower than the previous estimates and it is\naround the 30% level of the IGRB intensity."
                },
                "authors": [
                    {
                        "name": "A. Circiello"
                    },
                    {
                        "name": "A. McDaniel"
                    },
                    {
                        "name": "M. Di Mauro"
                    },
                    {
                        "name": "C. Karwin"
                    },
                    {
                        "name": "N. Khatiya"
                    },
                    {
                        "name": "M. Ajello"
                    },
                    {
                        "name": "F. Donato"
                    },
                    {
                        "name": "D. Hartmann"
                    },
                    {
                        "name": "A. Strong"
                    }
                ],
                "author_detail": {
                    "name": "A. Strong"
                },
                "author": "A. Strong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08596v2",
                "updated": "2025-10-07T15:40:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    40,
                    54,
                    1,
                    280,
                    0
                ],
                "published": "2025-04-11T14:55:15Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    14,
                    55,
                    15,
                    4,
                    101,
                    0
                ],
                "title": "MedHal: An Evaluation Dataset for Medical Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedHal: An Evaluation Dataset for Medical Hallucination Detection"
                },
                "summary": "We present MedHal, a novel large-scale dataset specifically designed to\nevaluate if models can detect hallucinations in medical texts. Current\nhallucination detection methods face significant limitations when applied to\nspecialized domains like medicine, where they can have disastrous consequences.\nExisting medical datasets are either too small, containing only a few hundred\nsamples, or focus on a single task like Question Answering or Natural Language\nInference. MedHal addresses these gaps by: (1) incorporating diverse medical\ntext sources and tasks; (2) providing a substantial volume of annotated samples\nsuitable for training medical hallucination detection models; and (3) including\nexplanations for factual inconsistencies to guide model learning. We\ndemonstrate MedHal's utility by training and evaluating a baseline medical\nhallucination detection model, showing improvements over general-purpose\nhallucination detection approaches. This resource enables more efficient\nevaluation of medical text generation systems while reducing reliance on costly\nexpert review, potentially accelerating the development of medical AI research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MedHal, a novel large-scale dataset specifically designed to\nevaluate if models can detect hallucinations in medical texts. Current\nhallucination detection methods face significant limitations when applied to\nspecialized domains like medicine, where they can have disastrous consequences.\nExisting medical datasets are either too small, containing only a few hundred\nsamples, or focus on a single task like Question Answering or Natural Language\nInference. MedHal addresses these gaps by: (1) incorporating diverse medical\ntext sources and tasks; (2) providing a substantial volume of annotated samples\nsuitable for training medical hallucination detection models; and (3) including\nexplanations for factual inconsistencies to guide model learning. We\ndemonstrate MedHal's utility by training and evaluating a baseline medical\nhallucination detection model, showing improvements over general-purpose\nhallucination detection approaches. This resource enables more efficient\nevaluation of medical text generation systems while reducing reliance on costly\nexpert review, potentially accelerating the development of medical AI research."
                },
                "authors": [
                    {
                        "name": "Gaya Mehenni"
                    },
                    {
                        "name": "Fabrice Lamarche"
                    },
                    {
                        "name": "Odette Rios-Ibacache"
                    },
                    {
                        "name": "John Kildea"
                    },
                    {
                        "name": "Amal Zouaq"
                    }
                ],
                "author_detail": {
                    "name": "Amal Zouaq"
                },
                "author": "Amal Zouaq",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06046v1",
                "updated": "2025-10-07T15:40:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    40,
                    10,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:40:10Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    40,
                    10,
                    1,
                    280,
                    0
                ],
                "title": "GLVD: Guided Learned Vertex Descent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLVD: Guided Learned Vertex Descent"
                },
                "summary": "Existing 3D face modeling methods usually depend on 3D Morphable Models,\nwhich inherently constrain the representation capacity to fixed shape priors.\nOptimization-based approaches offer high-quality reconstructions but tend to be\ncomputationally expensive. In this work, we introduce GLVD, a hybrid method for\n3D face reconstruction from few-shot images that extends Learned Vertex Descent\n(LVD) by integrating per-vertex neural field optimization with global\nstructural guidance from dynamically predicted 3D keypoints. By incorporating\nrelative spatial encoding, GLVD iteratively refines mesh vertices without\nrequiring dense 3D supervision. This enables expressive and adaptable geometry\nreconstruction while maintaining computational efficiency. GLVD achieves\nstate-of-the-art performance in single-view settings and remains highly\ncompetitive in multi-view scenarios, all while substantially reducing inference\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing 3D face modeling methods usually depend on 3D Morphable Models,\nwhich inherently constrain the representation capacity to fixed shape priors.\nOptimization-based approaches offer high-quality reconstructions but tend to be\ncomputationally expensive. In this work, we introduce GLVD, a hybrid method for\n3D face reconstruction from few-shot images that extends Learned Vertex Descent\n(LVD) by integrating per-vertex neural field optimization with global\nstructural guidance from dynamically predicted 3D keypoints. By incorporating\nrelative spatial encoding, GLVD iteratively refines mesh vertices without\nrequiring dense 3D supervision. This enables expressive and adaptable geometry\nreconstruction while maintaining computational efficiency. GLVD achieves\nstate-of-the-art performance in single-view settings and remains highly\ncompetitive in multi-view scenarios, all while substantially reducing inference\ntime."
                },
                "authors": [
                    {
                        "name": "Pol Caselles Rico"
                    },
                    {
                        "name": "Francesc Moreno Noguer"
                    }
                ],
                "author_detail": {
                    "name": "Francesc Moreno Noguer"
                },
                "author": "Francesc Moreno Noguer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06042v1",
                "updated": "2025-10-07T15:36:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    36,
                    4,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:36:04Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    36,
                    4,
                    1,
                    280,
                    0
                ],
                "title": "Agent+P: Guiding UI Agents via Symbolic Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent+P: Guiding UI Agents via Symbolic Planning"
                },
                "summary": "Large Language Model (LLM)-based UI agents show great promise for UI\nautomation but often hallucinate in long-horizon tasks due to their lack of\nunderstanding of the global UI transition structure. To address this, we\nintroduce AGENT+P, a novel framework that leverages symbolic planning to guide\nLLM-based UI agents. Specifically, we model an app's UI transition structure as\na UI Transition Graph (UTG), which allows us to reformulate the UI automation\ntask as a pathfinding problem on the UTG. This further enables an off-the-shelf\nsymbolic planner to generate a provably correct and optimal high-level plan,\npreventing the agent from redundant exploration and guiding the agent to\nachieve the automation goals. AGENT+P is designed as a plug-and-play framework\nto enhance existing UI agents. Evaluation on the AndroidWorld benchmark\ndemonstrates that AGENT+P improves the success rates of state-of-the-art UI\nagents by up to 14% and reduces the action steps by 37.7%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based UI agents show great promise for UI\nautomation but often hallucinate in long-horizon tasks due to their lack of\nunderstanding of the global UI transition structure. To address this, we\nintroduce AGENT+P, a novel framework that leverages symbolic planning to guide\nLLM-based UI agents. Specifically, we model an app's UI transition structure as\na UI Transition Graph (UTG), which allows us to reformulate the UI automation\ntask as a pathfinding problem on the UTG. This further enables an off-the-shelf\nsymbolic planner to generate a provably correct and optimal high-level plan,\npreventing the agent from redundant exploration and guiding the agent to\nachieve the automation goals. AGENT+P is designed as a plug-and-play framework\nto enhance existing UI agents. Evaluation on the AndroidWorld benchmark\ndemonstrates that AGENT+P improves the success rates of state-of-the-art UI\nagents by up to 14% and reduces the action steps by 37.7%."
                },
                "authors": [
                    {
                        "name": "Shang Ma"
                    },
                    {
                        "name": "Xusheng Xiao"
                    },
                    {
                        "name": "Yanfang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Yanfang Ye"
                },
                "author": "Yanfang Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05016v2",
                "updated": "2025-10-07T15:34:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    34,
                    59,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-06T16:58:47Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    58,
                    47,
                    0,
                    279,
                    0
                ],
                "title": "Large Language Models Achieve Gold Medal Performance at the\n  International Olympiad on Astronomy & Astrophysics (IOAA)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Achieve Gold Medal Performance at the\n  International Olympiad on Astronomy & Astrophysics (IOAA)"
                },
                "summary": "While task-specific demonstrations show early success in applying large\nlanguage models (LLMs) to automate some astronomical research tasks, they only\nprovide incomplete views of all necessary capabilities in solving astronomy\nproblems, calling for more thorough understanding of LLMs' strengths and\nlimitations. So far, existing benchmarks and evaluations focus on simple\nquestion-answering that primarily tests astronomical knowledge and fails to\nevaluate the complex reasoning required for real-world research in the\ndiscipline. Here, we address this gap by systematically benchmarking five\nstate-of-the-art LLMs on the International Olympiad on Astronomy and\nAstrophysics (IOAA) exams, which are designed to examine deep conceptual\nunderstanding, multi-step derivations, and multimodal analysis. With average\nscores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing\nmodels) not only achieve gold medal level performance but also rank in the top\ntwo among ~200-300 participants in all four IOAA theory exams evaluated\n(2022-2025). In comparison, results on the data analysis exams show more\ndivergence. GPT-5 still excels in the exams with an 88.5% average score,\nranking top 10 among the participants in the four most recent IOAAs, while\nother models' performances drop to 48-76%. Furthermore, our in-depth error\nanalysis underscores conceptual reasoning, geometric reasoning, and spatial\nvisualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,\nalthough LLMs approach peak human performance in theory exams, critical gaps\nmust be addressed before they can serve as autonomous research agents in\nastronomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While task-specific demonstrations show early success in applying large\nlanguage models (LLMs) to automate some astronomical research tasks, they only\nprovide incomplete views of all necessary capabilities in solving astronomy\nproblems, calling for more thorough understanding of LLMs' strengths and\nlimitations. So far, existing benchmarks and evaluations focus on simple\nquestion-answering that primarily tests astronomical knowledge and fails to\nevaluate the complex reasoning required for real-world research in the\ndiscipline. Here, we address this gap by systematically benchmarking five\nstate-of-the-art LLMs on the International Olympiad on Astronomy and\nAstrophysics (IOAA) exams, which are designed to examine deep conceptual\nunderstanding, multi-step derivations, and multimodal analysis. With average\nscores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing\nmodels) not only achieve gold medal level performance but also rank in the top\ntwo among ~200-300 participants in all four IOAA theory exams evaluated\n(2022-2025). In comparison, results on the data analysis exams show more\ndivergence. GPT-5 still excels in the exams with an 88.5% average score,\nranking top 10 among the participants in the four most recent IOAAs, while\nother models' performances drop to 48-76%. Furthermore, our in-depth error\nanalysis underscores conceptual reasoning, geometric reasoning, and spatial\nvisualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,\nalthough LLMs approach peak human performance in theory exams, critical gaps\nmust be addressed before they can serve as autonomous research agents in\nastronomy."
                },
                "authors": [
                    {
                        "name": "Lucas Carrit Delgado Pinheiro"
                    },
                    {
                        "name": "Ziru Chen"
                    },
                    {
                        "name": "Bruno Caixeta Piazza"
                    },
                    {
                        "name": "Ness Shroff"
                    },
                    {
                        "name": "Yingbin Liang"
                    },
                    {
                        "name": "Yuan-Sen Ting"
                    },
                    {
                        "name": "Huan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Huan Sun"
                },
                "author": "Huan Sun",
                "arxiv_comment": "18 pages, 6 figures, to be submitted, comments are welcome.\n  Reproducibility details can be found at:\n  https://github.com/OSU-NLP-Group/LLM-IOAA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06040v1",
                "updated": "2025-10-07T15:34:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    34,
                    46,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:34:46Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    34,
                    46,
                    1,
                    280,
                    0
                ],
                "title": "VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via\n  Tree-based Group Relative Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via\n  Tree-based Group Relative Policy Optimization"
                },
                "summary": "Understanding hour-long videos with multi-modal large language models\n(MM-LLMs) enriches the landscape of human-centered AI applications. However,\nfor end-to-end video understanding with LLMs, uniformly sampling video frames\nresults in LLMs being overwhelmed by a vast amount of irrelevant information as\nvideo length increases. Existing hierarchical key frame extraction methods\nimprove the accuracy of video understanding but still face two critical\nchallenges. 1) How can the interference of extensive redundant information in\nlong videos be mitigated? 2) How can a model dynamically adapt to complex\nhierarchical structures while accurately identifying key frames? To address\nthese issues, we propose VideoMiner, which iteratively segments, captions, and\nclusters long videos, forming a hierarchical tree structure. The proposed\nVideoMiner progresses from long videos to events to frames while preserving\ntemporal coherence, effectively addressing the first challenge. To precisely\nlocate key frames, we introduce T-GRPO, a tree-based group relative policy\noptimization in reinforcement learning method that guides the exploration of\nthe VideoMiner. The proposed T-GRPO is specifically designed for tree\nstructures, integrating spatiotemporal information at the event level while\nbeing guided by the question, thus solving the second challenge. We achieve\nsuperior performance in all long-video understanding tasks and uncover several\ninteresting insights. Our proposed T-GRPO surprisingly incentivizes the model\nto spontaneously generate a reasoning chain. Additionally, the designed tree\ngrowth auxin dynamically adjusts the expansion depth, obtaining accuracy and\nefficiency gains. The code is publicly available at\nhttps://github.com/caoxinye/VideoMiner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding hour-long videos with multi-modal large language models\n(MM-LLMs) enriches the landscape of human-centered AI applications. However,\nfor end-to-end video understanding with LLMs, uniformly sampling video frames\nresults in LLMs being overwhelmed by a vast amount of irrelevant information as\nvideo length increases. Existing hierarchical key frame extraction methods\nimprove the accuracy of video understanding but still face two critical\nchallenges. 1) How can the interference of extensive redundant information in\nlong videos be mitigated? 2) How can a model dynamically adapt to complex\nhierarchical structures while accurately identifying key frames? To address\nthese issues, we propose VideoMiner, which iteratively segments, captions, and\nclusters long videos, forming a hierarchical tree structure. The proposed\nVideoMiner progresses from long videos to events to frames while preserving\ntemporal coherence, effectively addressing the first challenge. To precisely\nlocate key frames, we introduce T-GRPO, a tree-based group relative policy\noptimization in reinforcement learning method that guides the exploration of\nthe VideoMiner. The proposed T-GRPO is specifically designed for tree\nstructures, integrating spatiotemporal information at the event level while\nbeing guided by the question, thus solving the second challenge. We achieve\nsuperior performance in all long-video understanding tasks and uncover several\ninteresting insights. Our proposed T-GRPO surprisingly incentivizes the model\nto spontaneously generate a reasoning chain. Additionally, the designed tree\ngrowth auxin dynamically adjusts the expansion depth, obtaining accuracy and\nefficiency gains. The code is publicly available at\nhttps://github.com/caoxinye/VideoMiner."
                },
                "authors": [
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Hongcan Guo"
                    },
                    {
                        "name": "Jiawen Qian"
                    },
                    {
                        "name": "Guoshun Nan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Yuqi Pan"
                    },
                    {
                        "name": "Tianhao Hou"
                    },
                    {
                        "name": "Xiaojuan Wang"
                    },
                    {
                        "name": "Yutong Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Gao"
                },
                "author": "Yutong Gao",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06039v1",
                "updated": "2025-10-07T15:33:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    33,
                    52,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:33:52Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    33,
                    52,
                    1,
                    280,
                    0
                ],
                "title": "CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive\n  Evaluation of Chinese LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive\n  Evaluation of Chinese LLMs"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across a wide\nrange of natural language processing tasks. However, Chinese LLMs face unique\nchallenges, primarily due to the dominance of unstructured free text and the\nlack of structured representations in Chinese corpora. While existing\nbenchmarks for LLMs partially assess Chinese LLMs, they are still predominantly\nEnglish-centric and fail to address the unique linguistic characteristics of\nChinese, lacking structured datasets essential for robust evaluation. To\naddress these challenges, we present a Comprehensive Benchmark for Evaluating\nChinese Large Language Models (CB-ECLLM) based on the newly constructed Chinese\nData-Text Pair (CDTP) dataset. Specifically, CDTP comprises over 7 million\naligned text pairs, each consisting of unstructured text coupled with one or\nmore corresponding triples, alongside a total of 15 million triples spanning\nfour critical domains. The core contributions of CDTP are threefold: (i)\nenriching Chinese corpora with high-quality structured information; (ii)\nenabling fine-grained evaluation tailored to knowledge-driven tasks; and (iii)\nsupporting multi-task fine-tuning to assess generalization and robustness\nacross scenarios, including Knowledge Graph Completion, Triple-to-Text\ngeneration, and Question Answering. Furthermore, we conduct rigorous\nevaluations through extensive experiments and ablation studies to assess the\neffectiveness, Supervised Fine-Tuning (SFT), and robustness of the benchmark.\nTo support reproducible research, we offer an open-source codebase and outline\npotential directions for future investigations based on our insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across a wide\nrange of natural language processing tasks. However, Chinese LLMs face unique\nchallenges, primarily due to the dominance of unstructured free text and the\nlack of structured representations in Chinese corpora. While existing\nbenchmarks for LLMs partially assess Chinese LLMs, they are still predominantly\nEnglish-centric and fail to address the unique linguistic characteristics of\nChinese, lacking structured datasets essential for robust evaluation. To\naddress these challenges, we present a Comprehensive Benchmark for Evaluating\nChinese Large Language Models (CB-ECLLM) based on the newly constructed Chinese\nData-Text Pair (CDTP) dataset. Specifically, CDTP comprises over 7 million\naligned text pairs, each consisting of unstructured text coupled with one or\nmore corresponding triples, alongside a total of 15 million triples spanning\nfour critical domains. The core contributions of CDTP are threefold: (i)\nenriching Chinese corpora with high-quality structured information; (ii)\nenabling fine-grained evaluation tailored to knowledge-driven tasks; and (iii)\nsupporting multi-task fine-tuning to assess generalization and robustness\nacross scenarios, including Knowledge Graph Completion, Triple-to-Text\ngeneration, and Question Answering. Furthermore, we conduct rigorous\nevaluations through extensive experiments and ablation studies to assess the\neffectiveness, Supervised Fine-Tuning (SFT), and robustness of the benchmark.\nTo support reproducible research, we offer an open-source codebase and outline\npotential directions for future investigations based on our insights."
                },
                "authors": [
                    {
                        "name": "Chengwei Wu"
                    },
                    {
                        "name": "Jiapu Wang"
                    },
                    {
                        "name": "Mingyang Gao"
                    },
                    {
                        "name": "Xingrui Zhuo"
                    },
                    {
                        "name": "Jipeng Guo"
                    },
                    {
                        "name": "Runlin Lei"
                    },
                    {
                        "name": "Haoran Luo"
                    },
                    {
                        "name": "Tianyu Chen"
                    },
                    {
                        "name": "Haoyi Zhou"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Zechao Li"
                    }
                ],
                "author_detail": {
                    "name": "Zechao Li"
                },
                "author": "Zechao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12961v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12961v3",
                "updated": "2025-10-07T15:30:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    30,
                    8,
                    1,
                    280,
                    0
                ],
                "published": "2025-04-17T14:07:11Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    7,
                    11,
                    3,
                    107,
                    0
                ],
                "title": "QLLM: Do We Really Need a Mixing Network for Credit Assignment in\n  Multi-Agent Reinforcement Learning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QLLM: Do We Really Need a Mixing Network for Credit Assignment in\n  Multi-Agent Reinforcement Learning?"
                },
                "summary": "Credit assignment has remained a fundamental challenge in multi-agent\nreinforcement learning (MARL). Previous studies have primarily addressed this\nissue through value decomposition methods under the centralized training with\ndecentralized execution paradigm, where neural networks are utilized to\napproximate the nonlinear relationship between individual Q-values and the\nglobal Q-value. Although these approaches have achieved considerable success in\nvarious benchmark tasks, they still suffer from several limitations, including\nimprecise attribution of contributions, limited interpretability, and poor\nscalability in high-dimensional state spaces. To address these challenges, we\npropose a novel algorithm, \\textbf{QLLM}, which facilitates the automatic\nconstruction of credit assignment functions using large language models (LLMs).\nSpecifically, the concept of \\textbf{TFCAF} is introduced, wherein the credit\nallocation process is represented as a direct and expressive nonlinear\nfunctional formulation. A custom-designed \\textit{coder-evaluator} framework is\nfurther employed to guide the generation, verification, and refinement of\nexecutable code by LLMs, significantly mitigating issues such as hallucination\nand shallow reasoning during inference. Extensive experiments conducted on\nseveral standard MARL benchmarks demonstrate that the proposed method\nconsistently outperforms existing state-of-the-art baselines. Moreover, QLLM\nexhibits strong generalization capability and maintains compatibility with a\nwide range of MARL algorithms that utilize mixing networks, positioning it as a\npromising and versatile solution for complex multi-agent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Credit assignment has remained a fundamental challenge in multi-agent\nreinforcement learning (MARL). Previous studies have primarily addressed this\nissue through value decomposition methods under the centralized training with\ndecentralized execution paradigm, where neural networks are utilized to\napproximate the nonlinear relationship between individual Q-values and the\nglobal Q-value. Although these approaches have achieved considerable success in\nvarious benchmark tasks, they still suffer from several limitations, including\nimprecise attribution of contributions, limited interpretability, and poor\nscalability in high-dimensional state spaces. To address these challenges, we\npropose a novel algorithm, \\textbf{QLLM}, which facilitates the automatic\nconstruction of credit assignment functions using large language models (LLMs).\nSpecifically, the concept of \\textbf{TFCAF} is introduced, wherein the credit\nallocation process is represented as a direct and expressive nonlinear\nfunctional formulation. A custom-designed \\textit{coder-evaluator} framework is\nfurther employed to guide the generation, verification, and refinement of\nexecutable code by LLMs, significantly mitigating issues such as hallucination\nand shallow reasoning during inference. Extensive experiments conducted on\nseveral standard MARL benchmarks demonstrate that the proposed method\nconsistently outperforms existing state-of-the-art baselines. Moreover, QLLM\nexhibits strong generalization capability and maintains compatibility with a\nwide range of MARL algorithms that utilize mixing networks, positioning it as a\npromising and versatile solution for complex multi-agent scenarios."
                },
                "authors": [
                    {
                        "name": "Zhouyang Jiang"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Airong Wei"
                    },
                    {
                        "name": "Zhiwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Xu"
                },
                "author": "Zhiwei Xu",
                "arxiv_comment": "We are withdrawing this manuscript due to experimental errors and\n  mistakes in data preprocessing. These issues materially affect the results\n  and could mislead subsequent studies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12961v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12961v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13786v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13786v6",
                "updated": "2025-10-07T15:29:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    29,
                    38,
                    1,
                    280,
                    0
                ],
                "published": "2023-10-20T19:32:54Z",
                "published_parsed": [
                    2023,
                    10,
                    20,
                    19,
                    32,
                    54,
                    4,
                    293,
                    0
                ],
                "title": "Fundamental Limits of Membership Inference Attacks on Machine Learning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Membership Inference Attacks on Machine Learning\n  Models"
                },
                "summary": "Membership inference attacks (MIA) can reveal whether a particular data point\nwas part of the training dataset, potentially exposing sensitive information\nabout individuals. This article provides theoretical guarantees by exploring\nthe fundamental statistical limitations associated with MIAs on machine\nlearning models at large. More precisely, we first derive the statistical\nquantity that governs the effectiveness and success of such attacks. We then\ntheoretically prove that in a non-linear regression setting with overfitting\nlearning procedures, attacks may have a high probability of success. Finally,\nwe investigate several situations for which we provide bounds on this quantity\nof interest. Interestingly, our findings indicate that discretizing the data\nmight enhance the learning procedure's security. Specifically, it is\ndemonstrated to be limited by a constant, which quantifies the diversity of the\nunderlying data distribution. We illustrate those results through simple\nsimulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership inference attacks (MIA) can reveal whether a particular data point\nwas part of the training dataset, potentially exposing sensitive information\nabout individuals. This article provides theoretical guarantees by exploring\nthe fundamental statistical limitations associated with MIAs on machine\nlearning models at large. More precisely, we first derive the statistical\nquantity that governs the effectiveness and success of such attacks. We then\ntheoretically prove that in a non-linear regression setting with overfitting\nlearning procedures, attacks may have a high probability of success. Finally,\nwe investigate several situations for which we provide bounds on this quantity\nof interest. Interestingly, our findings indicate that discretizing the data\nmight enhance the learning procedure's security. Specifically, it is\ndemonstrated to be limited by a constant, which quantifies the diversity of the\nunderlying data distribution. We illustrate those results through simple\nsimulations."
                },
                "authors": [
                    {
                        "name": "Eric Aubinais"
                    },
                    {
                        "name": "Elisabeth Gassiat"
                    },
                    {
                        "name": "Pablo Piantanida"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Piantanida"
                },
                "author": "Pablo Piantanida",
                "arxiv_comment": "Accepted for publication in JMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.13786v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13786v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19252v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19252v3",
                "updated": "2025-10-07T15:22:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    22,
                    5,
                    1,
                    280,
                    0
                ],
                "published": "2025-01-31T16:09:30Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    9,
                    30,
                    4,
                    31,
                    0
                ],
                "title": "Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search"
                },
                "summary": "The remarkable progress in text-to-video diffusion models enables the\ngeneration of photorealistic videos, although the content of these generated\nvideos often includes unnatural movement or deformation, reverse playback, and\nmotionless scenes. Recently, an alignment problem has attracted huge attention,\nwhere we steer the output of diffusion models based on some measure of the\ncontent's goodness. Because there is a large room for improvement of perceptual\nquality along the frame direction, we should address which metrics we should\noptimize and how we can optimize them in the video generation. In this paper,\nwe propose diffusion latent beam search with lookahead estimator, which can\nselect a better diffusion latent to maximize a given alignment reward at\ninference time. We then point out that improving perceptual video quality with\nrespect to alignment to prompts requires reward calibration by weighting\nexisting metrics. This is because when humans or vision language models\nevaluate outputs, many previous metrics to quantify the naturalness of video do\nnot always correlate with the evaluation. We demonstrate that our method\nimproves the perceptual quality evaluated on the calibrated reward, VLMs, and\nhuman assessment, without model parameter update, and outputs the best\ngeneration compared to greedy search and best-of-N sampling under much more\nefficient computational cost. The experiments highlight that our method is\nbeneficial to many capable generative models, and provide a practical\nguideline: we should prioritize the inference-time compute allocation into\nenabling the lookahead estimator and increasing the search budget, rather than\nexpanding the denoising steps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable progress in text-to-video diffusion models enables the\ngeneration of photorealistic videos, although the content of these generated\nvideos often includes unnatural movement or deformation, reverse playback, and\nmotionless scenes. Recently, an alignment problem has attracted huge attention,\nwhere we steer the output of diffusion models based on some measure of the\ncontent's goodness. Because there is a large room for improvement of perceptual\nquality along the frame direction, we should address which metrics we should\noptimize and how we can optimize them in the video generation. In this paper,\nwe propose diffusion latent beam search with lookahead estimator, which can\nselect a better diffusion latent to maximize a given alignment reward at\ninference time. We then point out that improving perceptual video quality with\nrespect to alignment to prompts requires reward calibration by weighting\nexisting metrics. This is because when humans or vision language models\nevaluate outputs, many previous metrics to quantify the naturalness of video do\nnot always correlate with the evaluation. We demonstrate that our method\nimproves the perceptual quality evaluated on the calibrated reward, VLMs, and\nhuman assessment, without model parameter update, and outputs the best\ngeneration compared to greedy search and best-of-N sampling under much more\nefficient computational cost. The experiments highlight that our method is\nbeneficial to many capable generative models, and provide a practical\nguideline: we should prioritize the inference-time compute allocation into\nenabling the lookahead estimator and increasing the search budget, rather than\nexpanding the denoising steps."
                },
                "authors": [
                    {
                        "name": "Yuta Oshima"
                    },
                    {
                        "name": "Masahiro Suzuki"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Hiroki Furuta"
                    }
                ],
                "author_detail": {
                    "name": "Hiroki Furuta"
                },
                "author": "Hiroki Furuta",
                "arxiv_comment": "Accepted to NeurIPS2025. Website:\n  https://sites.google.com/view/t2v-dlbs and Code:\n  https://github.com/shim0114/T2V-Diffusion-Search",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19252v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19252v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06018v1",
                "updated": "2025-10-07T15:16:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    16,
                    47,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:16:47Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    16,
                    47,
                    1,
                    280,
                    0
                ],
                "title": "Evaluating The Impact of Stimulus Quality in Investigations of LLM\n  Language Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating The Impact of Stimulus Quality in Investigations of LLM\n  Language Performance"
                },
                "summary": "Recent studies employing Large Language Models (LLMs) to test the Argument\nfrom the Poverty of the Stimulus (APS) have yielded contrasting results across\nsyntactic phenomena. This paper investigates the hypothesis that\ncharacteristics of the stimuli used in recent studies, including lexical\nambiguities and structural complexities, may confound model performance. A\nmethodology is proposed for re-evaluating LLM competence on syntactic\nprediction, focusing on GPT-2. This involves: 1) establishing a baseline on\npreviously used (both filtered and unfiltered) stimuli, and 2) generating a\nnew, refined dataset using a state-of-the-art (SOTA) generative LLM (Gemini 2.5\nPro Preview) guided by linguistically-informed templates designed to mitigate\nidentified confounds. Our preliminary findings indicate that GPT-2 demonstrates\nnotably improved performance on these refined PG stimuli compared to baselines,\nsuggesting that stimulus quality significantly influences outcomes in\nsurprisal-based evaluations of LLM syntactic competency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies employing Large Language Models (LLMs) to test the Argument\nfrom the Poverty of the Stimulus (APS) have yielded contrasting results across\nsyntactic phenomena. This paper investigates the hypothesis that\ncharacteristics of the stimuli used in recent studies, including lexical\nambiguities and structural complexities, may confound model performance. A\nmethodology is proposed for re-evaluating LLM competence on syntactic\nprediction, focusing on GPT-2. This involves: 1) establishing a baseline on\npreviously used (both filtered and unfiltered) stimuli, and 2) generating a\nnew, refined dataset using a state-of-the-art (SOTA) generative LLM (Gemini 2.5\nPro Preview) guided by linguistically-informed templates designed to mitigate\nidentified confounds. Our preliminary findings indicate that GPT-2 demonstrates\nnotably improved performance on these refined PG stimuli compared to baselines,\nsuggesting that stimulus quality significantly influences outcomes in\nsurprisal-based evaluations of LLM syntactic competency."
                },
                "authors": [
                    {
                        "name": "Timothy Pistotti"
                    },
                    {
                        "name": "Jason Brown"
                    },
                    {
                        "name": "Michael Witbrock"
                    }
                ],
                "author_detail": {
                    "name": "Michael Witbrock"
                },
                "author": "Michael Witbrock",
                "arxiv_comment": "Presented at https://brigap-workshop.github.io/ Information to be\n  updated upon publication of proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18842v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18842v4",
                "updated": "2025-10-07T15:13:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    13,
                    12,
                    1,
                    280,
                    0
                ],
                "published": "2025-05-24T19:30:47Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    19,
                    30,
                    47,
                    5,
                    144,
                    0
                ],
                "title": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning"
                },
                "summary": "When thinking with images, humans rarely rely on a single glance: they\nrevisit visual information repeatedly during reasoning. However, existing\nmodels typically process images only once and thereafter generate reasoning\nentirely in text, lacking mechanisms to re-access or ground inference in visual\nrepresentations. We empirically confirm this: as reasoning chains lengthen,\nmodels progressively lose focus on relevant regions. In response, we introduce\nv1, a lightweight extension that enables active visual referencing through a\nsimple point-and-copy approach. This allows the model to identify relevant\nimage patches and copy their embeddings back into the reasoning stream,\nensuring that evolving hypotheses remain grounded in perceptual evidence.\nCrucially, our pointing strategy lets the MLLM directly select image patches\nusing their semantic representations as keys, keeping perceptual evidence\nembedded in the same space as the model's reasoning. To train this capability,\nwe construct v1g, a dataset of 300K multimodal reasoning traces with\ninterleaved visual grounding annotations. Across various multimodal\nmathematical reasoning benchmarks, v1 consistently outperforms comparable\nbaselines, establishing point-and-copy as a practical mechanism for grounded\nreasoning. The model checkpoint and dataset are available at\ngithub.com/jun297/v1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When thinking with images, humans rarely rely on a single glance: they\nrevisit visual information repeatedly during reasoning. However, existing\nmodels typically process images only once and thereafter generate reasoning\nentirely in text, lacking mechanisms to re-access or ground inference in visual\nrepresentations. We empirically confirm this: as reasoning chains lengthen,\nmodels progressively lose focus on relevant regions. In response, we introduce\nv1, a lightweight extension that enables active visual referencing through a\nsimple point-and-copy approach. This allows the model to identify relevant\nimage patches and copy their embeddings back into the reasoning stream,\nensuring that evolving hypotheses remain grounded in perceptual evidence.\nCrucially, our pointing strategy lets the MLLM directly select image patches\nusing their semantic representations as keys, keeping perceptual evidence\nembedded in the same space as the model's reasoning. To train this capability,\nwe construct v1g, a dataset of 300K multimodal reasoning traces with\ninterleaved visual grounding annotations. Across various multimodal\nmathematical reasoning benchmarks, v1 consistently outperforms comparable\nbaselines, establishing point-and-copy as a practical mechanism for grounded\nreasoning. The model checkpoint and dataset are available at\ngithub.com/jun297/v1."
                },
                "authors": [
                    {
                        "name": "Jiwan Chung"
                    },
                    {
                        "name": "Junhyeok Kim"
                    },
                    {
                        "name": "Siyeol Kim"
                    },
                    {
                        "name": "Jaeyoung Lee"
                    },
                    {
                        "name": "Min Soo Kim"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18842v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18842v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02418v2",
                "updated": "2025-10-07T15:12:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    12,
                    39,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-02T15:22:21Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    22,
                    21,
                    3,
                    275,
                    0
                ],
                "title": "BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks"
                },
                "summary": "LLM web agents now browse and take actions on the open web, yet current agent\nevaluations are constrained to sandboxed environments or artificial tasks. We\nintroduce BrowserArena, a live open-web agent evaluation platform that collects\nuser-submitted tasks, runs Arena-style head-to-head comparisons, and uses\nstep-level human feedback to surface failure modes. Collecting and analyzing\nstep-level annotations on the agent traces, we identify three consistent\nfailure modes: captcha resolution, pop-up banner removal, and direct navigation\nto URLs. By constructing targeted datasets to further study these tasks, we\ndiscover variations in how different language models navigate these failure\nmodes. We find, for example, that o4-mini deploys a wider variety of strategies\nto circumvent captcha resolution than other models and DeepSeek-R1 consistently\nmisleads users about pop-up banner closure. Our findings surface both the\ndiversity and brittleness of current web agents. More broadly, our benchmarking\nmethodology provides an approach to evaluating and understanding web agent\nfailure modes at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM web agents now browse and take actions on the open web, yet current agent\nevaluations are constrained to sandboxed environments or artificial tasks. We\nintroduce BrowserArena, a live open-web agent evaluation platform that collects\nuser-submitted tasks, runs Arena-style head-to-head comparisons, and uses\nstep-level human feedback to surface failure modes. Collecting and analyzing\nstep-level annotations on the agent traces, we identify three consistent\nfailure modes: captcha resolution, pop-up banner removal, and direct navigation\nto URLs. By constructing targeted datasets to further study these tasks, we\ndiscover variations in how different language models navigate these failure\nmodes. We find, for example, that o4-mini deploys a wider variety of strategies\nto circumvent captcha resolution than other models and DeepSeek-R1 consistently\nmisleads users about pop-up banner closure. Our findings surface both the\ndiversity and brittleness of current web agents. More broadly, our benchmarking\nmethodology provides an approach to evaluating and understanding web agent\nfailure modes at scale."
                },
                "authors": [
                    {
                        "name": "Sagnik Anupam"
                    },
                    {
                        "name": "Davis Brown"
                    },
                    {
                        "name": "Shuo Li"
                    },
                    {
                        "name": "Eric Wong"
                    },
                    {
                        "name": "Hamed Hassani"
                    },
                    {
                        "name": "Osbert Bastani"
                    }
                ],
                "author_detail": {
                    "name": "Osbert Bastani"
                },
                "author": "Osbert Bastani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06014v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06014v1",
                "updated": "2025-10-07T15:10:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    10,
                    51,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:10:51Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    10,
                    51,
                    1,
                    280,
                    0
                ],
                "title": "ARISE: An Adaptive Resolution-Aware Metric for Test-Time Scaling\n  Evaluation in Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARISE: An Adaptive Resolution-Aware Metric for Test-Time Scaling\n  Evaluation in Large Reasoning Models"
                },
                "summary": "Test-time scaling has emerged as a transformative paradigm for enhancing the\nperformance of large reasoning models, enabling dynamic allocation of\ncomputational resources during inference. However, as the landscape of\nreasoning models rapidly expands, a critical question remains: how can we\nsystematically compare and evaluate the test-time scaling capabilities across\ndifferent models? In this paper, we introduce ARISE (Adaptive Resolution-aware\nScaling Evaluation), a novel metric specifically designed to assess the\ntest-time scaling effectiveness of large reasoning models. Unlike existing\nevaluation approaches, ARISE incorporates two key innovations: (1) sample-level\nawareness that effectively penalizes negative scaling behaviors where increased\ncomputation leads to performance degradation, and (2) a dynamic sampling\nmechanism that mitigates the impact of accuracy fluctuations and token count\ninstability on the final assessment. We conduct comprehensive experiments\nevaluating state-of-the-art reasoning models across diverse domains including\nmathematical reasoning, code generation, and agentic tasks. Our results\ndemonstrate that ARISE provides a reliable and fine-grained measurement of\ntest-time scaling capabilities, revealing significant variations in scaling\nefficiency across models. Notably, our evaluation identifies Claude Opus as\nexhibiting superior scaling characteristics compared to other contemporary\nreasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling has emerged as a transformative paradigm for enhancing the\nperformance of large reasoning models, enabling dynamic allocation of\ncomputational resources during inference. However, as the landscape of\nreasoning models rapidly expands, a critical question remains: how can we\nsystematically compare and evaluate the test-time scaling capabilities across\ndifferent models? In this paper, we introduce ARISE (Adaptive Resolution-aware\nScaling Evaluation), a novel metric specifically designed to assess the\ntest-time scaling effectiveness of large reasoning models. Unlike existing\nevaluation approaches, ARISE incorporates two key innovations: (1) sample-level\nawareness that effectively penalizes negative scaling behaviors where increased\ncomputation leads to performance degradation, and (2) a dynamic sampling\nmechanism that mitigates the impact of accuracy fluctuations and token count\ninstability on the final assessment. We conduct comprehensive experiments\nevaluating state-of-the-art reasoning models across diverse domains including\nmathematical reasoning, code generation, and agentic tasks. Our results\ndemonstrate that ARISE provides a reliable and fine-grained measurement of\ntest-time scaling capabilities, revealing significant variations in scaling\nefficiency across models. Notably, our evaluation identifies Claude Opus as\nexhibiting superior scaling characteristics compared to other contemporary\nreasoning models."
                },
                "authors": [
                    {
                        "name": "Zhangyue Yin"
                    },
                    {
                        "name": "Qiushi Sun"
                    },
                    {
                        "name": "Zhiyuan Zeng"
                    },
                    {
                        "name": "Zhiyuan Yu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "19 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06014v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06014v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06009v1",
                "updated": "2025-10-07T15:08:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    8,
                    26,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:08:26Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    8,
                    26,
                    1,
                    280,
                    0
                ],
                "title": "Continual Learning for Image Captioning through Improved Image-Text\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Learning for Image Captioning through Improved Image-Text\n  Alignment"
                },
                "summary": "Generating accurate and coherent image captions in a continual learning\nsetting remains a major challenge due to catastrophic forgetting and the\ndifficulty of aligning evolving visual concepts with language over time. In\nthis work, we propose a novel multi-loss framework for continual image\ncaptioning that integrates semantic guidance through prompt-based continual\nlearning and contrastive alignment. Built upon a pretrained ViT-GPT-2 backbone,\nour approach combines standard cross-entropy loss with three additional\ncomponents: (1) a prompt-based cosine similarity loss that aligns image\nembeddings with synthetically constructed prompts encoding objects, attributes,\nand actions; (2) a CLIP-style loss that promotes alignment between image\nembeddings and target caption embedding; and (3) a language-guided contrastive\nloss that employs a triplet loss to enhance class-level discriminability\nbetween tasks. Notably, our approach introduces no additional overhead at\ninference time and requires no prompts during caption generation. We find that\nthis approach mitigates catastrophic forgetting, while achieving better\nsemantic caption alignment compared to state-of-the-art methods. The code can\nbe found via the following link https://github.com/\nGepardius/Taetz_Bordelius_Continual_ImageCaptioning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating accurate and coherent image captions in a continual learning\nsetting remains a major challenge due to catastrophic forgetting and the\ndifficulty of aligning evolving visual concepts with language over time. In\nthis work, we propose a novel multi-loss framework for continual image\ncaptioning that integrates semantic guidance through prompt-based continual\nlearning and contrastive alignment. Built upon a pretrained ViT-GPT-2 backbone,\nour approach combines standard cross-entropy loss with three additional\ncomponents: (1) a prompt-based cosine similarity loss that aligns image\nembeddings with synthetically constructed prompts encoding objects, attributes,\nand actions; (2) a CLIP-style loss that promotes alignment between image\nembeddings and target caption embedding; and (3) a language-guided contrastive\nloss that employs a triplet loss to enhance class-level discriminability\nbetween tasks. Notably, our approach introduces no additional overhead at\ninference time and requires no prompts during caption generation. We find that\nthis approach mitigates catastrophic forgetting, while achieving better\nsemantic caption alignment compared to state-of-the-art methods. The code can\nbe found via the following link https://github.com/\nGepardius/Taetz_Bordelius_Continual_ImageCaptioning."
                },
                "authors": [
                    {
                        "name": "Bertram Taetz"
                    },
                    {
                        "name": "Gal Bordelius"
                    }
                ],
                "author_detail": {
                    "name": "Gal Bordelius"
                },
                "author": "Gal Bordelius",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02362v2",
                "updated": "2025-10-07T15:07:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    7,
                    33,
                    1,
                    280,
                    0
                ],
                "published": "2024-10-03T10:23:03Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    23,
                    3,
                    3,
                    277,
                    0
                ],
                "title": "A Comprehensive Survey of Mamba Architectures for Medical Image\n  Analysis: Classification, Segmentation, Restoration and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Mamba Architectures for Medical Image\n  Analysis: Classification, Segmentation, Restoration and Beyond"
                },
                "summary": "Mamba, a special case of the State Space Model, is gaining popularity as an\nalternative to template-based deep learning approaches in medical image\nanalysis. While transformers are powerful architectures, they have drawbacks,\nincluding quadratic computational complexity and an inability to address\nlong-range dependencies efficiently. This limitation affects the analysis of\nlarge and complex datasets in medical imaging, where there are many spatial and\ntemporal relationships. In contrast, Mamba offers benefits that make it\nwell-suited for medical image analysis. It has linear time complexity, which is\na significant improvement over transformers. Mamba processes longer sequences\nwithout attention mechanisms, enabling faster inference and requiring less\nmemory. Mamba also demonstrates strong performance in merging multimodal data,\nimproving diagnosis accuracy and patient outcomes. The organization of this\npaper allows readers to appreciate the capabilities of Mamba in medical imaging\nstep by step. We begin by defining core concepts of SSMs and models, including\nS4, S5, and S6, followed by an exploration of Mamba architectures such as pure\nMamba, U-Net variants, and hybrid models with convolutional neural networks,\ntransformers, and Graph Neural Networks. We also cover Mamba optimizations,\ntechniques and adaptations, scanning, datasets, applications, experimental\nresults, and conclude with its challenges and future directions in medical\nimaging. This review aims to demonstrate the transformative potential of Mamba\nin overcoming existing barriers within medical imaging while paving the way for\ninnovative advancements in the field. A comprehensive list of Mamba\narchitectures applied in the medical field, reviewed in this work, is available\nat Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mamba, a special case of the State Space Model, is gaining popularity as an\nalternative to template-based deep learning approaches in medical image\nanalysis. While transformers are powerful architectures, they have drawbacks,\nincluding quadratic computational complexity and an inability to address\nlong-range dependencies efficiently. This limitation affects the analysis of\nlarge and complex datasets in medical imaging, where there are many spatial and\ntemporal relationships. In contrast, Mamba offers benefits that make it\nwell-suited for medical image analysis. It has linear time complexity, which is\na significant improvement over transformers. Mamba processes longer sequences\nwithout attention mechanisms, enabling faster inference and requiring less\nmemory. Mamba also demonstrates strong performance in merging multimodal data,\nimproving diagnosis accuracy and patient outcomes. The organization of this\npaper allows readers to appreciate the capabilities of Mamba in medical imaging\nstep by step. We begin by defining core concepts of SSMs and models, including\nS4, S5, and S6, followed by an exploration of Mamba architectures such as pure\nMamba, U-Net variants, and hybrid models with convolutional neural networks,\ntransformers, and Graph Neural Networks. We also cover Mamba optimizations,\ntechniques and adaptations, scanning, datasets, applications, experimental\nresults, and conclude with its challenges and future directions in medical\nimaging. This review aims to demonstrate the transformative potential of Mamba\nin overcoming existing barriers within medical imaging while paving the way for\ninnovative advancements in the field. A comprehensive list of Mamba\narchitectures applied in the medical field, reviewed in this work, is available\nat Github."
                },
                "authors": [
                    {
                        "name": "Shubhi Bansal"
                    },
                    {
                        "name": "Sreeharish A"
                    },
                    {
                        "name": "Madhava Prasath J"
                    },
                    {
                        "name": "Manikandan S"
                    },
                    {
                        "name": "Sreekanth Madisetty"
                    },
                    {
                        "name": "Mohammad Zia Ur Rehman"
                    },
                    {
                        "name": "Chandravardhan Singh Raghaw"
                    },
                    {
                        "name": "Gaurav Duggal"
                    },
                    {
                        "name": "Nagendra Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Nagendra Kumar"
                },
                "author": "Nagendra Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06001v1",
                "updated": "2025-10-07T15:03:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    3,
                    9,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:03:09Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    3,
                    9,
                    1,
                    280,
                    0
                ],
                "title": "Exploring Gaps in the APS: Direct Minimal Pair Analysis in LLM Syntactic\n  Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Gaps in the APS: Direct Minimal Pair Analysis in LLM Syntactic\n  Assessments"
                },
                "summary": "Recent studies probing the Argument from the Poverty of the Stimulus (APS)\nhave applied Large Language Models (LLMs) to test the learnability of complex\nsyntax through surprisal-based metrics. However, divergent conclusions raise\nquestions concerning the insights these metrics offer. While Wilcox et al.\n(2024) used direct minimal pair comparisons (the \"wh-effect\") to demonstrate\nthat models successfully generalise knowledge of filler-gap dependencies, Lan\net al. (2024) used a Difference-in-Differences (DiD) metric and found that\nmodels largely fail on parasitic gaps (PGs). This paper argues that the direct\nminimal pair approach offers greater diagnostic transparency. We demonstrate\nthis by generating a full 8-permutation paradigm of refined PG stimuli and\nevaluating the GPT-2 model used in previous studies with a systematic\nWilcox-style wh-effect analysis. Our results show that GPT-2 succeeds across\nall four tested conditions, indicating robust knowledge of filler-gap licensing\nprinciples even in complex PG environments. This finding, which contrasts with\nthe more ambiguous results from DiD-style metrics, suggests that the choice of\nevaluation metric is critical for assessing an LLM's syntactic competence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies probing the Argument from the Poverty of the Stimulus (APS)\nhave applied Large Language Models (LLMs) to test the learnability of complex\nsyntax through surprisal-based metrics. However, divergent conclusions raise\nquestions concerning the insights these metrics offer. While Wilcox et al.\n(2024) used direct minimal pair comparisons (the \"wh-effect\") to demonstrate\nthat models successfully generalise knowledge of filler-gap dependencies, Lan\net al. (2024) used a Difference-in-Differences (DiD) metric and found that\nmodels largely fail on parasitic gaps (PGs). This paper argues that the direct\nminimal pair approach offers greater diagnostic transparency. We demonstrate\nthis by generating a full 8-permutation paradigm of refined PG stimuli and\nevaluating the GPT-2 model used in previous studies with a systematic\nWilcox-style wh-effect analysis. Our results show that GPT-2 succeeds across\nall four tested conditions, indicating robust knowledge of filler-gap licensing\nprinciples even in complex PG environments. This finding, which contrasts with\nthe more ambiguous results from DiD-style metrics, suggests that the choice of\nevaluation metric is critical for assessing an LLM's syntactic competence."
                },
                "authors": [
                    {
                        "name": "Timothy Pistotti"
                    },
                    {
                        "name": "Jason Brown"
                    },
                    {
                        "name": "Michael Witbrock"
                    }
                ],
                "author_detail": {
                    "name": "Michael Witbrock"
                },
                "author": "Michael Witbrock",
                "arxiv_comment": "Presented at the https://brigap-workshop.github.io/ Information to be\n  updated after publication of proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02444v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02444v6",
                "updated": "2025-10-07T14:57:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    57,
                    19,
                    1,
                    280,
                    0
                ],
                "published": "2025-02-04T16:10:55Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    10,
                    55,
                    1,
                    35,
                    0
                ],
                "title": "Generative Psycho-Lexical Approach for Constructing Value Systems in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Psycho-Lexical Approach for Constructing Value Systems in\n  Large Language Models"
                },
                "summary": "Values are core drivers of individual and collective perception, cognition,\nand behavior. Value systems, such as Schwartz's Theory of Basic Human Values,\ndelineate the hierarchy and interplay among these values, enabling\ncross-disciplinary investigations into decision-making and societal dynamics.\nRecently, the rise of Large Language Models (LLMs) has raised concerns\nregarding their elusive intrinsic values. Despite growing efforts in\nevaluating, understanding, and aligning LLM values, a psychologically grounded\nLLM value system remains underexplored. This study addresses the gap by\nintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,\nadaptable, and theoretically informed method for constructing value systems.\nLeveraging GPLA, we propose a psychologically grounded five-factor value system\ntailored for LLMs. For systematic validation, we present three benchmarking\ntasks that integrate psychological principles with cutting-edge AI priorities.\nOur results reveal that the proposed value system meets standard psychological\ncriteria, better captures LLM values, improves LLM safety prediction, and\nenhances LLM alignment, when compared to the canonical Schwartz's values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Values are core drivers of individual and collective perception, cognition,\nand behavior. Value systems, such as Schwartz's Theory of Basic Human Values,\ndelineate the hierarchy and interplay among these values, enabling\ncross-disciplinary investigations into decision-making and societal dynamics.\nRecently, the rise of Large Language Models (LLMs) has raised concerns\nregarding their elusive intrinsic values. Despite growing efforts in\nevaluating, understanding, and aligning LLM values, a psychologically grounded\nLLM value system remains underexplored. This study addresses the gap by\nintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,\nadaptable, and theoretically informed method for constructing value systems.\nLeveraging GPLA, we propose a psychologically grounded five-factor value system\ntailored for LLMs. For systematic validation, we present three benchmarking\ntasks that integrate psychological principles with cutting-edge AI priorities.\nOur results reveal that the proposed value system meets standard psychological\ncriteria, better captures LLM values, improves LLM safety prediction, and\nenhances LLM alignment, when compared to the canonical Schwartz's values."
                },
                "authors": [
                    {
                        "name": "Haoran Ye"
                    },
                    {
                        "name": "Tianze Zhang"
                    },
                    {
                        "name": "Yuhang Xie"
                    },
                    {
                        "name": "Liyuan Zhang"
                    },
                    {
                        "name": "Yuanyi Ren"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Guojie Song"
                    }
                ],
                "author_detail": {
                    "name": "Guojie Song"
                },
                "author": "Guojie Song",
                "arxiv_comment": "ACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02444v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02444v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11773v2",
                "updated": "2025-10-07T14:55:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    55,
                    30,
                    1,
                    280,
                    0
                ],
                "published": "2025-09-15T10:53:05Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    10,
                    53,
                    5,
                    0,
                    258,
                    0
                ],
                "title": "AgenticIE: An Adaptive Agent for Information Extraction from Complex\n  Regulatory Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgenticIE: An Adaptive Agent for Information Extraction from Complex\n  Regulatory Documents"
                },
                "summary": "Declaration of Performance (DoP) documents, mandated by EU regulation,\ncertify the performance of construction products. There are two challenges to\nmake DoPs machine and human accessible through automated key-value pair\nextraction (KVP) and question answering (QA): (1) While some of their content\nis standardized, DoPs vary widely in layout, schema, and format; (2) Both users\nand documents are multilingual. Existing static or LLM-only Information\nExtraction (IE) pipelines fail to adapt to this structural document and user\ndiversity. Our domain-specific, agentic system addresses these challenges\nthrough a planner-executor-responder architecture. The system infers user\nintent, detects document language and modality, and orchestrates tools\ndynamically for robust, traceable reasoning while avoiding tool misuse or\nexecution loops. Our agent outperforms baselines (ROUGE: 0.783 vs. 0.703/0.608)\nwith better cross-lingual stability (17-point vs. 21-26-point variation).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Declaration of Performance (DoP) documents, mandated by EU regulation,\ncertify the performance of construction products. There are two challenges to\nmake DoPs machine and human accessible through automated key-value pair\nextraction (KVP) and question answering (QA): (1) While some of their content\nis standardized, DoPs vary widely in layout, schema, and format; (2) Both users\nand documents are multilingual. Existing static or LLM-only Information\nExtraction (IE) pipelines fail to adapt to this structural document and user\ndiversity. Our domain-specific, agentic system addresses these challenges\nthrough a planner-executor-responder architecture. The system infers user\nintent, detects document language and modality, and orchestrates tools\ndynamically for robust, traceable reasoning while avoiding tool misuse or\nexecution loops. Our agent outperforms baselines (ROUGE: 0.783 vs. 0.703/0.608)\nwith better cross-lingual stability (17-point vs. 21-26-point variation)."
                },
                "authors": [
                    {
                        "name": "Gaye Colakoglu"
                    },
                    {
                        "name": "Gürkan Solmaz"
                    },
                    {
                        "name": "Jonathan Fürst"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Fürst"
                },
                "author": "Jonathan Fürst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05991v1",
                "updated": "2025-10-07T14:51:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    51,
                    5,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T14:51:05Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    51,
                    5,
                    1,
                    280,
                    0
                ],
                "title": "Robust Inference for Convex Pairwise Difference Estimators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Inference for Convex Pairwise Difference Estimators"
                },
                "summary": "This paper develops distribution theory and bootstrap-based inference methods\nfor a broad class of convex pairwise difference estimators. These estimators\nminimize a kernel-weighted convex-in-parameter function over observation pairs\nthat are similar in terms of certain covariates, where the similarity is\ngoverned by a localization (bandwidth) parameter. While classical results\nestablish asymptotic normality under restrictive bandwidth conditions, we show\nthat valid Gaussian and bootstrap-based inference remains possible under\nsubstantially weaker assumptions. First, we extend the theory of small\nbandwidth asymptotics to convex pairwise estimation settings, deriving robust\nGaussian approximations even when a smaller than standard bandwidth is used.\nSecond, we employ a debiasing procedure based on generalized jackknifing to\nenable inference with larger bandwidths, while preserving convexity of the\nobjective function. Third, we construct a novel bootstrap method that adjusts\nfor bandwidth-induced variance distortions, yielding valid inference across a\nwide range of bandwidth choices. Our proposed inference method enjoys\ndemonstrable more robustness, while retaining the practical appeal of convex\npairwise difference estimators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper develops distribution theory and bootstrap-based inference methods\nfor a broad class of convex pairwise difference estimators. These estimators\nminimize a kernel-weighted convex-in-parameter function over observation pairs\nthat are similar in terms of certain covariates, where the similarity is\ngoverned by a localization (bandwidth) parameter. While classical results\nestablish asymptotic normality under restrictive bandwidth conditions, we show\nthat valid Gaussian and bootstrap-based inference remains possible under\nsubstantially weaker assumptions. First, we extend the theory of small\nbandwidth asymptotics to convex pairwise estimation settings, deriving robust\nGaussian approximations even when a smaller than standard bandwidth is used.\nSecond, we employ a debiasing procedure based on generalized jackknifing to\nenable inference with larger bandwidths, while preserving convexity of the\nobjective function. Third, we construct a novel bootstrap method that adjusts\nfor bandwidth-induced variance distortions, yielding valid inference across a\nwide range of bandwidth choices. Our proposed inference method enjoys\ndemonstrable more robustness, while retaining the practical appeal of convex\npairwise difference estimators."
                },
                "authors": [
                    {
                        "name": "Matias D. Cattaneo"
                    },
                    {
                        "name": "Michael Jansson"
                    },
                    {
                        "name": "Kenichi Nagasawa"
                    }
                ],
                "author_detail": {
                    "name": "Kenichi Nagasawa"
                },
                "author": "Kenichi Nagasawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05987v1",
                "updated": "2025-10-07T14:46:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    46,
                    12,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T14:46:12Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    46,
                    12,
                    1,
                    280,
                    0
                ],
                "title": "Sample Smart, Not Hard: Correctness-First Decoding for Better Reasoning\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample Smart, Not Hard: Correctness-First Decoding for Better Reasoning\n  in LLMs"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied to complex tasks that\nrequire extended reasoning. In such settings, models often benefit from diverse\nchains-of-thought to arrive at multiple candidate solutions. This requires two\ncompeting objectives: to inject enough stochasticity to explore multiple\nreasoning chains, and to ensure sufficient accuracy and quality in each path.\nExisting works pursue the first objective by increasing exploration at highly\nuncertain steps with higher temperature or larger candidate token sets, while\nothers improve reliability by rejecting samples with low confidence\npost-generation, implying that low confidence correlates with low answer\nquality. These two lines of thought are in conflict, as they conflate different\nsources of uncertainty. To resolve this, we argue that the decoding rule should\nbe calibrated by correctness, not confidence alone. We should sample from\ntokens with higher estimated correctness, and reduce sampling where expected\ncorrectness is low. We propose simple strategies that achieve this goal:\nGreedy-Threshold makes sampling greedy at very low confidence steps.\nCalibrated-TopK and Calibrated-epsilon set truncation threshold based on\nestimated rank-wise correctness. Together, our findings challenge prevailing\nheuristics about decoding under uncertainty and show gains across math and\ngeneral reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied to complex tasks that\nrequire extended reasoning. In such settings, models often benefit from diverse\nchains-of-thought to arrive at multiple candidate solutions. This requires two\ncompeting objectives: to inject enough stochasticity to explore multiple\nreasoning chains, and to ensure sufficient accuracy and quality in each path.\nExisting works pursue the first objective by increasing exploration at highly\nuncertain steps with higher temperature or larger candidate token sets, while\nothers improve reliability by rejecting samples with low confidence\npost-generation, implying that low confidence correlates with low answer\nquality. These two lines of thought are in conflict, as they conflate different\nsources of uncertainty. To resolve this, we argue that the decoding rule should\nbe calibrated by correctness, not confidence alone. We should sample from\ntokens with higher estimated correctness, and reduce sampling where expected\ncorrectness is low. We propose simple strategies that achieve this goal:\nGreedy-Threshold makes sampling greedy at very low confidence steps.\nCalibrated-TopK and Calibrated-epsilon set truncation threshold based on\nestimated rank-wise correctness. Together, our findings challenge prevailing\nheuristics about decoding under uncertainty and show gains across math and\ngeneral reasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Xueyan Li"
                    },
                    {
                        "name": "Guinan Su"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05984v1",
                "updated": "2025-10-07T14:44:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    44,
                    5,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T14:44:05Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    44,
                    5,
                    1,
                    280,
                    0
                ],
                "title": "ECTSpeech: Enhancing Efficient Speech Synthesis via Easy Consistency\n  Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECTSpeech: Enhancing Efficient Speech Synthesis via Easy Consistency\n  Tuning"
                },
                "summary": "Diffusion models have demonstrated remarkable performance in speech\nsynthesis, but typically require multi-step sampling, resulting in low\ninference efficiency. Recent studies address this issue by distilling diffusion\nmodels into consistency models, enabling efficient one-step generation.\nHowever, these approaches introduce additional training costs and rely heavily\non the performance of pre-trained teacher models. In this paper, we propose\nECTSpeech, a simple and effective one-step speech synthesis framework that, for\nthe first time, incorporates the Easy Consistency Tuning (ECT) strategy into\nspeech synthesis. By progressively tightening consistency constraints on a\npre-trained diffusion model, ECTSpeech achieves high-quality one-step\ngeneration while significantly reducing training complexity. In addition, we\ndesign a multi-scale gate module (MSGate) to enhance the denoiser's ability to\nfuse features at different scales. Experimental results on the LJSpeech dataset\ndemonstrate that ECTSpeech achieves audio quality comparable to\nstate-of-the-art methods under single-step sampling, while substantially\nreducing the model's training cost and complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated remarkable performance in speech\nsynthesis, but typically require multi-step sampling, resulting in low\ninference efficiency. Recent studies address this issue by distilling diffusion\nmodels into consistency models, enabling efficient one-step generation.\nHowever, these approaches introduce additional training costs and rely heavily\non the performance of pre-trained teacher models. In this paper, we propose\nECTSpeech, a simple and effective one-step speech synthesis framework that, for\nthe first time, incorporates the Easy Consistency Tuning (ECT) strategy into\nspeech synthesis. By progressively tightening consistency constraints on a\npre-trained diffusion model, ECTSpeech achieves high-quality one-step\ngeneration while significantly reducing training complexity. In addition, we\ndesign a multi-scale gate module (MSGate) to enhance the denoiser's ability to\nfuse features at different scales. Experimental results on the LJSpeech dataset\ndemonstrate that ECTSpeech achieves audio quality comparable to\nstate-of-the-art methods under single-step sampling, while substantially\nreducing the model's training cost and complexity."
                },
                "authors": [
                    {
                        "name": "Tao Zhu"
                    },
                    {
                        "name": "Yinfeng Yu"
                    },
                    {
                        "name": "Liejun Wang"
                    },
                    {
                        "name": "Fuchun Sun"
                    },
                    {
                        "name": "Wendong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Wendong Zheng"
                },
                "author": "Wendong Zheng",
                "arxiv_comment": "Accepted for publication by Proceedings of the 2025 ACM Multimedia\n  Asia Conference(MMAsia '25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05982v1",
                "updated": "2025-10-07T14:42:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    42,
                    47,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T14:42:47Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    42,
                    47,
                    1,
                    280,
                    0
                ],
                "title": "Dormant BH candidates from Gaia DR3 summary diagnostics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dormant BH candidates from Gaia DR3 summary diagnostics"
                },
                "summary": "We present a rigorous identification of candidates for dormant black holes\n(BHs) and neutron stars (NSs) in binaries using summary statistics from Gaia\nDR3, rather than full orbital solutions. Although Gaia astrometric orbits have\nalready revealed a small sample of compact object binaries, many systems remain\nundetected due to stringent quality cuts imposed on the published orbits. Using\na forward-modelling framework that simulates Gaia observables, in particular\nthe renormalised unit weight error (ruwe) and radial velocity (RV) scatter, we\ninfer posterior distributions for companion mass and orbital period via MCMC\nsampling, marginalising over nuisance orbital parameters. We validate our\napproach by comparing the predicted masses and periods against full orbit\nsolutions from DR3, and by successfully recovering known compact object\nbinaries as promising candidates. The method is best suited for systems with\nred giant primaries, which have more reliable Gaia RV scatter and a light\ncentroid more likely dominated by one component, compared to main-sequence\nstars. And they are less likely to be triples with short-period inner binaries,\nwhich produce confounding signatures. We apply the method to three million\ngiants and identify 556 systems with best-fit companion masses $\\gtrsim\n3\\,M_\\odot$. Recovery simulations suggest our selection method is substantially\nmore sensitive than the DR3 non-single-star catalogue, particularly for\nbinaries with periods below 1 year and above $\\sim 6$ years. These candidates\nrepresent promising targets for spectroscopic follow-up and Gaia DR4 analysis\nto confirm the presence of compact objects. Candidate main-sequence stars with\nmassive companions face a larger set of confounding effects. Therefore, we\npresent an analogous catalogue of 279 additional `main sequence' candidates\nonly as an appendix.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a rigorous identification of candidates for dormant black holes\n(BHs) and neutron stars (NSs) in binaries using summary statistics from Gaia\nDR3, rather than full orbital solutions. Although Gaia astrometric orbits have\nalready revealed a small sample of compact object binaries, many systems remain\nundetected due to stringent quality cuts imposed on the published orbits. Using\na forward-modelling framework that simulates Gaia observables, in particular\nthe renormalised unit weight error (ruwe) and radial velocity (RV) scatter, we\ninfer posterior distributions for companion mass and orbital period via MCMC\nsampling, marginalising over nuisance orbital parameters. We validate our\napproach by comparing the predicted masses and periods against full orbit\nsolutions from DR3, and by successfully recovering known compact object\nbinaries as promising candidates. The method is best suited for systems with\nred giant primaries, which have more reliable Gaia RV scatter and a light\ncentroid more likely dominated by one component, compared to main-sequence\nstars. And they are less likely to be triples with short-period inner binaries,\nwhich produce confounding signatures. We apply the method to three million\ngiants and identify 556 systems with best-fit companion masses $\\gtrsim\n3\\,M_\\odot$. Recovery simulations suggest our selection method is substantially\nmore sensitive than the DR3 non-single-star catalogue, particularly for\nbinaries with periods below 1 year and above $\\sim 6$ years. These candidates\nrepresent promising targets for spectroscopic follow-up and Gaia DR4 analysis\nto confirm the presence of compact objects. Candidate main-sequence stars with\nmassive companions face a larger set of confounding effects. Therefore, we\npresent an analogous catalogue of 279 additional `main sequence' candidates\nonly as an appendix."
                },
                "authors": [
                    {
                        "name": "Johanna Müller-Horn"
                    },
                    {
                        "name": "Hans-Walter Rix"
                    },
                    {
                        "name": "Kareem El-Badry"
                    },
                    {
                        "name": "Ben Pennell"
                    },
                    {
                        "name": "Matthew Green"
                    },
                    {
                        "name": "Jiadong Li"
                    },
                    {
                        "name": "Rhys Seeburger"
                    }
                ],
                "author_detail": {
                    "name": "Rhys Seeburger"
                },
                "author": "Rhys Seeburger",
                "arxiv_comment": "17 pages, 15 figures, submitted to A&A, comments welcome, catalogues\n  are available here: https://zenodo.org/records/17271786",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05976v1",
                "updated": "2025-10-07T14:30:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    30,
                    36,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T14:30:36Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    30,
                    36,
                    1,
                    280,
                    0
                ],
                "title": "Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective\n  Taxonomy and Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective\n  Taxonomy and Performance Analysis"
                },
                "summary": "Low-light image enhancement (LLIE) is vital for safety-critical applications\nsuch as surveillance, autonomous navigation, and medical imaging, where\nvisibility degradation can impair downstream task performance. Recently,\ndiffusion models have emerged as a promising generative paradigm for LLIE due\nto their capacity to model complex image distributions via iterative denoising.\nThis survey provides an up-to-date critical analysis of diffusion models for\nLLIE, distinctively featuring an in-depth comparative performance evaluation\nagainst Generative Adversarial Network and Transformer-based state-of-the-art\nmethods, a thorough examination of practical deployment challenges, and a\nforward-looking perspective on the role of emerging paradigms like foundation\nmodels. We propose a multi-perspective taxonomy encompassing six categories:\nIntrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal,\nand Autonomous; that map enhancement methods across physical priors,\nconditioning schemes, and computational efficiency. Our taxonomy is grounded in\na hybrid view of both the model mechanism and the conditioning signals. We\nevaluate qualitative failure modes, benchmark inconsistencies, and trade-offs\nbetween interpretability, generalization, and inference efficiency. We also\ndiscuss real-world deployment constraints (e.g., memory, energy use) and\nethical considerations. This survey aims to guide the next generation of\ndiffusion-based LLIE research by highlighting trends and surfacing open\nresearch questions, including novel conditioning, real-time adaptation, and the\npotential of foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-light image enhancement (LLIE) is vital for safety-critical applications\nsuch as surveillance, autonomous navigation, and medical imaging, where\nvisibility degradation can impair downstream task performance. Recently,\ndiffusion models have emerged as a promising generative paradigm for LLIE due\nto their capacity to model complex image distributions via iterative denoising.\nThis survey provides an up-to-date critical analysis of diffusion models for\nLLIE, distinctively featuring an in-depth comparative performance evaluation\nagainst Generative Adversarial Network and Transformer-based state-of-the-art\nmethods, a thorough examination of practical deployment challenges, and a\nforward-looking perspective on the role of emerging paradigms like foundation\nmodels. We propose a multi-perspective taxonomy encompassing six categories:\nIntrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal,\nand Autonomous; that map enhancement methods across physical priors,\nconditioning schemes, and computational efficiency. Our taxonomy is grounded in\na hybrid view of both the model mechanism and the conditioning signals. We\nevaluate qualitative failure modes, benchmark inconsistencies, and trade-offs\nbetween interpretability, generalization, and inference efficiency. We also\ndiscuss real-world deployment constraints (e.g., memory, energy use) and\nethical considerations. This survey aims to guide the next generation of\ndiffusion-based LLIE research by highlighting trends and surfacing open\nresearch questions, including novel conditioning, real-time adaptation, and the\npotential of foundation models."
                },
                "authors": [
                    {
                        "name": "Eashan Adhikarla"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Brian D. Davison"
                    }
                ],
                "author_detail": {
                    "name": "Brian D. Davison"
                },
                "author": "Brian D. Davison",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05972v1",
                "updated": "2025-10-07T14:28:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    28,
                    30,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T14:28:30Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    28,
                    30,
                    1,
                    280,
                    0
                ],
                "title": "LexiCon: a Benchmark for Planning under Temporal Constraints in Natural\n  Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LexiCon: a Benchmark for Planning under Temporal Constraints in Natural\n  Language"
                },
                "summary": "Owing to their reasoning capabilities, large language models (LLMs) have been\nevaluated on planning tasks described in natural language. However, LLMs have\nlargely been tested on planning domains without constraints. In order to deploy\nthem in real-world settings where adherence to constraints, in particular\nsafety constraints, is critical, we need to evaluate their performance on\nconstrained planning tasks. We introduce LexiCon -- a natural language-based\n(Lexi) constrained (Con) planning benchmark, consisting of a suite of\nenvironments, that can be used to evaluate the planning capabilities of LLMs in\na principled fashion. The core idea behind LexiCon is to take existing planning\nenvironments and impose temporal constraints on the states. These constrained\nproblems are then translated into natural language and given to an LLM to\nsolve. A key feature of LexiCon is its extensibility. That is, the set of\nsupported environments can be extended with new (unconstrained) environment\ngenerators, for which temporal constraints are constructed automatically. This\nrenders LexiCon future-proof: the hardness of the generated planning problems\ncan be increased as the planning capabilities of LLMs improve. Our experiments\nreveal that the performance of state-of-the-art LLMs, including reasoning\nmodels like GPT-5, o3, and R1, deteriorates as the degree of constrainedness of\nthe planning tasks increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Owing to their reasoning capabilities, large language models (LLMs) have been\nevaluated on planning tasks described in natural language. However, LLMs have\nlargely been tested on planning domains without constraints. In order to deploy\nthem in real-world settings where adherence to constraints, in particular\nsafety constraints, is critical, we need to evaluate their performance on\nconstrained planning tasks. We introduce LexiCon -- a natural language-based\n(Lexi) constrained (Con) planning benchmark, consisting of a suite of\nenvironments, that can be used to evaluate the planning capabilities of LLMs in\na principled fashion. The core idea behind LexiCon is to take existing planning\nenvironments and impose temporal constraints on the states. These constrained\nproblems are then translated into natural language and given to an LLM to\nsolve. A key feature of LexiCon is its extensibility. That is, the set of\nsupported environments can be extended with new (unconstrained) environment\ngenerators, for which temporal constraints are constructed automatically. This\nrenders LexiCon future-proof: the hardness of the generated planning problems\ncan be increased as the planning capabilities of LLMs improve. Our experiments\nreveal that the performance of state-of-the-art LLMs, including reasoning\nmodels like GPT-5, o3, and R1, deteriorates as the degree of constrainedness of\nthe planning tasks increases."
                },
                "authors": [
                    {
                        "name": "Periklis Mantenoglou"
                    },
                    {
                        "name": "Rishi Hazra"
                    },
                    {
                        "name": "Pedro Zuidberg Dos Martires"
                    },
                    {
                        "name": "Luc De Raedt"
                    }
                ],
                "author_detail": {
                    "name": "Luc De Raedt"
                },
                "author": "Luc De Raedt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05969v1",
                "updated": "2025-10-07T14:24:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    24,
                    32,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T14:24:32Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    24,
                    32,
                    1,
                    280,
                    0
                ],
                "title": "Probing the Difficulty Perception Mechanism of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Difficulty Perception Mechanism of Large Language Models"
                },
                "summary": "Large language models (LLMs) are increasingly deployed on complex reasoning\ntasks, yet little is known about their ability to internally evaluate problem\ndifficulty, which is an essential capability for adaptive reasoning and\nefficient resource allocation. In this work, we investigate whether LLMs\nimplicitly encode problem difficulty in their internal representations. Using a\nlinear probe on the final-token representations of LLMs, we demonstrate that\nthe difficulty level of math problems can be linearly modeled. We further\nlocate the specific attention heads of the final Transformer layer: these\nattention heads have opposite activation patterns for simple and difficult\nproblems, thus achieving perception of difficulty. Our ablation experiments\nprove the accuracy of the location. Crucially, our experiments provide\npractical support for using LLMs as automatic difficulty annotators,\npotentially substantially reducing reliance on costly human labeling in\nbenchmark construction and curriculum learning. We also uncover that there is a\nsignificant difference in entropy and difficulty perception at the token level.\nOur study reveals that difficulty perception in LLMs is not only present but\nalso structurally organized, offering new theoretical insights and practical\ndirections for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed on complex reasoning\ntasks, yet little is known about their ability to internally evaluate problem\ndifficulty, which is an essential capability for adaptive reasoning and\nefficient resource allocation. In this work, we investigate whether LLMs\nimplicitly encode problem difficulty in their internal representations. Using a\nlinear probe on the final-token representations of LLMs, we demonstrate that\nthe difficulty level of math problems can be linearly modeled. We further\nlocate the specific attention heads of the final Transformer layer: these\nattention heads have opposite activation patterns for simple and difficult\nproblems, thus achieving perception of difficulty. Our ablation experiments\nprove the accuracy of the location. Crucially, our experiments provide\npractical support for using LLMs as automatic difficulty annotators,\npotentially substantially reducing reliance on costly human labeling in\nbenchmark construction and curriculum learning. We also uncover that there is a\nsignificant difference in entropy and difficulty perception at the token level.\nOur study reveals that difficulty perception in LLMs is not only present but\nalso structurally organized, offering new theoretical insights and practical\ndirections for future research."
                },
                "authors": [
                    {
                        "name": "Sunbowen Lee"
                    },
                    {
                        "name": "Qingyu Yin"
                    },
                    {
                        "name": "Chak Tou Leong"
                    },
                    {
                        "name": "Jialiang Zhang"
                    },
                    {
                        "name": "Yicheng Gong"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05968v1",
                "updated": "2025-10-07T14:23:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    23,
                    24,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T14:23:24Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    23,
                    24,
                    1,
                    280,
                    0
                ],
                "title": "Extending ResourceLink: Patterns for Large Dataset Processing in MCP\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending ResourceLink: Patterns for Large Dataset Processing in MCP\n  Applications"
                },
                "summary": "Large language models translate natural language into database queries, yet\ncontext window limitations prevent direct deployment in reporting systems where\ncomplete datasets exhaust available tokens. The Model Context Protocol\nspecification defines ResourceLink for referencing external resources, but\npractical patterns for implementing scalable reporting architectures remain\nundocumented. This paper presents patterns for building LLM-powered reporting\nsystems that decouple query generation from data retrieval. We introduce a\ndual-response pattern extending ResourceLink to support both iterative query\nrefinement and out-of-band data access, accompanied by patterns for\nmulti-tenant security and resource lifecycle management. These patterns address\nfundamental challenges in LLM-driven reporting applications and provide\npractical guidance for developers building them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models translate natural language into database queries, yet\ncontext window limitations prevent direct deployment in reporting systems where\ncomplete datasets exhaust available tokens. The Model Context Protocol\nspecification defines ResourceLink for referencing external resources, but\npractical patterns for implementing scalable reporting architectures remain\nundocumented. This paper presents patterns for building LLM-powered reporting\nsystems that decouple query generation from data retrieval. We introduce a\ndual-response pattern extending ResourceLink to support both iterative query\nrefinement and out-of-band data access, accompanied by patterns for\nmulti-tenant security and resource lifecycle management. These patterns address\nfundamental challenges in LLM-driven reporting applications and provide\npractical guidance for developers building them."
                },
                "authors": [
                    {
                        "name": "Scott Frees"
                    }
                ],
                "author_detail": {
                    "name": "Scott Frees"
                },
                "author": "Scott Frees",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05959v1",
                "updated": "2025-10-07T14:16:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    16,
                    59,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T14:16:59Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    16,
                    59,
                    1,
                    280,
                    0
                ],
                "title": "Distributed Platoon Control Under Quantization: Stability Analysis and\n  Privacy Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Platoon Control Under Quantization: Stability Analysis and\n  Privacy Preservation"
                },
                "summary": "Distributed control of connected and automated vehicles has attracted\nconsiderable interest for its potential to improve traffic efficiency and\nsafety. However, such control schemes require sharing privacy-sensitive vehicle\ndata, which introduces risks of information leakage and potential malicious\nactivities. This paper investigates the stability and privacy-preserving\nproperties of distributed platoon control under two types of quantizers:\ndeterministic and probabilistic. For deterministic quantization, we show that\nthe resulting control strategy ensures the system errors remain uniformly\nultimately bounded. Moreover, in the absence of auxiliary information, an\neavesdropper cannot uniquely infer sensitive vehicle states. In contrast, the\nuse of probabilistic quantization enables asymptotic convergence of the vehicle\nplatoon in expectation with bounded variance. Importantly, probabilistic\nquantizers can satisfy differential privacy guarantees, thereby preserving\nprivacy even when the eavesdropper possesses arbitrary auxiliary information.\nWe further analyze the trade-off between control performance and privacy by\nformulating an optimization problem that characterizes the impact of the\nquantization step on both metrics. Numerical simulations are provided to\nillustrate the performance differences between the two quantization strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed control of connected and automated vehicles has attracted\nconsiderable interest for its potential to improve traffic efficiency and\nsafety. However, such control schemes require sharing privacy-sensitive vehicle\ndata, which introduces risks of information leakage and potential malicious\nactivities. This paper investigates the stability and privacy-preserving\nproperties of distributed platoon control under two types of quantizers:\ndeterministic and probabilistic. For deterministic quantization, we show that\nthe resulting control strategy ensures the system errors remain uniformly\nultimately bounded. Moreover, in the absence of auxiliary information, an\neavesdropper cannot uniquely infer sensitive vehicle states. In contrast, the\nuse of probabilistic quantization enables asymptotic convergence of the vehicle\nplatoon in expectation with bounded variance. Importantly, probabilistic\nquantizers can satisfy differential privacy guarantees, thereby preserving\nprivacy even when the eavesdropper possesses arbitrary auxiliary information.\nWe further analyze the trade-off between control performance and privacy by\nformulating an optimization problem that characterizes the impact of the\nquantization step on both metrics. Numerical simulations are provided to\nillustrate the performance differences between the two quantization strategies."
                },
                "authors": [
                    {
                        "name": "Kaixiang Zhang"
                    },
                    {
                        "name": "Zhaojian Li"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05957v1",
                "updated": "2025-10-07T14:14:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    14,
                    7,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T14:14:07Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    14,
                    7,
                    1,
                    280,
                    0
                ],
                "title": "Learning to Crawl: Latent Model-Based Reinforcement Learning for Soft\n  Robotic Adaptive Locomotion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Crawl: Latent Model-Based Reinforcement Learning for Soft\n  Robotic Adaptive Locomotion"
                },
                "summary": "Soft robotic crawlers are mobile robots that utilize soft body deformability\nand compliance to achieve locomotion through surface contact. Designing control\nstrategies for such systems is challenging due to model inaccuracies, sensor\nnoise, and the need to discover locomotor gaits. In this work, we present a\nmodel-based reinforcement learning (MB-RL) framework in which latent dynamics\ninferred from onboard sensors serve as a predictive model that guides an\nactor-critic algorithm to optimize locomotor policies. We evaluate the\nframework on a minimal crawler model in simulation using inertial measurement\nunits and time-of-flight sensors as observations. The learned latent dynamics\nenable short-horizon motion prediction while the actor-critic discovers\neffective locomotor policies. This approach highlights the potential of\nlatent-dynamics MB-RL for enabling embodied soft robotic adaptive locomotion\nbased solely on noisy sensor feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft robotic crawlers are mobile robots that utilize soft body deformability\nand compliance to achieve locomotion through surface contact. Designing control\nstrategies for such systems is challenging due to model inaccuracies, sensor\nnoise, and the need to discover locomotor gaits. In this work, we present a\nmodel-based reinforcement learning (MB-RL) framework in which latent dynamics\ninferred from onboard sensors serve as a predictive model that guides an\nactor-critic algorithm to optimize locomotor policies. We evaluate the\nframework on a minimal crawler model in simulation using inertial measurement\nunits and time-of-flight sensors as observations. The learned latent dynamics\nenable short-horizon motion prediction while the actor-critic discovers\neffective locomotor policies. This approach highlights the potential of\nlatent-dynamics MB-RL for enabling embodied soft robotic adaptive locomotion\nbased solely on noisy sensor feedback."
                },
                "authors": [
                    {
                        "name": "Vaughn Gzenda"
                    },
                    {
                        "name": "Robin Chhabra"
                    }
                ],
                "author_detail": {
                    "name": "Robin Chhabra"
                },
                "author": "Robin Chhabra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05950v1",
                "updated": "2025-10-07T14:07:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    7,
                    43,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T14:07:43Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    7,
                    43,
                    1,
                    280,
                    0
                ],
                "title": "Training-Free Time Series Classification via In-Context Reasoning with\n  LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Time Series Classification via In-Context Reasoning with\n  LLM Agents"
                },
                "summary": "Time series classification (TSC) spans diverse application scenarios, yet\nlabeled data are often scarce, making task-specific training costly and\ninflexible. Recent reasoning-oriented large language models (LLMs) show promise\nin understanding temporal patterns, but purely zero-shot usage remains\nsuboptimal. We propose FETA, a multi-agent framework for training-free TSC via\nexemplar-based in-context reasoning. FETA decomposes a multivariate series into\nchannel-wise subproblems, retrieves a few structurally similar labeled examples\nfor each channel, and leverages a reasoning LLM to compare the query against\nthese exemplars, producing channel-level labels with self-assessed confidences;\na confidence-weighted aggregator then fuses all channel decisions. This design\neliminates the need for pretraining or fine-tuning, improves efficiency by\npruning irrelevant channels and controlling input length, and enhances\ninterpretability through exemplar grounding and confidence estimation. On nine\nchallenging UEA datasets, FETA achieves strong accuracy under a fully\ntraining-free setting, surpassing multiple trained baselines. These results\ndemonstrate that a multi-agent in-context reasoning framework can transform\nLLMs into competitive, plug-and-play TSC solvers without any parameter\ntraining. The code is available at https://github.com/SongyuanSui/FETATSC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series classification (TSC) spans diverse application scenarios, yet\nlabeled data are often scarce, making task-specific training costly and\ninflexible. Recent reasoning-oriented large language models (LLMs) show promise\nin understanding temporal patterns, but purely zero-shot usage remains\nsuboptimal. We propose FETA, a multi-agent framework for training-free TSC via\nexemplar-based in-context reasoning. FETA decomposes a multivariate series into\nchannel-wise subproblems, retrieves a few structurally similar labeled examples\nfor each channel, and leverages a reasoning LLM to compare the query against\nthese exemplars, producing channel-level labels with self-assessed confidences;\na confidence-weighted aggregator then fuses all channel decisions. This design\neliminates the need for pretraining or fine-tuning, improves efficiency by\npruning irrelevant channels and controlling input length, and enhances\ninterpretability through exemplar grounding and confidence estimation. On nine\nchallenging UEA datasets, FETA achieves strong accuracy under a fully\ntraining-free setting, surpassing multiple trained baselines. These results\ndemonstrate that a multi-agent in-context reasoning framework can transform\nLLMs into competitive, plug-and-play TSC solvers without any parameter\ntraining. The code is available at https://github.com/SongyuanSui/FETATSC."
                },
                "authors": [
                    {
                        "name": "Songyuan Sui"
                    },
                    {
                        "name": "Zihang Xu"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Kwei-Herng Lai"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_comment": "8 pages main content, 12 pages total including appendix, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14763v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14763v2",
                "updated": "2025-10-07T13:59:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    59,
                    20,
                    1,
                    280,
                    0
                ],
                "published": "2025-05-20T17:25:23Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    25,
                    23,
                    1,
                    140,
                    0
                ],
                "title": "Unifying Inference-Time Planning Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Inference-Time Planning Language Generation"
                },
                "summary": "A line of work in planning uses LLM not to generate a plan, but to generate a\nformal representation in some planning language, which can be input into a\nsymbolic solver to deterministically find a plan. While showing improved trust\nand promising performance, dozens of recent publications have proposed\nscattered methods on a variety of benchmarks under different experimental\nsettings. We attempt to unify the inference-time LLM-as-formalizer methodology\nfor classical planning by proposing a unifying framework based on intermediate\nrepresentations. We thus systematically evaluate more than a dozen pipelines\nthat subsume most existing work, while proposing novel ones that involve\nsyntactically similar but high resource intermediate languages (such as a\nPython wrapper of PDDL). We provide recipes for planning language generation\npipelines, draw a series of conclusions showing the efficacy of their various\ncomponents, and evidence their robustness against problem complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A line of work in planning uses LLM not to generate a plan, but to generate a\nformal representation in some planning language, which can be input into a\nsymbolic solver to deterministically find a plan. While showing improved trust\nand promising performance, dozens of recent publications have proposed\nscattered methods on a variety of benchmarks under different experimental\nsettings. We attempt to unify the inference-time LLM-as-formalizer methodology\nfor classical planning by proposing a unifying framework based on intermediate\nrepresentations. We thus systematically evaluate more than a dozen pipelines\nthat subsume most existing work, while proposing novel ones that involve\nsyntactically similar but high resource intermediate languages (such as a\nPython wrapper of PDDL). We provide recipes for planning language generation\npipelines, draw a series of conclusions showing the efficacy of their various\ncomponents, and evidence their robustness against problem complexity."
                },
                "authors": [
                    {
                        "name": "Prabhu Prakash Kagitha"
                    },
                    {
                        "name": "Bo Sun"
                    },
                    {
                        "name": "Ishan Desai"
                    },
                    {
                        "name": "Andrew Zhu"
                    },
                    {
                        "name": "Cassie Huang"
                    },
                    {
                        "name": "Manling Li"
                    },
                    {
                        "name": "Ziyang Li"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14763v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14763v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15165v2",
                "updated": "2025-10-07T13:55:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    55,
                    24,
                    1,
                    280,
                    0
                ],
                "published": "2025-09-18T17:16:58Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    16,
                    58,
                    3,
                    261,
                    0
                ],
                "title": "Invariant Modeling for Joint Distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invariant Modeling for Joint Distributions"
                },
                "summary": "A common theme underlying many problems in statistics and economics involves\nthe determination of a systematic method of selecting a joint distribution\nconsistent with a specified list of categorical marginals, some of which have\nan ordinal structure. We propose guidance in narrowing down the set of possible\nmethods by introducing Invariant Aggregation (IA), a natural property that\nrequires merging adjacent categories in one marginal not to alter the joint\ndistribution over unaffected values. We prove that a model satisfies IA if and\nonly if it is a copula model. This characterization ensures i) robustness\nagainst data manipulation and survey design, and ii) allows seamless\nincorporation of new variables. Our results provide both theoretical clarity\nand practical safeguards for inference under marginal constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common theme underlying many problems in statistics and economics involves\nthe determination of a systematic method of selecting a joint distribution\nconsistent with a specified list of categorical marginals, some of which have\nan ordinal structure. We propose guidance in narrowing down the set of possible\nmethods by introducing Invariant Aggregation (IA), a natural property that\nrequires merging adjacent categories in one marginal not to alter the joint\ndistribution over unaffected values. We prove that a model satisfies IA if and\nonly if it is a copula model. This characterization ensures i) robustness\nagainst data manipulation and survey design, and ii) allows seamless\nincorporation of new variables. Our results provide both theoretical clarity\nand practical safeguards for inference under marginal constraints."
                },
                "authors": [
                    {
                        "name": "Christopher P. Chambers"
                    },
                    {
                        "name": "Yusufcan Masatlioglu"
                    },
                    {
                        "name": "Ruodu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ruodu Wang"
                },
                "author": "Ruodu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.TH",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05946v1",
                "updated": "2025-10-07T13:55:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    55,
                    6,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T13:55:06Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    55,
                    6,
                    1,
                    280,
                    0
                ],
                "title": "N-Parties Private Structure and Parameter Learning for Sum-Product\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N-Parties Private Structure and Parameter Learning for Sum-Product\n  Networks"
                },
                "summary": "A sum-product network (SPN) is a graphical model that allows several types of\nprobabilistic inference to be performed efficiently. In this paper, we propose\na privacy-preserving protocol which tackles structure generation and parameter\nlearning of SPNs. Additionally, we provide a protocol for private inference on\nSPNs, subsequent to training. To preserve the privacy of the participants, we\nderive our protocol based on secret sharing, which guarantees privacy in the\nhonest-but-curious setting even when at most half of the parties cooperate to\ndisclose the data. The protocol makes use of a forest of randomly generated\nSPNs, which is trained and weighted privately and can then be used for private\ninference on data points. Our experiments indicate that preserving the privacy\nof all participants does not decrease log-likelihood performance on both\nhomogeneously and heterogeneously partitioned data. We furthermore show that\nour protocol's performance is comparable to current state-of-the-art SPN\nlearners in homogeneously partitioned data settings. In terms of runtime and\nmemory usage, we demonstrate that our implementation scales well when\nincreasing the number of parties, comparing favorably to protocols for neural\nnetworks, when they are trained to reproduce the input-output behavior of SPNs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A sum-product network (SPN) is a graphical model that allows several types of\nprobabilistic inference to be performed efficiently. In this paper, we propose\na privacy-preserving protocol which tackles structure generation and parameter\nlearning of SPNs. Additionally, we provide a protocol for private inference on\nSPNs, subsequent to training. To preserve the privacy of the participants, we\nderive our protocol based on secret sharing, which guarantees privacy in the\nhonest-but-curious setting even when at most half of the parties cooperate to\ndisclose the data. The protocol makes use of a forest of randomly generated\nSPNs, which is trained and weighted privately and can then be used for private\ninference on data points. Our experiments indicate that preserving the privacy\nof all participants does not decrease log-likelihood performance on both\nhomogeneously and heterogeneously partitioned data. We furthermore show that\nour protocol's performance is comparable to current state-of-the-art SPN\nlearners in homogeneously partitioned data settings. In terms of runtime and\nmemory usage, we demonstrate that our implementation scales well when\nincreasing the number of parties, comparing favorably to protocols for neural\nnetworks, when they are trained to reproduce the input-output behavior of SPNs."
                },
                "authors": [
                    {
                        "name": "Xenia Heilmann"
                    },
                    {
                        "name": "Ernst Althaus"
                    },
                    {
                        "name": "Mattia Cerrato"
                    },
                    {
                        "name": "Nick Johannes Peter Rassau"
                    },
                    {
                        "name": "Mohammad Sadeq Dousti"
                    },
                    {
                        "name": "Stefan Kramer"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Kramer"
                },
                "author": "Stefan Kramer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05943v1",
                "updated": "2025-10-07T13:52:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    52,
                    51,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T13:52:51Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    52,
                    51,
                    1,
                    280,
                    0
                ],
                "title": "EARL: Efficient Agentic Reinforcement Learning Systems for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EARL: Efficient Agentic Reinforcement Learning Systems for Large\n  Language Models"
                },
                "summary": "Reinforcement learning (RL) has become a pivotal component of large language\nmodel (LLM) post-training, and agentic RL extends this paradigm to operate as\nagents through multi-turn interaction and tool use. Scaling such systems\nexposes two practical bottlenecks: (1) context length grows rapidly during\ntraining, inflating memory usage and latency, and triggering out-of-memory\n(OOM) failures; and (2) intermediate tensors accumulate with context length,\nmaking cross-device data movement a major system bottleneck.\n  We present EARL, a scalable system for efficient agentic RL. EARL designs a\nparallelism selector that dynamically adapts model and training parallelism\nacross RL stages based on sequence length and system load, and a data\ndispatcher that performs layout-aware, decentralized exchange of intermediate\ndata batches. Together, these components increase throughput, reduce\nlong-context failures, and enable stable large-scale training of agentic LLMs\nwithout relying on hard limits or penalties of context length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a pivotal component of large language\nmodel (LLM) post-training, and agentic RL extends this paradigm to operate as\nagents through multi-turn interaction and tool use. Scaling such systems\nexposes two practical bottlenecks: (1) context length grows rapidly during\ntraining, inflating memory usage and latency, and triggering out-of-memory\n(OOM) failures; and (2) intermediate tensors accumulate with context length,\nmaking cross-device data movement a major system bottleneck.\n  We present EARL, a scalable system for efficient agentic RL. EARL designs a\nparallelism selector that dynamically adapts model and training parallelism\nacross RL stages based on sequence length and system load, and a data\ndispatcher that performs layout-aware, decentralized exchange of intermediate\ndata batches. Together, these components increase throughput, reduce\nlong-context failures, and enable stable large-scale training of agentic LLMs\nwithout relying on hard limits or penalties of context length."
                },
                "authors": [
                    {
                        "name": "Zheyue Tan"
                    },
                    {
                        "name": "Mustapha Abdullahi"
                    },
                    {
                        "name": "Tuo Shi"
                    },
                    {
                        "name": "Huining Yuan"
                    },
                    {
                        "name": "Zelai Xu"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Boxun Li"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05942v2",
                "updated": "2025-10-08T08:03:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    8,
                    3,
                    38,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-07T13:52:16Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    52,
                    16,
                    1,
                    280,
                    0
                ],
                "title": "EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation\n  for Moral Alignment in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation\n  for Moral Alignment in Large Language Models"
                },
                "summary": "We present EvalMORAAL, a transparent chain-of-thought (CoT) framework that\nuses two scoring methods (log-probabilities and direct ratings) plus a\nmodel-as-judge peer review to evaluate moral alignment in 20 large language\nmodels. We assess models on the World Values Survey (55 countries, 19 topics)\nand the PEW Global Attitudes Survey (39 countries, 8 topics). With EvalMORAAL,\ntop models align closely with survey responses (Pearson's r approximately 0.90\non WVS). Yet we find a clear regional difference: Western regions average\nr=0.82 while non-Western regions average r=0.61 (a 0.21 absolute gap),\nindicating consistent regional bias. Our framework adds three parts: (1) two\nscoring methods for all models to enable fair comparison, (2) a structured\nchain-of-thought protocol with self-consistency checks, and (3) a\nmodel-as-judge peer review that flags 348 conflicts using a data-driven\nthreshold. Peer agreement relates to survey alignment (WVS r=0.74, PEW r=0.39,\nboth p<.001), supporting automated quality checks. These results show real\nprogress toward culture-aware AI while highlighting open challenges for use\nacross regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present EvalMORAAL, a transparent chain-of-thought (CoT) framework that\nuses two scoring methods (log-probabilities and direct ratings) plus a\nmodel-as-judge peer review to evaluate moral alignment in 20 large language\nmodels. We assess models on the World Values Survey (55 countries, 19 topics)\nand the PEW Global Attitudes Survey (39 countries, 8 topics). With EvalMORAAL,\ntop models align closely with survey responses (Pearson's r approximately 0.90\non WVS). Yet we find a clear regional difference: Western regions average\nr=0.82 while non-Western regions average r=0.61 (a 0.21 absolute gap),\nindicating consistent regional bias. Our framework adds three parts: (1) two\nscoring methods for all models to enable fair comparison, (2) a structured\nchain-of-thought protocol with self-consistency checks, and (3) a\nmodel-as-judge peer review that flags 348 conflicts using a data-driven\nthreshold. Peer agreement relates to survey alignment (WVS r=0.74, PEW r=0.39,\nboth p<.001), supporting automated quality checks. These results show real\nprogress toward culture-aware AI while highlighting open challenges for use\nacross regions."
                },
                "authors": [
                    {
                        "name": "Hadi Mohammadi"
                    },
                    {
                        "name": "Anastasia Giachanou"
                    },
                    {
                        "name": "Ayoub Bagheri"
                    }
                ],
                "author_detail": {
                    "name": "Ayoub Bagheri"
                },
                "author": "Ayoub Bagheri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09946v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09946v3",
                "updated": "2025-10-07T13:51:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    51,
                    9,
                    1,
                    280,
                    0
                ],
                "published": "2024-08-19T12:35:23Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    35,
                    23,
                    0,
                    232,
                    0
                ],
                "title": "Fine-Grained and Thematic Evaluation of LLMs in Social Deduction Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained and Thematic Evaluation of LLMs in Social Deduction Game"
                },
                "summary": "Recent studies have investigated whether large language models (LLMs) can\nsupport obscured communication, which is characterized by core aspects such as\ninferring subtext and evading suspicions. To conduct the investigation,\nresearchers have used social deduction games (SDGs) as their experimental\nenvironment, in which players conceal and infer specific information. However,\nprior work has often overlooked how LLMs should be evaluated in such settings.\nSpecifically, we point out two limitations with the evaluation methods they\nemployed. First, metrics used in prior studies are coarse-grained as they are\nbased on overall game outcomes that often fail to capture event-level\nbehaviors; Second, error analyses have lacked structured methodologies capable\nof producing insights that meaningfully support evaluation outcomes. To address\nthese limitations, we propose a microscopic and systematic approach to the\ninvestigation. Specifically, we introduce six fine-grained metrics that resolve\nthe first issue. To tackle the second issue, we conducted a thematic analysis\nand identified four major reasoning failures that undermine LLMs' performance\nin obscured communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have investigated whether large language models (LLMs) can\nsupport obscured communication, which is characterized by core aspects such as\ninferring subtext and evading suspicions. To conduct the investigation,\nresearchers have used social deduction games (SDGs) as their experimental\nenvironment, in which players conceal and infer specific information. However,\nprior work has often overlooked how LLMs should be evaluated in such settings.\nSpecifically, we point out two limitations with the evaluation methods they\nemployed. First, metrics used in prior studies are coarse-grained as they are\nbased on overall game outcomes that often fail to capture event-level\nbehaviors; Second, error analyses have lacked structured methodologies capable\nof producing insights that meaningfully support evaluation outcomes. To address\nthese limitations, we propose a microscopic and systematic approach to the\ninvestigation. Specifically, we introduce six fine-grained metrics that resolve\nthe first issue. To tackle the second issue, we conducted a thematic analysis\nand identified four major reasoning failures that undermine LLMs' performance\nin obscured communication."
                },
                "authors": [
                    {
                        "name": "Byungjun Kim"
                    },
                    {
                        "name": "Dayeon Seo"
                    },
                    {
                        "name": "Minju Kim"
                    },
                    {
                        "name": "Bugeun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Bugeun Kim"
                },
                "author": "Bugeun Kim",
                "arxiv_doi": "10.1109/ACCESS.2025.3611399",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3611399",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.09946v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09946v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in IEEE Access",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05935v1",
                "updated": "2025-10-07T13:46:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    46,
                    6,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T13:46:06Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    46,
                    6,
                    1,
                    280,
                    0
                ],
                "title": "LLM-FS-Agent: A Deliberative Role-based Large Language Model\n  Architecture for Transparent Feature Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-FS-Agent: A Deliberative Role-based Large Language Model\n  Architecture for Transparent Feature Selection"
                },
                "summary": "High-dimensional data remains a pervasive challenge in machine learning,\noften undermining model interpretability and computational efficiency. While\nLarge Language Models (LLMs) have shown promise for dimensionality reduction\nthrough feature selection, existing LLM-based approaches frequently lack\nstructured reasoning and transparent justification for their decisions. This\npaper introduces LLM-FS-Agent, a novel multi-agent architecture designed for\ninterpretable and robust feature selection. The system orchestrates a\ndeliberative \"debate\" among multiple LLM agents, each assigned a specific role,\nenabling collective evaluation of feature relevance and generation of detailed\njustifications. We evaluate LLM-FS-Agent in the cybersecurity domain using the\nCIC-DIAD 2024 IoT intrusion detection dataset and compare its performance\nagainst strong baselines, including LLM-Select and traditional methods such as\nPCA. Experimental results demonstrate that LLM-FS-Agent consistently achieves\nsuperior or comparable classification performance while reducing downstream\ntraining time by an average of 46% (statistically significant improvement, p =\n0.028 for XGBoost). These findings highlight that the proposed deliberative\narchitecture enhances both decision transparency and computational efficiency,\nestablishing LLM-FS-Agent as a practical and reliable solution for real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional data remains a pervasive challenge in machine learning,\noften undermining model interpretability and computational efficiency. While\nLarge Language Models (LLMs) have shown promise for dimensionality reduction\nthrough feature selection, existing LLM-based approaches frequently lack\nstructured reasoning and transparent justification for their decisions. This\npaper introduces LLM-FS-Agent, a novel multi-agent architecture designed for\ninterpretable and robust feature selection. The system orchestrates a\ndeliberative \"debate\" among multiple LLM agents, each assigned a specific role,\nenabling collective evaluation of feature relevance and generation of detailed\njustifications. We evaluate LLM-FS-Agent in the cybersecurity domain using the\nCIC-DIAD 2024 IoT intrusion detection dataset and compare its performance\nagainst strong baselines, including LLM-Select and traditional methods such as\nPCA. Experimental results demonstrate that LLM-FS-Agent consistently achieves\nsuperior or comparable classification performance while reducing downstream\ntraining time by an average of 46% (statistically significant improvement, p =\n0.028 for XGBoost). These findings highlight that the proposed deliberative\narchitecture enhances both decision transparency and computational efficiency,\nestablishing LLM-FS-Agent as a practical and reliable solution for real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Mohamed Bal-Ghaoui"
                    },
                    {
                        "name": "Fayssal Sabri"
                    }
                ],
                "author_detail": {
                    "name": "Fayssal Sabri"
                },
                "author": "Fayssal Sabri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05921v1",
                "updated": "2025-10-07T13:30:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    30,
                    18,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T13:30:18Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    30,
                    18,
                    1,
                    280,
                    0
                ],
                "title": "Prompt reinforcing for long-term planning of large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt reinforcing for long-term planning of large language models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in a wide range\nof natural language processing tasks and can be adapted through prompting.\nHowever, they remain suboptimal in multi-turn interactions, often relying on\nincorrect early assumptions and failing to track user goals over time, which\nmakes such tasks particularly challenging. Prior works in dialogue systems have\nshown that long-term planning is essential for handling interactive tasks. In\nthis work, we propose a prompt optimisation framework inspired by reinforcement\nlearning, which enables such planning to take place by only modifying the task\ninstruction prompt of the LLM-based agent. By generating turn-by-turn feedback\nand leveraging experience replay for prompt rewriting, our proposed method\nshows significant improvement in multi-turn tasks such as text-to-SQL and\ntask-oriented dialogue. Moreover, it generalises across different LLM-based\nagents and can leverage diverse LLMs as meta-prompting agents. This warrants\nfuture research in reinforcement learning-inspired parameter-free optimisation\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in a wide range\nof natural language processing tasks and can be adapted through prompting.\nHowever, they remain suboptimal in multi-turn interactions, often relying on\nincorrect early assumptions and failing to track user goals over time, which\nmakes such tasks particularly challenging. Prior works in dialogue systems have\nshown that long-term planning is essential for handling interactive tasks. In\nthis work, we propose a prompt optimisation framework inspired by reinforcement\nlearning, which enables such planning to take place by only modifying the task\ninstruction prompt of the LLM-based agent. By generating turn-by-turn feedback\nand leveraging experience replay for prompt rewriting, our proposed method\nshows significant improvement in multi-turn tasks such as text-to-SQL and\ntask-oriented dialogue. Moreover, it generalises across different LLM-based\nagents and can leverage diverse LLMs as meta-prompting agents. This warrants\nfuture research in reinforcement learning-inspired parameter-free optimisation\nmethods."
                },
                "authors": [
                    {
                        "name": "Hsien-Chin Lin"
                    },
                    {
                        "name": "Benjamin Matthias Ruppik"
                    },
                    {
                        "name": "Carel van Niekerk"
                    },
                    {
                        "name": "Chia-Hao Shen"
                    },
                    {
                        "name": "Michael Heck"
                    },
                    {
                        "name": "Nurul Lubis"
                    },
                    {
                        "name": "Renato Vukovic"
                    },
                    {
                        "name": "Shutong Feng"
                    },
                    {
                        "name": "Milica Gašić"
                    }
                ],
                "author_detail": {
                    "name": "Milica Gašić"
                },
                "author": "Milica Gašić",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03597v2",
                "updated": "2025-10-07T13:29:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    29,
                    15,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-04T01:20:30Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    1,
                    20,
                    30,
                    5,
                    277,
                    0
                ],
                "title": "Neon: Negative Extrapolation From Self-Training Improves Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neon: Negative Extrapolation From Self-Training Improves Image\n  Generation"
                },
                "summary": "Scaling generative AI models is bottlenecked by the scarcity of high-quality\ntraining data. The ease of synthesizing from a generative model suggests using\n(unverified) synthetic data to augment a limited corpus of real data for the\npurpose of fine-tuning in the hope of improving performance. Unfortunately,\nhowever, the resulting positive feedback loop leads to model autophagy disorder\n(MAD, aka model collapse) that results in a rapid degradation in sample quality\nand/or diversity. In this paper, we introduce Neon (for Negative Extrapolation\nfrOm self-traiNing), a new learning method that turns the degradation from\nself-training into a powerful signal for self-improvement. Given a base model,\nNeon first fine-tunes it on its own self-synthesized data but then,\ncounterintuitively, reverses its gradient updates to extrapolate away from the\ndegraded weights. We prove that Neon works because typical inference samplers\nthat favor high-probability regions create a predictable anti-alignment between\nthe synthetic and real data population gradients, which negative extrapolation\ncorrects to better align the model with the true data distribution. Neon is\nremarkably easy to implement via a simple post-hoc merge that requires no new\nreal data, works effectively with as few as 1k synthetic samples, and typically\nuses less than 1% additional training compute. We demonstrate Neon's\nuniversality across a range of architectures (diffusion, flow matching,\nautoregressive, and inductive moment matching models) and datasets (ImageNet,\nCIFAR-10, and FFHQ). In particular, on ImageNet 256x256, Neon elevates the\nxAR-L model to a new state-of-the-art FID of 1.02 with only 0.36% additional\ntraining compute. Code is available at https://github.com/VITA-Group/Neon",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling generative AI models is bottlenecked by the scarcity of high-quality\ntraining data. The ease of synthesizing from a generative model suggests using\n(unverified) synthetic data to augment a limited corpus of real data for the\npurpose of fine-tuning in the hope of improving performance. Unfortunately,\nhowever, the resulting positive feedback loop leads to model autophagy disorder\n(MAD, aka model collapse) that results in a rapid degradation in sample quality\nand/or diversity. In this paper, we introduce Neon (for Negative Extrapolation\nfrOm self-traiNing), a new learning method that turns the degradation from\nself-training into a powerful signal for self-improvement. Given a base model,\nNeon first fine-tunes it on its own self-synthesized data but then,\ncounterintuitively, reverses its gradient updates to extrapolate away from the\ndegraded weights. We prove that Neon works because typical inference samplers\nthat favor high-probability regions create a predictable anti-alignment between\nthe synthetic and real data population gradients, which negative extrapolation\ncorrects to better align the model with the true data distribution. Neon is\nremarkably easy to implement via a simple post-hoc merge that requires no new\nreal data, works effectively with as few as 1k synthetic samples, and typically\nuses less than 1% additional training compute. We demonstrate Neon's\nuniversality across a range of architectures (diffusion, flow matching,\nautoregressive, and inductive moment matching models) and datasets (ImageNet,\nCIFAR-10, and FFHQ). In particular, on ImageNet 256x256, Neon elevates the\nxAR-L model to a new state-of-the-art FID of 1.02 with only 0.36% additional\ntraining compute. Code is available at https://github.com/VITA-Group/Neon"
                },
                "authors": [
                    {
                        "name": "Sina Alemohammad"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Richard G. Baraniuk"
                    }
                ],
                "author_detail": {
                    "name": "Richard G. Baraniuk"
                },
                "author": "Richard G. Baraniuk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05909v1",
                "updated": "2025-10-07T13:20:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    20,
                    51,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T13:20:51Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    20,
                    51,
                    1,
                    280,
                    0
                ],
                "title": "Optimizing for Persuasion Improves LLM Generalization: Evidence from\n  Quality-Diversity Evolution of Debate Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing for Persuasion Improves LLM Generalization: Evidence from\n  Quality-Diversity Evolution of Debate Strategies"
                },
                "summary": "Large Language Models (LLMs) optimized to output truthful answers often\noverfit, producing brittle reasoning that fails to generalize. While\npersuasion-based optimization has shown promise in debate settings, it has not\nbeen systematically compared against mainstream truth-based approaches. We\nintroduce DebateQD, a minimal Quality-Diversity (QD) evolutionary algorithm\nthat evolves diverse debate strategies across different categories\n(rationality, authority, emotional appeal, etc.) through tournament-style\ncompetitions where two LLMs debate while a third judges. Unlike previously\nproposed methods that require a population of LLMs, our approach maintains\ndiversity of opponents through prompt-based strategies within a single LLM\narchitecture, making it more accessible for experiments while preserving the\nkey benefits of population-based optimization. In contrast to prior work, we\nexplicitly isolate the role of the optimization objective by fixing the debate\nprotocol and swapping only the fitness function: persuasion rewards strategies\nthat convince the judge irrespective of truth, whereas truth rewards\ncollaborative correctness. Across three model scales (7B, 32B, 72B parameters)\nand multiple dataset sizes from the QuALITY benchmark, persuasion-optimized\nstrategies achieve up to 13.94% smaller train-test generalization gaps, while\nmatching or exceeding truth optimization's test performance. These results\nprovide the first controlled evidence that competitive pressure to persuade,\nrather than seek the truth collaboratively, fosters more transferable reasoning\nskills, offering a promising path for improving LLM generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) optimized to output truthful answers often\noverfit, producing brittle reasoning that fails to generalize. While\npersuasion-based optimization has shown promise in debate settings, it has not\nbeen systematically compared against mainstream truth-based approaches. We\nintroduce DebateQD, a minimal Quality-Diversity (QD) evolutionary algorithm\nthat evolves diverse debate strategies across different categories\n(rationality, authority, emotional appeal, etc.) through tournament-style\ncompetitions where two LLMs debate while a third judges. Unlike previously\nproposed methods that require a population of LLMs, our approach maintains\ndiversity of opponents through prompt-based strategies within a single LLM\narchitecture, making it more accessible for experiments while preserving the\nkey benefits of population-based optimization. In contrast to prior work, we\nexplicitly isolate the role of the optimization objective by fixing the debate\nprotocol and swapping only the fitness function: persuasion rewards strategies\nthat convince the judge irrespective of truth, whereas truth rewards\ncollaborative correctness. Across three model scales (7B, 32B, 72B parameters)\nand multiple dataset sizes from the QuALITY benchmark, persuasion-optimized\nstrategies achieve up to 13.94% smaller train-test generalization gaps, while\nmatching or exceeding truth optimization's test performance. These results\nprovide the first controlled evidence that competitive pressure to persuade,\nrather than seek the truth collaboratively, fosters more transferable reasoning\nskills, offering a promising path for improving LLM generalization."
                },
                "authors": [
                    {
                        "name": "Aksel Joonas Reedi"
                    },
                    {
                        "name": "Corentin Léger"
                    },
                    {
                        "name": "Julien Pourcel"
                    },
                    {
                        "name": "Loris Gaven"
                    },
                    {
                        "name": "Perrine Charriau"
                    },
                    {
                        "name": "Guillaume Pourcel"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Pourcel"
                },
                "author": "Guillaume Pourcel",
                "arxiv_comment": "Open-source code available at\n  https://github.com/flowersteam/llm_persuasion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04002v2",
                "updated": "2025-10-07T13:17:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    17,
                    8,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-05T02:30:11Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    2,
                    30,
                    11,
                    6,
                    278,
                    0
                ],
                "title": "AgriGPT-VL: Agricultural Vision-Language Understanding Suite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgriGPT-VL: Agricultural Vision-Language Understanding Suite"
                },
                "summary": "Despite rapid advances in multimodal large language models, agricultural\napplications remain constrained by the scarcity of domain-tailored models,\ncurated vision-language corpora, and rigorous evaluation. To address these\nchallenges, we present the AgriGPT-VL Suite, a unified multimodal framework for\nagriculture. Our contributions are threefold. First, we introduce Agri-3M-VL,\nthe largest vision-language corpus for agriculture to our knowledge, curated by\na scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M\nimage-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO\nreinforcement learning samples. Second, we develop AgriGPT-VL, an\nagriculture-specialized vision-language model trained via a progressive\ncurriculum of textual grounding, multimodal shallow/deep alignment, and GRPO\nrefinement. This method achieves strong multimodal reasoning while preserving\ntext-only capability. Third, we establish AgriBench-VL-4K, a compact yet\nchallenging evaluation suite with open-ended and image-grounded questions,\npaired with multi-metric evaluation and an LLM-as-a-judge framework.\nExperiments show that AgriGPT-VL outperforms leading general-purpose VLMs on\nAgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge\nevaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K\nwith no noticeable degradation of language ability. Ablation studies further\nconfirm consistent gains from our alignment and GRPO refinement stages. We will\nopen source all of the resources to support reproducible research and\ndeployment in low-resource agricultural settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rapid advances in multimodal large language models, agricultural\napplications remain constrained by the scarcity of domain-tailored models,\ncurated vision-language corpora, and rigorous evaluation. To address these\nchallenges, we present the AgriGPT-VL Suite, a unified multimodal framework for\nagriculture. Our contributions are threefold. First, we introduce Agri-3M-VL,\nthe largest vision-language corpus for agriculture to our knowledge, curated by\na scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M\nimage-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO\nreinforcement learning samples. Second, we develop AgriGPT-VL, an\nagriculture-specialized vision-language model trained via a progressive\ncurriculum of textual grounding, multimodal shallow/deep alignment, and GRPO\nrefinement. This method achieves strong multimodal reasoning while preserving\ntext-only capability. Third, we establish AgriBench-VL-4K, a compact yet\nchallenging evaluation suite with open-ended and image-grounded questions,\npaired with multi-metric evaluation and an LLM-as-a-judge framework.\nExperiments show that AgriGPT-VL outperforms leading general-purpose VLMs on\nAgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge\nevaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K\nwith no noticeable degradation of language ability. Ablation studies further\nconfirm consistent gains from our alignment and GRPO refinement stages. We will\nopen source all of the resources to support reproducible research and\ndeployment in low-resource agricultural settings."
                },
                "authors": [
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Yunkui Chen"
                    },
                    {
                        "name": "Lanfei Feng"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Xiao Xu"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Nueraili Aierken"
                    },
                    {
                        "name": "Runhe Huang"
                    },
                    {
                        "name": "Hongjian Lin"
                    },
                    {
                        "name": "Yibin Ying"
                    },
                    {
                        "name": "Shijian Li"
                    }
                ],
                "author_detail": {
                    "name": "Shijian Li"
                },
                "author": "Shijian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02298v3",
                "updated": "2025-10-08T02:10:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    2,
                    10,
                    47,
                    2,
                    281,
                    0
                ],
                "published": "2025-08-04T11:06:08Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    6,
                    8,
                    0,
                    216,
                    0
                ],
                "title": "CAPO: Towards Enhancing LLM Reasoning through Generative Credit\n  Assignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAPO: Towards Enhancing LLM Reasoning through Generative Credit\n  Assignment"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has improved the\nreasoning abilities of Large Language Models (LLMs) by using rule-based binary\nfeedback. However, current RLVR methods typically assign the same reward to\nevery token. This coarse-grained feedback hampers precise credit assignment,\nmaking it hard for models to identify which reasoning steps lead to success or\nfailure, and often results in suboptimal policies. Methods like PPO provide\ncredit assignment by value estimation, but yield inaccurate and unverifiable\nsignals due to limited sampling. On the other hand, methods using Process\nReward Models can provide step-wise rewards but suffer from several key\nlimitations: they require high-quality process supervision labels, the feedback\nis unreliable due to probabilistic reward modeling, and their application in\nonline reinforcement learning (RL) is time-consuming. To overcome these\nlimitations, we introduce a simple but efficient method-Credit Assignment\nPolicy Optimization (CAPO). Instead of training auxiliary models, CAPO directly\nleverages an off-the-shelf, general-purpose LLM as a Generative Process Reward\nModel (LLM-as-GenPRM) to generate all step-wise critique by one pass only based\non the correctness of the step itself, providing deterministic token-level\ncredits to refine the tokens that were originally assigned identical rule-based\nrewards. To further enhance the accuracy and robustness, we employ voting\nmechanisms that scale with the number of generated critiques. Extensive\nexperiments on various backbones like Llama and Qwen models show that CAPO\nconsistently outperforms supervised learning-based and RL-based fine-tuning\nmethods across four challenging mathematical benchmarks and three out-of-domain\nbenchmarks. Further analysis shows that CAPO can help the model to foster the\nlearning of correct reasoning pathways leading to correct answers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has improved the\nreasoning abilities of Large Language Models (LLMs) by using rule-based binary\nfeedback. However, current RLVR methods typically assign the same reward to\nevery token. This coarse-grained feedback hampers precise credit assignment,\nmaking it hard for models to identify which reasoning steps lead to success or\nfailure, and often results in suboptimal policies. Methods like PPO provide\ncredit assignment by value estimation, but yield inaccurate and unverifiable\nsignals due to limited sampling. On the other hand, methods using Process\nReward Models can provide step-wise rewards but suffer from several key\nlimitations: they require high-quality process supervision labels, the feedback\nis unreliable due to probabilistic reward modeling, and their application in\nonline reinforcement learning (RL) is time-consuming. To overcome these\nlimitations, we introduce a simple but efficient method-Credit Assignment\nPolicy Optimization (CAPO). Instead of training auxiliary models, CAPO directly\nleverages an off-the-shelf, general-purpose LLM as a Generative Process Reward\nModel (LLM-as-GenPRM) to generate all step-wise critique by one pass only based\non the correctness of the step itself, providing deterministic token-level\ncredits to refine the tokens that were originally assigned identical rule-based\nrewards. To further enhance the accuracy and robustness, we employ voting\nmechanisms that scale with the number of generated critiques. Extensive\nexperiments on various backbones like Llama and Qwen models show that CAPO\nconsistently outperforms supervised learning-based and RL-based fine-tuning\nmethods across four challenging mathematical benchmarks and three out-of-domain\nbenchmarks. Further analysis shows that CAPO can help the model to foster the\nlearning of correct reasoning pathways leading to correct answers."
                },
                "authors": [
                    {
                        "name": "Guofu Xie"
                    },
                    {
                        "name": "Yunsheng Shi"
                    },
                    {
                        "name": "Hongtao Tian"
                    },
                    {
                        "name": "Ting Yao"
                    },
                    {
                        "name": "Xiao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Zhang"
                },
                "author": "Xiao Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05902v1",
                "updated": "2025-10-07T13:11:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    11,
                    18,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T13:11:18Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    11,
                    18,
                    1,
                    280,
                    0
                ],
                "title": "A subsampling approach for large data sets when the Generalised Linear\n  Model is potentially misspecified",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A subsampling approach for large data sets when the Generalised Linear\n  Model is potentially misspecified"
                },
                "summary": "Subsampling is a computationally efficient and scalable method to draw\ninference in large data settings based on a subset of the data rather than\nneeding to consider the whole dataset. When employing subsampling techniques, a\ncrucial consideration is how to select an informative subset based on the\nqueries posed by the data analyst. A recently proposed method for this purpose\ninvolves randomly selecting samples from the large dataset based on subsampling\nprobabilities. However, a major drawback of this approach is that the derived\nsubsampling probabilities are typically based on an assumed statistical model\nwhich may be difficult to correctly specify in practice. To address this\nlimitation, we propose to determine subsampling probabilities based on a\nstatistical model that we acknowledge may be misspecified. To do so, we propose\nto evaluate the subsampling probabilities based on the Mean Squared Error (MSE)\nof the predictions from a model that is not assumed to completely describe the\nlarge dataset. We apply our subsampling approach in a simulation study and for\nthe analysis of two real-world large datasets, where its performance is\nbenchmarked against existing subsampling techniques. The findings suggest that\nthere is value in adopting our approach over current practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subsampling is a computationally efficient and scalable method to draw\ninference in large data settings based on a subset of the data rather than\nneeding to consider the whole dataset. When employing subsampling techniques, a\ncrucial consideration is how to select an informative subset based on the\nqueries posed by the data analyst. A recently proposed method for this purpose\ninvolves randomly selecting samples from the large dataset based on subsampling\nprobabilities. However, a major drawback of this approach is that the derived\nsubsampling probabilities are typically based on an assumed statistical model\nwhich may be difficult to correctly specify in practice. To address this\nlimitation, we propose to determine subsampling probabilities based on a\nstatistical model that we acknowledge may be misspecified. To do so, we propose\nto evaluate the subsampling probabilities based on the Mean Squared Error (MSE)\nof the predictions from a model that is not assumed to completely describe the\nlarge dataset. We apply our subsampling approach in a simulation study and for\nthe analysis of two real-world large datasets, where its performance is\nbenchmarked against existing subsampling techniques. The findings suggest that\nthere is value in adopting our approach over current practice."
                },
                "authors": [
                    {
                        "name": "Amalan Mahendran"
                    },
                    {
                        "name": "Helen Thompson"
                    },
                    {
                        "name": "James M. McGree"
                    }
                ],
                "author_detail": {
                    "name": "James M. McGree"
                },
                "author": "James M. McGree",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.06214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06214v1",
                "updated": "2025-10-07T17:59:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    59,
                    13,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:59:13Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    59,
                    13,
                    1,
                    280,
                    0
                ],
                "title": "Stratified GRPO: Handling Structural Heterogeneity in Reinforcement\n  Learning of LLM Search Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stratified GRPO: Handling Structural Heterogeneity in Reinforcement\n  Learning of LLM Search Agents"
                },
                "summary": "Large language model (LLM) agents increasingly rely on external tools such as\nsearch engines to solve complex, multi-step problems, and reinforcement\nlearning (RL) has become a key paradigm for training them. However, the\ntrajectories of search agents are structurally heterogeneous, where variations\nin the number, placement, and outcomes of search calls lead to fundamentally\ndifferent answer directions and reward distributions. Standard policy gradient\nmethods, which use a single global baseline, suffer from what we identify and\nformalize as cross-stratum bias-an \"apples-to-oranges\" comparison of\nheterogeneous trajectories. This cross-stratum bias distorts credit assignment\nand hinders exploration of complex, multi-step search strategies. To address\nthis, we propose Stratified GRPO, whose central component, Stratified Advantage\nNormalization (SAN), partitions trajectories into homogeneous strata based on\ntheir structural properties and computes advantages locally within each\nstratum. This ensures that trajectories are evaluated only against their true\npeers. Our analysis proves that SAN eliminates cross-stratum bias, yields\nconditionally unbiased unit-variance estimates inside each stratum, and retains\nthe global unbiasedness and unit-variance properties enjoyed by standard\nnormalization, resulting in a more pure and scale-stable learning signal. To\nimprove practical stability under finite-sample regimes, we further linearly\nblend SAN with the global estimator. Extensive experiments on diverse\nsingle-hop and multi-hop question-answering benchmarks demonstrate that\nStratified GRPO consistently and substantially outperforms GRPO by up to 11.3\npoints, achieving higher training rewards, greater training stability, and more\neffective search policies. These results establish stratification as a\nprincipled remedy for structural heterogeneity in RL for LLM search agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents increasingly rely on external tools such as\nsearch engines to solve complex, multi-step problems, and reinforcement\nlearning (RL) has become a key paradigm for training them. However, the\ntrajectories of search agents are structurally heterogeneous, where variations\nin the number, placement, and outcomes of search calls lead to fundamentally\ndifferent answer directions and reward distributions. Standard policy gradient\nmethods, which use a single global baseline, suffer from what we identify and\nformalize as cross-stratum bias-an \"apples-to-oranges\" comparison of\nheterogeneous trajectories. This cross-stratum bias distorts credit assignment\nand hinders exploration of complex, multi-step search strategies. To address\nthis, we propose Stratified GRPO, whose central component, Stratified Advantage\nNormalization (SAN), partitions trajectories into homogeneous strata based on\ntheir structural properties and computes advantages locally within each\nstratum. This ensures that trajectories are evaluated only against their true\npeers. Our analysis proves that SAN eliminates cross-stratum bias, yields\nconditionally unbiased unit-variance estimates inside each stratum, and retains\nthe global unbiasedness and unit-variance properties enjoyed by standard\nnormalization, resulting in a more pure and scale-stable learning signal. To\nimprove practical stability under finite-sample regimes, we further linearly\nblend SAN with the global estimator. Extensive experiments on diverse\nsingle-hop and multi-hop question-answering benchmarks demonstrate that\nStratified GRPO consistently and substantially outperforms GRPO by up to 11.3\npoints, achieving higher training rewards, greater training stability, and more\neffective search policies. These results establish stratification as a\nprincipled remedy for structural heterogeneity in RL for LLM search agents."
                },
                "authors": [
                    {
                        "name": "Mingkang Zhu"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06213v1",
                "updated": "2025-10-07T17:59:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    59,
                    7,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:59:07Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    59,
                    7,
                    1,
                    280,
                    0
                ],
                "title": "Training Dynamics Impact Post-Training Quantization Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Dynamics Impact Post-Training Quantization Robustness"
                },
                "summary": "While post-training quantization is widely adopted for efficient deployment\nof large language models, the mechanisms underlying quantization robustness\nremain unclear. We conduct a comprehensive analysis of quantization degradation\nacross open-source language model training trajectories up to 32B parameters\nand 15T training tokens to accurately assess the relationship between training\ndynamics and quantization performance. Our key finding is that quantization\nerrors in large-scale training runs are driven by a complex interplay between\nlearning rate and other training hyperparameters. Specifically, once learning\nrates decay, validation loss and quantization error diverge, largely\nindependent of training data scale. To investigate interventions on the\ntraining dynamics and identify specific configurations that can modulate\nquantization robustness favorably, we train our own models in controlled\nexperiments up to 100B tokens. Our results challenge the assumption that\nincreasing dataset scale inherently compromises quantization effectiveness,\ndemonstrating instead that strategic training hyperparameter interventions can\nimprove quantization quality at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While post-training quantization is widely adopted for efficient deployment\nof large language models, the mechanisms underlying quantization robustness\nremain unclear. We conduct a comprehensive analysis of quantization degradation\nacross open-source language model training trajectories up to 32B parameters\nand 15T training tokens to accurately assess the relationship between training\ndynamics and quantization performance. Our key finding is that quantization\nerrors in large-scale training runs are driven by a complex interplay between\nlearning rate and other training hyperparameters. Specifically, once learning\nrates decay, validation loss and quantization error diverge, largely\nindependent of training data scale. To investigate interventions on the\ntraining dynamics and identify specific configurations that can modulate\nquantization robustness favorably, we train our own models in controlled\nexperiments up to 100B tokens. Our results challenge the assumption that\nincreasing dataset scale inherently compromises quantization effectiveness,\ndemonstrating instead that strategic training hyperparameter interventions can\nimprove quantization quality at scale."
                },
                "authors": [
                    {
                        "name": "Albert Catalan-Tatjer"
                    },
                    {
                        "name": "Niccolò Ajroldi"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04573v2",
                "updated": "2025-10-07T17:58:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    58,
                    48,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-06T08:15:03Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    8,
                    15,
                    3,
                    0,
                    279,
                    0
                ],
                "title": "LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning"
                },
                "summary": "Large Language Models (LLMs) demonstrate their reasoning ability through\nchain-of-thought (CoT) generation. However, LLM's autoregressive decoding may\nlimit the ability to revisit and refine earlier tokens in a holistic manner,\nwhich can also lead to inefficient exploration for diverse solutions. In this\npaper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning\nframework that unifies the expressiveness of continuous latent representation\nwith the iterative refinement capabilities of latent diffusion models for an\nexisting LLM. We first construct a structured latent reasoning space using a\nVariational Autoencoder (VAE) that encodes text reasoning steps into blocks of\nthought tokens, preserving semantic information and interpretability while\noffering compact but expressive representations. Subsequently, we utilize a\nlatent diffusion model that learns to denoise a block of latent thought tokens\nwith a blockwise bidirectional attention mask, enabling longer horizon and\niterative refinement with adaptive test-time compute. This design allows\nefficient parallel generation of diverse reasoning trajectories, allowing the\nmodel to plan and revise the reasoning process holistically. We conduct\nevaluations on a suite of mathematical reasoning and planning benchmarks.\nEmpirical results show that LaDiR consistently improves accuracy, diversity,\nand interpretability over existing autoregressive, diffusion-based, and latent\nreasoning methods, revealing a new paradigm for text reasoning with latent\ndiffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate their reasoning ability through\nchain-of-thought (CoT) generation. However, LLM's autoregressive decoding may\nlimit the ability to revisit and refine earlier tokens in a holistic manner,\nwhich can also lead to inefficient exploration for diverse solutions. In this\npaper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning\nframework that unifies the expressiveness of continuous latent representation\nwith the iterative refinement capabilities of latent diffusion models for an\nexisting LLM. We first construct a structured latent reasoning space using a\nVariational Autoencoder (VAE) that encodes text reasoning steps into blocks of\nthought tokens, preserving semantic information and interpretability while\noffering compact but expressive representations. Subsequently, we utilize a\nlatent diffusion model that learns to denoise a block of latent thought tokens\nwith a blockwise bidirectional attention mask, enabling longer horizon and\niterative refinement with adaptive test-time compute. This design allows\nefficient parallel generation of diverse reasoning trajectories, allowing the\nmodel to plan and revise the reasoning process holistically. We conduct\nevaluations on a suite of mathematical reasoning and planning benchmarks.\nEmpirical results show that LaDiR consistently improves accuracy, diversity,\nand interpretability over existing autoregressive, diffusion-based, and latent\nreasoning methods, revealing a new paradigm for text reasoning with latent\ndiffusion."
                },
                "authors": [
                    {
                        "name": "Haoqiang Kang"
                    },
                    {
                        "name": "Yizhe Zhang"
                    },
                    {
                        "name": "Nikki Lijing Kuang"
                    },
                    {
                        "name": "Nicklas Majamaki"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    },
                    {
                        "name": "Yi-An Ma"
                    },
                    {
                        "name": "Lianhui Qin"
                    }
                ],
                "author_detail": {
                    "name": "Lianhui Qin"
                },
                "author": "Lianhui Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19227v2",
                "updated": "2025-10-07T17:57:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    57,
                    11,
                    1,
                    280,
                    0
                ],
                "published": "2025-08-26T17:43:20Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    43,
                    20,
                    1,
                    238,
                    0
                ],
                "title": "Generative Interfaces for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Interfaces for Language Models"
                },
                "summary": "Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with up to a 72% improvement in\nhuman preference. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with up to a 72% improvement in\nhuman preference. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction."
                },
                "authors": [
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Yanzhe Zhang"
                    },
                    {
                        "name": "Yutong Zhang"
                    },
                    {
                        "name": "Yijia Shao"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14824v2",
                "updated": "2025-10-07T17:56:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    56,
                    22,
                    1,
                    280,
                    0
                ],
                "published": "2025-05-20T18:39:56Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    18,
                    39,
                    56,
                    1,
                    140,
                    0
                ],
                "title": "Tracing Multilingual Factual Knowledge Acquisition in Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracing Multilingual Factual Knowledge Acquisition in Pretraining"
                },
                "summary": "Large Language Models (LLMs) are capable of recalling multilingual factual\nknowledge present in their pretraining data. However, most studies evaluate\nonly the final model, leaving the development of factual recall and\ncrosslingual consistency throughout pretraining largely unexplored. In this\nwork, we trace how factual recall and crosslingual consistency evolve during\npretraining, focusing on OLMo-7B as a case study. We find that both accuracy\nand consistency improve over time for most languages. We show that this\nimprovement is primarily driven by the fact frequency in the pretraining\ncorpus: more frequent facts are more likely to be recalled correctly,\nregardless of language. Yet, some low-frequency facts in non-English languages\ncan still be correctly recalled. Our analysis reveals that these instances\nlargely benefit from crosslingual transfer of their English counterparts -- an\neffect that emerges predominantly in the early stages of pretraining. We\npinpoint two distinct pathways through which multilingual factual knowledge\nacquisition occurs: (1) frequency-driven learning, which is dominant and\nlanguage-agnostic, and (2) crosslingual transfer, which is limited in scale and\ntypically constrained to relation types involving named entities. We release\nour code and data to facilitate further research at\nhttps://github.com/cisnlp/multilingual-fact-tracing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are capable of recalling multilingual factual\nknowledge present in their pretraining data. However, most studies evaluate\nonly the final model, leaving the development of factual recall and\ncrosslingual consistency throughout pretraining largely unexplored. In this\nwork, we trace how factual recall and crosslingual consistency evolve during\npretraining, focusing on OLMo-7B as a case study. We find that both accuracy\nand consistency improve over time for most languages. We show that this\nimprovement is primarily driven by the fact frequency in the pretraining\ncorpus: more frequent facts are more likely to be recalled correctly,\nregardless of language. Yet, some low-frequency facts in non-English languages\ncan still be correctly recalled. Our analysis reveals that these instances\nlargely benefit from crosslingual transfer of their English counterparts -- an\neffect that emerges predominantly in the early stages of pretraining. We\npinpoint two distinct pathways through which multilingual factual knowledge\nacquisition occurs: (1) frequency-driven learning, which is dominant and\nlanguage-agnostic, and (2) crosslingual transfer, which is limited in scale and\ntypically constrained to relation types involving named entities. We release\nour code and data to facilitate further research at\nhttps://github.com/cisnlp/multilingual-fact-tracing."
                },
                "authors": [
                    {
                        "name": "Yihong Liu"
                    },
                    {
                        "name": "Mingyang Wang"
                    },
                    {
                        "name": "Amir Hossein Kargaran"
                    },
                    {
                        "name": "Felicia Körner"
                    },
                    {
                        "name": "Ercong Nie"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "François Yvon"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "arxiv_comment": "EMNLP Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18427v2",
                "updated": "2025-10-07T17:56:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    56,
                    0,
                    1,
                    280,
                    0
                ],
                "published": "2025-08-25T19:18:50Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    19,
                    18,
                    50,
                    0,
                    237,
                    0
                ],
                "title": "Tracing Positional Bias in Financial Decision-Making: Mechanistic\n  Insights from Qwen2.5",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracing Positional Bias in Financial Decision-Making: Mechanistic\n  Insights from Qwen2.5"
                },
                "summary": "The growing adoption of large language models (LLMs) in finance exposes\nhigh-stakes decision-making to subtle, underexamined positional biases. The\ncomplexity and opacity of modern model architectures compound this risk. We\npresent the first unified framework and benchmark that not only detects and\nquantifies positional bias in binary financial decisions but also pinpoints its\nmechanistic origins within open-source Qwen2.5-instruct models (1.5B-14B). Our\nempirical analysis covers a novel, finance-authentic dataset revealing that\npositional bias is pervasive, scale-sensitive, and prone to resurfacing under\nnuanced prompt designs and investment scenarios, with recency and primacy\neffects revealing new vulnerabilities in risk-laden contexts. Through\ntransparent mechanistic interpretability, we map how and where bias emerges and\npropagates within the models to deliver actionable, generalizable insights\nacross prompt types and scales. By bridging domain-specific audit with model\ninterpretability, our work provides a new methodological standard for both\nrigorous bias diagnosis and practical mitigation, establishing essential\nguidance for responsible and trustworthy deployment of LLMs in financial\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of large language models (LLMs) in finance exposes\nhigh-stakes decision-making to subtle, underexamined positional biases. The\ncomplexity and opacity of modern model architectures compound this risk. We\npresent the first unified framework and benchmark that not only detects and\nquantifies positional bias in binary financial decisions but also pinpoints its\nmechanistic origins within open-source Qwen2.5-instruct models (1.5B-14B). Our\nempirical analysis covers a novel, finance-authentic dataset revealing that\npositional bias is pervasive, scale-sensitive, and prone to resurfacing under\nnuanced prompt designs and investment scenarios, with recency and primacy\neffects revealing new vulnerabilities in risk-laden contexts. Through\ntransparent mechanistic interpretability, we map how and where bias emerges and\npropagates within the models to deliver actionable, generalizable insights\nacross prompt types and scales. By bridging domain-specific audit with model\ninterpretability, our work provides a new methodological standard for both\nrigorous bias diagnosis and practical mitigation, establishing essential\nguidance for responsible and trustworthy deployment of LLMs in financial\nsystems."
                },
                "authors": [
                    {
                        "name": "Fabrizio Dimino"
                    },
                    {
                        "name": "Krati Saxena"
                    },
                    {
                        "name": "Bhaskarjit Sarmah"
                    },
                    {
                        "name": "Stefano Pasquali"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Pasquali"
                },
                "author": "Stefano Pasquali",
                "arxiv_doi": "10.1145/3768292.3770394",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3768292.3770394",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14252v2",
                "updated": "2025-10-07T17:55:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    55,
                    14,
                    1,
                    280,
                    0
                ],
                "published": "2025-09-11T03:03:57Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    3,
                    3,
                    57,
                    3,
                    254,
                    0
                ],
                "title": "LLM-JEPA: Large Language Models Meet Joint Embedding Predictive\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-JEPA: Large Language Models Meet Joint Embedding Predictive\n  Architectures"
                },
                "summary": "Large Language Model (LLM) pretraining, finetuning, and evaluation rely on\ninput-space reconstruction and generative capabilities. Yet, it has been\nobserved in vision that embedding-space training objectives, e.g., with Joint\nEmbedding Predictive Architectures (JEPAs), are far superior to their\ninput-space counterpart. That mismatch in how training is achieved between\nlanguage and vision opens up a natural question: {\\em can language training\nmethods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is\na testimony of the challenge in designing such objectives for language. In this\nwork, we propose a first step in that direction where we develop LLM-JEPA, a\nJEPA based solution for LLMs applicable both to finetuning and pretraining.\nThus far, LLM-JEPA is able to outperform the standard LLM training objectives\nby a significant margin across models, all while being robust to overfiting.\nThose findings are observed across numerous datasets (NL-RX, GSM8K, Spider,\nRottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo\nfamilies. Code: https://github.com/rbalestr-lab/llm-jepa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) pretraining, finetuning, and evaluation rely on\ninput-space reconstruction and generative capabilities. Yet, it has been\nobserved in vision that embedding-space training objectives, e.g., with Joint\nEmbedding Predictive Architectures (JEPAs), are far superior to their\ninput-space counterpart. That mismatch in how training is achieved between\nlanguage and vision opens up a natural question: {\\em can language training\nmethods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is\na testimony of the challenge in designing such objectives for language. In this\nwork, we propose a first step in that direction where we develop LLM-JEPA, a\nJEPA based solution for LLMs applicable both to finetuning and pretraining.\nThus far, LLM-JEPA is able to outperform the standard LLM training objectives\nby a significant margin across models, all while being robust to overfiting.\nThose findings are observed across numerous datasets (NL-RX, GSM8K, Spider,\nRottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo\nfamilies. Code: https://github.com/rbalestr-lab/llm-jepa."
                },
                "authors": [
                    {
                        "name": "Hai Huang"
                    },
                    {
                        "name": "Yann LeCun"
                    },
                    {
                        "name": "Randall Balestriero"
                    }
                ],
                "author_detail": {
                    "name": "Randall Balestriero"
                },
                "author": "Randall Balestriero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06198v1",
                "updated": "2025-10-07T17:53:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    53,
                    55,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:53:55Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    53,
                    55,
                    1,
                    280,
                    0
                ],
                "title": "Peeking inside the Black-Box: Reinforcement Learning for Explainable and\n  Accurate Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peeking inside the Black-Box: Reinforcement Learning for Explainable and\n  Accurate Relation Extraction"
                },
                "summary": "This paper introduces a framework for relation extraction (RE) that enhances\nboth accuracy and explainability. The framework has two key components: (i) a\nreasoning mechanism that formulates relation extraction as a series of\ntext-processing steps inspired by cognitive science, and (ii) an optimization\nprocess driven by reinforcement learning (RL) with a novel reward function\ndesigned to improve both task accuracy and explanation quality. We call our\napproach CogRE. Our framework addresses the lack of supervision for\nlanguage-based explanations in traditional RE by promoting outputs that include\nimportant relation keywords. These keywords are drawn from a high-quality\ndictionary that is automatically constructed using an LLM. We evaluate our\napproach for the task of one-shot RE using two LLMs and two RE datasets. Our\nexperiments show that CogRE improves explanation quality by addressing two\ncommon failure patterns in one-shot RE: poor attention focus and limited\none-shot learning capability. For example, our cognitive-structured reasoning\nwith Qwen2.5-15B-Instruct on One-shot NYT29 achieves 24.65% F1, surpassing\nprior reasoning-based designs. Optimizing this approach with RL using our\nreward further improves performance by +23.46% (absolute). Finally, human\nevaluation shows that our best model generates relational keywords closely\naligned with gold labels, increasing human explanation quality ratings by 54%\n(relative).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a framework for relation extraction (RE) that enhances\nboth accuracy and explainability. The framework has two key components: (i) a\nreasoning mechanism that formulates relation extraction as a series of\ntext-processing steps inspired by cognitive science, and (ii) an optimization\nprocess driven by reinforcement learning (RL) with a novel reward function\ndesigned to improve both task accuracy and explanation quality. We call our\napproach CogRE. Our framework addresses the lack of supervision for\nlanguage-based explanations in traditional RE by promoting outputs that include\nimportant relation keywords. These keywords are drawn from a high-quality\ndictionary that is automatically constructed using an LLM. We evaluate our\napproach for the task of one-shot RE using two LLMs and two RE datasets. Our\nexperiments show that CogRE improves explanation quality by addressing two\ncommon failure patterns in one-shot RE: poor attention focus and limited\none-shot learning capability. For example, our cognitive-structured reasoning\nwith Qwen2.5-15B-Instruct on One-shot NYT29 achieves 24.65% F1, surpassing\nprior reasoning-based designs. Optimizing this approach with RL using our\nreward further improves performance by +23.46% (absolute). Finally, human\nevaluation shows that our best model generates relational keywords closely\naligned with gold labels, increasing human explanation quality ratings by 54%\n(relative)."
                },
                "authors": [
                    {
                        "name": "Xinyu Guo"
                    },
                    {
                        "name": "Zhengliang Shi"
                    },
                    {
                        "name": "Minglai Yang"
                    },
                    {
                        "name": "Mahdi Rahimi"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    }
                ],
                "author_detail": {
                    "name": "Mihai Surdeanu"
                },
                "author": "Mihai Surdeanu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06190v1",
                "updated": "2025-10-07T17:49:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    49,
                    30,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:49:30Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    49,
                    30,
                    1,
                    280,
                    0
                ],
                "title": "On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond"
                },
                "summary": "This paper formally studies generation processes, including auto-regressive\nnext-token prediction and masked diffusion, that abstract beyond architectural\nspecifics. At this level of abstraction, we quantify their benefits and\nlimitations through measurable criteria such as computational hardness and\nlearnability. In particular, we demonstrate that allowing generation to proceed\nbeyond autoregression and current masked diffusion, with capabilities to\nrewrite and length-variable edit, can bring significant theoretical and\nempirical advantages, with important implications for frontier LLMs that aspire\nto tackle increasingly hard problems and work universally across domains beyond\nnatural language, such as coding and science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper formally studies generation processes, including auto-regressive\nnext-token prediction and masked diffusion, that abstract beyond architectural\nspecifics. At this level of abstraction, we quantify their benefits and\nlimitations through measurable criteria such as computational hardness and\nlearnability. In particular, we demonstrate that allowing generation to proceed\nbeyond autoregression and current masked diffusion, with capabilities to\nrewrite and length-variable edit, can bring significant theoretical and\nempirical advantages, with important implications for frontier LLMs that aspire\nto tackle increasingly hard problems and work universally across domains beyond\nnatural language, such as coding and science."
                },
                "authors": [
                    {
                        "name": "Chenxiao Yang"
                    },
                    {
                        "name": "Cai Zhou"
                    },
                    {
                        "name": "David Wipf"
                    },
                    {
                        "name": "Zhiyuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Li"
                },
                "author": "Zhiyuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06189v2",
                "updated": "2025-10-08T01:21:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    1,
                    21,
                    49,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-07T17:49:24Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    49,
                    24,
                    1,
                    280,
                    0
                ],
                "title": "Barbarians at the Gate: How AI is Upending Systems Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Barbarians at the Gate: How AI is Upending Systems Research"
                },
                "summary": "Artificial Intelligence (AI) is starting to transform the research process as\nwe know it by automating the discovery of new solutions. Given a task, the\ntypical AI-driven approach is (i) to generate a set of diverse solutions, and\nthen (ii) to verify these solutions and select one that solves the problem.\nCrucially, this approach assumes the existence of a reliable verifier, i.e.,\none that can accurately determine whether a solution solves the given problem.\nWe argue that systems research, long focused on designing and evaluating new\nperformance-oriented algorithms, is particularly well-suited for AI-driven\nsolution discovery. This is because system performance problems naturally admit\nreliable verifiers: solutions are typically implemented in real systems or\nsimulators, and verification reduces to running these software artifacts\nagainst predefined workloads and measuring performance. We term this approach\nas AI-Driven Research for Systems (ADRS), which iteratively generates,\nevaluates, and refines solutions. Using penEvolve, an existing open-source ADRS\ninstance, we present case studies across diverse domains, including load\nbalancing for multi-region cloud scheduling, Mixture-of-Experts inference,\nLLM-based SQL queries, and transaction scheduling. In multiple instances, ADRS\ndiscovers algorithms that outperform state-of-the-art human designs (e.g.,\nachieving up to 5.0x runtime improvements or 50% cost reductions). We distill\nbest practices for guiding algorithm evolution, from prompt design to evaluator\nconstruction, for existing frameworks. We then discuss the broader implications\nfor the systems community: as AI assumes a central role in algorithm design, we\nargue that human researchers will increasingly focus on problem formulation and\nstrategic guidance. Our results highlight both the disruptive potential and the\nurgent need to adapt systems research practices in the age of AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) is starting to transform the research process as\nwe know it by automating the discovery of new solutions. Given a task, the\ntypical AI-driven approach is (i) to generate a set of diverse solutions, and\nthen (ii) to verify these solutions and select one that solves the problem.\nCrucially, this approach assumes the existence of a reliable verifier, i.e.,\none that can accurately determine whether a solution solves the given problem.\nWe argue that systems research, long focused on designing and evaluating new\nperformance-oriented algorithms, is particularly well-suited for AI-driven\nsolution discovery. This is because system performance problems naturally admit\nreliable verifiers: solutions are typically implemented in real systems or\nsimulators, and verification reduces to running these software artifacts\nagainst predefined workloads and measuring performance. We term this approach\nas AI-Driven Research for Systems (ADRS), which iteratively generates,\nevaluates, and refines solutions. Using penEvolve, an existing open-source ADRS\ninstance, we present case studies across diverse domains, including load\nbalancing for multi-region cloud scheduling, Mixture-of-Experts inference,\nLLM-based SQL queries, and transaction scheduling. In multiple instances, ADRS\ndiscovers algorithms that outperform state-of-the-art human designs (e.g.,\nachieving up to 5.0x runtime improvements or 50% cost reductions). We distill\nbest practices for guiding algorithm evolution, from prompt design to evaluator\nconstruction, for existing frameworks. We then discuss the broader implications\nfor the systems community: as AI assumes a central role in algorithm design, we\nargue that human researchers will increasingly focus on problem formulation and\nstrategic guidance. Our results highlight both the disruptive potential and the\nurgent need to adapt systems research practices in the age of AI."
                },
                "authors": [
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Melissa Pan"
                    },
                    {
                        "name": "Zhifei Li"
                    },
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Alex Krentsel"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Mert Cemri"
                    },
                    {
                        "name": "Jongseok Park"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Jeff Chen"
                    },
                    {
                        "name": "Lakshya Agrawal"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Koushik Sen"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06187v1",
                "updated": "2025-10-07T17:46:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    46,
                    33,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:46:33Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    46,
                    33,
                    1,
                    280,
                    0
                ],
                "title": "Automated Program Repair of Uncompilable Student Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Program Repair of Uncompilable Student Code"
                },
                "summary": "A significant portion of student programming submissions in CS1 learning\nenvironments are uncompilable, limiting their use in student modeling and\ndownstream knowledge tracing. Traditional modeling pipelines often exclude\nthese cases, discarding observations of student learning. This study\ninvestigates automated program repair as a strategy to recover uncompilable\ncode while preserving students' structural intent for use in student modeling.\nWithin this framework, we assess large language models (LLMs) as repair agents,\nincluding GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash\n(Google), under high- and low-context prompting conditions. Repairs were\nevaluated for compilability, edit distance, and preservation of students'\noriginal structure and logic. We find that while all three LLMs are capable of\nproducing compilable repairs, their behavior diverges in how well they preserve\nstudents' control flow and code structure, which affects their pedagogical\nutility. By recovering uncompilable submissions, this work enables richer and\nmore comprehensive analyses of learners' coding processes and development over\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A significant portion of student programming submissions in CS1 learning\nenvironments are uncompilable, limiting their use in student modeling and\ndownstream knowledge tracing. Traditional modeling pipelines often exclude\nthese cases, discarding observations of student learning. This study\ninvestigates automated program repair as a strategy to recover uncompilable\ncode while preserving students' structural intent for use in student modeling.\nWithin this framework, we assess large language models (LLMs) as repair agents,\nincluding GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash\n(Google), under high- and low-context prompting conditions. Repairs were\nevaluated for compilability, edit distance, and preservation of students'\noriginal structure and logic. We find that while all three LLMs are capable of\nproducing compilable repairs, their behavior diverges in how well they preserve\nstudents' control flow and code structure, which affects their pedagogical\nutility. By recovering uncompilable submissions, this work enables richer and\nmore comprehensive analyses of learners' coding processes and development over\ntime."
                },
                "authors": [
                    {
                        "name": "Griffin Pitts"
                    },
                    {
                        "name": "Aum Pandya"
                    },
                    {
                        "name": "Darsh Rank"
                    },
                    {
                        "name": "Tirth Bhatt"
                    },
                    {
                        "name": "Muntasir Hoq"
                    },
                    {
                        "name": "Bita Akram"
                    }
                ],
                "author_detail": {
                    "name": "Bita Akram"
                },
                "author": "Bita Akram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06186v1",
                "updated": "2025-10-07T17:45:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    45,
                    35,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:45:35Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    45,
                    35,
                    1,
                    280,
                    0
                ],
                "title": "RECODE-H: A Benchmark for Research Code Development with Interactive\n  Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RECODE-H: A Benchmark for Research Code Development with Interactive\n  Human Feedback"
                },
                "summary": "Large language models (LLMs) show the promise in supporting scientific\nresearch implementation, yet their ability to generate correct and executable\ncode remains limited. Existing works largely adopt one-shot settings, ignoring\nthe iterative and feedback-driven nature of realistic workflows of scientific\nresearch development. To address this gap, we present RECODE-H, a benchmark of\n102 tasks from research papers and repositories that evaluates LLM agents\nthrough multi-turn interactions with LLM-simulated human feedback. It includes\nstructured instructions,unit tests, and a five-level feedback hierarchy to\nreflect realistic researcher-agent collaboration. We further present\nReCodeAgent, a framework that integrates feedback into iterative code\ngeneration. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,\nDeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer\nfeedback, while also highlighting ongoing challenges in the generation of\ncomplex research code. RECODE-H establishes a foundation for developing\nadaptive, feedback-driven LLM agents in scientific research implementation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show the promise in supporting scientific\nresearch implementation, yet their ability to generate correct and executable\ncode remains limited. Existing works largely adopt one-shot settings, ignoring\nthe iterative and feedback-driven nature of realistic workflows of scientific\nresearch development. To address this gap, we present RECODE-H, a benchmark of\n102 tasks from research papers and repositories that evaluates LLM agents\nthrough multi-turn interactions with LLM-simulated human feedback. It includes\nstructured instructions,unit tests, and a five-level feedback hierarchy to\nreflect realistic researcher-agent collaboration. We further present\nReCodeAgent, a framework that integrates feedback into iterative code\ngeneration. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,\nDeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer\nfeedback, while also highlighting ongoing challenges in the generation of\ncomplex research code. RECODE-H establishes a foundation for developing\nadaptive, feedback-driven LLM agents in scientific research implementation"
                },
                "authors": [
                    {
                        "name": "Chunyu Miao"
                    },
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Yankai Chen"
                    },
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Fangxin Wang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Wooseong Yang"
                    },
                    {
                        "name": "Bowei He"
                    },
                    {
                        "name": "Xinni Zhang"
                    },
                    {
                        "name": "Dianzhi Yu"
                    },
                    {
                        "name": "Hanchen Yang"
                    },
                    {
                        "name": "Hoang H Nguyen"
                    },
                    {
                        "name": "Yue Zhou"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jizhou Guo"
                    },
                    {
                        "name": "Wenzhe Fan"
                    },
                    {
                        "name": "Chin-Yuan Yeh"
                    },
                    {
                        "name": "Panpan Meng"
                    },
                    {
                        "name": "Liancheng Fang"
                    },
                    {
                        "name": "Jinhu Qi"
                    },
                    {
                        "name": "Wei-Chieh Huang"
                    },
                    {
                        "name": "Zhengyao Gu"
                    },
                    {
                        "name": "Yuwei Han"
                    },
                    {
                        "name": "Langzhou He"
                    },
                    {
                        "name": "Yuyao Yang"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Irwin King"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "Code and dataset are available at github.com/ChunyuMiao98/RECODE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22945v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22945v2",
                "updated": "2025-10-07T17:39:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    39,
                    5,
                    1,
                    280,
                    0
                ],
                "published": "2025-05-28T23:57:03Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    23,
                    57,
                    3,
                    2,
                    148,
                    0
                ],
                "title": "OWL: Probing Cross-Lingual Recall of Memorized Texts via World\n  Literature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OWL: Probing Cross-Lingual Recall of Memorized Texts via World\n  Literature"
                },
                "summary": "Large language models (LLMs) are known to memorize and recall English text\nfrom their pretraining data. However, the extent to which this ability\ngeneralizes to non-English languages or transfers across languages remains\nunclear. This paper investigates multilingual and cross-lingual memorization in\nLLMs, probing if memorized content in one language (e.g., English) can be\nrecalled when presented in translation. To do so, we introduce OWL, a dataset\nof 31.5K aligned excerpts from 20 books in ten languages, including English\noriginals, official translations (Vietnamese, Spanish, Turkish), and new\ntranslations in six low-resource languages (Sesotho, Yoruba, Maithili,\nMalagasy, Setswana, Tahitian). We evaluate memorization across model families\nand sizes through three tasks: (1) direct probing, which asks the model to\nidentify a book's title and author; (2) name cloze, which requires predicting\nmasked character names; and (3) prefix probing, which involves generating\ncontinuations. We find that LLMs consistently recall content across languages,\neven for texts without direct translation in pretraining data. GPT-4o, for\nexample, identifies authors and titles 69% of the time and masked entities 6%\nof the time in newly translated excerpts. Perturbations (e.g., masking\ncharacters, shuffling words) modestly reduce direct probing accuracy (7% drop\nfor shuffled official translations). Our results highlight the extent of\ncross-lingual memorization and provide insights on the differences between the\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are known to memorize and recall English text\nfrom their pretraining data. However, the extent to which this ability\ngeneralizes to non-English languages or transfers across languages remains\nunclear. This paper investigates multilingual and cross-lingual memorization in\nLLMs, probing if memorized content in one language (e.g., English) can be\nrecalled when presented in translation. To do so, we introduce OWL, a dataset\nof 31.5K aligned excerpts from 20 books in ten languages, including English\noriginals, official translations (Vietnamese, Spanish, Turkish), and new\ntranslations in six low-resource languages (Sesotho, Yoruba, Maithili,\nMalagasy, Setswana, Tahitian). We evaluate memorization across model families\nand sizes through three tasks: (1) direct probing, which asks the model to\nidentify a book's title and author; (2) name cloze, which requires predicting\nmasked character names; and (3) prefix probing, which involves generating\ncontinuations. We find that LLMs consistently recall content across languages,\neven for texts without direct translation in pretraining data. GPT-4o, for\nexample, identifies authors and titles 69% of the time and masked entities 6%\nof the time in newly translated excerpts. Perturbations (e.g., masking\ncharacters, shuffling words) modestly reduce direct probing accuracy (7% drop\nfor shuffled official translations). Our results highlight the extent of\ncross-lingual memorization and provide insights on the differences between the\nmodels."
                },
                "authors": [
                    {
                        "name": "Alisha Srivastava"
                    },
                    {
                        "name": "Emir Korukluoglu"
                    },
                    {
                        "name": "Minh Nhat Le"
                    },
                    {
                        "name": "Duyen Tran"
                    },
                    {
                        "name": "Chau Minh Pham"
                    },
                    {
                        "name": "Marzena Karpinska"
                    },
                    {
                        "name": "Mohit Iyyer"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Iyyer"
                },
                "author": "Mohit Iyyer",
                "arxiv_comment": "Accepted to EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22945v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22945v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06175v1",
                "updated": "2025-10-07T17:35:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    35,
                    28,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:35:28Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    35,
                    28,
                    1,
                    280,
                    0
                ],
                "title": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via\n  Outlier-Suppressed Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via\n  Outlier-Suppressed Vector Quantization"
                },
                "summary": "The Key-Value (KV) cache introduces substantial memory overhead during large\nlanguage model (LLM) inference. Although existing vector quantization (VQ)\nmethods reduce KV cache usage and provide flexible representational capacity\nacross bit-widths, they suffer severe performance degradation at ultra-low\nbit-widths due to key cache outliers that hinder effective codebook\nutilization. To address this challenge, we propose VecInfer, a novel VQ method\nfor aggressive KV cache compression while enabling efficient inference. By\napplying smooth and Hadamard transformations, VecInfer suppresses outliers in\nthe key cache, enabling the codebook to comprehensively cover the original data\ndistribution and thereby reducing quantization difficulty. To facilitate\nefficient deployment, we design an optimized CUDA kernel that fuses computation\nwith dequantization to minimize memory access overhead. Extensive evaluations\ndemonstrate that VecInfer consistently outperforms existing quantization\nbaselines across both long-context understanding and mathematical reasoning\ntasks. With only 2-bit quantization, VecInfer achieves performance comparable\nto full precision, while delivering up to $\\mathbf{2.7\\times}$ speedup in\nlarge-batch self-attention computation and $\\mathbf{8.3\\times}$ reduction in\nsingle-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache introduces substantial memory overhead during large\nlanguage model (LLM) inference. Although existing vector quantization (VQ)\nmethods reduce KV cache usage and provide flexible representational capacity\nacross bit-widths, they suffer severe performance degradation at ultra-low\nbit-widths due to key cache outliers that hinder effective codebook\nutilization. To address this challenge, we propose VecInfer, a novel VQ method\nfor aggressive KV cache compression while enabling efficient inference. By\napplying smooth and Hadamard transformations, VecInfer suppresses outliers in\nthe key cache, enabling the codebook to comprehensively cover the original data\ndistribution and thereby reducing quantization difficulty. To facilitate\nefficient deployment, we design an optimized CUDA kernel that fuses computation\nwith dequantization to minimize memory access overhead. Extensive evaluations\ndemonstrate that VecInfer consistently outperforms existing quantization\nbaselines across both long-context understanding and mathematical reasoning\ntasks. With only 2-bit quantization, VecInfer achieves performance comparable\nto full precision, while delivering up to $\\mathbf{2.7\\times}$ speedup in\nlarge-batch self-attention computation and $\\mathbf{8.3\\times}$ reduction in\nsingle-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length."
                },
                "authors": [
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Chenxu Yang"
                    },
                    {
                        "name": "Zhengyang Tong"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06151v1",
                "updated": "2025-10-07T17:21:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    21,
                    20,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:21:20Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    21,
                    20,
                    1,
                    280,
                    0
                ],
                "title": "LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design\n  for Heterogeneous Agent Teams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design\n  for Heterogeneous Agent Teams"
                },
                "summary": "A critical challenge in modelling Heterogeneous-Agent Teams is training\nagents to collaborate with teammates whose policies are inaccessible or\nnon-stationary, such as humans. Traditional approaches rely on expensive\nhuman-in-the-loop data, which limits scalability. We propose using Large\nLanguage Models (LLMs) as policy-agnostic human proxies to generate synthetic\ndata that mimics human decision-making. To evaluate this, we conduct three\nexperiments in a grid-world capture game inspired by Stag Hunt, a game theory\nparadigm that balances risk and reward. In Experiment 1, we compare decisions\nfrom 30 human participants and 2 expert judges with outputs from LLaMA 3.1 and\nMixtral 8x22B models. LLMs, prompted with game-state observations and reward\nstructures, align more closely with experts than participants, demonstrating\nconsistency in applying underlying decision criteria. Experiment 2 modifies\nprompts to induce risk-sensitive strategies (e.g. \"be risk averse\"). LLM\noutputs mirror human participants' variability, shifting between risk-averse\nand risk-seeking behaviours. Finally, Experiment 3 tests LLMs in a dynamic\ngrid-world where the LLM agents generate movement actions. LLMs produce\ntrajectories resembling human participants' paths. While LLMs cannot yet fully\nreplicate human adaptability, their prompt-guided diversity offers a scalable\nfoundation for simulating policy-agnostic teammates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical challenge in modelling Heterogeneous-Agent Teams is training\nagents to collaborate with teammates whose policies are inaccessible or\nnon-stationary, such as humans. Traditional approaches rely on expensive\nhuman-in-the-loop data, which limits scalability. We propose using Large\nLanguage Models (LLMs) as policy-agnostic human proxies to generate synthetic\ndata that mimics human decision-making. To evaluate this, we conduct three\nexperiments in a grid-world capture game inspired by Stag Hunt, a game theory\nparadigm that balances risk and reward. In Experiment 1, we compare decisions\nfrom 30 human participants and 2 expert judges with outputs from LLaMA 3.1 and\nMixtral 8x22B models. LLMs, prompted with game-state observations and reward\nstructures, align more closely with experts than participants, demonstrating\nconsistency in applying underlying decision criteria. Experiment 2 modifies\nprompts to induce risk-sensitive strategies (e.g. \"be risk averse\"). LLM\noutputs mirror human participants' variability, shifting between risk-averse\nand risk-seeking behaviours. Finally, Experiment 3 tests LLMs in a dynamic\ngrid-world where the LLM agents generate movement actions. LLMs produce\ntrajectories resembling human participants' paths. While LLMs cannot yet fully\nreplicate human adaptability, their prompt-guided diversity offers a scalable\nfoundation for simulating policy-agnostic teammates."
                },
                "authors": [
                    {
                        "name": "Aju Ani Justus"
                    },
                    {
                        "name": "Chris Baber"
                    }
                ],
                "author_detail": {
                    "name": "Chris Baber"
                },
                "author": "Chris Baber",
                "arxiv_comment": "This is a preprint of a paper presented at the \\textit{European\n  Conference on Artificial Intelligence (ECAI 2025)}. It is made publicly\n  available for the benefit of the research community and should be regarded as\n  a preprint rather than a formally reviewed publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03498v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03498v3",
                "updated": "2025-10-07T17:20:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    20,
                    52,
                    1,
                    280,
                    0
                ],
                "published": "2025-09-03T17:29:50Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    29,
                    50,
                    2,
                    246,
                    0
                ],
                "title": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and\n  Generation"
                },
                "summary": "We introduce OneCAT, a unified multimodal model that seamlessly integrates\nunderstanding, generation, and editing within a novel, pure decoder-only\ntransformer architecture. Our framework uniquely eliminates the need for\nexternal components such as Vision Transformers (ViT) or vision tokenizer\nduring inference, leading to significant efficiency gains, especially for\nhigh-resolution inputs. This is achieved through a modality-specific\nMixture-of-Experts (MoE) structure trained with a single autoregressive (AR)\nobjective, which also natively supports dynamic resolutions. Furthermore, we\npioneer a multi-scale visual autoregressive mechanism within the Large Language\nModel (LLM) that drastically reduces decoding steps compared to diffusion-based\nmethods while maintaining state-of-the-art performance. Our findings\ndemonstrate the powerful potential of pure autoregressive modeling as a\nsufficient and elegant foundation for unified multimodal intelligence. As a\nresult, OneCAT sets a new performance standard, outperforming existing\nopen-source unified multimodal models across benchmarks for multimodal\ngeneration, editing, and understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce OneCAT, a unified multimodal model that seamlessly integrates\nunderstanding, generation, and editing within a novel, pure decoder-only\ntransformer architecture. Our framework uniquely eliminates the need for\nexternal components such as Vision Transformers (ViT) or vision tokenizer\nduring inference, leading to significant efficiency gains, especially for\nhigh-resolution inputs. This is achieved through a modality-specific\nMixture-of-Experts (MoE) structure trained with a single autoregressive (AR)\nobjective, which also natively supports dynamic resolutions. Furthermore, we\npioneer a multi-scale visual autoregressive mechanism within the Large Language\nModel (LLM) that drastically reduces decoding steps compared to diffusion-based\nmethods while maintaining state-of-the-art performance. Our findings\ndemonstrate the powerful potential of pure autoregressive modeling as a\nsufficient and elegant foundation for unified multimodal intelligence. As a\nresult, OneCAT sets a new performance standard, outperforming existing\nopen-source unified multimodal models across benchmarks for multimodal\ngeneration, editing, and understanding."
                },
                "authors": [
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Xinyu Peng"
                    },
                    {
                        "name": "Yaoming Wang"
                    },
                    {
                        "name": "Zelin Peng"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Wenrui Dai"
                    },
                    {
                        "name": "Hongkai Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hongkai Xiong"
                },
                "author": "Hongkai Xiong",
                "arxiv_comment": "technical report, project url:https://onecat-ai.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03498v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03498v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15510v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15510v4",
                "updated": "2025-10-07T17:20:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    20,
                    30,
                    1,
                    280,
                    0
                ],
                "published": "2024-08-28T03:45:49Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    45,
                    49,
                    2,
                    241,
                    0
                ],
                "title": "How Reliable are Causal Probing Interventions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Reliable are Causal Probing Interventions?"
                },
                "summary": "Causal probing aims to analyze foundation models by examining how intervening\non their representation of various latent properties impacts their outputs.\nRecent works have cast doubt on the theoretical basis of several leading causal\nprobing methods, but it has been unclear how to systematically evaluate the\neffectiveness of these methods in practice. To address this, we define two key\ncausal probing desiderata: completeness (how thoroughly the representation of\nthe target property has been transformed) and selectivity (how little\nnon-targeted properties have been impacted). We find that there is an inherent\ntradeoff between the two, which we define as reliability, their harmonic mean.\nWe introduce an empirical analysis framework to measure and evaluate these\nquantities, allowing us to make the first direct comparisons between different\nfamilies of leading causal probing methods (e.g., linear vs. nonlinear, or\nconcept removal vs. counterfactual interventions). We find that: (1) all\nmethods show a clear tradeoff between completeness and selectivity; (2) more\ncomplete and reliable methods have a greater impact on LLM behavior; and (3)\nnonlinear interventions are almost always more reliable than linear\ninterventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal probing aims to analyze foundation models by examining how intervening\non their representation of various latent properties impacts their outputs.\nRecent works have cast doubt on the theoretical basis of several leading causal\nprobing methods, but it has been unclear how to systematically evaluate the\neffectiveness of these methods in practice. To address this, we define two key\ncausal probing desiderata: completeness (how thoroughly the representation of\nthe target property has been transformed) and selectivity (how little\nnon-targeted properties have been impacted). We find that there is an inherent\ntradeoff between the two, which we define as reliability, their harmonic mean.\nWe introduce an empirical analysis framework to measure and evaluate these\nquantities, allowing us to make the first direct comparisons between different\nfamilies of leading causal probing methods (e.g., linear vs. nonlinear, or\nconcept removal vs. counterfactual interventions). We find that: (1) all\nmethods show a clear tradeoff between completeness and selectivity; (2) more\ncomplete and reliable methods have a greater impact on LLM behavior; and (3)\nnonlinear interventions are almost always more reliable than linear\ninterventions."
                },
                "authors": [
                    {
                        "name": "Marc Canby"
                    },
                    {
                        "name": "Adam Davies"
                    },
                    {
                        "name": "Chirag Rastogi"
                    },
                    {
                        "name": "Julia Hockenmaier"
                    }
                ],
                "author_detail": {
                    "name": "Julia Hockenmaier"
                },
                "author": "Julia Hockenmaier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15510v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15510v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03408v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03408v2",
                "updated": "2025-10-07T17:20:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    20,
                    13,
                    1,
                    280,
                    0
                ],
                "published": "2025-06-03T21:36:56Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    21,
                    36,
                    56,
                    1,
                    154,
                    0
                ],
                "title": "Trajectory Prediction Meets Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Prediction Meets Large Language Models: A Survey"
                },
                "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin integrating language-driven techniques into trajectory prediction. By\nleveraging their semantic and reasoning capabilities, LLMs are reshaping how\nautonomous systems perceive, model, and predict trajectories. This survey\nprovides a comprehensive overview of this emerging field, categorizing recent\nwork into five directions: (1) Trajectory prediction via language modeling\nparadigms, (2) Direct trajectory prediction with pretrained language models,\n(3) Language-guided scene understanding for trajectory prediction, (4)\nLanguage-driven data generation for trajectory prediction, (5) Language-based\nreasoning and interpretability for trajectory prediction. For each, we analyze\nrepresentative methods, highlight core design choices, and identify open\nchallenges. This survey bridges natural language processing and trajectory\nprediction, offering a unified perspective on how language can enrich\ntrajectory prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have sparked growing interest\nin integrating language-driven techniques into trajectory prediction. By\nleveraging their semantic and reasoning capabilities, LLMs are reshaping how\nautonomous systems perceive, model, and predict trajectories. This survey\nprovides a comprehensive overview of this emerging field, categorizing recent\nwork into five directions: (1) Trajectory prediction via language modeling\nparadigms, (2) Direct trajectory prediction with pretrained language models,\n(3) Language-guided scene understanding for trajectory prediction, (4)\nLanguage-driven data generation for trajectory prediction, (5) Language-based\nreasoning and interpretability for trajectory prediction. For each, we analyze\nrepresentative methods, highlight core design choices, and identify open\nchallenges. This survey bridges natural language processing and trajectory\nprediction, offering a unified perspective on how language can enrich\ntrajectory prediction."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Ruining Yang"
                    },
                    {
                        "name": "Yitian Zhang"
                    },
                    {
                        "name": "Jianglin Lu"
                    },
                    {
                        "name": "Mingyuan Zhang"
                    },
                    {
                        "name": "Yizhou Wang"
                    },
                    {
                        "name": "Lili Su"
                    },
                    {
                        "name": "Yun Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yun Fu"
                },
                "author": "Yun Fu",
                "arxiv_comment": "16 pages, GitHub:\n  https://github.com/colorfulfuture/Awesome-Trajectory-Motion-Prediction-Papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03408v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03408v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06143v1",
                "updated": "2025-10-07T17:17:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    17,
                    14,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:17:14Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    17,
                    14,
                    1,
                    280,
                    0
                ],
                "title": "RoSE: Round-robin Synthetic Data Evaluation for Selecting LLM Generators\n  without Human Test Sets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSE: Round-robin Synthetic Data Evaluation for Selecting LLM Generators\n  without Human Test Sets"
                },
                "summary": "LLMs are powerful generators of synthetic data, which are used for training\nsmaller, specific models. This is especially valuable for low-resource\nlanguages, where human-labelled data is scarce but LLMs can still produce\nhigh-quality text. However, LLMs differ in how useful their outputs are for\ntraining. Selecting the best LLM as a generator is challenging because\nextrinsic evaluation requires costly human annotations (which are often\nunavailable for low-resource languages), while intrinsic metrics correlate\npoorly with downstream performance. We introduce Round robin Synthetic data\nEvaluation (RoSE), a proxy metric for selecting the best LLM generator without\nhuman test sets. RoSE trains a small model on the outputs of a candidate\ngenerator (LLM) and then evaluates it on generated synthetic examples from all\nother candidate LLMs. The final RoSE score is the mean performance of this\nsmall model. Across six LLMs, eleven languages, and three tasks (sentiment,\ntopic, intent), RoSE identifies the optimal generator more often than any other\nintrinsic heuristics. RoSE outperforms intrinsic heuristics and comes within\n0.76 percentage points of the optimal generator baseline. This result is\nmeasured in terms of downstream performance, obtained by training a small model\non the chosen generator's outputs (optimal vs. proxy metric selected) and\nevaluating it on human-labelled test data. Additionally, RoSE is the only\nmetric to achieve a positive correlation with performance on human test data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are powerful generators of synthetic data, which are used for training\nsmaller, specific models. This is especially valuable for low-resource\nlanguages, where human-labelled data is scarce but LLMs can still produce\nhigh-quality text. However, LLMs differ in how useful their outputs are for\ntraining. Selecting the best LLM as a generator is challenging because\nextrinsic evaluation requires costly human annotations (which are often\nunavailable for low-resource languages), while intrinsic metrics correlate\npoorly with downstream performance. We introduce Round robin Synthetic data\nEvaluation (RoSE), a proxy metric for selecting the best LLM generator without\nhuman test sets. RoSE trains a small model on the outputs of a candidate\ngenerator (LLM) and then evaluates it on generated synthetic examples from all\nother candidate LLMs. The final RoSE score is the mean performance of this\nsmall model. Across six LLMs, eleven languages, and three tasks (sentiment,\ntopic, intent), RoSE identifies the optimal generator more often than any other\nintrinsic heuristics. RoSE outperforms intrinsic heuristics and comes within\n0.76 percentage points of the optimal generator baseline. This result is\nmeasured in terms of downstream performance, obtained by training a small model\non the chosen generator's outputs (optimal vs. proxy metric selected) and\nevaluating it on human-labelled test data. Additionally, RoSE is the only\nmetric to achieve a positive correlation with performance on human test data."
                },
                "authors": [
                    {
                        "name": "Jan Cegin"
                    },
                    {
                        "name": "Branislav Pecher"
                    },
                    {
                        "name": "Ivan Srba"
                    },
                    {
                        "name": "Jakub Simko"
                    }
                ],
                "author_detail": {
                    "name": "Jakub Simko"
                },
                "author": "Jakub Simko",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09947v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09947v2",
                "updated": "2025-10-07T17:06:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    6,
                    21,
                    1,
                    280,
                    0
                ],
                "published": "2025-09-12T03:38:15Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    3,
                    38,
                    15,
                    4,
                    255,
                    0
                ],
                "title": "Toward Green Code: Prompting Small Language Models for Energy-Efficient\n  Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Green Code: Prompting Small Language Models for Energy-Efficient\n  Code Generation"
                },
                "summary": "There is a growing concern about the environmental impact of large language\nmodels (LLMs) in software development, particularly due to their high energy\nuse and carbon footprint. Small Language Models (SLMs) offer a more sustainable\nalternative, requiring fewer computational resources while remaining effective\nfor fundamental programming tasks. In this study, we investigate whether prompt\nengineering can improve the energy efficiency of SLMs in code generation. We\nevaluate four open-source SLMs, StableCode-Instruct-3B,\nQwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, and Phi-3-Mini-4K-Instruct,\nacross 150 Python problems from LeetCode, evenly distributed into easy, medium,\nand hard categories. Each model is tested under four prompting strategies: role\nprompting, zero-shot, few-shot, and chain-of-thought (CoT). For every generated\nsolution, we measure runtime, memory usage, and energy consumption, comparing\nthe results with a human-written baseline. Our findings show that CoT prompting\nprovides consistent energy savings for Qwen2.5-Coder and StableCode-3B, while\nCodeLlama-7B and Phi-3-Mini-4K fail to outperform the baseline under any\nprompting strategy. These results highlight that the benefits of prompting are\nmodel-dependent and that carefully designed prompts can guide SLMs toward\ngreener software development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing concern about the environmental impact of large language\nmodels (LLMs) in software development, particularly due to their high energy\nuse and carbon footprint. Small Language Models (SLMs) offer a more sustainable\nalternative, requiring fewer computational resources while remaining effective\nfor fundamental programming tasks. In this study, we investigate whether prompt\nengineering can improve the energy efficiency of SLMs in code generation. We\nevaluate four open-source SLMs, StableCode-Instruct-3B,\nQwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, and Phi-3-Mini-4K-Instruct,\nacross 150 Python problems from LeetCode, evenly distributed into easy, medium,\nand hard categories. Each model is tested under four prompting strategies: role\nprompting, zero-shot, few-shot, and chain-of-thought (CoT). For every generated\nsolution, we measure runtime, memory usage, and energy consumption, comparing\nthe results with a human-written baseline. Our findings show that CoT prompting\nprovides consistent energy savings for Qwen2.5-Coder and StableCode-3B, while\nCodeLlama-7B and Phi-3-Mini-4K fail to outperform the baseline under any\nprompting strategy. These results highlight that the benefits of prompting are\nmodel-dependent and that carefully designed prompts can guide SLMs toward\ngreener software development."
                },
                "authors": [
                    {
                        "name": "Humza Ashraf"
                    },
                    {
                        "name": "Syed Muhammad Danish"
                    },
                    {
                        "name": "Shadikur Rahman"
                    },
                    {
                        "name": "Zeeshan Sattar"
                    }
                ],
                "author_detail": {
                    "name": "Zeeshan Sattar"
                },
                "author": "Zeeshan Sattar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09947v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09947v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06126v1",
                "updated": "2025-10-07T17:05:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    5,
                    30,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:05:30Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    5,
                    30,
                    1,
                    280,
                    0
                ],
                "title": "lm-Meter: Unveiling Runtime Inference Latency for On-Device Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "lm-Meter: Unveiling Runtime Inference Latency for On-Device Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly integrated into everyday\napplications, but their prevalent cloud-based deployment raises growing\nconcerns around data privacy and long-term sustainability. Running LLMs locally\non mobile and edge devices (on-device LLMs) offers the promise of enhanced\nprivacy, reliability, and reduced communication costs. However, realizing this\nvision remains challenging due to substantial memory and compute demands, as\nwell as limited visibility into performance-efficiency trade-offs on\nresource-constrained hardware. We propose lm-Meter, the first lightweight,\nonline latency profiler tailored for on-device LLM inference. lm-Meter captures\nfine-grained, real-time latency at both phase (e.g., embedding, prefill,\ndecode, softmax, sampling) and kernel levels without auxiliary devices. We\nimplement lm-Meter on commercial mobile platforms and demonstrate its high\nprofiling accuracy with minimal system overhead, e.g., only 2.58% throughput\nreduction in prefill and 0.99% in decode under the most constrained Powersave\ngovernor. Leveraging lm-Meter, we conduct comprehensive empirical studies\nrevealing phase- and kernel-level bottlenecks in on-device LLM inference,\nquantifying accuracy-efficiency trade-offs, and identifying systematic\noptimization opportunities. lm-Meter provides unprecedented visibility into the\nruntime behavior of LLMs on constrained platforms, laying the foundation for\ninformed optimization and accelerating the democratization of on-device LLM\nsystems. Code and tutorials are available at\nhttps://github.com/amai-gsu/LM-Meter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly integrated into everyday\napplications, but their prevalent cloud-based deployment raises growing\nconcerns around data privacy and long-term sustainability. Running LLMs locally\non mobile and edge devices (on-device LLMs) offers the promise of enhanced\nprivacy, reliability, and reduced communication costs. However, realizing this\nvision remains challenging due to substantial memory and compute demands, as\nwell as limited visibility into performance-efficiency trade-offs on\nresource-constrained hardware. We propose lm-Meter, the first lightweight,\nonline latency profiler tailored for on-device LLM inference. lm-Meter captures\nfine-grained, real-time latency at both phase (e.g., embedding, prefill,\ndecode, softmax, sampling) and kernel levels without auxiliary devices. We\nimplement lm-Meter on commercial mobile platforms and demonstrate its high\nprofiling accuracy with minimal system overhead, e.g., only 2.58% throughput\nreduction in prefill and 0.99% in decode under the most constrained Powersave\ngovernor. Leveraging lm-Meter, we conduct comprehensive empirical studies\nrevealing phase- and kernel-level bottlenecks in on-device LLM inference,\nquantifying accuracy-efficiency trade-offs, and identifying systematic\noptimization opportunities. lm-Meter provides unprecedented visibility into the\nruntime behavior of LLMs on constrained platforms, laying the foundation for\ninformed optimization and accelerating the democratization of on-device LLM\nsystems. Code and tutorials are available at\nhttps://github.com/amai-gsu/LM-Meter."
                },
                "authors": [
                    {
                        "name": "Haoxin Wang"
                    },
                    {
                        "name": "Xiaolong Tu"
                    },
                    {
                        "name": "Hongyu Ke"
                    },
                    {
                        "name": "Huirong Chai"
                    },
                    {
                        "name": "Dawei Chen"
                    },
                    {
                        "name": "Kyungtae Han"
                    }
                ],
                "author_detail": {
                    "name": "Kyungtae Han"
                },
                "author": "Kyungtae Han",
                "arxiv_doi": "10.1145/3769012.3770614",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3769012.3770614",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.06126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is the preprint version of the paper accepted to The 10th\n  ACM/IEEE Symposium on Edge Computing (SEC 2025)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17242v2",
                "updated": "2025-10-07T16:58:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    58,
                    55,
                    1,
                    280,
                    0
                ],
                "published": "2025-05-22T19:43:37Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    19,
                    43,
                    37,
                    3,
                    142,
                    0
                ],
                "title": "Optimal Policy Minimum Bayesian Risk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Policy Minimum Bayesian Risk"
                },
                "summary": "Inference scaling helps LLMs solve complex reasoning problems through\nextended runtime computation. On top of long chain-of-thought (long-CoT)\nmodels, purely inference-time techniques such as best-of-N (BoN) sampling,\nmajority voting, or more generally, minimum Bayes risk decoding (MBRD), can\nfurther improve LLM accuracy by generating multiple candidate solutions and\naggregating over them. These methods typically leverage additional signals in\nthe form of reward models and risk/similarity functions that compare generated\nsamples, e.g., exact match in some normalized space or standard similarity\nmetrics such as Rouge. Here we present a novel method for incorporating reward\nand risk/similarity signals into MBRD. Based on the concept of optimal policy\nin KL-controlled reinforcement learning, our framework provides a simple and\nwell-defined mechanism for leveraging such signals, offering several advantages\nover traditional inference-time methods: higher robustness, improved accuracy,\nand well-understood asymptotic behavior. In addition, it allows for the\ndevelopment of a sample-efficient variant of MBRD that can adjust the number of\nsamples to generate according to the difficulty of the problem, without relying\non majority vote counts. We empirically demonstrate the advantages of our\napproach on math (MATH-$500$) and coding (HumanEval) tasks using recent\nopen-source models. We also present a comprehensive analysis of its\naccuracy-compute trade-offs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference scaling helps LLMs solve complex reasoning problems through\nextended runtime computation. On top of long chain-of-thought (long-CoT)\nmodels, purely inference-time techniques such as best-of-N (BoN) sampling,\nmajority voting, or more generally, minimum Bayes risk decoding (MBRD), can\nfurther improve LLM accuracy by generating multiple candidate solutions and\naggregating over them. These methods typically leverage additional signals in\nthe form of reward models and risk/similarity functions that compare generated\nsamples, e.g., exact match in some normalized space or standard similarity\nmetrics such as Rouge. Here we present a novel method for incorporating reward\nand risk/similarity signals into MBRD. Based on the concept of optimal policy\nin KL-controlled reinforcement learning, our framework provides a simple and\nwell-defined mechanism for leveraging such signals, offering several advantages\nover traditional inference-time methods: higher robustness, improved accuracy,\nand well-understood asymptotic behavior. In addition, it allows for the\ndevelopment of a sample-efficient variant of MBRD that can adjust the number of\nsamples to generate according to the difficulty of the problem, without relying\non majority vote counts. We empirically demonstrate the advantages of our\napproach on math (MATH-$500$) and coding (HumanEval) tasks using recent\nopen-source models. We also present a comprehensive analysis of its\naccuracy-compute trade-offs."
                },
                "authors": [
                    {
                        "name": "Ramón Fernandez Astudillo"
                    },
                    {
                        "name": "Md Arafat Sultan"
                    },
                    {
                        "name": "Aashka Trivedi"
                    },
                    {
                        "name": "Yousef El-Kurdi"
                    },
                    {
                        "name": "Tahira Naseem"
                    },
                    {
                        "name": "Radu Florian"
                    },
                    {
                        "name": "Salim Roukos"
                    }
                ],
                "author_detail": {
                    "name": "Salim Roukos"
                },
                "author": "Salim Roukos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06108v1",
                "updated": "2025-10-07T16:40:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    40,
                    42,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:40:42Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    40,
                    42,
                    1,
                    280,
                    0
                ],
                "title": "Influence Functions for Efficient Data Selection in Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence Functions for Efficient Data Selection in Reasoning"
                },
                "summary": "Fine-tuning large language models (LLMs) on chain-of-thought (CoT) data shows\nthat a small amount of high-quality data can outperform massive datasets. Yet,\nwhat constitutes \"quality\" remains ill-defined. Existing reasoning methods rely\non indirect heuristics such as problem difficulty or trace length, while\ninstruction-tuning has explored a broader range of automated selection\nstrategies, but rarely in the context of reasoning. We propose to define\nreasoning data quality using influence functions, which measure the causal\neffect of individual CoT examples on downstream accuracy, and introduce\ninfluence-based pruning, which consistently outperforms perplexity and\nembedding-based baselines on math reasoning within a model family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on chain-of-thought (CoT) data shows\nthat a small amount of high-quality data can outperform massive datasets. Yet,\nwhat constitutes \"quality\" remains ill-defined. Existing reasoning methods rely\non indirect heuristics such as problem difficulty or trace length, while\ninstruction-tuning has explored a broader range of automated selection\nstrategies, but rarely in the context of reasoning. We propose to define\nreasoning data quality using influence functions, which measure the causal\neffect of individual CoT examples on downstream accuracy, and introduce\ninfluence-based pruning, which consistently outperforms perplexity and\nembedding-based baselines on math reasoning within a model family."
                },
                "authors": [
                    {
                        "name": "Prateek Humane"
                    },
                    {
                        "name": "Paolo Cudrano"
                    },
                    {
                        "name": "Daniel Z. Kaplan"
                    },
                    {
                        "name": "Matteo Matteucci"
                    },
                    {
                        "name": "Supriyo Chakraborty"
                    },
                    {
                        "name": "Irina Rish"
                    }
                ],
                "author_detail": {
                    "name": "Irina Rish"
                },
                "author": "Irina Rish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06107v1",
                "updated": "2025-10-07T16:40:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    40,
                    31,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:40:31Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    40,
                    31,
                    1,
                    280,
                    0
                ],
                "title": "Distributional Semantics Tracing: A Framework for Explaining\n  Hallucinations in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributional Semantics Tracing: A Framework for Explaining\n  Hallucinations in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are prone to hallucination, the generation of\nplausible yet factually incorrect statements. This work investigates the\nintrinsic, architectural origins of this failure mode through three primary\ncontributions.First, to enable the reliable tracing of internal semantic\nfailures, we propose \\textbf{Distributional Semantics Tracing (DST)}, a unified\nframework that integrates established interpretability techniques to produce a\ncausal map of a model's reasoning, treating meaning as a function of context\n(distributional semantics). Second, we pinpoint the model's layer at which a\nhallucination becomes inevitable, identifying a specific \\textbf{commitment\nlayer} where a model's internal representations irreversibly diverge from\nfactuality. Third, we identify the underlying mechanism for these failures. We\nobserve a conflict between distinct computational pathways, which we interpret\nusing the lens of dual-process theory: a fast, heuristic \\textbf{associative\npathway} (akin to System 1) and a slow, deliberate \\textbf{contextual pathway}\n(akin to System 2), leading to predictable failure modes such as\n\\textit{Reasoning Shortcut Hijacks}. Our framework's ability to quantify the\ncoherence of the contextual pathway reveals a strong negative correlation\n($\\rho = -0.863$) with hallucination rates, implying that these failures are\npredictable consequences of internal semantic weakness. The result is a\nmechanistic account of how, when, and why hallucinations occur within the\nTransformer architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are prone to hallucination, the generation of\nplausible yet factually incorrect statements. This work investigates the\nintrinsic, architectural origins of this failure mode through three primary\ncontributions.First, to enable the reliable tracing of internal semantic\nfailures, we propose \\textbf{Distributional Semantics Tracing (DST)}, a unified\nframework that integrates established interpretability techniques to produce a\ncausal map of a model's reasoning, treating meaning as a function of context\n(distributional semantics). Second, we pinpoint the model's layer at which a\nhallucination becomes inevitable, identifying a specific \\textbf{commitment\nlayer} where a model's internal representations irreversibly diverge from\nfactuality. Third, we identify the underlying mechanism for these failures. We\nobserve a conflict between distinct computational pathways, which we interpret\nusing the lens of dual-process theory: a fast, heuristic \\textbf{associative\npathway} (akin to System 1) and a slow, deliberate \\textbf{contextual pathway}\n(akin to System 2), leading to predictable failure modes such as\n\\textit{Reasoning Shortcut Hijacks}. Our framework's ability to quantify the\ncoherence of the contextual pathway reveals a strong negative correlation\n($\\rho = -0.863$) with hallucination rates, implying that these failures are\npredictable consequences of internal semantic weakness. The result is a\nmechanistic account of how, when, and why hallucinations occur within the\nTransformer architecture."
                },
                "authors": [
                    {
                        "name": "Gagan Bhatia"
                    },
                    {
                        "name": "Somayajulu G Sripada"
                    },
                    {
                        "name": "Kevin Allan"
                    },
                    {
                        "name": "Jacobo Azcona"
                    }
                ],
                "author_detail": {
                    "name": "Jacobo Azcona"
                },
                "author": "Jacobo Azcona",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06105v1",
                "updated": "2025-10-07T16:37:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    37,
                    15,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:37:15Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    37,
                    15,
                    1,
                    280,
                    0
                ],
                "title": "Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences"
                },
                "summary": "Large language models (LLMs) are increasingly shaping how information is\ncreated and disseminated, from companies using them to craft persuasive\nadvertisements, to election campaigns optimizing messaging to gain votes, to\nsocial media influencers boosting engagement. These settings are inherently\ncompetitive, with sellers, candidates, and influencers vying for audience\napproval, yet it remains poorly understood how competitive feedback loops\ninfluence LLM behavior. We show that optimizing LLMs for competitive success\ncan inadvertently drive misalignment. Using simulated environments across these\nscenarios, we find that, 6.3% increase in sales is accompanied by a 14.0% rise\nin deceptive marketing; in elections, a 4.9% gain in vote share coincides with\n22.3% more disinformation and 12.5% more populist rhetoric; and on social\nmedia, a 7.5% engagement boost comes with 188.6% more disinformation and a\n16.3% increase in promotion of harmful behaviors. We call this phenomenon\nMoloch's Bargain for AI--competitive success achieved at the cost of alignment.\nThese misaligned behaviors emerge even when models are explicitly instructed to\nremain truthful and grounded, revealing the fragility of current alignment\nsafeguards. Our findings highlight how market-driven optimization pressures can\nsystematically erode alignment, creating a race to the bottom, and suggest that\nsafe deployment of AI systems will require stronger governance and carefully\ndesigned incentives to prevent competitive dynamics from undermining societal\ntrust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly shaping how information is\ncreated and disseminated, from companies using them to craft persuasive\nadvertisements, to election campaigns optimizing messaging to gain votes, to\nsocial media influencers boosting engagement. These settings are inherently\ncompetitive, with sellers, candidates, and influencers vying for audience\napproval, yet it remains poorly understood how competitive feedback loops\ninfluence LLM behavior. We show that optimizing LLMs for competitive success\ncan inadvertently drive misalignment. Using simulated environments across these\nscenarios, we find that, 6.3% increase in sales is accompanied by a 14.0% rise\nin deceptive marketing; in elections, a 4.9% gain in vote share coincides with\n22.3% more disinformation and 12.5% more populist rhetoric; and on social\nmedia, a 7.5% engagement boost comes with 188.6% more disinformation and a\n16.3% increase in promotion of harmful behaviors. We call this phenomenon\nMoloch's Bargain for AI--competitive success achieved at the cost of alignment.\nThese misaligned behaviors emerge even when models are explicitly instructed to\nremain truthful and grounded, revealing the fragility of current alignment\nsafeguards. Our findings highlight how market-driven optimization pressures can\nsystematically erode alignment, creating a race to the bottom, and suggest that\nsafe deployment of AI systems will require stronger governance and carefully\ndesigned incentives to prevent competitive dynamics from undermining societal\ntrust."
                },
                "authors": [
                    {
                        "name": "Batu El"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "author": "James Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06104v1",
                "updated": "2025-10-07T16:36:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    36,
                    1,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:36:01Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    36,
                    1,
                    1,
                    280,
                    0
                ],
                "title": "Explaining Code Risk in OSS: Towards LLM-Generated Fault Prediction\n  Interpretations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining Code Risk in OSS: Towards LLM-Generated Fault Prediction\n  Interpretations"
                },
                "summary": "Open Source Software (OSS) has become a very important and crucial\ninfrastructure worldwide because of the value it provides. OSS typically\ndepends on contributions from developers across diverse backgrounds and levels\nof experience. Making safe changes, such as fixing a bug or implementing a new\nfeature, can be challenging, especially in object-oriented systems where\ncomponents are interdependent. Static analysis and defect-prediction tools\nproduce metrics (e.g., complexity,coupling) that flag potentially fault-prone\ncomponents, but these signals are often hard for contributors new or unfamiliar\nwith the codebase to interpret. Large Language Models (LLMs) have shown strong\nperformance on software engineering tasks such as code summarization and\ndocumentation generation. Building on this progress, we investigate whether\nLLMs can translate fault-prediction metrics into clear, human-readable risk\nexplanations and actionable guidance to help OSS contributors plan and review\ncode modifications. We outline explanation types that an LLM-generated\nassistant could provide (descriptive, contextual, and actionable explanations).\nWe also outline our next steps to assess usefulness through a task-based study\nwith OSS contributors, comparing metric-only baselines to LLM-generated\nexplanations on decision quality, time-to-completion, and error rates",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Source Software (OSS) has become a very important and crucial\ninfrastructure worldwide because of the value it provides. OSS typically\ndepends on contributions from developers across diverse backgrounds and levels\nof experience. Making safe changes, such as fixing a bug or implementing a new\nfeature, can be challenging, especially in object-oriented systems where\ncomponents are interdependent. Static analysis and defect-prediction tools\nproduce metrics (e.g., complexity,coupling) that flag potentially fault-prone\ncomponents, but these signals are often hard for contributors new or unfamiliar\nwith the codebase to interpret. Large Language Models (LLMs) have shown strong\nperformance on software engineering tasks such as code summarization and\ndocumentation generation. Building on this progress, we investigate whether\nLLMs can translate fault-prediction metrics into clear, human-readable risk\nexplanations and actionable guidance to help OSS contributors plan and review\ncode modifications. We outline explanation types that an LLM-generated\nassistant could provide (descriptive, contextual, and actionable explanations).\nWe also outline our next steps to assess usefulness through a task-based study\nwith OSS contributors, comparing metric-only baselines to LLM-generated\nexplanations on decision quality, time-to-completion, and error rates"
                },
                "authors": [
                    {
                        "name": "Elijah Kayode Adejumo"
                    },
                    {
                        "name": "Brittany Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Brittany Johnson"
                },
                "author": "Brittany Johnson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06101v1",
                "updated": "2025-10-07T16:32:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    32,
                    9,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:32:09Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    32,
                    9,
                    1,
                    280,
                    0
                ],
                "title": "The Valley of Code Reasoning: Scaling Knowledge Distillation of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Valley of Code Reasoning: Scaling Knowledge Distillation of Large\n  Language Models"
                },
                "summary": "Distilling the thinking traces of a Large Language Model (LLM) with reasoning\ncapabilities into a smaller model has been proven effective. Yet, there is a\nscarcity of work done on how model performances scale with the quantity of\ndistillation data. In this work, we study the scaling trend of distilling\ncompetitive coding skills on two small non-reasoning LLMs. We validate the\nhypothesis that there is a $\\textit{valley of code reasoning}$: downstream\nperformance on competitive coding first drops as data quantity increases, then\nit steadily increases in a sharper-than-log-linear fashion. Having identified\nthe trend, we further fine-tune the models at two different distillation stages\non the same data to ground conclusions on their respective learning phases. We\nlearn that across stages in the low and medium-low data regimes, small models\nbenefit significantly from easier coding questions than from harder ones. We\nalso find that, surprisingly, the correctness of outputs in training data makes\nno difference to distillation outcomes. Our work represents a step forward in\nunderstanding the training dynamics of code reasoning distillation outside\nintuition",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling the thinking traces of a Large Language Model (LLM) with reasoning\ncapabilities into a smaller model has been proven effective. Yet, there is a\nscarcity of work done on how model performances scale with the quantity of\ndistillation data. In this work, we study the scaling trend of distilling\ncompetitive coding skills on two small non-reasoning LLMs. We validate the\nhypothesis that there is a $\\textit{valley of code reasoning}$: downstream\nperformance on competitive coding first drops as data quantity increases, then\nit steadily increases in a sharper-than-log-linear fashion. Having identified\nthe trend, we further fine-tune the models at two different distillation stages\non the same data to ground conclusions on their respective learning phases. We\nlearn that across stages in the low and medium-low data regimes, small models\nbenefit significantly from easier coding questions than from harder ones. We\nalso find that, surprisingly, the correctness of outputs in training data makes\nno difference to distillation outcomes. Our work represents a step forward in\nunderstanding the training dynamics of code reasoning distillation outside\nintuition"
                },
                "authors": [
                    {
                        "name": "Muyu He"
                    },
                    {
                        "name": "Muhammad Ali Shafique"
                    },
                    {
                        "name": "Anand Kumar"
                    },
                    {
                        "name": "Tsach Mackey"
                    },
                    {
                        "name": "Nazneen Rajani"
                    }
                ],
                "author_detail": {
                    "name": "Nazneen Rajani"
                },
                "author": "Nazneen Rajani",
                "arxiv_comment": "NeurIPS 2025 Workshop on Deep Learning for Code (DL4C), Project page:\n  https://collinear.ai/valley-of-reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06096v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06096v2",
                "updated": "2025-10-08T10:07:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    10,
                    7,
                    14,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-07T16:25:14Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    25,
                    14,
                    1,
                    280,
                    0
                ],
                "title": "The Alignment Auditor: A Bayesian Framework for Verifying and Refining\n  LLM Objectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Alignment Auditor: A Bayesian Framework for Verifying and Refining\n  LLM Objectives"
                },
                "summary": "The objectives that Large Language Models (LLMs) implicitly optimize remain\ndangerously opaque, making trustworthy alignment and auditing a grand\nchallenge. While Inverse Reinforcement Learning (IRL) can infer reward\nfunctions from behaviour, existing approaches either produce a single,\noverconfident reward estimate or fail to address the fundamental ambiguity of\nthe task (non-identifiability). This paper introduces a principled auditing\nframework that re-frames reward inference from a simple estimation task to a\ncomprehensive process for verification. Our framework leverages Bayesian IRL to\nnot only recover a distribution over objectives but to enable three critical\naudit capabilities: (i) Quantifying and systematically reducing\nnon-identifiability by demonstrating posterior contraction over sequential\nrounds of evidence; (ii) Providing actionable, uncertainty-aware diagnostics\nthat expose spurious shortcuts and identify out-of-distribution prompts where\nthe inferred objective cannot be trusted; and (iii) Validating policy-level\nutility by showing that the refined, low-uncertainty reward can be used\ndirectly in RLHF to achieve training dynamics and toxicity reductions\ncomparable to the ground-truth alignment process. Empirically, our framework\nsuccessfully audits a detoxified LLM, yielding a well-calibrated and\ninterpretable objective that strengthens alignment guarantees. Overall, this\nwork provides a practical toolkit for auditors, safety teams, and regulators to\nverify what LLMs are truly trying to achieve, moving us toward more trustworthy\nand accountable AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The objectives that Large Language Models (LLMs) implicitly optimize remain\ndangerously opaque, making trustworthy alignment and auditing a grand\nchallenge. While Inverse Reinforcement Learning (IRL) can infer reward\nfunctions from behaviour, existing approaches either produce a single,\noverconfident reward estimate or fail to address the fundamental ambiguity of\nthe task (non-identifiability). This paper introduces a principled auditing\nframework that re-frames reward inference from a simple estimation task to a\ncomprehensive process for verification. Our framework leverages Bayesian IRL to\nnot only recover a distribution over objectives but to enable three critical\naudit capabilities: (i) Quantifying and systematically reducing\nnon-identifiability by demonstrating posterior contraction over sequential\nrounds of evidence; (ii) Providing actionable, uncertainty-aware diagnostics\nthat expose spurious shortcuts and identify out-of-distribution prompts where\nthe inferred objective cannot be trusted; and (iii) Validating policy-level\nutility by showing that the refined, low-uncertainty reward can be used\ndirectly in RLHF to achieve training dynamics and toxicity reductions\ncomparable to the ground-truth alignment process. Empirically, our framework\nsuccessfully audits a detoxified LLM, yielding a well-calibrated and\ninterpretable objective that strengthens alignment guarantees. Overall, this\nwork provides a practical toolkit for auditors, safety teams, and regulators to\nverify what LLMs are truly trying to achieve, moving us toward more trustworthy\nand accountable AI."
                },
                "authors": [
                    {
                        "name": "Matthieu Bou"
                    },
                    {
                        "name": "Nyal Patel"
                    },
                    {
                        "name": "Arjun Jagota"
                    },
                    {
                        "name": "Satyapriya Krishna"
                    },
                    {
                        "name": "Sonali Parbhoo"
                    }
                ],
                "author_detail": {
                    "name": "Sonali Parbhoo"
                },
                "author": "Sonali Parbhoo",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06096v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06096v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06093v1",
                "updated": "2025-10-07T16:21:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    21,
                    52,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:21:52Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    21,
                    52,
                    1,
                    280,
                    0
                ],
                "title": "Classical AI vs. LLMs for Decision-Maker Alignment in Health Insurance\n  Choices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical AI vs. LLMs for Decision-Maker Alignment in Health Insurance\n  Choices"
                },
                "summary": "As algorithmic decision-makers are increasingly applied to high-stakes\ndomains, AI alignment research has evolved from a focus on universal value\nalignment to context-specific approaches that account for decision-maker\nattributes. Prior work on Decision-Maker Alignment (DMA) has explored two\nprimary strategies: (1) classical AI methods integrating case-based reasoning,\nBayesian reasoning, and naturalistic decision-making, and (2) large language\nmodel (LLM)-based methods leveraging prompt engineering. While both approaches\nhave shown promise in limited domains such as medical triage, their\ngeneralizability to novel contexts remains underexplored. In this work, we\nimplement a prior classical AI model and develop an LLM-based algorithmic\ndecision-maker evaluated using a large reasoning model (GPT-5) and a\nnon-reasoning model (GPT-4) with weighted self-consistency under a zero-shot\nprompting framework, as proposed in recent literature. We evaluate both\napproaches on a health insurance decision-making dataset annotated for three\ntarget decision-makers with varying levels of risk tolerance (0.0, 0.5, 1.0).\nIn the experiments reported herein, classical AI and LLM-based models achieved\ncomparable alignment with attribute-based targets, with classical AI exhibiting\nslightly better alignment for a moderate risk profile. The dataset and\nopen-source implementation are publicly available at:\nhttps://github.com/TeX-Base/ClassicalAIvsLLMsforDMAlignment and\nhttps://github.com/Parallax-Advanced-Research/ITM/tree/feature_insurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As algorithmic decision-makers are increasingly applied to high-stakes\ndomains, AI alignment research has evolved from a focus on universal value\nalignment to context-specific approaches that account for decision-maker\nattributes. Prior work on Decision-Maker Alignment (DMA) has explored two\nprimary strategies: (1) classical AI methods integrating case-based reasoning,\nBayesian reasoning, and naturalistic decision-making, and (2) large language\nmodel (LLM)-based methods leveraging prompt engineering. While both approaches\nhave shown promise in limited domains such as medical triage, their\ngeneralizability to novel contexts remains underexplored. In this work, we\nimplement a prior classical AI model and develop an LLM-based algorithmic\ndecision-maker evaluated using a large reasoning model (GPT-5) and a\nnon-reasoning model (GPT-4) with weighted self-consistency under a zero-shot\nprompting framework, as proposed in recent literature. We evaluate both\napproaches on a health insurance decision-making dataset annotated for three\ntarget decision-makers with varying levels of risk tolerance (0.0, 0.5, 1.0).\nIn the experiments reported herein, classical AI and LLM-based models achieved\ncomparable alignment with attribute-based targets, with classical AI exhibiting\nslightly better alignment for a moderate risk profile. The dataset and\nopen-source implementation are publicly available at:\nhttps://github.com/TeX-Base/ClassicalAIvsLLMsforDMAlignment and\nhttps://github.com/Parallax-Advanced-Research/ITM/tree/feature_insurance."
                },
                "authors": [
                    {
                        "name": "Mallika Mainali"
                    },
                    {
                        "name": "Harsha Sureshbabu"
                    },
                    {
                        "name": "Anik Sen"
                    },
                    {
                        "name": "Christopher B. Rauch"
                    },
                    {
                        "name": "Noah D. Reifsnyder"
                    },
                    {
                        "name": "John Meyer"
                    },
                    {
                        "name": "J. T. Turner"
                    },
                    {
                        "name": "Michael W. Floyd"
                    },
                    {
                        "name": "Matthew Molineaux"
                    },
                    {
                        "name": "Rosina O. Weber"
                    }
                ],
                "author_detail": {
                    "name": "Rosina O. Weber"
                },
                "author": "Rosina O. Weber",
                "arxiv_comment": "15 pages, 3 figures. Accepted at the Twelfth Annual Conference on\n  Advances in Cognitive Systems (ACS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06092v1",
                "updated": "2025-10-07T16:20:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    20,
                    14,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:20:14Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    20,
                    14,
                    1,
                    280,
                    0
                ],
                "title": "Learning from Failures: Understanding LLM Alignment through\n  Failure-Aware Inverse RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Failures: Understanding LLM Alignment through\n  Failure-Aware Inverse RL"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) aligns Large Language\nModels (LLMs) with human preferences, yet the underlying reward signals they\ninternalize remain hidden, posing a critical challenge for interpretability and\nsafety. Existing approaches attempt to extract these latent incentives using\nInverse Reinforcement Learning (IRL), but treat all preference pairs equally,\noften overlooking the most informative signals: those examples the extracted\nreward model misclassifies or assigns nearly equal scores, which we term\n\\emph{failures}. We introduce a novel \\emph{failure-aware} IRL algorithm that\nfocuses on misclassified or difficult examples to recover the latent rewards\ndefining model behaviors. By learning from these failures, our failure-aware\nIRL extracts reward functions that better reflect the true objectives behind\nRLHF. We demonstrate that failure-aware IRL outperforms existing IRL baselines\nacross multiple metrics when applied to LLM detoxification, without requiring\nexternal classifiers or supervision. Crucially, failure-aware IRL yields\nrewards that better capture the true incentives learned during RLHF, enabling\nmore effective re-RLHF training than standard IRL. This establishes\nfailure-aware IRL as a robust, scalable method for auditing model alignment and\nreducing ambiguity in the IRL process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) aligns Large Language\nModels (LLMs) with human preferences, yet the underlying reward signals they\ninternalize remain hidden, posing a critical challenge for interpretability and\nsafety. Existing approaches attempt to extract these latent incentives using\nInverse Reinforcement Learning (IRL), but treat all preference pairs equally,\noften overlooking the most informative signals: those examples the extracted\nreward model misclassifies or assigns nearly equal scores, which we term\n\\emph{failures}. We introduce a novel \\emph{failure-aware} IRL algorithm that\nfocuses on misclassified or difficult examples to recover the latent rewards\ndefining model behaviors. By learning from these failures, our failure-aware\nIRL extracts reward functions that better reflect the true objectives behind\nRLHF. We demonstrate that failure-aware IRL outperforms existing IRL baselines\nacross multiple metrics when applied to LLM detoxification, without requiring\nexternal classifiers or supervision. Crucially, failure-aware IRL yields\nrewards that better capture the true incentives learned during RLHF, enabling\nmore effective re-RLHF training than standard IRL. This establishes\nfailure-aware IRL as a robust, scalable method for auditing model alignment and\nreducing ambiguity in the IRL process."
                },
                "authors": [
                    {
                        "name": "Nyal Patel"
                    },
                    {
                        "name": "Matthieu Bou"
                    },
                    {
                        "name": "Arjun Jagota"
                    },
                    {
                        "name": "Satyapriya Krishna"
                    },
                    {
                        "name": "Sonali Parbhoo"
                    }
                ],
                "author_detail": {
                    "name": "Sonali Parbhoo"
                },
                "author": "Sonali Parbhoo",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04226v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04226v3",
                "updated": "2025-10-08T07:35:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    7,
                    35,
                    57,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-05T14:29:15Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    14,
                    29,
                    15,
                    6,
                    278,
                    0
                ],
                "title": "Epistemic Diversity and Knowledge Collapse in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Epistemic Diversity and Knowledge Collapse in Large Language Models"
                },
                "summary": "Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation"
                },
                "authors": [
                    {
                        "name": "Dustin Wright"
                    },
                    {
                        "name": "Sarah Masud"
                    },
                    {
                        "name": "Jared Moore"
                    },
                    {
                        "name": "Srishti Yadav"
                    },
                    {
                        "name": "Maria Antoniak"
                    },
                    {
                        "name": "Chan Young Park"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "arxiv_comment": "16 pages; 8 figures, 4 tables; v2 changelog: Fixed the modeling for\n  table 3, random effect is the model version; v3 changelog: Fixed minor\n  formatting issues in tables 2 and 3;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04226v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04226v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21961v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21961v3",
                "updated": "2025-10-07T16:06:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    6,
                    25,
                    1,
                    280,
                    0
                ],
                "published": "2025-03-27T20:18:22Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    20,
                    18,
                    22,
                    3,
                    86,
                    0
                ],
                "title": "Entropy-Gated Branching for Efficient Test-Time Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy-Gated Branching for Efficient Test-Time Reasoning"
                },
                "summary": "Test-time compute methods can significantly improve the reasoning\ncapabilities and problem-solving accuracy of large language models (LLMs).\nHowever, these approaches require substantially more computational resources,\nwith most compute wasted on exploring low-diversity branches where the model\nalready exhibits high confidence. We observe that a small subset of uncertain\nreasoning steps has a disproportionately large impact on final prediction\naccuracy, and branching at these critical junctures tends to yield more diverse\nand higher-quality candidate reasoning steps. We propose Entropy-Gated\nBranching (EGB), which branches only at high-uncertainty steps and prunes\nexpansions with a lightweight verifier. On mathematical and financial reasoning\nbenchmarks, EGB improves accuracy by 22.6% over standard inference while\noperating 31%-75% faster across math benchmarks than test-time beam search with\nhigher performance. Our results show that dynamic resource allocation during\ninference can substantially improve both efficiency and effectiveness, offering\na more scalable pathway to enhanced LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute methods can significantly improve the reasoning\ncapabilities and problem-solving accuracy of large language models (LLMs).\nHowever, these approaches require substantially more computational resources,\nwith most compute wasted on exploring low-diversity branches where the model\nalready exhibits high confidence. We observe that a small subset of uncertain\nreasoning steps has a disproportionately large impact on final prediction\naccuracy, and branching at these critical junctures tends to yield more diverse\nand higher-quality candidate reasoning steps. We propose Entropy-Gated\nBranching (EGB), which branches only at high-uncertainty steps and prunes\nexpansions with a lightweight verifier. On mathematical and financial reasoning\nbenchmarks, EGB improves accuracy by 22.6% over standard inference while\noperating 31%-75% faster across math benchmarks than test-time beam search with\nhigher performance. Our results show that dynamic resource allocation during\ninference can substantially improve both efficiency and effectiveness, offering\na more scalable pathway to enhanced LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Xianzhi Li"
                    },
                    {
                        "name": "Ethan Callanan"
                    },
                    {
                        "name": "Abdellah Ghassel"
                    },
                    {
                        "name": "Xiaodan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Zhu"
                },
                "author": "Xiaodan Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21961v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21961v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06078v1",
                "updated": "2025-10-07T16:03:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    3,
                    57,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T16:03:57Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    16,
                    3,
                    57,
                    1,
                    280,
                    0
                ],
                "title": "Constraint-Aware Route Recommendation from Natural Language via\n  Hierarchical LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint-Aware Route Recommendation from Natural Language via\n  Hierarchical LLM Agents"
                },
                "summary": "Route recommendation aims to provide users with optimal travel plans that\nsatisfy diverse and complex requirements. Classical routing algorithms (e.g.,\nshortest-path and constraint-aware search) are efficient but assume structured\ninputs and fixed objectives, limiting adaptability to natural-language queries.\nRecent LLM-based approaches enhance flexibility but struggle with spatial\nreasoning and the joint modeling of route-level and POI-level preferences. To\naddress these limitations, we propose RouteLLM, a hierarchical multi-agent\nframework that grounds natural-language intents into constraint-aware routes.\nIt first parses user queries into structured intents including POIs, paths, and\nconstraints. A manager agent then coordinates specialized sub-agents: a\nconstraint agent that resolves and formally check constraints, a POI agent that\nretrieves and ranks candidate POIs, and a path refinement agent that refines\nroutes via a routing engine with preference-conditioned costs. A final verifier\nagent ensures constraint satisfaction and produces the final route with an\ninterpretable rationale. This design bridges linguistic flexibility and spatial\nstructure, enabling reasoning over route feasibility and user preferences.\nExperiments show that our method reliably grounds textual preferences into\nconstraint-aware routes, improving route quality and preference satisfaction\nover classical methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Route recommendation aims to provide users with optimal travel plans that\nsatisfy diverse and complex requirements. Classical routing algorithms (e.g.,\nshortest-path and constraint-aware search) are efficient but assume structured\ninputs and fixed objectives, limiting adaptability to natural-language queries.\nRecent LLM-based approaches enhance flexibility but struggle with spatial\nreasoning and the joint modeling of route-level and POI-level preferences. To\naddress these limitations, we propose RouteLLM, a hierarchical multi-agent\nframework that grounds natural-language intents into constraint-aware routes.\nIt first parses user queries into structured intents including POIs, paths, and\nconstraints. A manager agent then coordinates specialized sub-agents: a\nconstraint agent that resolves and formally check constraints, a POI agent that\nretrieves and ranks candidate POIs, and a path refinement agent that refines\nroutes via a routing engine with preference-conditioned costs. A final verifier\nagent ensures constraint satisfaction and produces the final route with an\ninterpretable rationale. This design bridges linguistic flexibility and spatial\nstructure, enabling reasoning over route feasibility and user preferences.\nExperiments show that our method reliably grounds textual preferences into\nconstraint-aware routes, improving route quality and preference satisfaction\nover classical methods."
                },
                "authors": [
                    {
                        "name": "Tao Zhe"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Fateme Memar"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Xinyue Ye"
                    },
                    {
                        "name": "Zhongren Peng"
                    },
                    {
                        "name": "Dongjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dongjie Wang"
                },
                "author": "Dongjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02322v2",
                "updated": "2025-10-07T15:56:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    56,
                    15,
                    1,
                    280,
                    0
                ],
                "published": "2025-08-04T11:42:48Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    42,
                    48,
                    0,
                    216,
                    0
                ],
                "title": "CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert\n  Redundancy Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert\n  Redundancy Analysis"
                },
                "summary": "Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are\ndistinguished by their strong performance scaling with increasing parameters\nacross a wide range of tasks, yet they also suffer from substantial\ncomputational and storage overheads. Notably, the performance gains of MoE\nmodels do not scale proportionally with the growth in expert parameters. While\nprior works attempt to reduce parameters via expert-level pruning, merging, or\ndecomposition, they still suffer from challenges in both performance and\ncomputational efficiency. In this paper, we address these challenges by\nintroducing micro-expert as a finer-grained compression unit that spans across\nmatrices. We first establish a more fundamental perspective, viewing MoE layers\nas mixtures of micro-experts, and present CAMERA, a lightweight and\ntraining-free framework for identifying micro-expert redundancy. Our analysis\nuncovers significant variance in micro-expert contributions during decoding.\nBased on this insight, we further propose CAMERA-P, a structured micro-expert\npruning framework, and CAMERA-Q, a mixed-precision quantization idea designed\nfor micro-experts. Extensive experiments on nine downstream tasks show that\nCAMERA-P consistently outperforms strong baselines under pruning ratios ranging\nfrom 20% to 60%. Furthermore, CAMERA-Q achieves superior results under\naggressive 2-bit quantization, surpassing existing matrix- and channel-level\nideas. Notably, our method enables complete micro-expert analysis of\nQwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are\ndistinguished by their strong performance scaling with increasing parameters\nacross a wide range of tasks, yet they also suffer from substantial\ncomputational and storage overheads. Notably, the performance gains of MoE\nmodels do not scale proportionally with the growth in expert parameters. While\nprior works attempt to reduce parameters via expert-level pruning, merging, or\ndecomposition, they still suffer from challenges in both performance and\ncomputational efficiency. In this paper, we address these challenges by\nintroducing micro-expert as a finer-grained compression unit that spans across\nmatrices. We first establish a more fundamental perspective, viewing MoE layers\nas mixtures of micro-experts, and present CAMERA, a lightweight and\ntraining-free framework for identifying micro-expert redundancy. Our analysis\nuncovers significant variance in micro-expert contributions during decoding.\nBased on this insight, we further propose CAMERA-P, a structured micro-expert\npruning framework, and CAMERA-Q, a mixed-precision quantization idea designed\nfor micro-experts. Extensive experiments on nine downstream tasks show that\nCAMERA-P consistently outperforms strong baselines under pruning ratios ranging\nfrom 20% to 60%. Furthermore, CAMERA-Q achieves superior results under\naggressive 2-bit quantization, surpassing existing matrix- and channel-level\nideas. Notably, our method enables complete micro-expert analysis of\nQwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU."
                },
                "authors": [
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Yuanchi Zhang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "16 pages, 9 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06062v1",
                "updated": "2025-10-07T15:54:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    54,
                    24,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:54:24Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    54,
                    24,
                    1,
                    280,
                    0
                ],
                "title": "ASPO: Asymmetric Importance Sampling Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASPO: Asymmetric Importance Sampling Policy Optimization"
                },
                "summary": "Recent Large Language Model (LLM) post-training methods rely on token-level\nclipping mechanisms during Reinforcement Learning (RL). However, we identify a\nfundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance\nSampling (IS) ratios of positive-advantage tokens are mismatched, leading to\nunbalanced token weighting for positive and negative tokens. This mismatch\nsuppresses the update of low-probability tokens while over-amplifying already\nhigh-probability ones. To address this, we propose Asymmetric Importance\nSampling Policy Optimization (ASPO), which uses a simple yet effective strategy\nthat flips the IS ratios of positive-advantage tokens, aligning their update\ndirection with the learning dynamics of negative ones. AIS further incorporates\na soft dual-clipping mechanism to stabilize extreme updates while maintaining\ngradient flow. Comprehensive experiments on coding and mathematical reasoning\nbenchmarks demonstrate that ASPO significantly mitigates premature convergence,\nimproves training stability, and enhances final performance over strong\nGRPO-based baselines. Our analysis provides new insights into the role of\ntoken-level weighting in OSRL and highlights the critical importance of\ncorrecting IS in LLM RL. The code and models of ASPO are available at\nhttps://github.com/wizard-III/Archer2.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Large Language Model (LLM) post-training methods rely on token-level\nclipping mechanisms during Reinforcement Learning (RL). However, we identify a\nfundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance\nSampling (IS) ratios of positive-advantage tokens are mismatched, leading to\nunbalanced token weighting for positive and negative tokens. This mismatch\nsuppresses the update of low-probability tokens while over-amplifying already\nhigh-probability ones. To address this, we propose Asymmetric Importance\nSampling Policy Optimization (ASPO), which uses a simple yet effective strategy\nthat flips the IS ratios of positive-advantage tokens, aligning their update\ndirection with the learning dynamics of negative ones. AIS further incorporates\na soft dual-clipping mechanism to stabilize extreme updates while maintaining\ngradient flow. Comprehensive experiments on coding and mathematical reasoning\nbenchmarks demonstrate that ASPO significantly mitigates premature convergence,\nimproves training stability, and enhances final performance over strong\nGRPO-based baselines. Our analysis provides new insights into the role of\ntoken-level weighting in OSRL and highlights the critical importance of\ncorrecting IS in LLM RL. The code and models of ASPO are available at\nhttps://github.com/wizard-III/Archer2.0."
                },
                "authors": [
                    {
                        "name": "Jiakang Wang"
                    },
                    {
                        "name": "Runze Liu"
                    },
                    {
                        "name": "Lei Lin"
                    },
                    {
                        "name": "Wenping Hu"
                    },
                    {
                        "name": "Xiu Li"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    },
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00207v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00207v2",
                "updated": "2025-10-07T15:54:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    54,
                    8,
                    1,
                    280,
                    0
                ],
                "published": "2025-09-30T19:31:35Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    31,
                    35,
                    1,
                    273,
                    0
                ],
                "title": "FlowMoE: A Scalable Pipeline Scheduling Framework for Distributed\n  Mixture-of-Experts Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowMoE: A Scalable Pipeline Scheduling Framework for Distributed\n  Mixture-of-Experts Training"
                },
                "summary": "The parameter size of modern large language models (LLMs) can be scaled up\nvia the sparsely-activated Mixture-of-Experts (MoE) technique to avoid\nexcessive increase of the computational costs. To further improve training\nefficiency, pipelining computation and communication has become a promising\nsolution for distributed MoE training. However, existing work primarily focuses\non scheduling tasks within the MoE layer, such as expert computing and\nall-to-all (A2A) communication, while neglecting other key operations including\nmulti-head attention (MHA) computing, gating, and all-reduce communication. In\nthis paper, we propose FlowMoE, a scalable framework for scheduling multi-type\ntask pipelines. First, FlowMoE constructs a unified pipeline to consistently\nscheduling MHA computing, gating, expert computing, and A2A communication.\nSecond, FlowMoE introduces a tensor chunk-based priority scheduling mechanism\nto overlap the all-reduce communication with all computing tasks. We implement\nFlowMoE as an adaptive and generic framework atop PyTorch. Extensive\nexperiments with 675 typical MoE layers and four real-world MoE models across\ntwo GPU clusters demonstrate that our proposed FlowMoE framework outperforms\nstate-of-the-art MoE training frameworks, reducing training time by 13%-57%,\nenergy consumption by 10%-39%, and memory usage by 7%-32%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The parameter size of modern large language models (LLMs) can be scaled up\nvia the sparsely-activated Mixture-of-Experts (MoE) technique to avoid\nexcessive increase of the computational costs. To further improve training\nefficiency, pipelining computation and communication has become a promising\nsolution for distributed MoE training. However, existing work primarily focuses\non scheduling tasks within the MoE layer, such as expert computing and\nall-to-all (A2A) communication, while neglecting other key operations including\nmulti-head attention (MHA) computing, gating, and all-reduce communication. In\nthis paper, we propose FlowMoE, a scalable framework for scheduling multi-type\ntask pipelines. First, FlowMoE constructs a unified pipeline to consistently\nscheduling MHA computing, gating, expert computing, and A2A communication.\nSecond, FlowMoE introduces a tensor chunk-based priority scheduling mechanism\nto overlap the all-reduce communication with all computing tasks. We implement\nFlowMoE as an adaptive and generic framework atop PyTorch. Extensive\nexperiments with 675 typical MoE layers and four real-world MoE models across\ntwo GPU clusters demonstrate that our proposed FlowMoE framework outperforms\nstate-of-the-art MoE training frameworks, reducing training time by 13%-57%,\nenergy consumption by 10%-39%, and memory usage by 7%-32%."
                },
                "authors": [
                    {
                        "name": "Yunqi Gao"
                    },
                    {
                        "name": "Bing Hu"
                    },
                    {
                        "name": "Mahdi Boloursaz Mashhadi"
                    },
                    {
                        "name": "A-Long Jin"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Pei Xiao"
                    },
                    {
                        "name": "Rahim Tafazolli"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00207v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00207v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17355v2",
                "updated": "2025-10-07T15:53:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    53,
                    55,
                    1,
                    280,
                    0
                ],
                "published": "2025-02-24T17:33:18Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    33,
                    18,
                    0,
                    55,
                    0
                ],
                "title": "On Relation-Specific Neurons in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Relation-Specific Neurons in Large Language Models"
                },
                "summary": "In large language models (LLMs), certain \\emph{neurons} can store distinct\npieces of knowledge learned during pretraining. While factual knowledge\ntypically appears as a combination of \\emph{relations} and \\emph{entities}, it\nremains unclear whether some neurons focus on a relation itself -- independent\nof any entity. We hypothesize such neurons \\emph{detect} a relation in the\ninput text and \\emph{guide} generation involving such a relation. To\ninvestigate this, we study the LLama-2 family on a chosen set of relations,\nwith a \\textit{statistics}-based method. Our experiments demonstrate the\nexistence of relation-specific neurons. We measure the effect of selectively\ndeactivating candidate neurons specific to relation $r$ on the LLM's ability to\nhandle (1) facts involving relation $r$ and (2) facts involving a different\nrelation $r' \\neq r$. With respect to their capacity for encoding relation\ninformation, we give evidence for the following three properties of\nrelation-specific neurons. \\textbf{(i) Neuron cumulativity.} Multiple neurons\njointly contribute to processing facts involving relation $r$, with no single\nneuron fully encoding a fact in $r$ on its own. \\textbf{(ii) Neuron\nversatility.} Neurons can be shared across multiple closely related as well as\nless related relations. In addition, some relation neurons transfer across\nlanguages. \\textbf{(iii) Neuron interference.} Deactivating neurons specific to\none relation can improve LLMs' factual recall performance for facts of other\nrelations. We make our code and data publicly available at\nhttps://github.com/cisnlp/relation-specific-neurons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large language models (LLMs), certain \\emph{neurons} can store distinct\npieces of knowledge learned during pretraining. While factual knowledge\ntypically appears as a combination of \\emph{relations} and \\emph{entities}, it\nremains unclear whether some neurons focus on a relation itself -- independent\nof any entity. We hypothesize such neurons \\emph{detect} a relation in the\ninput text and \\emph{guide} generation involving such a relation. To\ninvestigate this, we study the LLama-2 family on a chosen set of relations,\nwith a \\textit{statistics}-based method. Our experiments demonstrate the\nexistence of relation-specific neurons. We measure the effect of selectively\ndeactivating candidate neurons specific to relation $r$ on the LLM's ability to\nhandle (1) facts involving relation $r$ and (2) facts involving a different\nrelation $r' \\neq r$. With respect to their capacity for encoding relation\ninformation, we give evidence for the following three properties of\nrelation-specific neurons. \\textbf{(i) Neuron cumulativity.} Multiple neurons\njointly contribute to processing facts involving relation $r$, with no single\nneuron fully encoding a fact in $r$ on its own. \\textbf{(ii) Neuron\nversatility.} Neurons can be shared across multiple closely related as well as\nless related relations. In addition, some relation neurons transfer across\nlanguages. \\textbf{(iii) Neuron interference.} Deactivating neurons specific to\none relation can improve LLMs' factual recall performance for facts of other\nrelations. We make our code and data publicly available at\nhttps://github.com/cisnlp/relation-specific-neurons."
                },
                "authors": [
                    {
                        "name": "Yihong Liu"
                    },
                    {
                        "name": "Runsheng Chen"
                    },
                    {
                        "name": "Lea Hirlimann"
                    },
                    {
                        "name": "Ahmad Dawar Hakimi"
                    },
                    {
                        "name": "Mingyang Wang"
                    },
                    {
                        "name": "Amir Hossein Kargaran"
                    },
                    {
                        "name": "Sascha Rothe"
                    },
                    {
                        "name": "François Yvon"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06056v1",
                "updated": "2025-10-07T15:49:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    49,
                    51,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:49:51Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    49,
                    51,
                    1,
                    280,
                    0
                ],
                "title": "Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep\n  Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep\n  Research"
                },
                "summary": "Large language models hold promise as scientific assistants, yet existing\nagents either rely solely on algorithm evolution or on deep research in\nisolation, both of which face critical limitations. Pure algorithm evolution,\nas in AlphaEvolve, depends only on the internal knowledge of LLMs and quickly\nplateaus in complex domains, while pure deep research proposes ideas without\nvalidation, resulting in unrealistic or unimplementable solutions. We present\nDeepEvolve, an agent that integrates deep research with algorithm evolution,\nuniting external knowledge retrieval, cross-file code editing, and systematic\ndebugging under a feedback-driven iterative loop. Each iteration not only\nproposes new hypotheses but also refines, implements, and tests them, avoiding\nboth shallow improvements and unproductive over-refinements. Across nine\nbenchmarks in chemistry, mathematics, biology, materials, and patents,\nDeepEvolve consistently improves the initial algorithm, producing executable\nnew algorithms with sustained gains. By bridging the gap between unguided\nevolution and research without grounding, DeepEvolve provides a reliable\nframework for advancing scientific algorithm discovery. Our code is available\nat https://github.com/liugangcode/deepevolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models hold promise as scientific assistants, yet existing\nagents either rely solely on algorithm evolution or on deep research in\nisolation, both of which face critical limitations. Pure algorithm evolution,\nas in AlphaEvolve, depends only on the internal knowledge of LLMs and quickly\nplateaus in complex domains, while pure deep research proposes ideas without\nvalidation, resulting in unrealistic or unimplementable solutions. We present\nDeepEvolve, an agent that integrates deep research with algorithm evolution,\nuniting external knowledge retrieval, cross-file code editing, and systematic\ndebugging under a feedback-driven iterative loop. Each iteration not only\nproposes new hypotheses but also refines, implements, and tests them, avoiding\nboth shallow improvements and unproductive over-refinements. Across nine\nbenchmarks in chemistry, mathematics, biology, materials, and patents,\nDeepEvolve consistently improves the initial algorithm, producing executable\nnew algorithms with sustained gains. By bridging the gap between unguided\nevolution and research without grounding, DeepEvolve provides a reliable\nframework for advancing scientific algorithm discovery. Our code is available\nat https://github.com/liugangcode/deepevolve."
                },
                "authors": [
                    {
                        "name": "Gang Liu"
                    },
                    {
                        "name": "Yihan Zhu"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Meng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Jiang"
                },
                "author": "Meng Jiang",
                "arxiv_comment": "25 pages, 17 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06048v1",
                "updated": "2025-10-07T15:42:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    42,
                    33,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:42:33Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    42,
                    33,
                    1,
                    280,
                    0
                ],
                "title": "BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection\n  in Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection\n  in Language Model Pretraining"
                },
                "summary": "Effective data selection is essential for pretraining large language models\n(LLMs), enhancing efficiency and improving generalization to downstream tasks.\nHowever, existing approaches often require leveraging external pretrained\nmodels, making it difficult to disentangle the effects of data selection from\nthose of the external pretrained models. In addition, they often overlook the\nlong-term impact of selected data if the model is trained to convergence,\nprimarily due to the prohibitive cost of full-scale LLM pretraining. In this\npaper, we introduce BLISS (\\textbf{B}ileve\\textbf{L} \\textbf{I}nfluence\n\\textbf{S}coring method for data \\textbf{S}election): a lightweight data\nselection method that operates entirely \\emph{from scratch}, without relying on\nany external pretrained oracle models, while explicitly accounting for the\nlong-term impact of selected data. BLISS leverages a small proxy model as a\nsurrogate for the LLM and employs a score model to estimate the long-term\ninfluence of training samples if the proxy model is trained to convergence. We\nformulate data selection as a bilevel optimization problem, where the\nupper-level objective optimizes the score model to assign importance weights to\ntraining samples, ensuring that minimizing the lower-level objective (i.e.,\ntraining the proxy model over the weighted training loss until convergence)\nleads to best validation performance. Once optimized, the trained score model\npredicts influence scores for the dataset, enabling efficient selection of\nhigh-quality samples for LLM pretraining. We validate BLISS by pretraining\n410M/1B/2.8B Pythia and LLaMA-0.5B models on selected subsets of the C4\ndataset. Notably, under the 1B model setting, BLISS achieves $1.7\\times$\nspeedup in reaching the same performance as the state-of-the-art method,\ndemonstrating superior performance across multiple downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective data selection is essential for pretraining large language models\n(LLMs), enhancing efficiency and improving generalization to downstream tasks.\nHowever, existing approaches often require leveraging external pretrained\nmodels, making it difficult to disentangle the effects of data selection from\nthose of the external pretrained models. In addition, they often overlook the\nlong-term impact of selected data if the model is trained to convergence,\nprimarily due to the prohibitive cost of full-scale LLM pretraining. In this\npaper, we introduce BLISS (\\textbf{B}ileve\\textbf{L} \\textbf{I}nfluence\n\\textbf{S}coring method for data \\textbf{S}election): a lightweight data\nselection method that operates entirely \\emph{from scratch}, without relying on\nany external pretrained oracle models, while explicitly accounting for the\nlong-term impact of selected data. BLISS leverages a small proxy model as a\nsurrogate for the LLM and employs a score model to estimate the long-term\ninfluence of training samples if the proxy model is trained to convergence. We\nformulate data selection as a bilevel optimization problem, where the\nupper-level objective optimizes the score model to assign importance weights to\ntraining samples, ensuring that minimizing the lower-level objective (i.e.,\ntraining the proxy model over the weighted training loss until convergence)\nleads to best validation performance. Once optimized, the trained score model\npredicts influence scores for the dataset, enabling efficient selection of\nhigh-quality samples for LLM pretraining. We validate BLISS by pretraining\n410M/1B/2.8B Pythia and LLaMA-0.5B models on selected subsets of the C4\ndataset. Notably, under the 1B model setting, BLISS achieves $1.7\\times$\nspeedup in reaching the same performance as the state-of-the-art method,\ndemonstrating superior performance across multiple downstream tasks."
                },
                "authors": [
                    {
                        "name": "Jie Hao"
                    },
                    {
                        "name": "Rui Yu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Huixia Wang"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Mingrui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mingrui Liu"
                },
                "author": "Mingrui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06042v1",
                "updated": "2025-10-07T15:36:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    36,
                    4,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:36:04Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    36,
                    4,
                    1,
                    280,
                    0
                ],
                "title": "Agent+P: Guiding UI Agents via Symbolic Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent+P: Guiding UI Agents via Symbolic Planning"
                },
                "summary": "Large Language Model (LLM)-based UI agents show great promise for UI\nautomation but often hallucinate in long-horizon tasks due to their lack of\nunderstanding of the global UI transition structure. To address this, we\nintroduce AGENT+P, a novel framework that leverages symbolic planning to guide\nLLM-based UI agents. Specifically, we model an app's UI transition structure as\na UI Transition Graph (UTG), which allows us to reformulate the UI automation\ntask as a pathfinding problem on the UTG. This further enables an off-the-shelf\nsymbolic planner to generate a provably correct and optimal high-level plan,\npreventing the agent from redundant exploration and guiding the agent to\nachieve the automation goals. AGENT+P is designed as a plug-and-play framework\nto enhance existing UI agents. Evaluation on the AndroidWorld benchmark\ndemonstrates that AGENT+P improves the success rates of state-of-the-art UI\nagents by up to 14% and reduces the action steps by 37.7%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based UI agents show great promise for UI\nautomation but often hallucinate in long-horizon tasks due to their lack of\nunderstanding of the global UI transition structure. To address this, we\nintroduce AGENT+P, a novel framework that leverages symbolic planning to guide\nLLM-based UI agents. Specifically, we model an app's UI transition structure as\na UI Transition Graph (UTG), which allows us to reformulate the UI automation\ntask as a pathfinding problem on the UTG. This further enables an off-the-shelf\nsymbolic planner to generate a provably correct and optimal high-level plan,\npreventing the agent from redundant exploration and guiding the agent to\nachieve the automation goals. AGENT+P is designed as a plug-and-play framework\nto enhance existing UI agents. Evaluation on the AndroidWorld benchmark\ndemonstrates that AGENT+P improves the success rates of state-of-the-art UI\nagents by up to 14% and reduces the action steps by 37.7%."
                },
                "authors": [
                    {
                        "name": "Shang Ma"
                    },
                    {
                        "name": "Xusheng Xiao"
                    },
                    {
                        "name": "Yanfang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Yanfang Ye"
                },
                "author": "Yanfang Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05016v2",
                "updated": "2025-10-07T15:34:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    34,
                    59,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-06T16:58:47Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    58,
                    47,
                    0,
                    279,
                    0
                ],
                "title": "Large Language Models Achieve Gold Medal Performance at the\n  International Olympiad on Astronomy & Astrophysics (IOAA)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Achieve Gold Medal Performance at the\n  International Olympiad on Astronomy & Astrophysics (IOAA)"
                },
                "summary": "While task-specific demonstrations show early success in applying large\nlanguage models (LLMs) to automate some astronomical research tasks, they only\nprovide incomplete views of all necessary capabilities in solving astronomy\nproblems, calling for more thorough understanding of LLMs' strengths and\nlimitations. So far, existing benchmarks and evaluations focus on simple\nquestion-answering that primarily tests astronomical knowledge and fails to\nevaluate the complex reasoning required for real-world research in the\ndiscipline. Here, we address this gap by systematically benchmarking five\nstate-of-the-art LLMs on the International Olympiad on Astronomy and\nAstrophysics (IOAA) exams, which are designed to examine deep conceptual\nunderstanding, multi-step derivations, and multimodal analysis. With average\nscores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing\nmodels) not only achieve gold medal level performance but also rank in the top\ntwo among ~200-300 participants in all four IOAA theory exams evaluated\n(2022-2025). In comparison, results on the data analysis exams show more\ndivergence. GPT-5 still excels in the exams with an 88.5% average score,\nranking top 10 among the participants in the four most recent IOAAs, while\nother models' performances drop to 48-76%. Furthermore, our in-depth error\nanalysis underscores conceptual reasoning, geometric reasoning, and spatial\nvisualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,\nalthough LLMs approach peak human performance in theory exams, critical gaps\nmust be addressed before they can serve as autonomous research agents in\nastronomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While task-specific demonstrations show early success in applying large\nlanguage models (LLMs) to automate some astronomical research tasks, they only\nprovide incomplete views of all necessary capabilities in solving astronomy\nproblems, calling for more thorough understanding of LLMs' strengths and\nlimitations. So far, existing benchmarks and evaluations focus on simple\nquestion-answering that primarily tests astronomical knowledge and fails to\nevaluate the complex reasoning required for real-world research in the\ndiscipline. Here, we address this gap by systematically benchmarking five\nstate-of-the-art LLMs on the International Olympiad on Astronomy and\nAstrophysics (IOAA) exams, which are designed to examine deep conceptual\nunderstanding, multi-step derivations, and multimodal analysis. With average\nscores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing\nmodels) not only achieve gold medal level performance but also rank in the top\ntwo among ~200-300 participants in all four IOAA theory exams evaluated\n(2022-2025). In comparison, results on the data analysis exams show more\ndivergence. GPT-5 still excels in the exams with an 88.5% average score,\nranking top 10 among the participants in the four most recent IOAAs, while\nother models' performances drop to 48-76%. Furthermore, our in-depth error\nanalysis underscores conceptual reasoning, geometric reasoning, and spatial\nvisualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,\nalthough LLMs approach peak human performance in theory exams, critical gaps\nmust be addressed before they can serve as autonomous research agents in\nastronomy."
                },
                "authors": [
                    {
                        "name": "Lucas Carrit Delgado Pinheiro"
                    },
                    {
                        "name": "Ziru Chen"
                    },
                    {
                        "name": "Bruno Caixeta Piazza"
                    },
                    {
                        "name": "Ness Shroff"
                    },
                    {
                        "name": "Yingbin Liang"
                    },
                    {
                        "name": "Yuan-Sen Ting"
                    },
                    {
                        "name": "Huan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Huan Sun"
                },
                "author": "Huan Sun",
                "arxiv_comment": "18 pages, 6 figures, to be submitted, comments are welcome.\n  Reproducibility details can be found at:\n  https://github.com/OSU-NLP-Group/LLM-IOAA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06040v1",
                "updated": "2025-10-07T15:34:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    34,
                    46,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:34:46Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    34,
                    46,
                    1,
                    280,
                    0
                ],
                "title": "VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via\n  Tree-based Group Relative Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via\n  Tree-based Group Relative Policy Optimization"
                },
                "summary": "Understanding hour-long videos with multi-modal large language models\n(MM-LLMs) enriches the landscape of human-centered AI applications. However,\nfor end-to-end video understanding with LLMs, uniformly sampling video frames\nresults in LLMs being overwhelmed by a vast amount of irrelevant information as\nvideo length increases. Existing hierarchical key frame extraction methods\nimprove the accuracy of video understanding but still face two critical\nchallenges. 1) How can the interference of extensive redundant information in\nlong videos be mitigated? 2) How can a model dynamically adapt to complex\nhierarchical structures while accurately identifying key frames? To address\nthese issues, we propose VideoMiner, which iteratively segments, captions, and\nclusters long videos, forming a hierarchical tree structure. The proposed\nVideoMiner progresses from long videos to events to frames while preserving\ntemporal coherence, effectively addressing the first challenge. To precisely\nlocate key frames, we introduce T-GRPO, a tree-based group relative policy\noptimization in reinforcement learning method that guides the exploration of\nthe VideoMiner. The proposed T-GRPO is specifically designed for tree\nstructures, integrating spatiotemporal information at the event level while\nbeing guided by the question, thus solving the second challenge. We achieve\nsuperior performance in all long-video understanding tasks and uncover several\ninteresting insights. Our proposed T-GRPO surprisingly incentivizes the model\nto spontaneously generate a reasoning chain. Additionally, the designed tree\ngrowth auxin dynamically adjusts the expansion depth, obtaining accuracy and\nefficiency gains. The code is publicly available at\nhttps://github.com/caoxinye/VideoMiner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding hour-long videos with multi-modal large language models\n(MM-LLMs) enriches the landscape of human-centered AI applications. However,\nfor end-to-end video understanding with LLMs, uniformly sampling video frames\nresults in LLMs being overwhelmed by a vast amount of irrelevant information as\nvideo length increases. Existing hierarchical key frame extraction methods\nimprove the accuracy of video understanding but still face two critical\nchallenges. 1) How can the interference of extensive redundant information in\nlong videos be mitigated? 2) How can a model dynamically adapt to complex\nhierarchical structures while accurately identifying key frames? To address\nthese issues, we propose VideoMiner, which iteratively segments, captions, and\nclusters long videos, forming a hierarchical tree structure. The proposed\nVideoMiner progresses from long videos to events to frames while preserving\ntemporal coherence, effectively addressing the first challenge. To precisely\nlocate key frames, we introduce T-GRPO, a tree-based group relative policy\noptimization in reinforcement learning method that guides the exploration of\nthe VideoMiner. The proposed T-GRPO is specifically designed for tree\nstructures, integrating spatiotemporal information at the event level while\nbeing guided by the question, thus solving the second challenge. We achieve\nsuperior performance in all long-video understanding tasks and uncover several\ninteresting insights. Our proposed T-GRPO surprisingly incentivizes the model\nto spontaneously generate a reasoning chain. Additionally, the designed tree\ngrowth auxin dynamically adjusts the expansion depth, obtaining accuracy and\nefficiency gains. The code is publicly available at\nhttps://github.com/caoxinye/VideoMiner."
                },
                "authors": [
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Hongcan Guo"
                    },
                    {
                        "name": "Jiawen Qian"
                    },
                    {
                        "name": "Guoshun Nan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Yuqi Pan"
                    },
                    {
                        "name": "Tianhao Hou"
                    },
                    {
                        "name": "Xiaojuan Wang"
                    },
                    {
                        "name": "Yutong Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Gao"
                },
                "author": "Yutong Gao",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06039v1",
                "updated": "2025-10-07T15:33:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    33,
                    52,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:33:52Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    33,
                    52,
                    1,
                    280,
                    0
                ],
                "title": "CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive\n  Evaluation of Chinese LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive\n  Evaluation of Chinese LLMs"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across a wide\nrange of natural language processing tasks. However, Chinese LLMs face unique\nchallenges, primarily due to the dominance of unstructured free text and the\nlack of structured representations in Chinese corpora. While existing\nbenchmarks for LLMs partially assess Chinese LLMs, they are still predominantly\nEnglish-centric and fail to address the unique linguistic characteristics of\nChinese, lacking structured datasets essential for robust evaluation. To\naddress these challenges, we present a Comprehensive Benchmark for Evaluating\nChinese Large Language Models (CB-ECLLM) based on the newly constructed Chinese\nData-Text Pair (CDTP) dataset. Specifically, CDTP comprises over 7 million\naligned text pairs, each consisting of unstructured text coupled with one or\nmore corresponding triples, alongside a total of 15 million triples spanning\nfour critical domains. The core contributions of CDTP are threefold: (i)\nenriching Chinese corpora with high-quality structured information; (ii)\nenabling fine-grained evaluation tailored to knowledge-driven tasks; and (iii)\nsupporting multi-task fine-tuning to assess generalization and robustness\nacross scenarios, including Knowledge Graph Completion, Triple-to-Text\ngeneration, and Question Answering. Furthermore, we conduct rigorous\nevaluations through extensive experiments and ablation studies to assess the\neffectiveness, Supervised Fine-Tuning (SFT), and robustness of the benchmark.\nTo support reproducible research, we offer an open-source codebase and outline\npotential directions for future investigations based on our insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across a wide\nrange of natural language processing tasks. However, Chinese LLMs face unique\nchallenges, primarily due to the dominance of unstructured free text and the\nlack of structured representations in Chinese corpora. While existing\nbenchmarks for LLMs partially assess Chinese LLMs, they are still predominantly\nEnglish-centric and fail to address the unique linguistic characteristics of\nChinese, lacking structured datasets essential for robust evaluation. To\naddress these challenges, we present a Comprehensive Benchmark for Evaluating\nChinese Large Language Models (CB-ECLLM) based on the newly constructed Chinese\nData-Text Pair (CDTP) dataset. Specifically, CDTP comprises over 7 million\naligned text pairs, each consisting of unstructured text coupled with one or\nmore corresponding triples, alongside a total of 15 million triples spanning\nfour critical domains. The core contributions of CDTP are threefold: (i)\nenriching Chinese corpora with high-quality structured information; (ii)\nenabling fine-grained evaluation tailored to knowledge-driven tasks; and (iii)\nsupporting multi-task fine-tuning to assess generalization and robustness\nacross scenarios, including Knowledge Graph Completion, Triple-to-Text\ngeneration, and Question Answering. Furthermore, we conduct rigorous\nevaluations through extensive experiments and ablation studies to assess the\neffectiveness, Supervised Fine-Tuning (SFT), and robustness of the benchmark.\nTo support reproducible research, we offer an open-source codebase and outline\npotential directions for future investigations based on our insights."
                },
                "authors": [
                    {
                        "name": "Chengwei Wu"
                    },
                    {
                        "name": "Jiapu Wang"
                    },
                    {
                        "name": "Mingyang Gao"
                    },
                    {
                        "name": "Xingrui Zhuo"
                    },
                    {
                        "name": "Jipeng Guo"
                    },
                    {
                        "name": "Runlin Lei"
                    },
                    {
                        "name": "Haoran Luo"
                    },
                    {
                        "name": "Tianyu Chen"
                    },
                    {
                        "name": "Haoyi Zhou"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Zechao Li"
                    }
                ],
                "author_detail": {
                    "name": "Zechao Li"
                },
                "author": "Zechao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12961v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12961v3",
                "updated": "2025-10-07T15:30:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    30,
                    8,
                    1,
                    280,
                    0
                ],
                "published": "2025-04-17T14:07:11Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    7,
                    11,
                    3,
                    107,
                    0
                ],
                "title": "QLLM: Do We Really Need a Mixing Network for Credit Assignment in\n  Multi-Agent Reinforcement Learning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QLLM: Do We Really Need a Mixing Network for Credit Assignment in\n  Multi-Agent Reinforcement Learning?"
                },
                "summary": "Credit assignment has remained a fundamental challenge in multi-agent\nreinforcement learning (MARL). Previous studies have primarily addressed this\nissue through value decomposition methods under the centralized training with\ndecentralized execution paradigm, where neural networks are utilized to\napproximate the nonlinear relationship between individual Q-values and the\nglobal Q-value. Although these approaches have achieved considerable success in\nvarious benchmark tasks, they still suffer from several limitations, including\nimprecise attribution of contributions, limited interpretability, and poor\nscalability in high-dimensional state spaces. To address these challenges, we\npropose a novel algorithm, \\textbf{QLLM}, which facilitates the automatic\nconstruction of credit assignment functions using large language models (LLMs).\nSpecifically, the concept of \\textbf{TFCAF} is introduced, wherein the credit\nallocation process is represented as a direct and expressive nonlinear\nfunctional formulation. A custom-designed \\textit{coder-evaluator} framework is\nfurther employed to guide the generation, verification, and refinement of\nexecutable code by LLMs, significantly mitigating issues such as hallucination\nand shallow reasoning during inference. Extensive experiments conducted on\nseveral standard MARL benchmarks demonstrate that the proposed method\nconsistently outperforms existing state-of-the-art baselines. Moreover, QLLM\nexhibits strong generalization capability and maintains compatibility with a\nwide range of MARL algorithms that utilize mixing networks, positioning it as a\npromising and versatile solution for complex multi-agent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Credit assignment has remained a fundamental challenge in multi-agent\nreinforcement learning (MARL). Previous studies have primarily addressed this\nissue through value decomposition methods under the centralized training with\ndecentralized execution paradigm, where neural networks are utilized to\napproximate the nonlinear relationship between individual Q-values and the\nglobal Q-value. Although these approaches have achieved considerable success in\nvarious benchmark tasks, they still suffer from several limitations, including\nimprecise attribution of contributions, limited interpretability, and poor\nscalability in high-dimensional state spaces. To address these challenges, we\npropose a novel algorithm, \\textbf{QLLM}, which facilitates the automatic\nconstruction of credit assignment functions using large language models (LLMs).\nSpecifically, the concept of \\textbf{TFCAF} is introduced, wherein the credit\nallocation process is represented as a direct and expressive nonlinear\nfunctional formulation. A custom-designed \\textit{coder-evaluator} framework is\nfurther employed to guide the generation, verification, and refinement of\nexecutable code by LLMs, significantly mitigating issues such as hallucination\nand shallow reasoning during inference. Extensive experiments conducted on\nseveral standard MARL benchmarks demonstrate that the proposed method\nconsistently outperforms existing state-of-the-art baselines. Moreover, QLLM\nexhibits strong generalization capability and maintains compatibility with a\nwide range of MARL algorithms that utilize mixing networks, positioning it as a\npromising and versatile solution for complex multi-agent scenarios."
                },
                "authors": [
                    {
                        "name": "Zhouyang Jiang"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Airong Wei"
                    },
                    {
                        "name": "Zhiwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Xu"
                },
                "author": "Zhiwei Xu",
                "arxiv_comment": "We are withdrawing this manuscript due to experimental errors and\n  mistakes in data preprocessing. These issues materially affect the results\n  and could mislead subsequent studies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12961v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12961v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06018v1",
                "updated": "2025-10-07T15:16:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    16,
                    47,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:16:47Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    16,
                    47,
                    1,
                    280,
                    0
                ],
                "title": "Evaluating The Impact of Stimulus Quality in Investigations of LLM\n  Language Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating The Impact of Stimulus Quality in Investigations of LLM\n  Language Performance"
                },
                "summary": "Recent studies employing Large Language Models (LLMs) to test the Argument\nfrom the Poverty of the Stimulus (APS) have yielded contrasting results across\nsyntactic phenomena. This paper investigates the hypothesis that\ncharacteristics of the stimuli used in recent studies, including lexical\nambiguities and structural complexities, may confound model performance. A\nmethodology is proposed for re-evaluating LLM competence on syntactic\nprediction, focusing on GPT-2. This involves: 1) establishing a baseline on\npreviously used (both filtered and unfiltered) stimuli, and 2) generating a\nnew, refined dataset using a state-of-the-art (SOTA) generative LLM (Gemini 2.5\nPro Preview) guided by linguistically-informed templates designed to mitigate\nidentified confounds. Our preliminary findings indicate that GPT-2 demonstrates\nnotably improved performance on these refined PG stimuli compared to baselines,\nsuggesting that stimulus quality significantly influences outcomes in\nsurprisal-based evaluations of LLM syntactic competency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies employing Large Language Models (LLMs) to test the Argument\nfrom the Poverty of the Stimulus (APS) have yielded contrasting results across\nsyntactic phenomena. This paper investigates the hypothesis that\ncharacteristics of the stimuli used in recent studies, including lexical\nambiguities and structural complexities, may confound model performance. A\nmethodology is proposed for re-evaluating LLM competence on syntactic\nprediction, focusing on GPT-2. This involves: 1) establishing a baseline on\npreviously used (both filtered and unfiltered) stimuli, and 2) generating a\nnew, refined dataset using a state-of-the-art (SOTA) generative LLM (Gemini 2.5\nPro Preview) guided by linguistically-informed templates designed to mitigate\nidentified confounds. Our preliminary findings indicate that GPT-2 demonstrates\nnotably improved performance on these refined PG stimuli compared to baselines,\nsuggesting that stimulus quality significantly influences outcomes in\nsurprisal-based evaluations of LLM syntactic competency."
                },
                "authors": [
                    {
                        "name": "Timothy Pistotti"
                    },
                    {
                        "name": "Jason Brown"
                    },
                    {
                        "name": "Michael Witbrock"
                    }
                ],
                "author_detail": {
                    "name": "Michael Witbrock"
                },
                "author": "Michael Witbrock",
                "arxiv_comment": "Presented at https://brigap-workshop.github.io/ Information to be\n  updated upon publication of proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02418v2",
                "updated": "2025-10-07T15:12:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    12,
                    39,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-02T15:22:21Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    22,
                    21,
                    3,
                    275,
                    0
                ],
                "title": "BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks"
                },
                "summary": "LLM web agents now browse and take actions on the open web, yet current agent\nevaluations are constrained to sandboxed environments or artificial tasks. We\nintroduce BrowserArena, a live open-web agent evaluation platform that collects\nuser-submitted tasks, runs Arena-style head-to-head comparisons, and uses\nstep-level human feedback to surface failure modes. Collecting and analyzing\nstep-level annotations on the agent traces, we identify three consistent\nfailure modes: captcha resolution, pop-up banner removal, and direct navigation\nto URLs. By constructing targeted datasets to further study these tasks, we\ndiscover variations in how different language models navigate these failure\nmodes. We find, for example, that o4-mini deploys a wider variety of strategies\nto circumvent captcha resolution than other models and DeepSeek-R1 consistently\nmisleads users about pop-up banner closure. Our findings surface both the\ndiversity and brittleness of current web agents. More broadly, our benchmarking\nmethodology provides an approach to evaluating and understanding web agent\nfailure modes at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM web agents now browse and take actions on the open web, yet current agent\nevaluations are constrained to sandboxed environments or artificial tasks. We\nintroduce BrowserArena, a live open-web agent evaluation platform that collects\nuser-submitted tasks, runs Arena-style head-to-head comparisons, and uses\nstep-level human feedback to surface failure modes. Collecting and analyzing\nstep-level annotations on the agent traces, we identify three consistent\nfailure modes: captcha resolution, pop-up banner removal, and direct navigation\nto URLs. By constructing targeted datasets to further study these tasks, we\ndiscover variations in how different language models navigate these failure\nmodes. We find, for example, that o4-mini deploys a wider variety of strategies\nto circumvent captcha resolution than other models and DeepSeek-R1 consistently\nmisleads users about pop-up banner closure. Our findings surface both the\ndiversity and brittleness of current web agents. More broadly, our benchmarking\nmethodology provides an approach to evaluating and understanding web agent\nfailure modes at scale."
                },
                "authors": [
                    {
                        "name": "Sagnik Anupam"
                    },
                    {
                        "name": "Davis Brown"
                    },
                    {
                        "name": "Shuo Li"
                    },
                    {
                        "name": "Eric Wong"
                    },
                    {
                        "name": "Hamed Hassani"
                    },
                    {
                        "name": "Osbert Bastani"
                    }
                ],
                "author_detail": {
                    "name": "Osbert Bastani"
                },
                "author": "Osbert Bastani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06001v1",
                "updated": "2025-10-07T15:03:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    3,
                    9,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T15:03:09Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    3,
                    9,
                    1,
                    280,
                    0
                ],
                "title": "Exploring Gaps in the APS: Direct Minimal Pair Analysis in LLM Syntactic\n  Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Gaps in the APS: Direct Minimal Pair Analysis in LLM Syntactic\n  Assessments"
                },
                "summary": "Recent studies probing the Argument from the Poverty of the Stimulus (APS)\nhave applied Large Language Models (LLMs) to test the learnability of complex\nsyntax through surprisal-based metrics. However, divergent conclusions raise\nquestions concerning the insights these metrics offer. While Wilcox et al.\n(2024) used direct minimal pair comparisons (the \"wh-effect\") to demonstrate\nthat models successfully generalise knowledge of filler-gap dependencies, Lan\net al. (2024) used a Difference-in-Differences (DiD) metric and found that\nmodels largely fail on parasitic gaps (PGs). This paper argues that the direct\nminimal pair approach offers greater diagnostic transparency. We demonstrate\nthis by generating a full 8-permutation paradigm of refined PG stimuli and\nevaluating the GPT-2 model used in previous studies with a systematic\nWilcox-style wh-effect analysis. Our results show that GPT-2 succeeds across\nall four tested conditions, indicating robust knowledge of filler-gap licensing\nprinciples even in complex PG environments. This finding, which contrasts with\nthe more ambiguous results from DiD-style metrics, suggests that the choice of\nevaluation metric is critical for assessing an LLM's syntactic competence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies probing the Argument from the Poverty of the Stimulus (APS)\nhave applied Large Language Models (LLMs) to test the learnability of complex\nsyntax through surprisal-based metrics. However, divergent conclusions raise\nquestions concerning the insights these metrics offer. While Wilcox et al.\n(2024) used direct minimal pair comparisons (the \"wh-effect\") to demonstrate\nthat models successfully generalise knowledge of filler-gap dependencies, Lan\net al. (2024) used a Difference-in-Differences (DiD) metric and found that\nmodels largely fail on parasitic gaps (PGs). This paper argues that the direct\nminimal pair approach offers greater diagnostic transparency. We demonstrate\nthis by generating a full 8-permutation paradigm of refined PG stimuli and\nevaluating the GPT-2 model used in previous studies with a systematic\nWilcox-style wh-effect analysis. Our results show that GPT-2 succeeds across\nall four tested conditions, indicating robust knowledge of filler-gap licensing\nprinciples even in complex PG environments. This finding, which contrasts with\nthe more ambiguous results from DiD-style metrics, suggests that the choice of\nevaluation metric is critical for assessing an LLM's syntactic competence."
                },
                "authors": [
                    {
                        "name": "Timothy Pistotti"
                    },
                    {
                        "name": "Jason Brown"
                    },
                    {
                        "name": "Michael Witbrock"
                    }
                ],
                "author_detail": {
                    "name": "Michael Witbrock"
                },
                "author": "Michael Witbrock",
                "arxiv_comment": "Presented at the https://brigap-workshop.github.io/ Information to be\n  updated after publication of proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15031v2",
                "updated": "2025-10-07T15:01:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    1,
                    35,
                    1,
                    280,
                    0
                ],
                "published": "2025-09-18T14:56:50Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    56,
                    50,
                    3,
                    261,
                    0
                ],
                "title": "AutoEdit: Automatic Hyperparameter Tuning for Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoEdit: Automatic Hyperparameter Tuning for Image Editing"
                },
                "summary": "Recent advances in diffusion models have revolutionized text-guided image\nediting, yet existing editing methods face critical challenges in\nhyperparameter identification. To get the reasonable editing performance, these\nmethods often require the user to brute-force tune multiple interdependent\nhyperparameters, such as inversion timesteps and attention modification. This\nprocess incurs high computational costs due to the huge hyperparameter search\nspace. We consider searching optimal editing's hyperparameters as a sequential\ndecision-making task within the diffusion denoising process. Specifically, we\npropose a reinforcement learning framework, which establishes a Markov Decision\nProcess that dynamically adjusts hyperparameters across denoising steps,\nintegrating editing objectives into a reward function. The method achieves time\nefficiency through proximal policy optimization while maintaining optimal\nhyperparameter configurations. Experiments demonstrate significant reduction in\nsearch time and computational overhead compared to existing brute-force\napproaches, advancing the practical deployment of a diffusion-based image\nediting framework in the real world. Codes can be found at\nhttps://github.com/chaupham1709/AutoEdit.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have revolutionized text-guided image\nediting, yet existing editing methods face critical challenges in\nhyperparameter identification. To get the reasonable editing performance, these\nmethods often require the user to brute-force tune multiple interdependent\nhyperparameters, such as inversion timesteps and attention modification. This\nprocess incurs high computational costs due to the huge hyperparameter search\nspace. We consider searching optimal editing's hyperparameters as a sequential\ndecision-making task within the diffusion denoising process. Specifically, we\npropose a reinforcement learning framework, which establishes a Markov Decision\nProcess that dynamically adjusts hyperparameters across denoising steps,\nintegrating editing objectives into a reward function. The method achieves time\nefficiency through proximal policy optimization while maintaining optimal\nhyperparameter configurations. Experiments demonstrate significant reduction in\nsearch time and computational overhead compared to existing brute-force\napproaches, advancing the practical deployment of a diffusion-based image\nediting framework in the real world. Codes can be found at\nhttps://github.com/chaupham1709/AutoEdit.git."
                },
                "authors": [
                    {
                        "name": "Chau Pham"
                    },
                    {
                        "name": "Quan Dao"
                    },
                    {
                        "name": "Mahesh Bhosale"
                    },
                    {
                        "name": "Yunjie Tian"
                    },
                    {
                        "name": "Dimitris Metaxas"
                    },
                    {
                        "name": "David Doermann"
                    }
                ],
                "author_detail": {
                    "name": "David Doermann"
                },
                "author": "David Doermann",
                "arxiv_comment": "Provided code link",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05995v1",
                "updated": "2025-10-07T14:57:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    57,
                    28,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T14:57:28Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    57,
                    28,
                    1,
                    280,
                    0
                ],
                "title": "A comprehensive comparison of neural operators for 3D industry-scale\n  engineering designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comprehensive comparison of neural operators for 3D industry-scale\n  engineering designs"
                },
                "summary": "Neural operators have emerged as powerful tools for learning nonlinear\nmappings between function spaces, enabling real-time prediction of complex\ndynamics in diverse scientific and engineering applications. With their growing\nadoption in engineering design evaluation, a wide range of neural operator\narchitectures have been proposed for various problem settings. However, model\nselection remains challenging due to the absence of fair and comprehensive\ncomparisons. To address this, we propose and standardize six representative 3D\nindustry-scale engineering design datasets spanning thermal analysis, linear\nelasticity, elasto-plasticity, time-dependent plastic problems, and\ncomputational fluid dynamics. All datasets include fully preprocessed inputs\nand outputs for model training, making them directly usable across diverse\nneural operator architectures. Using these datasets, we conduct a systematic\ncomparison of four types of neural operator variants, including\nBranch-Trunk-based Neural Operators inspired by DeepONet, Graph-based Neural\nOperators inspired by Graph Neural Networks, Grid-based Neural Operators\ninspired by Fourier Neural Operators, and Point-based Neural Operators inspired\nby PointNet. We further introduce practical enhancements to adapt these models\nto different engineering settings, improving the fairness of the comparison.\nOur benchmarking study evaluates each model strengths and limitations in terms\nof predictive performance, computational efficiency, memory usage, and\ndeployment complexity. The findings provide actionable insights to guide future\nneural operator development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural operators have emerged as powerful tools for learning nonlinear\nmappings between function spaces, enabling real-time prediction of complex\ndynamics in diverse scientific and engineering applications. With their growing\nadoption in engineering design evaluation, a wide range of neural operator\narchitectures have been proposed for various problem settings. However, model\nselection remains challenging due to the absence of fair and comprehensive\ncomparisons. To address this, we propose and standardize six representative 3D\nindustry-scale engineering design datasets spanning thermal analysis, linear\nelasticity, elasto-plasticity, time-dependent plastic problems, and\ncomputational fluid dynamics. All datasets include fully preprocessed inputs\nand outputs for model training, making them directly usable across diverse\nneural operator architectures. Using these datasets, we conduct a systematic\ncomparison of four types of neural operator variants, including\nBranch-Trunk-based Neural Operators inspired by DeepONet, Graph-based Neural\nOperators inspired by Graph Neural Networks, Grid-based Neural Operators\ninspired by Fourier Neural Operators, and Point-based Neural Operators inspired\nby PointNet. We further introduce practical enhancements to adapt these models\nto different engineering settings, improving the fairness of the comparison.\nOur benchmarking study evaluates each model strengths and limitations in terms\nof predictive performance, computational efficiency, memory usage, and\ndeployment complexity. The findings provide actionable insights to guide future\nneural operator development."
                },
                "authors": [
                    {
                        "name": "Weiheng Zhong"
                    },
                    {
                        "name": "Qibang Liu"
                    },
                    {
                        "name": "Diab Abueidda"
                    },
                    {
                        "name": "Seid Koric"
                    },
                    {
                        "name": "Hadi Meidani"
                    }
                ],
                "author_detail": {
                    "name": "Hadi Meidani"
                },
                "author": "Hadi Meidani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02444v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02444v6",
                "updated": "2025-10-07T14:57:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    57,
                    19,
                    1,
                    280,
                    0
                ],
                "published": "2025-02-04T16:10:55Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    10,
                    55,
                    1,
                    35,
                    0
                ],
                "title": "Generative Psycho-Lexical Approach for Constructing Value Systems in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Psycho-Lexical Approach for Constructing Value Systems in\n  Large Language Models"
                },
                "summary": "Values are core drivers of individual and collective perception, cognition,\nand behavior. Value systems, such as Schwartz's Theory of Basic Human Values,\ndelineate the hierarchy and interplay among these values, enabling\ncross-disciplinary investigations into decision-making and societal dynamics.\nRecently, the rise of Large Language Models (LLMs) has raised concerns\nregarding their elusive intrinsic values. Despite growing efforts in\nevaluating, understanding, and aligning LLM values, a psychologically grounded\nLLM value system remains underexplored. This study addresses the gap by\nintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,\nadaptable, and theoretically informed method for constructing value systems.\nLeveraging GPLA, we propose a psychologically grounded five-factor value system\ntailored for LLMs. For systematic validation, we present three benchmarking\ntasks that integrate psychological principles with cutting-edge AI priorities.\nOur results reveal that the proposed value system meets standard psychological\ncriteria, better captures LLM values, improves LLM safety prediction, and\nenhances LLM alignment, when compared to the canonical Schwartz's values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Values are core drivers of individual and collective perception, cognition,\nand behavior. Value systems, such as Schwartz's Theory of Basic Human Values,\ndelineate the hierarchy and interplay among these values, enabling\ncross-disciplinary investigations into decision-making and societal dynamics.\nRecently, the rise of Large Language Models (LLMs) has raised concerns\nregarding their elusive intrinsic values. Despite growing efforts in\nevaluating, understanding, and aligning LLM values, a psychologically grounded\nLLM value system remains underexplored. This study addresses the gap by\nintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,\nadaptable, and theoretically informed method for constructing value systems.\nLeveraging GPLA, we propose a psychologically grounded five-factor value system\ntailored for LLMs. For systematic validation, we present three benchmarking\ntasks that integrate psychological principles with cutting-edge AI priorities.\nOur results reveal that the proposed value system meets standard psychological\ncriteria, better captures LLM values, improves LLM safety prediction, and\nenhances LLM alignment, when compared to the canonical Schwartz's values."
                },
                "authors": [
                    {
                        "name": "Haoran Ye"
                    },
                    {
                        "name": "Tianze Zhang"
                    },
                    {
                        "name": "Yuhang Xie"
                    },
                    {
                        "name": "Liyuan Zhang"
                    },
                    {
                        "name": "Yuanyi Ren"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Guojie Song"
                    }
                ],
                "author_detail": {
                    "name": "Guojie Song"
                },
                "author": "Guojie Song",
                "arxiv_comment": "ACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02444v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02444v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11773v2",
                "updated": "2025-10-07T14:55:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    55,
                    30,
                    1,
                    280,
                    0
                ],
                "published": "2025-09-15T10:53:05Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    10,
                    53,
                    5,
                    0,
                    258,
                    0
                ],
                "title": "AgenticIE: An Adaptive Agent for Information Extraction from Complex\n  Regulatory Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgenticIE: An Adaptive Agent for Information Extraction from Complex\n  Regulatory Documents"
                },
                "summary": "Declaration of Performance (DoP) documents, mandated by EU regulation,\ncertify the performance of construction products. There are two challenges to\nmake DoPs machine and human accessible through automated key-value pair\nextraction (KVP) and question answering (QA): (1) While some of their content\nis standardized, DoPs vary widely in layout, schema, and format; (2) Both users\nand documents are multilingual. Existing static or LLM-only Information\nExtraction (IE) pipelines fail to adapt to this structural document and user\ndiversity. Our domain-specific, agentic system addresses these challenges\nthrough a planner-executor-responder architecture. The system infers user\nintent, detects document language and modality, and orchestrates tools\ndynamically for robust, traceable reasoning while avoiding tool misuse or\nexecution loops. Our agent outperforms baselines (ROUGE: 0.783 vs. 0.703/0.608)\nwith better cross-lingual stability (17-point vs. 21-26-point variation).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Declaration of Performance (DoP) documents, mandated by EU regulation,\ncertify the performance of construction products. There are two challenges to\nmake DoPs machine and human accessible through automated key-value pair\nextraction (KVP) and question answering (QA): (1) While some of their content\nis standardized, DoPs vary widely in layout, schema, and format; (2) Both users\nand documents are multilingual. Existing static or LLM-only Information\nExtraction (IE) pipelines fail to adapt to this structural document and user\ndiversity. Our domain-specific, agentic system addresses these challenges\nthrough a planner-executor-responder architecture. The system infers user\nintent, detects document language and modality, and orchestrates tools\ndynamically for robust, traceable reasoning while avoiding tool misuse or\nexecution loops. Our agent outperforms baselines (ROUGE: 0.783 vs. 0.703/0.608)\nwith better cross-lingual stability (17-point vs. 21-26-point variation)."
                },
                "authors": [
                    {
                        "name": "Gaye Colakoglu"
                    },
                    {
                        "name": "Gürkan Solmaz"
                    },
                    {
                        "name": "Jonathan Fürst"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Fürst"
                },
                "author": "Jonathan Fürst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05987v1",
                "updated": "2025-10-07T14:46:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    46,
                    12,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T14:46:12Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    46,
                    12,
                    1,
                    280,
                    0
                ],
                "title": "Sample Smart, Not Hard: Correctness-First Decoding for Better Reasoning\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample Smart, Not Hard: Correctness-First Decoding for Better Reasoning\n  in LLMs"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied to complex tasks that\nrequire extended reasoning. In such settings, models often benefit from diverse\nchains-of-thought to arrive at multiple candidate solutions. This requires two\ncompeting objectives: to inject enough stochasticity to explore multiple\nreasoning chains, and to ensure sufficient accuracy and quality in each path.\nExisting works pursue the first objective by increasing exploration at highly\nuncertain steps with higher temperature or larger candidate token sets, while\nothers improve reliability by rejecting samples with low confidence\npost-generation, implying that low confidence correlates with low answer\nquality. These two lines of thought are in conflict, as they conflate different\nsources of uncertainty. To resolve this, we argue that the decoding rule should\nbe calibrated by correctness, not confidence alone. We should sample from\ntokens with higher estimated correctness, and reduce sampling where expected\ncorrectness is low. We propose simple strategies that achieve this goal:\nGreedy-Threshold makes sampling greedy at very low confidence steps.\nCalibrated-TopK and Calibrated-epsilon set truncation threshold based on\nestimated rank-wise correctness. Together, our findings challenge prevailing\nheuristics about decoding under uncertainty and show gains across math and\ngeneral reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied to complex tasks that\nrequire extended reasoning. In such settings, models often benefit from diverse\nchains-of-thought to arrive at multiple candidate solutions. This requires two\ncompeting objectives: to inject enough stochasticity to explore multiple\nreasoning chains, and to ensure sufficient accuracy and quality in each path.\nExisting works pursue the first objective by increasing exploration at highly\nuncertain steps with higher temperature or larger candidate token sets, while\nothers improve reliability by rejecting samples with low confidence\npost-generation, implying that low confidence correlates with low answer\nquality. These two lines of thought are in conflict, as they conflate different\nsources of uncertainty. To resolve this, we argue that the decoding rule should\nbe calibrated by correctness, not confidence alone. We should sample from\ntokens with higher estimated correctness, and reduce sampling where expected\ncorrectness is low. We propose simple strategies that achieve this goal:\nGreedy-Threshold makes sampling greedy at very low confidence steps.\nCalibrated-TopK and Calibrated-epsilon set truncation threshold based on\nestimated rank-wise correctness. Together, our findings challenge prevailing\nheuristics about decoding under uncertainty and show gains across math and\ngeneral reasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Xueyan Li"
                    },
                    {
                        "name": "Guinan Su"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05976v1",
                "updated": "2025-10-07T14:30:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    30,
                    36,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T14:30:36Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    30,
                    36,
                    1,
                    280,
                    0
                ],
                "title": "Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective\n  Taxonomy and Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective\n  Taxonomy and Performance Analysis"
                },
                "summary": "Low-light image enhancement (LLIE) is vital for safety-critical applications\nsuch as surveillance, autonomous navigation, and medical imaging, where\nvisibility degradation can impair downstream task performance. Recently,\ndiffusion models have emerged as a promising generative paradigm for LLIE due\nto their capacity to model complex image distributions via iterative denoising.\nThis survey provides an up-to-date critical analysis of diffusion models for\nLLIE, distinctively featuring an in-depth comparative performance evaluation\nagainst Generative Adversarial Network and Transformer-based state-of-the-art\nmethods, a thorough examination of practical deployment challenges, and a\nforward-looking perspective on the role of emerging paradigms like foundation\nmodels. We propose a multi-perspective taxonomy encompassing six categories:\nIntrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal,\nand Autonomous; that map enhancement methods across physical priors,\nconditioning schemes, and computational efficiency. Our taxonomy is grounded in\na hybrid view of both the model mechanism and the conditioning signals. We\nevaluate qualitative failure modes, benchmark inconsistencies, and trade-offs\nbetween interpretability, generalization, and inference efficiency. We also\ndiscuss real-world deployment constraints (e.g., memory, energy use) and\nethical considerations. This survey aims to guide the next generation of\ndiffusion-based LLIE research by highlighting trends and surfacing open\nresearch questions, including novel conditioning, real-time adaptation, and the\npotential of foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-light image enhancement (LLIE) is vital for safety-critical applications\nsuch as surveillance, autonomous navigation, and medical imaging, where\nvisibility degradation can impair downstream task performance. Recently,\ndiffusion models have emerged as a promising generative paradigm for LLIE due\nto their capacity to model complex image distributions via iterative denoising.\nThis survey provides an up-to-date critical analysis of diffusion models for\nLLIE, distinctively featuring an in-depth comparative performance evaluation\nagainst Generative Adversarial Network and Transformer-based state-of-the-art\nmethods, a thorough examination of practical deployment challenges, and a\nforward-looking perspective on the role of emerging paradigms like foundation\nmodels. We propose a multi-perspective taxonomy encompassing six categories:\nIntrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal,\nand Autonomous; that map enhancement methods across physical priors,\nconditioning schemes, and computational efficiency. Our taxonomy is grounded in\na hybrid view of both the model mechanism and the conditioning signals. We\nevaluate qualitative failure modes, benchmark inconsistencies, and trade-offs\nbetween interpretability, generalization, and inference efficiency. We also\ndiscuss real-world deployment constraints (e.g., memory, energy use) and\nethical considerations. This survey aims to guide the next generation of\ndiffusion-based LLIE research by highlighting trends and surfacing open\nresearch questions, including novel conditioning, real-time adaptation, and the\npotential of foundation models."
                },
                "authors": [
                    {
                        "name": "Eashan Adhikarla"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Brian D. Davison"
                    }
                ],
                "author_detail": {
                    "name": "Brian D. Davison"
                },
                "author": "Brian D. Davison",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05972v1",
                "updated": "2025-10-07T14:28:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    28,
                    30,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T14:28:30Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    28,
                    30,
                    1,
                    280,
                    0
                ],
                "title": "LexiCon: a Benchmark for Planning under Temporal Constraints in Natural\n  Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LexiCon: a Benchmark for Planning under Temporal Constraints in Natural\n  Language"
                },
                "summary": "Owing to their reasoning capabilities, large language models (LLMs) have been\nevaluated on planning tasks described in natural language. However, LLMs have\nlargely been tested on planning domains without constraints. In order to deploy\nthem in real-world settings where adherence to constraints, in particular\nsafety constraints, is critical, we need to evaluate their performance on\nconstrained planning tasks. We introduce LexiCon -- a natural language-based\n(Lexi) constrained (Con) planning benchmark, consisting of a suite of\nenvironments, that can be used to evaluate the planning capabilities of LLMs in\na principled fashion. The core idea behind LexiCon is to take existing planning\nenvironments and impose temporal constraints on the states. These constrained\nproblems are then translated into natural language and given to an LLM to\nsolve. A key feature of LexiCon is its extensibility. That is, the set of\nsupported environments can be extended with new (unconstrained) environment\ngenerators, for which temporal constraints are constructed automatically. This\nrenders LexiCon future-proof: the hardness of the generated planning problems\ncan be increased as the planning capabilities of LLMs improve. Our experiments\nreveal that the performance of state-of-the-art LLMs, including reasoning\nmodels like GPT-5, o3, and R1, deteriorates as the degree of constrainedness of\nthe planning tasks increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Owing to their reasoning capabilities, large language models (LLMs) have been\nevaluated on planning tasks described in natural language. However, LLMs have\nlargely been tested on planning domains without constraints. In order to deploy\nthem in real-world settings where adherence to constraints, in particular\nsafety constraints, is critical, we need to evaluate their performance on\nconstrained planning tasks. We introduce LexiCon -- a natural language-based\n(Lexi) constrained (Con) planning benchmark, consisting of a suite of\nenvironments, that can be used to evaluate the planning capabilities of LLMs in\na principled fashion. The core idea behind LexiCon is to take existing planning\nenvironments and impose temporal constraints on the states. These constrained\nproblems are then translated into natural language and given to an LLM to\nsolve. A key feature of LexiCon is its extensibility. That is, the set of\nsupported environments can be extended with new (unconstrained) environment\ngenerators, for which temporal constraints are constructed automatically. This\nrenders LexiCon future-proof: the hardness of the generated planning problems\ncan be increased as the planning capabilities of LLMs improve. Our experiments\nreveal that the performance of state-of-the-art LLMs, including reasoning\nmodels like GPT-5, o3, and R1, deteriorates as the degree of constrainedness of\nthe planning tasks increases."
                },
                "authors": [
                    {
                        "name": "Periklis Mantenoglou"
                    },
                    {
                        "name": "Rishi Hazra"
                    },
                    {
                        "name": "Pedro Zuidberg Dos Martires"
                    },
                    {
                        "name": "Luc De Raedt"
                    }
                ],
                "author_detail": {
                    "name": "Luc De Raedt"
                },
                "author": "Luc De Raedt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05969v1",
                "updated": "2025-10-07T14:24:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    24,
                    32,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T14:24:32Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    24,
                    32,
                    1,
                    280,
                    0
                ],
                "title": "Probing the Difficulty Perception Mechanism of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Difficulty Perception Mechanism of Large Language Models"
                },
                "summary": "Large language models (LLMs) are increasingly deployed on complex reasoning\ntasks, yet little is known about their ability to internally evaluate problem\ndifficulty, which is an essential capability for adaptive reasoning and\nefficient resource allocation. In this work, we investigate whether LLMs\nimplicitly encode problem difficulty in their internal representations. Using a\nlinear probe on the final-token representations of LLMs, we demonstrate that\nthe difficulty level of math problems can be linearly modeled. We further\nlocate the specific attention heads of the final Transformer layer: these\nattention heads have opposite activation patterns for simple and difficult\nproblems, thus achieving perception of difficulty. Our ablation experiments\nprove the accuracy of the location. Crucially, our experiments provide\npractical support for using LLMs as automatic difficulty annotators,\npotentially substantially reducing reliance on costly human labeling in\nbenchmark construction and curriculum learning. We also uncover that there is a\nsignificant difference in entropy and difficulty perception at the token level.\nOur study reveals that difficulty perception in LLMs is not only present but\nalso structurally organized, offering new theoretical insights and practical\ndirections for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed on complex reasoning\ntasks, yet little is known about their ability to internally evaluate problem\ndifficulty, which is an essential capability for adaptive reasoning and\nefficient resource allocation. In this work, we investigate whether LLMs\nimplicitly encode problem difficulty in their internal representations. Using a\nlinear probe on the final-token representations of LLMs, we demonstrate that\nthe difficulty level of math problems can be linearly modeled. We further\nlocate the specific attention heads of the final Transformer layer: these\nattention heads have opposite activation patterns for simple and difficult\nproblems, thus achieving perception of difficulty. Our ablation experiments\nprove the accuracy of the location. Crucially, our experiments provide\npractical support for using LLMs as automatic difficulty annotators,\npotentially substantially reducing reliance on costly human labeling in\nbenchmark construction and curriculum learning. We also uncover that there is a\nsignificant difference in entropy and difficulty perception at the token level.\nOur study reveals that difficulty perception in LLMs is not only present but\nalso structurally organized, offering new theoretical insights and practical\ndirections for future research."
                },
                "authors": [
                    {
                        "name": "Sunbowen Lee"
                    },
                    {
                        "name": "Qingyu Yin"
                    },
                    {
                        "name": "Chak Tou Leong"
                    },
                    {
                        "name": "Jialiang Zhang"
                    },
                    {
                        "name": "Yicheng Gong"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05968v1",
                "updated": "2025-10-07T14:23:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    23,
                    24,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T14:23:24Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    23,
                    24,
                    1,
                    280,
                    0
                ],
                "title": "Extending ResourceLink: Patterns for Large Dataset Processing in MCP\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending ResourceLink: Patterns for Large Dataset Processing in MCP\n  Applications"
                },
                "summary": "Large language models translate natural language into database queries, yet\ncontext window limitations prevent direct deployment in reporting systems where\ncomplete datasets exhaust available tokens. The Model Context Protocol\nspecification defines ResourceLink for referencing external resources, but\npractical patterns for implementing scalable reporting architectures remain\nundocumented. This paper presents patterns for building LLM-powered reporting\nsystems that decouple query generation from data retrieval. We introduce a\ndual-response pattern extending ResourceLink to support both iterative query\nrefinement and out-of-band data access, accompanied by patterns for\nmulti-tenant security and resource lifecycle management. These patterns address\nfundamental challenges in LLM-driven reporting applications and provide\npractical guidance for developers building them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models translate natural language into database queries, yet\ncontext window limitations prevent direct deployment in reporting systems where\ncomplete datasets exhaust available tokens. The Model Context Protocol\nspecification defines ResourceLink for referencing external resources, but\npractical patterns for implementing scalable reporting architectures remain\nundocumented. This paper presents patterns for building LLM-powered reporting\nsystems that decouple query generation from data retrieval. We introduce a\ndual-response pattern extending ResourceLink to support both iterative query\nrefinement and out-of-band data access, accompanied by patterns for\nmulti-tenant security and resource lifecycle management. These patterns address\nfundamental challenges in LLM-driven reporting applications and provide\npractical guidance for developers building them."
                },
                "authors": [
                    {
                        "name": "Scott Frees"
                    }
                ],
                "author_detail": {
                    "name": "Scott Frees"
                },
                "author": "Scott Frees",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05961v1",
                "updated": "2025-10-07T14:18:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    18,
                    0,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T14:18:00Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    18,
                    0,
                    1,
                    280,
                    0
                ],
                "title": "msmJAX: Fast and Differentiable Electrostatics on the GPU in Python",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "msmJAX: Fast and Differentiable Electrostatics on the GPU in Python"
                },
                "summary": "We present msmJAX, a Python package implementing the multilevel summation\nmethod with B-spline interpolation, a linear-scaling algorithm for efficiently\nevaluating electrostatic and other long-range interactions in particle-based\nsimulations. Built on the JAX framework, msmJAX integrates naturally with the\nmachine-learning methods that are transforming chemistry and materials science,\nwhile also serving as a powerful tool in its own right. It combines high\nperformance with Python's accessibility, offers easy deployment on GPUs, and\nsupports automatic differentiation. We outline the modular design of msmJAX,\nenabling users to adapt or extend the code, and present benchmarks and\nexamples, including a verification of linear scaling, and demonstrations of its\nstability in molecular-dynamics simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present msmJAX, a Python package implementing the multilevel summation\nmethod with B-spline interpolation, a linear-scaling algorithm for efficiently\nevaluating electrostatic and other long-range interactions in particle-based\nsimulations. Built on the JAX framework, msmJAX integrates naturally with the\nmachine-learning methods that are transforming chemistry and materials science,\nwhile also serving as a powerful tool in its own right. It combines high\nperformance with Python's accessibility, offers easy deployment on GPUs, and\nsupports automatic differentiation. We outline the modular design of msmJAX,\nenabling users to adapt or extend the code, and present benchmarks and\nexamples, including a verification of linear scaling, and demonstrations of its\nstability in molecular-dynamics simulations."
                },
                "authors": [
                    {
                        "name": "Florian Buchner"
                    },
                    {
                        "name": "Johannes Schörghuber"
                    },
                    {
                        "name": "Nico Unglert"
                    },
                    {
                        "name": "Jesús Carrete"
                    },
                    {
                        "name": "Georg K. H. Madsen"
                    }
                ],
                "author_detail": {
                    "name": "Georg K. H. Madsen"
                },
                "author": "Georg K. H. Madsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05950v1",
                "updated": "2025-10-07T14:07:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    7,
                    43,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T14:07:43Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    14,
                    7,
                    43,
                    1,
                    280,
                    0
                ],
                "title": "Training-Free Time Series Classification via In-Context Reasoning with\n  LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Time Series Classification via In-Context Reasoning with\n  LLM Agents"
                },
                "summary": "Time series classification (TSC) spans diverse application scenarios, yet\nlabeled data are often scarce, making task-specific training costly and\ninflexible. Recent reasoning-oriented large language models (LLMs) show promise\nin understanding temporal patterns, but purely zero-shot usage remains\nsuboptimal. We propose FETA, a multi-agent framework for training-free TSC via\nexemplar-based in-context reasoning. FETA decomposes a multivariate series into\nchannel-wise subproblems, retrieves a few structurally similar labeled examples\nfor each channel, and leverages a reasoning LLM to compare the query against\nthese exemplars, producing channel-level labels with self-assessed confidences;\na confidence-weighted aggregator then fuses all channel decisions. This design\neliminates the need for pretraining or fine-tuning, improves efficiency by\npruning irrelevant channels and controlling input length, and enhances\ninterpretability through exemplar grounding and confidence estimation. On nine\nchallenging UEA datasets, FETA achieves strong accuracy under a fully\ntraining-free setting, surpassing multiple trained baselines. These results\ndemonstrate that a multi-agent in-context reasoning framework can transform\nLLMs into competitive, plug-and-play TSC solvers without any parameter\ntraining. The code is available at https://github.com/SongyuanSui/FETATSC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series classification (TSC) spans diverse application scenarios, yet\nlabeled data are often scarce, making task-specific training costly and\ninflexible. Recent reasoning-oriented large language models (LLMs) show promise\nin understanding temporal patterns, but purely zero-shot usage remains\nsuboptimal. We propose FETA, a multi-agent framework for training-free TSC via\nexemplar-based in-context reasoning. FETA decomposes a multivariate series into\nchannel-wise subproblems, retrieves a few structurally similar labeled examples\nfor each channel, and leverages a reasoning LLM to compare the query against\nthese exemplars, producing channel-level labels with self-assessed confidences;\na confidence-weighted aggregator then fuses all channel decisions. This design\neliminates the need for pretraining or fine-tuning, improves efficiency by\npruning irrelevant channels and controlling input length, and enhances\ninterpretability through exemplar grounding and confidence estimation. On nine\nchallenging UEA datasets, FETA achieves strong accuracy under a fully\ntraining-free setting, surpassing multiple trained baselines. These results\ndemonstrate that a multi-agent in-context reasoning framework can transform\nLLMs into competitive, plug-and-play TSC solvers without any parameter\ntraining. The code is available at https://github.com/SongyuanSui/FETATSC."
                },
                "authors": [
                    {
                        "name": "Songyuan Sui"
                    },
                    {
                        "name": "Zihang Xu"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Kwei-Herng Lai"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_comment": "8 pages main content, 12 pages total including appendix, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14763v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14763v2",
                "updated": "2025-10-07T13:59:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    59,
                    20,
                    1,
                    280,
                    0
                ],
                "published": "2025-05-20T17:25:23Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    25,
                    23,
                    1,
                    140,
                    0
                ],
                "title": "Unifying Inference-Time Planning Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Inference-Time Planning Language Generation"
                },
                "summary": "A line of work in planning uses LLM not to generate a plan, but to generate a\nformal representation in some planning language, which can be input into a\nsymbolic solver to deterministically find a plan. While showing improved trust\nand promising performance, dozens of recent publications have proposed\nscattered methods on a variety of benchmarks under different experimental\nsettings. We attempt to unify the inference-time LLM-as-formalizer methodology\nfor classical planning by proposing a unifying framework based on intermediate\nrepresentations. We thus systematically evaluate more than a dozen pipelines\nthat subsume most existing work, while proposing novel ones that involve\nsyntactically similar but high resource intermediate languages (such as a\nPython wrapper of PDDL). We provide recipes for planning language generation\npipelines, draw a series of conclusions showing the efficacy of their various\ncomponents, and evidence their robustness against problem complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A line of work in planning uses LLM not to generate a plan, but to generate a\nformal representation in some planning language, which can be input into a\nsymbolic solver to deterministically find a plan. While showing improved trust\nand promising performance, dozens of recent publications have proposed\nscattered methods on a variety of benchmarks under different experimental\nsettings. We attempt to unify the inference-time LLM-as-formalizer methodology\nfor classical planning by proposing a unifying framework based on intermediate\nrepresentations. We thus systematically evaluate more than a dozen pipelines\nthat subsume most existing work, while proposing novel ones that involve\nsyntactically similar but high resource intermediate languages (such as a\nPython wrapper of PDDL). We provide recipes for planning language generation\npipelines, draw a series of conclusions showing the efficacy of their various\ncomponents, and evidence their robustness against problem complexity."
                },
                "authors": [
                    {
                        "name": "Prabhu Prakash Kagitha"
                    },
                    {
                        "name": "Bo Sun"
                    },
                    {
                        "name": "Ishan Desai"
                    },
                    {
                        "name": "Andrew Zhu"
                    },
                    {
                        "name": "Cassie Huang"
                    },
                    {
                        "name": "Manling Li"
                    },
                    {
                        "name": "Ziyang Li"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14763v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14763v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05943v1",
                "updated": "2025-10-07T13:52:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    52,
                    51,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T13:52:51Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    52,
                    51,
                    1,
                    280,
                    0
                ],
                "title": "EARL: Efficient Agentic Reinforcement Learning Systems for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EARL: Efficient Agentic Reinforcement Learning Systems for Large\n  Language Models"
                },
                "summary": "Reinforcement learning (RL) has become a pivotal component of large language\nmodel (LLM) post-training, and agentic RL extends this paradigm to operate as\nagents through multi-turn interaction and tool use. Scaling such systems\nexposes two practical bottlenecks: (1) context length grows rapidly during\ntraining, inflating memory usage and latency, and triggering out-of-memory\n(OOM) failures; and (2) intermediate tensors accumulate with context length,\nmaking cross-device data movement a major system bottleneck.\n  We present EARL, a scalable system for efficient agentic RL. EARL designs a\nparallelism selector that dynamically adapts model and training parallelism\nacross RL stages based on sequence length and system load, and a data\ndispatcher that performs layout-aware, decentralized exchange of intermediate\ndata batches. Together, these components increase throughput, reduce\nlong-context failures, and enable stable large-scale training of agentic LLMs\nwithout relying on hard limits or penalties of context length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a pivotal component of large language\nmodel (LLM) post-training, and agentic RL extends this paradigm to operate as\nagents through multi-turn interaction and tool use. Scaling such systems\nexposes two practical bottlenecks: (1) context length grows rapidly during\ntraining, inflating memory usage and latency, and triggering out-of-memory\n(OOM) failures; and (2) intermediate tensors accumulate with context length,\nmaking cross-device data movement a major system bottleneck.\n  We present EARL, a scalable system for efficient agentic RL. EARL designs a\nparallelism selector that dynamically adapts model and training parallelism\nacross RL stages based on sequence length and system load, and a data\ndispatcher that performs layout-aware, decentralized exchange of intermediate\ndata batches. Together, these components increase throughput, reduce\nlong-context failures, and enable stable large-scale training of agentic LLMs\nwithout relying on hard limits or penalties of context length."
                },
                "authors": [
                    {
                        "name": "Zheyue Tan"
                    },
                    {
                        "name": "Mustapha Abdullahi"
                    },
                    {
                        "name": "Tuo Shi"
                    },
                    {
                        "name": "Huining Yuan"
                    },
                    {
                        "name": "Zelai Xu"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Boxun Li"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05942v2",
                "updated": "2025-10-08T08:03:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    8,
                    3,
                    38,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-07T13:52:16Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    52,
                    16,
                    1,
                    280,
                    0
                ],
                "title": "EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation\n  for Moral Alignment in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation\n  for Moral Alignment in Large Language Models"
                },
                "summary": "We present EvalMORAAL, a transparent chain-of-thought (CoT) framework that\nuses two scoring methods (log-probabilities and direct ratings) plus a\nmodel-as-judge peer review to evaluate moral alignment in 20 large language\nmodels. We assess models on the World Values Survey (55 countries, 19 topics)\nand the PEW Global Attitudes Survey (39 countries, 8 topics). With EvalMORAAL,\ntop models align closely with survey responses (Pearson's r approximately 0.90\non WVS). Yet we find a clear regional difference: Western regions average\nr=0.82 while non-Western regions average r=0.61 (a 0.21 absolute gap),\nindicating consistent regional bias. Our framework adds three parts: (1) two\nscoring methods for all models to enable fair comparison, (2) a structured\nchain-of-thought protocol with self-consistency checks, and (3) a\nmodel-as-judge peer review that flags 348 conflicts using a data-driven\nthreshold. Peer agreement relates to survey alignment (WVS r=0.74, PEW r=0.39,\nboth p<.001), supporting automated quality checks. These results show real\nprogress toward culture-aware AI while highlighting open challenges for use\nacross regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present EvalMORAAL, a transparent chain-of-thought (CoT) framework that\nuses two scoring methods (log-probabilities and direct ratings) plus a\nmodel-as-judge peer review to evaluate moral alignment in 20 large language\nmodels. We assess models on the World Values Survey (55 countries, 19 topics)\nand the PEW Global Attitudes Survey (39 countries, 8 topics). With EvalMORAAL,\ntop models align closely with survey responses (Pearson's r approximately 0.90\non WVS). Yet we find a clear regional difference: Western regions average\nr=0.82 while non-Western regions average r=0.61 (a 0.21 absolute gap),\nindicating consistent regional bias. Our framework adds three parts: (1) two\nscoring methods for all models to enable fair comparison, (2) a structured\nchain-of-thought protocol with self-consistency checks, and (3) a\nmodel-as-judge peer review that flags 348 conflicts using a data-driven\nthreshold. Peer agreement relates to survey alignment (WVS r=0.74, PEW r=0.39,\nboth p<.001), supporting automated quality checks. These results show real\nprogress toward culture-aware AI while highlighting open challenges for use\nacross regions."
                },
                "authors": [
                    {
                        "name": "Hadi Mohammadi"
                    },
                    {
                        "name": "Anastasia Giachanou"
                    },
                    {
                        "name": "Ayoub Bagheri"
                    }
                ],
                "author_detail": {
                    "name": "Ayoub Bagheri"
                },
                "author": "Ayoub Bagheri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09946v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09946v3",
                "updated": "2025-10-07T13:51:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    51,
                    9,
                    1,
                    280,
                    0
                ],
                "published": "2024-08-19T12:35:23Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    35,
                    23,
                    0,
                    232,
                    0
                ],
                "title": "Fine-Grained and Thematic Evaluation of LLMs in Social Deduction Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained and Thematic Evaluation of LLMs in Social Deduction Game"
                },
                "summary": "Recent studies have investigated whether large language models (LLMs) can\nsupport obscured communication, which is characterized by core aspects such as\ninferring subtext and evading suspicions. To conduct the investigation,\nresearchers have used social deduction games (SDGs) as their experimental\nenvironment, in which players conceal and infer specific information. However,\nprior work has often overlooked how LLMs should be evaluated in such settings.\nSpecifically, we point out two limitations with the evaluation methods they\nemployed. First, metrics used in prior studies are coarse-grained as they are\nbased on overall game outcomes that often fail to capture event-level\nbehaviors; Second, error analyses have lacked structured methodologies capable\nof producing insights that meaningfully support evaluation outcomes. To address\nthese limitations, we propose a microscopic and systematic approach to the\ninvestigation. Specifically, we introduce six fine-grained metrics that resolve\nthe first issue. To tackle the second issue, we conducted a thematic analysis\nand identified four major reasoning failures that undermine LLMs' performance\nin obscured communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have investigated whether large language models (LLMs) can\nsupport obscured communication, which is characterized by core aspects such as\ninferring subtext and evading suspicions. To conduct the investigation,\nresearchers have used social deduction games (SDGs) as their experimental\nenvironment, in which players conceal and infer specific information. However,\nprior work has often overlooked how LLMs should be evaluated in such settings.\nSpecifically, we point out two limitations with the evaluation methods they\nemployed. First, metrics used in prior studies are coarse-grained as they are\nbased on overall game outcomes that often fail to capture event-level\nbehaviors; Second, error analyses have lacked structured methodologies capable\nof producing insights that meaningfully support evaluation outcomes. To address\nthese limitations, we propose a microscopic and systematic approach to the\ninvestigation. Specifically, we introduce six fine-grained metrics that resolve\nthe first issue. To tackle the second issue, we conducted a thematic analysis\nand identified four major reasoning failures that undermine LLMs' performance\nin obscured communication."
                },
                "authors": [
                    {
                        "name": "Byungjun Kim"
                    },
                    {
                        "name": "Dayeon Seo"
                    },
                    {
                        "name": "Minju Kim"
                    },
                    {
                        "name": "Bugeun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Bugeun Kim"
                },
                "author": "Bugeun Kim",
                "arxiv_doi": "10.1109/ACCESS.2025.3611399",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3611399",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.09946v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09946v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in IEEE Access",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05935v1",
                "updated": "2025-10-07T13:46:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    46,
                    6,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T13:46:06Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    46,
                    6,
                    1,
                    280,
                    0
                ],
                "title": "LLM-FS-Agent: A Deliberative Role-based Large Language Model\n  Architecture for Transparent Feature Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-FS-Agent: A Deliberative Role-based Large Language Model\n  Architecture for Transparent Feature Selection"
                },
                "summary": "High-dimensional data remains a pervasive challenge in machine learning,\noften undermining model interpretability and computational efficiency. While\nLarge Language Models (LLMs) have shown promise for dimensionality reduction\nthrough feature selection, existing LLM-based approaches frequently lack\nstructured reasoning and transparent justification for their decisions. This\npaper introduces LLM-FS-Agent, a novel multi-agent architecture designed for\ninterpretable and robust feature selection. The system orchestrates a\ndeliberative \"debate\" among multiple LLM agents, each assigned a specific role,\nenabling collective evaluation of feature relevance and generation of detailed\njustifications. We evaluate LLM-FS-Agent in the cybersecurity domain using the\nCIC-DIAD 2024 IoT intrusion detection dataset and compare its performance\nagainst strong baselines, including LLM-Select and traditional methods such as\nPCA. Experimental results demonstrate that LLM-FS-Agent consistently achieves\nsuperior or comparable classification performance while reducing downstream\ntraining time by an average of 46% (statistically significant improvement, p =\n0.028 for XGBoost). These findings highlight that the proposed deliberative\narchitecture enhances both decision transparency and computational efficiency,\nestablishing LLM-FS-Agent as a practical and reliable solution for real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional data remains a pervasive challenge in machine learning,\noften undermining model interpretability and computational efficiency. While\nLarge Language Models (LLMs) have shown promise for dimensionality reduction\nthrough feature selection, existing LLM-based approaches frequently lack\nstructured reasoning and transparent justification for their decisions. This\npaper introduces LLM-FS-Agent, a novel multi-agent architecture designed for\ninterpretable and robust feature selection. The system orchestrates a\ndeliberative \"debate\" among multiple LLM agents, each assigned a specific role,\nenabling collective evaluation of feature relevance and generation of detailed\njustifications. We evaluate LLM-FS-Agent in the cybersecurity domain using the\nCIC-DIAD 2024 IoT intrusion detection dataset and compare its performance\nagainst strong baselines, including LLM-Select and traditional methods such as\nPCA. Experimental results demonstrate that LLM-FS-Agent consistently achieves\nsuperior or comparable classification performance while reducing downstream\ntraining time by an average of 46% (statistically significant improvement, p =\n0.028 for XGBoost). These findings highlight that the proposed deliberative\narchitecture enhances both decision transparency and computational efficiency,\nestablishing LLM-FS-Agent as a practical and reliable solution for real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Mohamed Bal-Ghaoui"
                    },
                    {
                        "name": "Fayssal Sabri"
                    }
                ],
                "author_detail": {
                    "name": "Fayssal Sabri"
                },
                "author": "Fayssal Sabri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21115v2",
                "updated": "2025-10-07T13:39:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    39,
                    23,
                    1,
                    280,
                    0
                ],
                "published": "2025-07-15T13:04:04Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    13,
                    4,
                    4,
                    1,
                    196,
                    0
                ],
                "title": "FedFlex: Federated Learning for Diverse Netflix Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedFlex: Federated Learning for Diverse Netflix Recommendations"
                },
                "summary": "The drive for personalization in recommender systems creates a tension\nbetween user privacy and the risk of \"filter bubbles\". Although federated\nlearning offers a promising paradigm for privacy-preserving recommendations,\nits impact on diversity remains unclear. We introduce FedFlex, a two-stage\nframework that combines local, on-device fine-tuning of matrix factorization\nmodels (SVD and BPR) with a lightweight Maximal Marginal Relevance (MMR)\nre-ranking step to promote diversity. We conducted the first live user study of\na federated recommender, collecting behavioral data and feedback during a\ntwo-week online deployment. Our results show that FedFlex successfully engages\nusers, with BPR outperforming SVD in click-through rate. Re-ranking with MMR\nconsistently improved ranking quality (nDCG) across both models, with\nstatistically significant gains, particularly for BPR. Diversity effects\nvaried: MMR increased coverage for both models and improved intra-list\ndiversity for BPR, but slightly reduced it for SVD, suggesting different\ninteractions between personalization and diversification across models. Our\nexit questionnaire responses indicated that most users expressed no clear\npreference between re-ranked and unprocessed lists, implying that increased\ndiversity did not substantially reduce user satisfaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The drive for personalization in recommender systems creates a tension\nbetween user privacy and the risk of \"filter bubbles\". Although federated\nlearning offers a promising paradigm for privacy-preserving recommendations,\nits impact on diversity remains unclear. We introduce FedFlex, a two-stage\nframework that combines local, on-device fine-tuning of matrix factorization\nmodels (SVD and BPR) with a lightweight Maximal Marginal Relevance (MMR)\nre-ranking step to promote diversity. We conducted the first live user study of\na federated recommender, collecting behavioral data and feedback during a\ntwo-week online deployment. Our results show that FedFlex successfully engages\nusers, with BPR outperforming SVD in click-through rate. Re-ranking with MMR\nconsistently improved ranking quality (nDCG) across both models, with\nstatistically significant gains, particularly for BPR. Diversity effects\nvaried: MMR increased coverage for both models and improved intra-list\ndiversity for BPR, but slightly reduced it for SVD, suggesting different\ninteractions between personalization and diversification across models. Our\nexit questionnaire responses indicated that most users expressed no clear\npreference between re-ranked and unprocessed lists, implying that increased\ndiversity did not substantially reduce user satisfaction."
                },
                "authors": [
                    {
                        "name": "Sven Lankester"
                    },
                    {
                        "name": "Gustavo de Carvalho Bertoli"
                    },
                    {
                        "name": "Matias Vizcaino"
                    },
                    {
                        "name": "Emmanuelle Beauxis Aussalet"
                    },
                    {
                        "name": "Manel Slokom"
                    }
                ],
                "author_detail": {
                    "name": "Manel Slokom"
                },
                "author": "Manel Slokom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05921v1",
                "updated": "2025-10-07T13:30:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    30,
                    18,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T13:30:18Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    30,
                    18,
                    1,
                    280,
                    0
                ],
                "title": "Prompt reinforcing for long-term planning of large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt reinforcing for long-term planning of large language models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in a wide range\nof natural language processing tasks and can be adapted through prompting.\nHowever, they remain suboptimal in multi-turn interactions, often relying on\nincorrect early assumptions and failing to track user goals over time, which\nmakes such tasks particularly challenging. Prior works in dialogue systems have\nshown that long-term planning is essential for handling interactive tasks. In\nthis work, we propose a prompt optimisation framework inspired by reinforcement\nlearning, which enables such planning to take place by only modifying the task\ninstruction prompt of the LLM-based agent. By generating turn-by-turn feedback\nand leveraging experience replay for prompt rewriting, our proposed method\nshows significant improvement in multi-turn tasks such as text-to-SQL and\ntask-oriented dialogue. Moreover, it generalises across different LLM-based\nagents and can leverage diverse LLMs as meta-prompting agents. This warrants\nfuture research in reinforcement learning-inspired parameter-free optimisation\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in a wide range\nof natural language processing tasks and can be adapted through prompting.\nHowever, they remain suboptimal in multi-turn interactions, often relying on\nincorrect early assumptions and failing to track user goals over time, which\nmakes such tasks particularly challenging. Prior works in dialogue systems have\nshown that long-term planning is essential for handling interactive tasks. In\nthis work, we propose a prompt optimisation framework inspired by reinforcement\nlearning, which enables such planning to take place by only modifying the task\ninstruction prompt of the LLM-based agent. By generating turn-by-turn feedback\nand leveraging experience replay for prompt rewriting, our proposed method\nshows significant improvement in multi-turn tasks such as text-to-SQL and\ntask-oriented dialogue. Moreover, it generalises across different LLM-based\nagents and can leverage diverse LLMs as meta-prompting agents. This warrants\nfuture research in reinforcement learning-inspired parameter-free optimisation\nmethods."
                },
                "authors": [
                    {
                        "name": "Hsien-Chin Lin"
                    },
                    {
                        "name": "Benjamin Matthias Ruppik"
                    },
                    {
                        "name": "Carel van Niekerk"
                    },
                    {
                        "name": "Chia-Hao Shen"
                    },
                    {
                        "name": "Michael Heck"
                    },
                    {
                        "name": "Nurul Lubis"
                    },
                    {
                        "name": "Renato Vukovic"
                    },
                    {
                        "name": "Shutong Feng"
                    },
                    {
                        "name": "Milica Gašić"
                    }
                ],
                "author_detail": {
                    "name": "Milica Gašić"
                },
                "author": "Milica Gašić",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05909v1",
                "updated": "2025-10-07T13:20:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    20,
                    51,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T13:20:51Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    20,
                    51,
                    1,
                    280,
                    0
                ],
                "title": "Optimizing for Persuasion Improves LLM Generalization: Evidence from\n  Quality-Diversity Evolution of Debate Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing for Persuasion Improves LLM Generalization: Evidence from\n  Quality-Diversity Evolution of Debate Strategies"
                },
                "summary": "Large Language Models (LLMs) optimized to output truthful answers often\noverfit, producing brittle reasoning that fails to generalize. While\npersuasion-based optimization has shown promise in debate settings, it has not\nbeen systematically compared against mainstream truth-based approaches. We\nintroduce DebateQD, a minimal Quality-Diversity (QD) evolutionary algorithm\nthat evolves diverse debate strategies across different categories\n(rationality, authority, emotional appeal, etc.) through tournament-style\ncompetitions where two LLMs debate while a third judges. Unlike previously\nproposed methods that require a population of LLMs, our approach maintains\ndiversity of opponents through prompt-based strategies within a single LLM\narchitecture, making it more accessible for experiments while preserving the\nkey benefits of population-based optimization. In contrast to prior work, we\nexplicitly isolate the role of the optimization objective by fixing the debate\nprotocol and swapping only the fitness function: persuasion rewards strategies\nthat convince the judge irrespective of truth, whereas truth rewards\ncollaborative correctness. Across three model scales (7B, 32B, 72B parameters)\nand multiple dataset sizes from the QuALITY benchmark, persuasion-optimized\nstrategies achieve up to 13.94% smaller train-test generalization gaps, while\nmatching or exceeding truth optimization's test performance. These results\nprovide the first controlled evidence that competitive pressure to persuade,\nrather than seek the truth collaboratively, fosters more transferable reasoning\nskills, offering a promising path for improving LLM generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) optimized to output truthful answers often\noverfit, producing brittle reasoning that fails to generalize. While\npersuasion-based optimization has shown promise in debate settings, it has not\nbeen systematically compared against mainstream truth-based approaches. We\nintroduce DebateQD, a minimal Quality-Diversity (QD) evolutionary algorithm\nthat evolves diverse debate strategies across different categories\n(rationality, authority, emotional appeal, etc.) through tournament-style\ncompetitions where two LLMs debate while a third judges. Unlike previously\nproposed methods that require a population of LLMs, our approach maintains\ndiversity of opponents through prompt-based strategies within a single LLM\narchitecture, making it more accessible for experiments while preserving the\nkey benefits of population-based optimization. In contrast to prior work, we\nexplicitly isolate the role of the optimization objective by fixing the debate\nprotocol and swapping only the fitness function: persuasion rewards strategies\nthat convince the judge irrespective of truth, whereas truth rewards\ncollaborative correctness. Across three model scales (7B, 32B, 72B parameters)\nand multiple dataset sizes from the QuALITY benchmark, persuasion-optimized\nstrategies achieve up to 13.94% smaller train-test generalization gaps, while\nmatching or exceeding truth optimization's test performance. These results\nprovide the first controlled evidence that competitive pressure to persuade,\nrather than seek the truth collaboratively, fosters more transferable reasoning\nskills, offering a promising path for improving LLM generalization."
                },
                "authors": [
                    {
                        "name": "Aksel Joonas Reedi"
                    },
                    {
                        "name": "Corentin Léger"
                    },
                    {
                        "name": "Julien Pourcel"
                    },
                    {
                        "name": "Loris Gaven"
                    },
                    {
                        "name": "Perrine Charriau"
                    },
                    {
                        "name": "Guillaume Pourcel"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Pourcel"
                },
                "author": "Guillaume Pourcel",
                "arxiv_comment": "Open-source code available at\n  https://github.com/flowersteam/llm_persuasion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04002v2",
                "updated": "2025-10-07T13:17:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    17,
                    8,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-05T02:30:11Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    2,
                    30,
                    11,
                    6,
                    278,
                    0
                ],
                "title": "AgriGPT-VL: Agricultural Vision-Language Understanding Suite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgriGPT-VL: Agricultural Vision-Language Understanding Suite"
                },
                "summary": "Despite rapid advances in multimodal large language models, agricultural\napplications remain constrained by the scarcity of domain-tailored models,\ncurated vision-language corpora, and rigorous evaluation. To address these\nchallenges, we present the AgriGPT-VL Suite, a unified multimodal framework for\nagriculture. Our contributions are threefold. First, we introduce Agri-3M-VL,\nthe largest vision-language corpus for agriculture to our knowledge, curated by\na scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M\nimage-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO\nreinforcement learning samples. Second, we develop AgriGPT-VL, an\nagriculture-specialized vision-language model trained via a progressive\ncurriculum of textual grounding, multimodal shallow/deep alignment, and GRPO\nrefinement. This method achieves strong multimodal reasoning while preserving\ntext-only capability. Third, we establish AgriBench-VL-4K, a compact yet\nchallenging evaluation suite with open-ended and image-grounded questions,\npaired with multi-metric evaluation and an LLM-as-a-judge framework.\nExperiments show that AgriGPT-VL outperforms leading general-purpose VLMs on\nAgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge\nevaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K\nwith no noticeable degradation of language ability. Ablation studies further\nconfirm consistent gains from our alignment and GRPO refinement stages. We will\nopen source all of the resources to support reproducible research and\ndeployment in low-resource agricultural settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rapid advances in multimodal large language models, agricultural\napplications remain constrained by the scarcity of domain-tailored models,\ncurated vision-language corpora, and rigorous evaluation. To address these\nchallenges, we present the AgriGPT-VL Suite, a unified multimodal framework for\nagriculture. Our contributions are threefold. First, we introduce Agri-3M-VL,\nthe largest vision-language corpus for agriculture to our knowledge, curated by\na scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M\nimage-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO\nreinforcement learning samples. Second, we develop AgriGPT-VL, an\nagriculture-specialized vision-language model trained via a progressive\ncurriculum of textual grounding, multimodal shallow/deep alignment, and GRPO\nrefinement. This method achieves strong multimodal reasoning while preserving\ntext-only capability. Third, we establish AgriBench-VL-4K, a compact yet\nchallenging evaluation suite with open-ended and image-grounded questions,\npaired with multi-metric evaluation and an LLM-as-a-judge framework.\nExperiments show that AgriGPT-VL outperforms leading general-purpose VLMs on\nAgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge\nevaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K\nwith no noticeable degradation of language ability. Ablation studies further\nconfirm consistent gains from our alignment and GRPO refinement stages. We will\nopen source all of the resources to support reproducible research and\ndeployment in low-resource agricultural settings."
                },
                "authors": [
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Yunkui Chen"
                    },
                    {
                        "name": "Lanfei Feng"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Xiao Xu"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Nueraili Aierken"
                    },
                    {
                        "name": "Runhe Huang"
                    },
                    {
                        "name": "Hongjian Lin"
                    },
                    {
                        "name": "Yibin Ying"
                    },
                    {
                        "name": "Shijian Li"
                    }
                ],
                "author_detail": {
                    "name": "Shijian Li"
                },
                "author": "Shijian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02298v3",
                "updated": "2025-10-08T02:10:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    2,
                    10,
                    47,
                    2,
                    281,
                    0
                ],
                "published": "2025-08-04T11:06:08Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    6,
                    8,
                    0,
                    216,
                    0
                ],
                "title": "CAPO: Towards Enhancing LLM Reasoning through Generative Credit\n  Assignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAPO: Towards Enhancing LLM Reasoning through Generative Credit\n  Assignment"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has improved the\nreasoning abilities of Large Language Models (LLMs) by using rule-based binary\nfeedback. However, current RLVR methods typically assign the same reward to\nevery token. This coarse-grained feedback hampers precise credit assignment,\nmaking it hard for models to identify which reasoning steps lead to success or\nfailure, and often results in suboptimal policies. Methods like PPO provide\ncredit assignment by value estimation, but yield inaccurate and unverifiable\nsignals due to limited sampling. On the other hand, methods using Process\nReward Models can provide step-wise rewards but suffer from several key\nlimitations: they require high-quality process supervision labels, the feedback\nis unreliable due to probabilistic reward modeling, and their application in\nonline reinforcement learning (RL) is time-consuming. To overcome these\nlimitations, we introduce a simple but efficient method-Credit Assignment\nPolicy Optimization (CAPO). Instead of training auxiliary models, CAPO directly\nleverages an off-the-shelf, general-purpose LLM as a Generative Process Reward\nModel (LLM-as-GenPRM) to generate all step-wise critique by one pass only based\non the correctness of the step itself, providing deterministic token-level\ncredits to refine the tokens that were originally assigned identical rule-based\nrewards. To further enhance the accuracy and robustness, we employ voting\nmechanisms that scale with the number of generated critiques. Extensive\nexperiments on various backbones like Llama and Qwen models show that CAPO\nconsistently outperforms supervised learning-based and RL-based fine-tuning\nmethods across four challenging mathematical benchmarks and three out-of-domain\nbenchmarks. Further analysis shows that CAPO can help the model to foster the\nlearning of correct reasoning pathways leading to correct answers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has improved the\nreasoning abilities of Large Language Models (LLMs) by using rule-based binary\nfeedback. However, current RLVR methods typically assign the same reward to\nevery token. This coarse-grained feedback hampers precise credit assignment,\nmaking it hard for models to identify which reasoning steps lead to success or\nfailure, and often results in suboptimal policies. Methods like PPO provide\ncredit assignment by value estimation, but yield inaccurate and unverifiable\nsignals due to limited sampling. On the other hand, methods using Process\nReward Models can provide step-wise rewards but suffer from several key\nlimitations: they require high-quality process supervision labels, the feedback\nis unreliable due to probabilistic reward modeling, and their application in\nonline reinforcement learning (RL) is time-consuming. To overcome these\nlimitations, we introduce a simple but efficient method-Credit Assignment\nPolicy Optimization (CAPO). Instead of training auxiliary models, CAPO directly\nleverages an off-the-shelf, general-purpose LLM as a Generative Process Reward\nModel (LLM-as-GenPRM) to generate all step-wise critique by one pass only based\non the correctness of the step itself, providing deterministic token-level\ncredits to refine the tokens that were originally assigned identical rule-based\nrewards. To further enhance the accuracy and robustness, we employ voting\nmechanisms that scale with the number of generated critiques. Extensive\nexperiments on various backbones like Llama and Qwen models show that CAPO\nconsistently outperforms supervised learning-based and RL-based fine-tuning\nmethods across four challenging mathematical benchmarks and three out-of-domain\nbenchmarks. Further analysis shows that CAPO can help the model to foster the\nlearning of correct reasoning pathways leading to correct answers."
                },
                "authors": [
                    {
                        "name": "Guofu Xie"
                    },
                    {
                        "name": "Yunsheng Shi"
                    },
                    {
                        "name": "Hongtao Tian"
                    },
                    {
                        "name": "Ting Yao"
                    },
                    {
                        "name": "Xiao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Zhang"
                },
                "author": "Xiao Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14064v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14064v3",
                "updated": "2025-10-07T13:10:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    10,
                    30,
                    1,
                    280,
                    0
                ],
                "published": "2025-04-18T20:36:10Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    20,
                    36,
                    10,
                    4,
                    108,
                    0
                ],
                "title": "DoomArena: A framework for Testing AI Agents Against Evolving Security\n  Threats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DoomArena: A framework for Testing AI Agents Against Evolving Security\n  Threats"
                },
                "summary": "We present DoomArena, a security evaluation framework for AI agents.\nDoomArena is designed on three principles: 1) It is a plug-in framework and\nintegrates easily into realistic agentic frameworks like BrowserGym (for web\nagents) and $\\tau$-bench (for tool calling agents); 2) It is configurable and\nallows for detailed threat modeling, allowing configuration of specific\ncomponents of the agentic framework being attackable, and specifying targets\nfor the attacker; and 3) It is modular and decouples the development of attacks\nfrom details of the environment in which the agent is deployed, allowing for\nthe same attacks to be applied across multiple environments. We illustrate\nseveral advantages of our framework, including the ability to adapt to new\nthreat models and environments easily, the ability to easily combine several\npreviously published attacks to enable comprehensive and fine-grained security\ntesting, and the ability to analyze trade-offs between various vulnerabilities\nand performance. We apply DoomArena to state-of-the-art (SOTA) web and\ntool-calling agents and find a number of surprising results: 1) SOTA agents\nhave varying levels of vulnerability to different threat models (malicious user\nvs malicious environment), and there is no Pareto dominant agent across all\nthreat models; 2) When multiple attacks are applied to an agent, they often\ncombine constructively; 3) Guardrail model-based defenses seem to fail, while\ndefenses based on powerful SOTA LLMs work better. DoomArena is available at\nhttps://github.com/ServiceNow/DoomArena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DoomArena, a security evaluation framework for AI agents.\nDoomArena is designed on three principles: 1) It is a plug-in framework and\nintegrates easily into realistic agentic frameworks like BrowserGym (for web\nagents) and $\\tau$-bench (for tool calling agents); 2) It is configurable and\nallows for detailed threat modeling, allowing configuration of specific\ncomponents of the agentic framework being attackable, and specifying targets\nfor the attacker; and 3) It is modular and decouples the development of attacks\nfrom details of the environment in which the agent is deployed, allowing for\nthe same attacks to be applied across multiple environments. We illustrate\nseveral advantages of our framework, including the ability to adapt to new\nthreat models and environments easily, the ability to easily combine several\npreviously published attacks to enable comprehensive and fine-grained security\ntesting, and the ability to analyze trade-offs between various vulnerabilities\nand performance. We apply DoomArena to state-of-the-art (SOTA) web and\ntool-calling agents and find a number of surprising results: 1) SOTA agents\nhave varying levels of vulnerability to different threat models (malicious user\nvs malicious environment), and there is no Pareto dominant agent across all\nthreat models; 2) When multiple attacks are applied to an agent, they often\ncombine constructively; 3) Guardrail model-based defenses seem to fail, while\ndefenses based on powerful SOTA LLMs work better. DoomArena is available at\nhttps://github.com/ServiceNow/DoomArena."
                },
                "authors": [
                    {
                        "name": "Leo Boisvert"
                    },
                    {
                        "name": "Mihir Bansal"
                    },
                    {
                        "name": "Chandra Kiran Reddy Evuru"
                    },
                    {
                        "name": "Gabriel Huang"
                    },
                    {
                        "name": "Abhay Puri"
                    },
                    {
                        "name": "Avinandan Bose"
                    },
                    {
                        "name": "Maryam Fazel"
                    },
                    {
                        "name": "Quentin Cappart"
                    },
                    {
                        "name": "Jason Stanley"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    },
                    {
                        "name": "Alexandre Drouin"
                    },
                    {
                        "name": "Krishnamurthy Dvijotham"
                    }
                ],
                "author_detail": {
                    "name": "Krishnamurthy Dvijotham"
                },
                "author": "Krishnamurthy Dvijotham",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14064v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14064v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09387v3",
                "updated": "2025-10-07T13:08:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    13,
                    8,
                    21,
                    1,
                    280,
                    0
                ],
                "published": "2025-09-11T12:06:34Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    6,
                    34,
                    3,
                    254,
                    0
                ],
                "title": "MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for\n  Hyper-parameters Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for\n  Hyper-parameters Optimization"
                },
                "summary": "Effective model and hyperparameter selection remains a major challenge in\ndeep learning, often requiring extensive expertise and computation. While\nAutoML and large language models (LLMs) promise automation, current LLM-based\napproaches rely on trial and error and expensive APIs, which provide limited\ninterpretability and generalizability. We propose MetaLLMiX, a zero-shot\nhyperparameter optimization framework combining meta-learning, explainable AI,\nand efficient LLM reasoning. By leveraging historical experiment outcomes with\nSHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained\nmodels without additional trials. We further employ an LLM-as-judge evaluation\nto control output format, accuracy, and completeness. Experiments on eight\nmedical imaging datasets using nine open-source lightweight LLMs show that\nMetaLLMiX achieves competitive or superior performance to traditional HPO\nmethods while drastically reducing computational cost. Our local deployment\noutperforms prior API-based approaches, achieving optimal results on 5 of 8\ntasks, response time reductions of 99.6-99.9%, and the fastest training times\non 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of\nbest-performing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective model and hyperparameter selection remains a major challenge in\ndeep learning, often requiring extensive expertise and computation. While\nAutoML and large language models (LLMs) promise automation, current LLM-based\napproaches rely on trial and error and expensive APIs, which provide limited\ninterpretability and generalizability. We propose MetaLLMiX, a zero-shot\nhyperparameter optimization framework combining meta-learning, explainable AI,\nand efficient LLM reasoning. By leveraging historical experiment outcomes with\nSHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained\nmodels without additional trials. We further employ an LLM-as-judge evaluation\nto control output format, accuracy, and completeness. Experiments on eight\nmedical imaging datasets using nine open-source lightweight LLMs show that\nMetaLLMiX achieves competitive or superior performance to traditional HPO\nmethods while drastically reducing computational cost. Our local deployment\noutperforms prior API-based approaches, achieving optimal results on 5 of 8\ntasks, response time reductions of 99.6-99.9%, and the fastest training times\non 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of\nbest-performing baselines."
                },
                "authors": [
                    {
                        "name": "Mohamed Bal-Ghaoui"
                    },
                    {
                        "name": "Mohammed Tiouti"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed Tiouti"
                },
                "author": "Mohammed Tiouti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05551v2",
                "updated": "2025-10-07T12:58:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    58,
                    4,
                    1,
                    280,
                    0
                ],
                "published": "2025-06-05T19:53:19Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    19,
                    53,
                    19,
                    3,
                    156,
                    0
                ],
                "title": "When Semantics Mislead Vision: Mitigating Large Multimodal Models\n  Hallucinations in Scene Text Spotting and Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Semantics Mislead Vision: Mitigating Large Multimodal Models\n  Hallucinations in Scene Text Spotting and Understanding"
                },
                "summary": "Large Multimodal Models (LMMs) have achieved impressive progress in visual\nperception and reasoning. However, when confronted with visually ambiguous or\nnon-semantic scene text, they often struggle to accurately spot and understand\nthe content, frequently generating semantically plausible yet visually\nincorrect answers, which we refer to as semantic hallucination. In this work,\nwe investigate the underlying causes of semantic hallucination and identify a\nkey finding: Transformer layers in LLM with stronger attention focus on scene\ntext regions are less prone to producing semantic hallucinations. Thus, we\npropose a training-free semantic hallucination mitigation framework comprising\ntwo key components: (1) ZoomText, a coarse-to-fine strategy that identifies\npotential text regions without external detectors; and (2) Grounded Layer\nCorrection, which adaptively leverages the internal representations from layers\nless prone to hallucination to guide decoding, correcting hallucinated outputs\nfor non-semantic samples while preserving the semantics of meaningful ones. To\nenable rigorous evaluation, we introduce TextHalu-Bench, a benchmark of 1,740\nsamples spanning both semantic and non-semantic cases, with manually curated\nquestion answer pairs designed to probe model hallucinations. Extensive\nexperiments demonstrate that our method not only effectively mitigates semantic\nhallucination but also achieves strong performance on public benchmarks for\nscene text spotting and understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) have achieved impressive progress in visual\nperception and reasoning. However, when confronted with visually ambiguous or\nnon-semantic scene text, they often struggle to accurately spot and understand\nthe content, frequently generating semantically plausible yet visually\nincorrect answers, which we refer to as semantic hallucination. In this work,\nwe investigate the underlying causes of semantic hallucination and identify a\nkey finding: Transformer layers in LLM with stronger attention focus on scene\ntext regions are less prone to producing semantic hallucinations. Thus, we\npropose a training-free semantic hallucination mitigation framework comprising\ntwo key components: (1) ZoomText, a coarse-to-fine strategy that identifies\npotential text regions without external detectors; and (2) Grounded Layer\nCorrection, which adaptively leverages the internal representations from layers\nless prone to hallucination to guide decoding, correcting hallucinated outputs\nfor non-semantic samples while preserving the semantics of meaningful ones. To\nenable rigorous evaluation, we introduce TextHalu-Bench, a benchmark of 1,740\nsamples spanning both semantic and non-semantic cases, with manually curated\nquestion answer pairs designed to probe model hallucinations. Extensive\nexperiments demonstrate that our method not only effectively mitigates semantic\nhallucination but also achieves strong performance on public benchmarks for\nscene text spotting and understanding."
                },
                "authors": [
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Hangui Lin"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Gangyan Zeng"
                    },
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    },
                    {
                        "name": "Nicu Sebe"
                    }
                ],
                "author_detail": {
                    "name": "Nicu Sebe"
                },
                "author": "Nicu Sebe",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20144v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20144v3",
                "updated": "2025-10-07T12:50:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    50,
                    57,
                    1,
                    280,
                    0
                ],
                "published": "2025-08-27T09:17:03Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    9,
                    17,
                    3,
                    2,
                    239,
                    0
                ],
                "title": "Navigating the EU AI Act: Foreseeable Challenges in Qualifying Deep\n  Learning-Based Automated Inspections of Class III Medical Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the EU AI Act: Foreseeable Challenges in Qualifying Deep\n  Learning-Based Automated Inspections of Class III Medical Devices"
                },
                "summary": "As deep learning (DL) technologies advance, their application in automated\nvisual inspection for Class III medical devices offers significant potential to\nenhance quality assurance and reduce human error. However, the adoption of such\nAI-based systems introduces new regulatory complexities-particularly under the\nEU Artificial Intelligence (AI) Act, which imposes high-risk system obligations\nthat differ in scope and depth from established regulatory frameworks such as\nthe Medical Device Regulation (MDR) and the U.S. FDA Quality System Regulation\n(QSR). This paper presents a high-level technical assessment of the foreseeable\nchallenges that manufacturers are likely to encounter when qualifying DL-based\nautomated inspections -- specifically static models -- within the existing\nmedical device compliance landscape. It examines divergences in risk management\nprinciples, dataset governance, model validation, explainability requirements,\nand post-deployment monitoring obligations. The discussion also explores\npotential implementation strategies and highlights areas of uncertainty,\nincluding data retention burdens, global compliance implications, and the\npractical difficulties of achieving statistical significance in validation with\nlimited defect data. Disclaimer: This paper presents a technical perspective\nand does not constitute legal or regulatory advice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As deep learning (DL) technologies advance, their application in automated\nvisual inspection for Class III medical devices offers significant potential to\nenhance quality assurance and reduce human error. However, the adoption of such\nAI-based systems introduces new regulatory complexities-particularly under the\nEU Artificial Intelligence (AI) Act, which imposes high-risk system obligations\nthat differ in scope and depth from established regulatory frameworks such as\nthe Medical Device Regulation (MDR) and the U.S. FDA Quality System Regulation\n(QSR). This paper presents a high-level technical assessment of the foreseeable\nchallenges that manufacturers are likely to encounter when qualifying DL-based\nautomated inspections -- specifically static models -- within the existing\nmedical device compliance landscape. It examines divergences in risk management\nprinciples, dataset governance, model validation, explainability requirements,\nand post-deployment monitoring obligations. The discussion also explores\npotential implementation strategies and highlights areas of uncertainty,\nincluding data retention burdens, global compliance implications, and the\npractical difficulties of achieving statistical significance in validation with\nlimited defect data. Disclaimer: This paper presents a technical perspective\nand does not constitute legal or regulatory advice."
                },
                "authors": [
                    {
                        "name": "Julio Zanon Diaz"
                    },
                    {
                        "name": "Tommy Brennan"
                    },
                    {
                        "name": "Peter Corcoran"
                    }
                ],
                "author_detail": {
                    "name": "Peter Corcoran"
                },
                "author": "Peter Corcoran",
                "arxiv_doi": "10.1007/s00170-025-16648-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00170-025-16648-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20144v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20144v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Critical Review article",
                "arxiv_journal_ref": "The International Journal of Advanced Manufacturing Technology,\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14313v2",
                "updated": "2025-10-07T12:44:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    44,
                    37,
                    1,
                    280,
                    0
                ],
                "published": "2025-05-20T13:00:48Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    13,
                    0,
                    48,
                    1,
                    140,
                    0
                ],
                "title": "Teaching Small Language Models to Learn Logic through Meta-Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Small Language Models to Learn Logic through Meta-Learning"
                },
                "summary": "Large language models (LLMs) are increasingly evaluated on reasoning tasks,\nyet their logical abilities remain contested. To address this, we study LLMs'\nreasoning in a well-defined fragment of logic: syllogistic reasoning. We cast\nthe problem as premise selection and construct controlled datasets to isolate\nlogical competence. Beyond evaluation, an open challenge is enabling LLMs to\nacquire abstract inference patterns that generalize to novel structures. We\npropose to apply few-shot meta-learning to this domain, thereby encouraging\nmodels to extract rules across tasks rather than memorize patterns within\ntasks. Although meta-learning has been little explored in the context of logic\nlearnability, our experiments show that it is effective: small models (1.5B-7B)\nfine-tuned with meta-learning demonstrate strong gains in generalization, with\nespecially pronounced benefits in low-data regimes. These meta-learned models\noutperform GPT-4o and o3-mini on our syllogistic reasoning task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly evaluated on reasoning tasks,\nyet their logical abilities remain contested. To address this, we study LLMs'\nreasoning in a well-defined fragment of logic: syllogistic reasoning. We cast\nthe problem as premise selection and construct controlled datasets to isolate\nlogical competence. Beyond evaluation, an open challenge is enabling LLMs to\nacquire abstract inference patterns that generalize to novel structures. We\npropose to apply few-shot meta-learning to this domain, thereby encouraging\nmodels to extract rules across tasks rather than memorize patterns within\ntasks. Although meta-learning has been little explored in the context of logic\nlearnability, our experiments show that it is effective: small models (1.5B-7B)\nfine-tuned with meta-learning demonstrate strong gains in generalization, with\nespecially pronounced benefits in low-data regimes. These meta-learned models\noutperform GPT-4o and o3-mini on our syllogistic reasoning task."
                },
                "authors": [
                    {
                        "name": "Leonardo Bertolazzi"
                    },
                    {
                        "name": "Manuel Vargas Guzmán"
                    },
                    {
                        "name": "Raffaella Bernardi"
                    },
                    {
                        "name": "Maciej Malicki"
                    },
                    {
                        "name": "Jakub Szymanik"
                    }
                ],
                "author_detail": {
                    "name": "Jakub Szymanik"
                },
                "author": "Jakub Szymanik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17753v2",
                "updated": "2025-10-07T12:40:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    40,
                    17,
                    1,
                    280,
                    0
                ],
                "published": "2025-03-22T12:34:15Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    12,
                    34,
                    15,
                    5,
                    81,
                    0
                ],
                "title": "Building Resource-Constrained Language Agents: A Korean Case Study on\n  Chemical Toxicity Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Resource-Constrained Language Agents: A Korean Case Study on\n  Chemical Toxicity Information"
                },
                "summary": "Language agents powered by large language models (LLMs) face significant\ndeployment challenges in resource-constrained environments, particularly for\nspecialized domains and less-common languages. This paper presents Tox-chat, a\nKorean chemical toxicity information agent devised within these limitations. We\npropose two key innovations: a context-efficient architecture that reduces\ntoken consumption through hierarchical section search, and a scenario-based\ndialogue generation methodology that effectively distills tool-using\ncapabilities from larger models. Experimental evaluations demonstrate that our\nfine-tuned 8B parameter model substantially outperforms both untuned models and\nbaseline approaches, in terms of DB faithfulness and preference. Our work\noffers valuable insights for researchers developing domain-specific language\nagents under practical constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language agents powered by large language models (LLMs) face significant\ndeployment challenges in resource-constrained environments, particularly for\nspecialized domains and less-common languages. This paper presents Tox-chat, a\nKorean chemical toxicity information agent devised within these limitations. We\npropose two key innovations: a context-efficient architecture that reduces\ntoken consumption through hierarchical section search, and a scenario-based\ndialogue generation methodology that effectively distills tool-using\ncapabilities from larger models. Experimental evaluations demonstrate that our\nfine-tuned 8B parameter model substantially outperforms both untuned models and\nbaseline approaches, in terms of DB faithfulness and preference. Our work\noffers valuable insights for researchers developing domain-specific language\nagents under practical constraints."
                },
                "authors": [
                    {
                        "name": "Hojun Cho"
                    },
                    {
                        "name": "Donghu Kim"
                    },
                    {
                        "name": "Soyoung Yang"
                    },
                    {
                        "name": "Chan Lee"
                    },
                    {
                        "name": "Hunjoo Lee"
                    },
                    {
                        "name": "Jaegul Choo"
                    }
                ],
                "author_detail": {
                    "name": "Jaegul Choo"
                },
                "author": "Jaegul Choo",
                "arxiv_comment": "EMNLP 2025 Industry track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05869v1",
                "updated": "2025-10-07T12:37:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    37,
                    6,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T12:37:06Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    37,
                    6,
                    1,
                    280,
                    0
                ],
                "title": "The fragility of \"cultural tendencies\" in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fragility of \"cultural tendencies\" in LLMs"
                },
                "summary": "In a recent study, Lu, Song, and Zhang (2025) (LSZ) propose that large\nlanguage models (LLMs), when prompted in different languages, display\nculturally specific tendencies. They report that the two models (i.e., GPT and\nERNIE) respond in more interdependent and holistic ways when prompted in\nChinese, and more independent and analytic ways when prompted in English. LSZ\nattribute these differences to deep-seated cultural patterns in the models,\nclaiming that prompt language alone can induce substantial cultural shifts.\nWhile we acknowledge the empirical patterns they observed, we find their\nexperiments, methods, and interpretations problematic. In this paper, we\ncritically re-evaluate the methodology, theoretical framing, and conclusions of\nLSZ. We argue that the reported \"cultural tendencies\" are not stable traits but\nfragile artifacts of specific models and task design. To test this, we\nconducted targeted replications using a broader set of LLMs and a larger number\nof test items. Our results show that prompt language has minimal effect on\noutputs, challenging LSZ's claim that these models encode grounded cultural\nbeliefs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a recent study, Lu, Song, and Zhang (2025) (LSZ) propose that large\nlanguage models (LLMs), when prompted in different languages, display\nculturally specific tendencies. They report that the two models (i.e., GPT and\nERNIE) respond in more interdependent and holistic ways when prompted in\nChinese, and more independent and analytic ways when prompted in English. LSZ\nattribute these differences to deep-seated cultural patterns in the models,\nclaiming that prompt language alone can induce substantial cultural shifts.\nWhile we acknowledge the empirical patterns they observed, we find their\nexperiments, methods, and interpretations problematic. In this paper, we\ncritically re-evaluate the methodology, theoretical framing, and conclusions of\nLSZ. We argue that the reported \"cultural tendencies\" are not stable traits but\nfragile artifacts of specific models and task design. To test this, we\nconducted targeted replications using a broader set of LLMs and a larger number\nof test items. Our results show that prompt language has minimal effect on\noutputs, challenging LSZ's claim that these models encode grounded cultural\nbeliefs."
                },
                "authors": [
                    {
                        "name": "Kun Sun"
                    },
                    {
                        "name": "Rong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rong Wang"
                },
                "author": "Rong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05864v1",
                "updated": "2025-10-07T12:33:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    33,
                    21,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T12:33:21Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    33,
                    21,
                    1,
                    280,
                    0
                ],
                "title": "Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input"
                },
                "summary": "Large language models (LLMs) increasingly support applications that rely on\nextended context, from document processing to retrieval-augmented generation.\nWhile their long-context capabilities are well studied for reasoning and\nretrieval, little is known about their behavior in safety-critical scenarios.\nWe evaluate LLMs' sensitivity to harmful content under extended context,\nvarying type (explicit vs. implicit), position (beginning, middle, end),\nprevalence (0.01-0.50 of the prompt), and context length (600-6000 tokens).\nAcross harmful content categories such as toxic, offensive, and hate speech,\nwith LLaMA-3, Qwen-2.5, and Mistral, we observe similar patterns: performance\npeaks at moderate harmful prevalence (0.25) but declines when content is very\nsparse or dominant; recall decreases with increasing context length; harmful\nsentences at the beginning are generally detected more reliably; and explicit\ncontent is more consistently recognized than implicit. These findings provide\nthe first systematic view of how LLMs prioritize and calibrate harmful content\nin long contexts, highlighting both their emerging strengths and the challenges\nthat remain for safety-critical use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly support applications that rely on\nextended context, from document processing to retrieval-augmented generation.\nWhile their long-context capabilities are well studied for reasoning and\nretrieval, little is known about their behavior in safety-critical scenarios.\nWe evaluate LLMs' sensitivity to harmful content under extended context,\nvarying type (explicit vs. implicit), position (beginning, middle, end),\nprevalence (0.01-0.50 of the prompt), and context length (600-6000 tokens).\nAcross harmful content categories such as toxic, offensive, and hate speech,\nwith LLaMA-3, Qwen-2.5, and Mistral, we observe similar patterns: performance\npeaks at moderate harmful prevalence (0.25) but declines when content is very\nsparse or dominant; recall decreases with increasing context length; harmful\nsentences at the beginning are generally detected more reliably; and explicit\ncontent is more consistently recognized than implicit. These findings provide\nthe first systematic view of how LLMs prioritize and calibrate harmful content\nin long contexts, highlighting both their emerging strengths and the challenges\nthat remain for safety-critical use."
                },
                "authors": [
                    {
                        "name": "Faeze Ghorbanpour"
                    },
                    {
                        "name": "Alexander Fraser"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Fraser"
                },
                "author": "Alexander Fraser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14271v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14271v2",
                "updated": "2025-10-07T12:31:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    31,
                    41,
                    1,
                    280,
                    0
                ],
                "published": "2025-05-20T12:23:31Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    12,
                    23,
                    31,
                    1,
                    140,
                    0
                ],
                "title": "FAID: Fine-Grained AI-Generated Text Detection Using Multi-Task\n  Auxiliary and Multi-Level Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAID: Fine-Grained AI-Generated Text Detection Using Multi-Task\n  Auxiliary and Multi-Level Contrastive Learning"
                },
                "summary": "The growing collaboration between humans and AI models in generative tasks\nhas introduced new challenges in distinguishing between human-written,\nLLM-generated, and human--LLM collaborative texts. In this work, we collect a\nmultilingual, multi-domain, multi-generator dataset FAIDSet. We further\nintroduce a fine-grained detection framework FAID to classify text into these\nthree categories, and also to identify the underlying LLM family of the\ngenerator. Unlike existing binary classifiers, FAID is built to capture both\nauthorship and model-specific characteristics. Our method combines multi-level\ncontrastive learning with multi-task auxiliary classification to learn subtle\nstylistic cues. By modeling LLM families as distinct stylistic entities, we\nincorporate an adaptation to address distributional shifts without retraining\nfor unseen data. Our experimental results demonstrate that FAID outperforms\nseveral baselines, particularly enhancing the generalization accuracy on unseen\ndomains and new LLMs, thus offering a potential solution for improving\ntransparency and accountability in AI-assisted writing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing collaboration between humans and AI models in generative tasks\nhas introduced new challenges in distinguishing between human-written,\nLLM-generated, and human--LLM collaborative texts. In this work, we collect a\nmultilingual, multi-domain, multi-generator dataset FAIDSet. We further\nintroduce a fine-grained detection framework FAID to classify text into these\nthree categories, and also to identify the underlying LLM family of the\ngenerator. Unlike existing binary classifiers, FAID is built to capture both\nauthorship and model-specific characteristics. Our method combines multi-level\ncontrastive learning with multi-task auxiliary classification to learn subtle\nstylistic cues. By modeling LLM families as distinct stylistic entities, we\nincorporate an adaptation to address distributional shifts without retraining\nfor unseen data. Our experimental results demonstrate that FAID outperforms\nseveral baselines, particularly enhancing the generalization accuracy on unseen\ndomains and new LLMs, thus offering a potential solution for improving\ntransparency and accountability in AI-assisted writing."
                },
                "authors": [
                    {
                        "name": "Minh Ngoc Ta"
                    },
                    {
                        "name": "Dong Cao Van"
                    },
                    {
                        "name": "Duc-Anh Hoang"
                    },
                    {
                        "name": "Minh Le-Anh"
                    },
                    {
                        "name": "Truong Nguyen"
                    },
                    {
                        "name": "My Anh Tran Nguyen"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Sang Dinh"
                    }
                ],
                "author_detail": {
                    "name": "Sang Dinh"
                },
                "author": "Sang Dinh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14271v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14271v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05860v1",
                "updated": "2025-10-07T12:30:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    30,
                    1,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T12:30:01Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    30,
                    1,
                    1,
                    280,
                    0
                ],
                "title": "Automated Boilerplate: Prevalence and Quality of Contract Generators in\n  the Context of Swiss Privacy Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Boilerplate: Prevalence and Quality of Contract Generators in\n  the Context of Swiss Privacy Policies"
                },
                "summary": "It has become increasingly challenging for firms to comply with a plethora of\nnovel digital regulations. This is especially true for smaller businesses that\noften lack both the resources and know-how to draft complex legal documents.\nInstead of seeking costly legal advice from attorneys, firms may turn to\ncheaper alternative legal service providers such as automated contract\ngenerators. While these services have a long-standing presence, there is little\nempirical evidence on their prevalence and output quality.\n  We address this gap in the context of a 2023 Swiss privacy law revision. To\nenable a systematic evaluation, we create and annotate a multilingual benchmark\ndataset that captures key compliance obligations under Swiss and EU privacy\nlaw. Using this dataset, we validate a novel GPT-5-based method for large-scale\ncompliance assessment of privacy policies, allowing us to measure the impact of\nthe revision. We observe compliance increases indicating an effect of the\nrevision. Generators, explicitly referenced by 18% of local websites, are\nassociated with substantially higher levels of compliance, with increases of up\nto 15 percentage points compared to privacy policies without generator use.\nThese findings contribute to three debates: the potential of LLMs for\ncross-lingual legal analysis, the Brussels Effect of EU regulations, and,\ncrucially, the role of automated tools in improving compliance and contractual\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has become increasingly challenging for firms to comply with a plethora of\nnovel digital regulations. This is especially true for smaller businesses that\noften lack both the resources and know-how to draft complex legal documents.\nInstead of seeking costly legal advice from attorneys, firms may turn to\ncheaper alternative legal service providers such as automated contract\ngenerators. While these services have a long-standing presence, there is little\nempirical evidence on their prevalence and output quality.\n  We address this gap in the context of a 2023 Swiss privacy law revision. To\nenable a systematic evaluation, we create and annotate a multilingual benchmark\ndataset that captures key compliance obligations under Swiss and EU privacy\nlaw. Using this dataset, we validate a novel GPT-5-based method for large-scale\ncompliance assessment of privacy policies, allowing us to measure the impact of\nthe revision. We observe compliance increases indicating an effect of the\nrevision. Generators, explicitly referenced by 18% of local websites, are\nassociated with substantially higher levels of compliance, with increases of up\nto 15 percentage points compared to privacy policies without generator use.\nThese findings contribute to three debates: the potential of LLMs for\ncross-lingual legal analysis, the Brussels Effect of EU regulations, and,\ncrucially, the role of automated tools in improving compliance and contractual\nquality."
                },
                "authors": [
                    {
                        "name": "Luka Nenadic"
                    },
                    {
                        "name": "David Rodriguez"
                    }
                ],
                "author_detail": {
                    "name": "David Rodriguez"
                },
                "author": "David Rodriguez",
                "arxiv_comment": "23 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05858v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05858v2",
                "updated": "2025-10-08T01:55:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    1,
                    55,
                    53,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-07T12:26:19Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    26,
                    19,
                    1,
                    280,
                    0
                ],
                "title": "DACP: Domain-Adaptive Continual Pre-Training of Large Language Models\n  for Phone Conversation Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACP: Domain-Adaptive Continual Pre-Training of Large Language Models\n  for Phone Conversation Summarization"
                },
                "summary": "Large language models (LLMs) have achieved impressive performance in text\nsummarization, yet their performance often falls short when applied to\nspecialized domains that differ from their original pre-training distribution.\nWhile fine-tuning can improve summarization quality, it typically relies on\ncostly and scarce high-quality labeled data. In this work, we explore continual\npre-training as a scalable, self-supervised approach to adapt LLMs for\ndownstream summarization tasks, particularly in the context of noisy real-world\nconversation transcripts. We conduct extensive experiments using large-scale,\nunlabeled business conversation data to investigate whether continual\npre-training enhances model capabilities in conversational summarization. Our\nresults demonstrate that continual pre-training yields substantial gains in\nboth in-domain and out-of-domain summarization benchmarks, while maintaining\nstrong generalization and robustness. We also analyze the effects of data\nselection strategies, providing practical guidelines for applying continual\npre-training in summarization-focused industrial applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive performance in text\nsummarization, yet their performance often falls short when applied to\nspecialized domains that differ from their original pre-training distribution.\nWhile fine-tuning can improve summarization quality, it typically relies on\ncostly and scarce high-quality labeled data. In this work, we explore continual\npre-training as a scalable, self-supervised approach to adapt LLMs for\ndownstream summarization tasks, particularly in the context of noisy real-world\nconversation transcripts. We conduct extensive experiments using large-scale,\nunlabeled business conversation data to investigate whether continual\npre-training enhances model capabilities in conversational summarization. Our\nresults demonstrate that continual pre-training yields substantial gains in\nboth in-domain and out-of-domain summarization benchmarks, while maintaining\nstrong generalization and robustness. We also analyze the effects of data\nselection strategies, providing practical guidelines for applying continual\npre-training in summarization-focused industrial applications."
                },
                "authors": [
                    {
                        "name": "Xue-Yong Fu"
                    },
                    {
                        "name": "Elena Khasanova"
                    },
                    {
                        "name": "Md Tahmid Rahman Laskar"
                    },
                    {
                        "name": "Harsh Saini"
                    },
                    {
                        "name": "Shashi Bhushan TN"
                    }
                ],
                "author_detail": {
                    "name": "Shashi Bhushan TN"
                },
                "author": "Shashi Bhushan TN",
                "arxiv_comment": "Accepted to the NewSumm Workshop at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05858v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05858v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21432v2",
                "updated": "2025-10-07T12:12:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    12,
                    13,
                    1,
                    280,
                    0
                ],
                "published": "2025-07-29T02:03:37Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    2,
                    3,
                    37,
                    1,
                    210,
                    0
                ],
                "title": "Towards Locally Deployable Fine-Tuned Causal Large Language Models for\n  Mode Choice Behaviour",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Locally Deployable Fine-Tuned Causal Large Language Models for\n  Mode Choice Behaviour"
                },
                "summary": "This study investigates the adoption of open-access, locally deployable\ncausal large language models (LLMs) for travel mode choice prediction and\nintroduces LiTransMC, the first fine-tuned causal LLM developed for this task.\nWe systematically benchmark eleven open-access LLMs (1-12B parameters) across\nthree stated and revealed preference datasets, testing 396 configurations and\ngenerating over 79,000 mode choice decisions. Beyond predictive accuracy, we\nevaluate models generated reasoning using BERTopic for topic modelling and a\nnovel Explanation Strength Index, providing the first structured analysis of\nhow LLMs articulate decision factors in alignment with behavioural theory.\nLiTransMC, fine-tuned using parameter efficient and loss masking strategy,\nachieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of\n0.000245, surpassing both untuned local models and larger proprietary systems,\nincluding GPT-4o with advanced persona inference and embedding-based loading,\nwhile also outperforming classical mode choice methods such as discrete choice\nmodels and machine learning classifiers for the same dataset. This dual\nimprovement, i.e., high instant-level accuracy and near-perfect distributional\ncalibration, demonstrates the feasibility of creating specialist, locally\ndeployable LLMs that integrate prediction and interpretability. Through\ncombining structured behavioural prediction with natural language reasoning,\nthis work unlocks the potential for conversational, multi-task transport models\ncapable of supporting agent-based simulations, policy testing, and behavioural\ninsight generation. These findings establish a pathway for transforming general\npurpose LLMs into specialized and explainable tools for transportation research\nand policy formulation, while maintaining privacy, reducing cost, and\nbroadening access through local deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the adoption of open-access, locally deployable\ncausal large language models (LLMs) for travel mode choice prediction and\nintroduces LiTransMC, the first fine-tuned causal LLM developed for this task.\nWe systematically benchmark eleven open-access LLMs (1-12B parameters) across\nthree stated and revealed preference datasets, testing 396 configurations and\ngenerating over 79,000 mode choice decisions. Beyond predictive accuracy, we\nevaluate models generated reasoning using BERTopic for topic modelling and a\nnovel Explanation Strength Index, providing the first structured analysis of\nhow LLMs articulate decision factors in alignment with behavioural theory.\nLiTransMC, fine-tuned using parameter efficient and loss masking strategy,\nachieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of\n0.000245, surpassing both untuned local models and larger proprietary systems,\nincluding GPT-4o with advanced persona inference and embedding-based loading,\nwhile also outperforming classical mode choice methods such as discrete choice\nmodels and machine learning classifiers for the same dataset. This dual\nimprovement, i.e., high instant-level accuracy and near-perfect distributional\ncalibration, demonstrates the feasibility of creating specialist, locally\ndeployable LLMs that integrate prediction and interpretability. Through\ncombining structured behavioural prediction with natural language reasoning,\nthis work unlocks the potential for conversational, multi-task transport models\ncapable of supporting agent-based simulations, policy testing, and behavioural\ninsight generation. These findings establish a pathway for transforming general\npurpose LLMs into specialized and explainable tools for transportation research\nand policy formulation, while maintaining privacy, reducing cost, and\nbroadening access through local deployment."
                },
                "authors": [
                    {
                        "name": "Tareq Alsaleh"
                    },
                    {
                        "name": "Bilal Farooq"
                    }
                ],
                "author_detail": {
                    "name": "Bilal Farooq"
                },
                "author": "Bilal Farooq",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05846v1",
                "updated": "2025-10-07T12:08:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    8,
                    25,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T12:08:25Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    8,
                    25,
                    1,
                    280,
                    0
                ],
                "title": "Luth: Efficient French Specialization for Small Language Models and\n  Cross-Lingual Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Luth: Efficient French Specialization for Small Language Models and\n  Cross-Lingual Transfer"
                },
                "summary": "The landscape of Large Language Models (LLMs) remains predominantly\nEnglish-centric, resulting in a significant performance gap for other major\nlanguages, such as French, especially in the context of Small Language Models\n(SLMs). Existing multilingual models demonstrate considerably lower performance\nin French compared to English, and research on efficient adaptation methods for\nFrench remains limited. To address this, we introduce \\textbf{Luth}, a family\nof French-specialized SLMs: through targeted post-training on curated,\nhigh-quality French data, our models outperform all open-source counterparts of\ncomparable size on multiple French benchmarks while retaining their original\nEnglish capabilities. We further show that strategic model merging enhances\nperformance in both languages, establishing Luth as a new state of the art for\nFrench SLMs and a robust baseline for future French-language research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The landscape of Large Language Models (LLMs) remains predominantly\nEnglish-centric, resulting in a significant performance gap for other major\nlanguages, such as French, especially in the context of Small Language Models\n(SLMs). Existing multilingual models demonstrate considerably lower performance\nin French compared to English, and research on efficient adaptation methods for\nFrench remains limited. To address this, we introduce \\textbf{Luth}, a family\nof French-specialized SLMs: through targeted post-training on curated,\nhigh-quality French data, our models outperform all open-source counterparts of\ncomparable size on multiple French benchmarks while retaining their original\nEnglish capabilities. We further show that strategic model merging enhances\nperformance in both languages, establishing Luth as a new state of the art for\nFrench SLMs and a robust baseline for future French-language research."
                },
                "authors": [
                    {
                        "name": "Maxence Lasbordes"
                    },
                    {
                        "name": "Sinoué Gad"
                    }
                ],
                "author_detail": {
                    "name": "Sinoué Gad"
                },
                "author": "Sinoué Gad",
                "arxiv_comment": "12 pages, 4 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05837v1",
                "updated": "2025-10-07T12:02:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    2,
                    3,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T12:02:03Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    2,
                    3,
                    1,
                    280,
                    0
                ],
                "title": "EEPO: Exploration-Enhanced Policy Optimization via Sample-Then-Forget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEPO: Exploration-Enhanced Policy Optimization via Sample-Then-Forget"
                },
                "summary": "Balancing exploration and exploitation remains a central challenge in\nreinforcement learning with verifiable rewards (RLVR) for large language models\n(LLMs). Current RLVR methods often overemphasize exploitation, leading to\nentropy collapse, diminished exploratory capacity, and ultimately limited\nperformance gains. Although techniques that increase policy stochasticity can\npromote exploration, they frequently fail to escape dominant behavioral modes.\nThis creates a self-reinforcing loop-repeatedly sampling and rewarding dominant\nmodes-that further erodes exploration. We introduce Exploration-Enhanced Policy\nOptimization (EEPO), a framework that promotes exploration via two-stage\nrollouts with adaptive unlearning. In the first stage, the model generates half\nof the trajectories; it then undergoes a lightweight unlearning step to\ntemporarily suppress these sampled responses, forcing the second stage to\nexplore different regions of the output space. This sample-then-forget\nmechanism disrupts the self-reinforcing loop and promotes wider exploration\nduring rollouts. Across five reasoning benchmarks, EEPO outperforms GRPO,\nachieving average relative gains of 24.3% on Qwen2.5-3B, 33.0% on\nLlama3.2-3B-Instruct, and 10.4% on Qwen3-8B-Base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing exploration and exploitation remains a central challenge in\nreinforcement learning with verifiable rewards (RLVR) for large language models\n(LLMs). Current RLVR methods often overemphasize exploitation, leading to\nentropy collapse, diminished exploratory capacity, and ultimately limited\nperformance gains. Although techniques that increase policy stochasticity can\npromote exploration, they frequently fail to escape dominant behavioral modes.\nThis creates a self-reinforcing loop-repeatedly sampling and rewarding dominant\nmodes-that further erodes exploration. We introduce Exploration-Enhanced Policy\nOptimization (EEPO), a framework that promotes exploration via two-stage\nrollouts with adaptive unlearning. In the first stage, the model generates half\nof the trajectories; it then undergoes a lightweight unlearning step to\ntemporarily suppress these sampled responses, forcing the second stage to\nexplore different regions of the output space. This sample-then-forget\nmechanism disrupts the self-reinforcing loop and promotes wider exploration\nduring rollouts. Across five reasoning benchmarks, EEPO outperforms GRPO,\nachieving average relative gains of 24.3% on Qwen2.5-3B, 33.0% on\nLlama3.2-3B-Instruct, and 10.4% on Qwen3-8B-Base."
                },
                "authors": [
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Xueting Han"
                    },
                    {
                        "name": "Qizhou Wang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Jing Bai"
                    },
                    {
                        "name": "Hinrich Schutze"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05836v1",
                "updated": "2025-10-07T12:01:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    1,
                    57,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T12:01:57Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    1,
                    57,
                    1,
                    280,
                    0
                ],
                "title": "Flow4Agent: Long-form Video Understanding via Motion Prior from Optical\n  Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow4Agent: Long-form Video Understanding via Motion Prior from Optical\n  Flow"
                },
                "summary": "Long-form video understanding has always been a challenging problem due to\nthe significant redundancy in both temporal and spatial contents. This\nchallenge is further exacerbated by the limited context length of Multimodal\nLarge Language Models (MLLMs). To address this issue, many previous works have\nattempted to extract key video information, where the \"key\" is typically\nsemantic-aware and heavily dependent on the CLIP model as prior. In this paper,\nwe propose Flow4Agent, a novel framework that pioneeringly incorporates motion\npriors from optical flow to facilitate LLM-based long video understanding.\nFlow4Agent mitigates the redundancy in long videos at both temporal and spatial\nlevels through two core modules: Temporal Granularity Optimization (TGO)\nadaptively refines framelevel hierarchies, which first leverages coarse flow\npriors to group similar visual contents and then applies semantic priors to\nfilter out highly irrelevant scene information. Motion Token Pruning (MTP)\nfurther refines the intra-frame visual representations, pruning high-redundancy\nvideo tokens using fine-grained optical flow information. Extensive experiments\ndemonstrate that our Flow4Agent outperforms existing methods across a wide\nrange of video MLLM benchmarks, especially for hour-level video understanding\ntasks, achieving 64.7% on Video-MME, 71.4% on MLVU and 60.4% on LongVideoBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form video understanding has always been a challenging problem due to\nthe significant redundancy in both temporal and spatial contents. This\nchallenge is further exacerbated by the limited context length of Multimodal\nLarge Language Models (MLLMs). To address this issue, many previous works have\nattempted to extract key video information, where the \"key\" is typically\nsemantic-aware and heavily dependent on the CLIP model as prior. In this paper,\nwe propose Flow4Agent, a novel framework that pioneeringly incorporates motion\npriors from optical flow to facilitate LLM-based long video understanding.\nFlow4Agent mitigates the redundancy in long videos at both temporal and spatial\nlevels through two core modules: Temporal Granularity Optimization (TGO)\nadaptively refines framelevel hierarchies, which first leverages coarse flow\npriors to group similar visual contents and then applies semantic priors to\nfilter out highly irrelevant scene information. Motion Token Pruning (MTP)\nfurther refines the intra-frame visual representations, pruning high-redundancy\nvideo tokens using fine-grained optical flow information. Extensive experiments\ndemonstrate that our Flow4Agent outperforms existing methods across a wide\nrange of video MLLM benchmarks, especially for hour-level video understanding\ntasks, achieving 64.7% on Video-MME, 71.4% on MLVU and 60.4% on LongVideoBench."
                },
                "authors": [
                    {
                        "name": "Ruyang Liu"
                    },
                    {
                        "name": "Shangkun Sun"
                    },
                    {
                        "name": "Haoran Tang"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "arxiv_comment": "Accepted to ICCV' 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01302v2",
                "updated": "2025-10-07T11:59:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    11,
                    59,
                    39,
                    1,
                    280,
                    0
                ],
                "published": "2025-08-02T10:25:36Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    10,
                    25,
                    36,
                    5,
                    214,
                    0
                ],
                "title": "Aligning Language Models with Real-time Knowledge Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Language Models with Real-time Knowledge Editing"
                },
                "summary": "Knowledge editing aims to modify outdated knowledge in large language models\n(LLMs) efficiently while retaining their original capabilities. Mainstream\nbenchmarks for knowledge editing are predominantly static and fail to keep in\npace with the evolving real-world knowledge. In this work, we introduce CRAFT,\nan ever-evolving real-world benchmark for knowledge editing. It features\nwell-designed paired edits for composite reasoning, and evaluates models on\nalias portability as well as temporal and common-sense locality, making it a\nchallenging knowledge editing benchmark on which previous knowledge editing\nmethods hardly achieve balanced performance. Towards flexible real-time\nediting, we propose KEDAS, a novel paradigm of knowledge editing alignment\nfeaturing diverse edit augmentation and self-adaptive post-alignment inference,\nwhich exhibits significant performance gain on CRAFT compared to previous\nmethods. All of our code and data are available at\nhttps://anonymous.4open.science/r/CRAFT-KEDAS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing aims to modify outdated knowledge in large language models\n(LLMs) efficiently while retaining their original capabilities. Mainstream\nbenchmarks for knowledge editing are predominantly static and fail to keep in\npace with the evolving real-world knowledge. In this work, we introduce CRAFT,\nan ever-evolving real-world benchmark for knowledge editing. It features\nwell-designed paired edits for composite reasoning, and evaluates models on\nalias portability as well as temporal and common-sense locality, making it a\nchallenging knowledge editing benchmark on which previous knowledge editing\nmethods hardly achieve balanced performance. Towards flexible real-time\nediting, we propose KEDAS, a novel paradigm of knowledge editing alignment\nfeaturing diverse edit augmentation and self-adaptive post-alignment inference,\nwhich exhibits significant performance gain on CRAFT compared to previous\nmethods. All of our code and data are available at\nhttps://anonymous.4open.science/r/CRAFT-KEDAS."
                },
                "authors": [
                    {
                        "name": "Chenming Tang"
                    },
                    {
                        "name": "Yutong Yang"
                    },
                    {
                        "name": "Kexue Wang"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "arxiv_comment": "Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07216v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07216v3",
                "updated": "2025-10-07T11:45:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    11,
                    45,
                    53,
                    1,
                    280,
                    0
                ],
                "published": "2025-08-10T07:36:44Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    7,
                    36,
                    44,
                    6,
                    222,
                    0
                ],
                "title": "Bridging Semantic Logic Gaps: A Cognition Inspired Multimodal Boundary\n  Preserving Network for Image Manipulation Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Semantic Logic Gaps: A Cognition Inspired Multimodal Boundary\n  Preserving Network for Image Manipulation Localization"
                },
                "summary": "The existing image manipulation localization (IML) models mainly relies on\nvisual cues, but ignores the semantic logical relationships between content\nfeatures. In fact, the content semantics conveyed by real images often conform\nto human cognitive laws. However, image manipulation technology usually\ndestroys the internal relationship between content features, thus leaving\nsemantic clues for IML. In this paper, we propose a cognition inspired\nmultimodal boundary preserving network (CMB-Net). Specifically, CMB-Net\nutilizes large language models (LLMs) to analyze manipulated regions within\nimages and generate prompt-based textual information to compensate for the lack\nof semantic relationships in the visual information. Considering that the\nerroneous texts induced by hallucination from LLMs will damage the accuracy of\nIML, we propose an image-text central ambiguity module (ITCAM). It assigns\nweights to the text features by quantifying the ambiguity between text and\nimage features, thereby ensuring the beneficial impact of textual information.\nWe also propose an image-text interaction module (ITIM) that aligns visual and\ntext features using a correlation matrix for fine-grained interaction. Finally,\ninspired by invertible neural networks, we propose a restoration edge decoder\n(RED) that mutually generates input and output features to preserve boundary\ninformation in manipulated regions without loss. Extensive experiments show\nthat CMB-Net outperforms most existing IML models. Our code is available on\nhttps://github.com/vpsg-research/CMB-Net.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The existing image manipulation localization (IML) models mainly relies on\nvisual cues, but ignores the semantic logical relationships between content\nfeatures. In fact, the content semantics conveyed by real images often conform\nto human cognitive laws. However, image manipulation technology usually\ndestroys the internal relationship between content features, thus leaving\nsemantic clues for IML. In this paper, we propose a cognition inspired\nmultimodal boundary preserving network (CMB-Net). Specifically, CMB-Net\nutilizes large language models (LLMs) to analyze manipulated regions within\nimages and generate prompt-based textual information to compensate for the lack\nof semantic relationships in the visual information. Considering that the\nerroneous texts induced by hallucination from LLMs will damage the accuracy of\nIML, we propose an image-text central ambiguity module (ITCAM). It assigns\nweights to the text features by quantifying the ambiguity between text and\nimage features, thereby ensuring the beneficial impact of textual information.\nWe also propose an image-text interaction module (ITIM) that aligns visual and\ntext features using a correlation matrix for fine-grained interaction. Finally,\ninspired by invertible neural networks, we propose a restoration edge decoder\n(RED) that mutually generates input and output features to preserve boundary\ninformation in manipulated regions without loss. Extensive experiments show\nthat CMB-Net outperforms most existing IML models. Our code is available on\nhttps://github.com/vpsg-research/CMB-Net."
                },
                "authors": [
                    {
                        "name": "Songlin Li"
                    },
                    {
                        "name": "Zhiqing Guo"
                    },
                    {
                        "name": "Yuanman Li"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yunfeng Diao"
                    },
                    {
                        "name": "Gaobo Yang"
                    },
                    {
                        "name": "Liejun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liejun Wang"
                },
                "author": "Liejun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07216v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07216v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19056v2",
                "updated": "2025-10-07T11:31:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    11,
                    31,
                    29,
                    1,
                    280,
                    0
                ],
                "published": "2025-05-25T09:18:24Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    9,
                    18,
                    24,
                    6,
                    145,
                    0
                ],
                "title": "An Embarrassingly Simple Defense Against LLM Abliteration Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Embarrassingly Simple Defense Against LLM Abliteration Attacks"
                },
                "summary": "Large language models (LLMs) are typically aligned to refuse harmful\ninstructions through safety fine-tuning. A recent attack, termed abliteration,\nidentifies and suppresses the single latent direction most responsible for\nrefusal behavior, thereby enabling models to generate harmful content. We\npropose a defense that fundamentally alters how models express refusal. We\nconstruct an extended-refusal dataset in which responses to harmful prompts\nprovide detailed justifications before refusing, distributing the refusal\nsignal across multiple token positions. Fine-tuning Llama-2-7B-Chat and\nQwen2.5-Instruct (1.5B and 3B parameters) on this dataset yields models that\nmaintain high refusal rates under abliteration: refusal rates drop by at most\n10%, compared to 70-80% drops in baseline models. Comprehensive evaluations of\nsafety and utility demonstrate that extended-refusal fine-tuning effectively\nneutralizes abliteration attacks while preserving general model performance and\nenhancing robustness across multiple alignment scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically aligned to refuse harmful\ninstructions through safety fine-tuning. A recent attack, termed abliteration,\nidentifies and suppresses the single latent direction most responsible for\nrefusal behavior, thereby enabling models to generate harmful content. We\npropose a defense that fundamentally alters how models express refusal. We\nconstruct an extended-refusal dataset in which responses to harmful prompts\nprovide detailed justifications before refusing, distributing the refusal\nsignal across multiple token positions. Fine-tuning Llama-2-7B-Chat and\nQwen2.5-Instruct (1.5B and 3B parameters) on this dataset yields models that\nmaintain high refusal rates under abliteration: refusal rates drop by at most\n10%, compared to 70-80% drops in baseline models. Comprehensive evaluations of\nsafety and utility demonstrate that extended-refusal fine-tuning effectively\nneutralizes abliteration attacks while preserving general model performance and\nenhancing robustness across multiple alignment scenarios."
                },
                "authors": [
                    {
                        "name": "Harethah Abu Shairah"
                    },
                    {
                        "name": "Hasan Abed Al Kader Hammoud"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "George Turkiyyah"
                    }
                ],
                "author_detail": {
                    "name": "George Turkiyyah"
                },
                "author": "George Turkiyyah",
                "arxiv_comment": "preprint - under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05807v1",
                "updated": "2025-10-07T11:24:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    11,
                    24,
                    51,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T11:24:51Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    11,
                    24,
                    51,
                    1,
                    280,
                    0
                ],
                "title": "Privacy-Preserving On-chain Permissioning for KYC-Compliant\n  Decentralized Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving On-chain Permissioning for KYC-Compliant\n  Decentralized Applications"
                },
                "summary": "Decentralized applications (dApps) in Decentralized Finance (DeFi) face a\nfundamental tension between regulatory compliance requirements like Know Your\nCustomer (KYC) and maintaining decentralization and privacy. Existing\npermissioned DeFi solutions often fail to adequately protect private attributes\nof dApp users and introduce implicit trust assumptions, undermining the\nblockchain's decentralization. Addressing these limitations, this paper\npresents a novel synthesis of Self-Sovereign Identity (SSI), Zero-Knowledge\nProofs (ZKPs), and Attribute-Based Access Control to enable privacy-preserving\non-chain permissioning based on decentralized policy decisions. We provide a\ncomprehensive framework for permissioned dApps that aligns decentralized trust,\nprivacy, and transparency, harmonizing blockchain principles with regulatory\ncompliance. Our framework supports multiple proof types (equality, range,\nmembership, and time-dependent) with efficient proof generation through a\ncommit-and-prove scheme that moves credential authenticity verification outside\nthe ZKP circuit. Experimental evaluation of our KYC-compliant DeFi\nimplementation shows considerable performance improvement for different proof\ntypes compared to baseline approaches. We advance the state-of-the-art through\na holistic approach, flexible proof mechanisms addressing diverse real-world\nrequirements, and optimized proof generation enabling practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized applications (dApps) in Decentralized Finance (DeFi) face a\nfundamental tension between regulatory compliance requirements like Know Your\nCustomer (KYC) and maintaining decentralization and privacy. Existing\npermissioned DeFi solutions often fail to adequately protect private attributes\nof dApp users and introduce implicit trust assumptions, undermining the\nblockchain's decentralization. Addressing these limitations, this paper\npresents a novel synthesis of Self-Sovereign Identity (SSI), Zero-Knowledge\nProofs (ZKPs), and Attribute-Based Access Control to enable privacy-preserving\non-chain permissioning based on decentralized policy decisions. We provide a\ncomprehensive framework for permissioned dApps that aligns decentralized trust,\nprivacy, and transparency, harmonizing blockchain principles with regulatory\ncompliance. Our framework supports multiple proof types (equality, range,\nmembership, and time-dependent) with efficient proof generation through a\ncommit-and-prove scheme that moves credential authenticity verification outside\nthe ZKP circuit. Experimental evaluation of our KYC-compliant DeFi\nimplementation shows considerable performance improvement for different proof\ntypes compared to baseline approaches. We advance the state-of-the-art through\na holistic approach, flexible proof mechanisms addressing diverse real-world\nrequirements, and optimized proof generation enabling practical deployment."
                },
                "authors": [
                    {
                        "name": "Fabian Piper"
                    },
                    {
                        "name": "Karl Wolf"
                    },
                    {
                        "name": "Jonathan Heiss"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Heiss"
                },
                "author": "Jonathan Heiss",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05799v1",
                "updated": "2025-10-07T11:18:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    11,
                    18,
                    4,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T11:18:04Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    11,
                    18,
                    4,
                    1,
                    280,
                    0
                ],
                "title": "Data-efficient Targeted Token-level Preference Optimization for\n  LLM-based Text-to-Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-efficient Targeted Token-level Preference Optimization for\n  LLM-based Text-to-Speech"
                },
                "summary": "Aligning text-to-speech (TTS) system outputs with human feedback through\npreference optimization has been shown to effectively improve the robustness\nand naturalness of language model-based TTS models. Current approaches\nprimarily require paired desirable and undesirable samples at the utterance\nlevel. However, such pairs are often limited in TTS output data, and\nutterance-level formulation prevents fine-grained token-level optimization\nneeded for accurate pronunciation alignment. In this study, we propose TKTO\nthat eliminates the need for paired data, enabling a more data-efficient\ntraining paradigm, and directly targets token-level units, automatically\nproviding fine-grained alignment signals without token-level annotations. TKTO\nimproves the challenging Japanese TTS accuracy by 39% and reduces CER by 54%,\nautomatically assigning 12.8 times stronger reward to targeted tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning text-to-speech (TTS) system outputs with human feedback through\npreference optimization has been shown to effectively improve the robustness\nand naturalness of language model-based TTS models. Current approaches\nprimarily require paired desirable and undesirable samples at the utterance\nlevel. However, such pairs are often limited in TTS output data, and\nutterance-level formulation prevents fine-grained token-level optimization\nneeded for accurate pronunciation alignment. In this study, we propose TKTO\nthat eliminates the need for paired data, enabling a more data-efficient\ntraining paradigm, and directly targets token-level units, automatically\nproviding fine-grained alignment signals without token-level annotations. TKTO\nimproves the challenging Japanese TTS accuracy by 39% and reduces CER by 54%,\nautomatically assigning 12.8 times stronger reward to targeted tokens."
                },
                "authors": [
                    {
                        "name": "Rikuto Kotoge"
                    },
                    {
                        "name": "Yuichi Sasaki"
                    }
                ],
                "author_detail": {
                    "name": "Yuichi Sasaki"
                },
                "author": "Yuichi Sasaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09098v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09098v4",
                "updated": "2025-10-07T11:13:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    11,
                    13,
                    56,
                    1,
                    280,
                    0
                ],
                "published": "2024-06-13T13:27:52Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    13,
                    27,
                    52,
                    3,
                    165,
                    0
                ],
                "title": "SciKnowEval: Evaluating Multi-level Scientific Knowledge of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciKnowEval: Evaluating Multi-level Scientific Knowledge of Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) are playing an increasingly important role in\nscientific research, yet there remains a lack of comprehensive benchmarks to\nevaluate the breadth and depth of scientific knowledge embedded in these\nmodels. To address this gap, we introduce SciKnowEval, a large-scale dataset\ndesigned to systematically assess LLMs across five progressive levels of\nscientific understanding: memory, comprehension, reasoning, discernment, and\napplication. SciKnowEval comprises 28K multi-level questions and solutions\nspanning biology, chemistry, physics, and materials science. Using this\nbenchmark, we evaluate 20 leading open-source and proprietary LLMs. The results\nshow that while proprietary models often achieve state-of-the-art performance,\nsubstantial challenges remain -- particularly in scientific reasoning and\nreal-world application. We envision SciKnowEval as a standard benchmark for\nevaluating scientific capabilities in LLMs and as a catalyst for advancing more\ncapable and reliable scientific language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are playing an increasingly important role in\nscientific research, yet there remains a lack of comprehensive benchmarks to\nevaluate the breadth and depth of scientific knowledge embedded in these\nmodels. To address this gap, we introduce SciKnowEval, a large-scale dataset\ndesigned to systematically assess LLMs across five progressive levels of\nscientific understanding: memory, comprehension, reasoning, discernment, and\napplication. SciKnowEval comprises 28K multi-level questions and solutions\nspanning biology, chemistry, physics, and materials science. Using this\nbenchmark, we evaluate 20 leading open-source and proprietary LLMs. The results\nshow that while proprietary models often achieve state-of-the-art performance,\nsubstantial challenges remain -- particularly in scientific reasoning and\nreal-world application. We envision SciKnowEval as a standard benchmark for\nevaluating scientific capabilities in LLMs and as a catalyst for advancing more\ncapable and reliable scientific language models."
                },
                "authors": [
                    {
                        "name": "Kehua Feng"
                    },
                    {
                        "name": "Xinyi Shen"
                    },
                    {
                        "name": "Weijie Wang"
                    },
                    {
                        "name": "Xiang Zhuang"
                    },
                    {
                        "name": "Yuqi Tang"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Keyan Ding"
                    }
                ],
                "author_detail": {
                    "name": "Keyan Ding"
                },
                "author": "Keyan Ding",
                "arxiv_comment": "33 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09098v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09098v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05788v1",
                "updated": "2025-10-07T11:09:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    11,
                    9,
                    11,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T11:09:11Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    11,
                    9,
                    11,
                    1,
                    280,
                    0
                ],
                "title": "Mellum: Production-Grade in-IDE Contextual Code Completion with\n  Multi-File Project Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mellum: Production-Grade in-IDE Contextual Code Completion with\n  Multi-File Project Understanding"
                },
                "summary": "We present the Mellum models family, open-weight code completion models\ndesigned for interactive use in JetBrains IDEs. Mellums have 4B parameters,\nadopt a Llama-style architecture, and are pre-trained on ~4T tokens of\npermissively licensed, multi-language code. Our studies show that (i) careful\ndata curation and staged training significantly improve the model's quality,\n(ii) editor-critical capabilities such as context packing are necessary for\nhigh-quality suggestions, and (iii) a compact, task-focused model can meet the\ncost and latency constraints of interactive completion.\n  In the paper, we describe an end-to-end industrial pipeline for producing\ncontextualized in-editor completion: disciplined data governance, multi-stage\ntraining that includes fill-in-the-middle and project context via supervised\nfine-tuning, and alignment via direct preference optimization using feedback\nfrom real-world scenarios. Our quality evaluations include both large-scale\noffline benchmarks and online telemetry from production deployments in\nJetBrains IDEs. Mellums are released under the Apache-2.0 license on\nHuggingFace, with a public model card providing a reproducible reference for\npractitioners. Our experience offers a pragmatic blueprint for taking a\nfocused, open model from a research prototype to at scale production for\nhundreds of thousands of users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the Mellum models family, open-weight code completion models\ndesigned for interactive use in JetBrains IDEs. Mellums have 4B parameters,\nadopt a Llama-style architecture, and are pre-trained on ~4T tokens of\npermissively licensed, multi-language code. Our studies show that (i) careful\ndata curation and staged training significantly improve the model's quality,\n(ii) editor-critical capabilities such as context packing are necessary for\nhigh-quality suggestions, and (iii) a compact, task-focused model can meet the\ncost and latency constraints of interactive completion.\n  In the paper, we describe an end-to-end industrial pipeline for producing\ncontextualized in-editor completion: disciplined data governance, multi-stage\ntraining that includes fill-in-the-middle and project context via supervised\nfine-tuning, and alignment via direct preference optimization using feedback\nfrom real-world scenarios. Our quality evaluations include both large-scale\noffline benchmarks and online telemetry from production deployments in\nJetBrains IDEs. Mellums are released under the Apache-2.0 license on\nHuggingFace, with a public model card providing a reproducible reference for\npractitioners. Our experience offers a pragmatic blueprint for taking a\nfocused, open model from a research prototype to at scale production for\nhundreds of thousands of users."
                },
                "authors": [
                    {
                        "name": "Nikita Pavlichenko"
                    },
                    {
                        "name": "Iurii Nazarov"
                    },
                    {
                        "name": "Ivan Dolgov"
                    },
                    {
                        "name": "Ekaterina Garanina"
                    },
                    {
                        "name": "Dmitry Ustalov"
                    },
                    {
                        "name": "Ivan Bondyrev"
                    },
                    {
                        "name": "Kseniia Lysaniuk"
                    },
                    {
                        "name": "Evgeniia Vu"
                    },
                    {
                        "name": "Kirill Chekmenev"
                    },
                    {
                        "name": "Joseph Shtok"
                    },
                    {
                        "name": "Yaroslav Golubev"
                    },
                    {
                        "name": "Anton Semenkin"
                    },
                    {
                        "name": "Uladzislau Sazanovich"
                    }
                ],
                "author_detail": {
                    "name": "Uladzislau Sazanovich"
                },
                "author": "Uladzislau Sazanovich",
                "arxiv_comment": "11 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02725v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02725v2",
                "updated": "2025-10-07T11:07:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    11,
                    7,
                    54,
                    1,
                    280,
                    0
                ],
                "published": "2025-04-03T16:07:38Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    7,
                    38,
                    3,
                    93,
                    0
                ],
                "title": "SAFER: Advancing Safety Alignment via Efficient Ex-Ante Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAFER: Advancing Safety Alignment via Efficient Ex-Ante Reasoning"
                },
                "summary": "Recent advancements in large language models (LLMs) have accelerated progress\ntoward artificial general intelligence, yet their potential to generate harmful\ncontent poses critical safety challenges. Existing alignment methods often\nstruggle to cover diverse safety scenarios and remain vulnerable to adversarial\nattacks. In this work, we propose SAFER, a framework for Safety Alignment via\neFficient Ex-Ante Reasoning. Our approach instantiates structured Ex-Ante\nreasoning through initial assessment, rule verification, and path calibration,\nand embeds predefined safety rules to provide transparent and verifiable safety\njudgments. Specifically, our approach consists of two training stages: (1)\nsupervised fine-tuning with synthetic traces to teach the multi-stage Ex-Ante\nreasoning, and (2) step-level reasoning preference optimization to jointly\nenhance safety, utility, and efficiency. Experiments on multiple open-source\nLLMs demonstrate that SAFER significantly enhances safety performance while\nmaintaining helpfulness and response efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have accelerated progress\ntoward artificial general intelligence, yet their potential to generate harmful\ncontent poses critical safety challenges. Existing alignment methods often\nstruggle to cover diverse safety scenarios and remain vulnerable to adversarial\nattacks. In this work, we propose SAFER, a framework for Safety Alignment via\neFficient Ex-Ante Reasoning. Our approach instantiates structured Ex-Ante\nreasoning through initial assessment, rule verification, and path calibration,\nand embeds predefined safety rules to provide transparent and verifiable safety\njudgments. Specifically, our approach consists of two training stages: (1)\nsupervised fine-tuning with synthetic traces to teach the multi-stage Ex-Ante\nreasoning, and (2) step-level reasoning preference optimization to jointly\nenhance safety, utility, and efficiency. Experiments on multiple open-source\nLLMs demonstrate that SAFER significantly enhances safety performance while\nmaintaining helpfulness and response efficiency."
                },
                "authors": [
                    {
                        "name": "Kehua Feng"
                    },
                    {
                        "name": "Keyan Ding"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Menghan Li"
                    },
                    {
                        "name": "Fanjunduo Wei"
                    },
                    {
                        "name": "Xinda Wang"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "22 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02725v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17906v2",
                "updated": "2025-10-07T11:01:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    11,
                    1,
                    58,
                    1,
                    280,
                    0
                ],
                "published": "2025-08-25T11:24:55Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    11,
                    24,
                    55,
                    0,
                    237,
                    0
                ],
                "title": "FinReflectKG: Agentic Construction and Evaluation of Financial Knowledge\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinReflectKG: Agentic Construction and Evaluation of Financial Knowledge\n  Graphs"
                },
                "summary": "The financial domain poses unique challenges for knowledge graph (KG)\nconstruction at scale due to the complexity and regulatory nature of financial\ndocuments. Despite the critical importance of structured financial knowledge,\nthe field lacks large-scale, open-source datasets capturing rich semantic\nrelationships from corporate disclosures. We introduce an open-source,\nlarge-scale financial knowledge graph dataset built from the latest annual SEC\n10-K filings of all S and P 100 companies - a comprehensive resource designed\nto catalyze research in financial AI. We propose a robust and generalizable\nknowledge graph (KG) construction framework that integrates intelligent\ndocument parsing, table-aware chunking, and schema-guided iterative extraction\nwith a reflection-driven feedback loop. Our system incorporates a comprehensive\nevaluation pipeline, combining rule-based checks, statistical validation, and\nLLM-as-a-Judge assessments to holistically measure extraction quality. We\nsupport three extraction modes - single-pass, multi-pass, and\nreflection-agent-based - allowing flexible trade-offs between efficiency,\naccuracy, and reliability based on user requirements. Empirical evaluations\ndemonstrate that the reflection-agent-based mode consistently achieves the best\nbalance, attaining a 64.8 percent compliance score against all rule-based\npolicies (CheckRules) and outperforming baseline methods (single-pass and\nmulti-pass) across key metrics such as precision, comprehensiveness, and\nrelevance in LLM-guided evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The financial domain poses unique challenges for knowledge graph (KG)\nconstruction at scale due to the complexity and regulatory nature of financial\ndocuments. Despite the critical importance of structured financial knowledge,\nthe field lacks large-scale, open-source datasets capturing rich semantic\nrelationships from corporate disclosures. We introduce an open-source,\nlarge-scale financial knowledge graph dataset built from the latest annual SEC\n10-K filings of all S and P 100 companies - a comprehensive resource designed\nto catalyze research in financial AI. We propose a robust and generalizable\nknowledge graph (KG) construction framework that integrates intelligent\ndocument parsing, table-aware chunking, and schema-guided iterative extraction\nwith a reflection-driven feedback loop. Our system incorporates a comprehensive\nevaluation pipeline, combining rule-based checks, statistical validation, and\nLLM-as-a-Judge assessments to holistically measure extraction quality. We\nsupport three extraction modes - single-pass, multi-pass, and\nreflection-agent-based - allowing flexible trade-offs between efficiency,\naccuracy, and reliability based on user requirements. Empirical evaluations\ndemonstrate that the reflection-agent-based mode consistently achieves the best\nbalance, attaining a 64.8 percent compliance score against all rule-based\npolicies (CheckRules) and outperforming baseline methods (single-pass and\nmulti-pass) across key metrics such as precision, comprehensiveness, and\nrelevance in LLM-guided evaluations."
                },
                "authors": [
                    {
                        "name": "Abhinav Arun"
                    },
                    {
                        "name": "Fabrizio Dimino"
                    },
                    {
                        "name": "Tejas Prakash Agarwal"
                    },
                    {
                        "name": "Bhaskarjit Sarmah"
                    },
                    {
                        "name": "Stefano Pasquali"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Pasquali"
                },
                "author": "Stefano Pasquali",
                "arxiv_doi": "10.1145/3768292.3770363.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3768292.3770363.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.17906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14146v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14146v3",
                "updated": "2025-10-07T10:58:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    10,
                    58,
                    44,
                    1,
                    280,
                    0
                ],
                "published": "2025-08-19T16:37:19Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    37,
                    19,
                    1,
                    231,
                    0
                ],
                "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based\n  Peer Review Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based\n  Peer Review Automation"
                },
                "summary": "With the rapid growth of academic publications, peer review has become an\nessential yet time-consuming responsibility within the research community.\nLarge Language Models (LLMs) have increasingly been adopted to assist in the\ngeneration of review comments; however, current LLM-based review tasks lack a\nunified evaluation benchmark to rigorously assess the models' ability to\nproduce comprehensive, accurate, and human-aligned assessments, particularly in\nscenarios involving multimodal content such as figures and tables. To address\nthis gap, we propose \\textbf{MMReview}, a comprehensive benchmark that spans\nmultiple disciplines and modalities. MMReview includes multimodal content and\nexpert-written review comments for 240 papers across 17 research domains within\nfour major academic disciplines: Artificial Intelligence, Natural Sciences,\nEngineering Sciences, and Social Sciences. We design a total of 13 tasks\ngrouped into four core categories, aimed at evaluating the performance of LLMs\nand Multimodal LLMs (MLLMs) in step-wise review generation, outcome\nformulation, alignment with human preferences, and robustness to adversarial\ninput manipulation. Extensive experiments conducted on 16 open-source models\nand 5 advanced closed-source models demonstrate the thoroughness of the\nbenchmark. We envision MMReview as a critical step toward establishing a\nstandardized foundation for the development of automated peer review systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of academic publications, peer review has become an\nessential yet time-consuming responsibility within the research community.\nLarge Language Models (LLMs) have increasingly been adopted to assist in the\ngeneration of review comments; however, current LLM-based review tasks lack a\nunified evaluation benchmark to rigorously assess the models' ability to\nproduce comprehensive, accurate, and human-aligned assessments, particularly in\nscenarios involving multimodal content such as figures and tables. To address\nthis gap, we propose \\textbf{MMReview}, a comprehensive benchmark that spans\nmultiple disciplines and modalities. MMReview includes multimodal content and\nexpert-written review comments for 240 papers across 17 research domains within\nfour major academic disciplines: Artificial Intelligence, Natural Sciences,\nEngineering Sciences, and Social Sciences. We design a total of 13 tasks\ngrouped into four core categories, aimed at evaluating the performance of LLMs\nand Multimodal LLMs (MLLMs) in step-wise review generation, outcome\nformulation, alignment with human preferences, and robustness to adversarial\ninput manipulation. Extensive experiments conducted on 16 open-source models\nand 5 advanced closed-source models demonstrate the thoroughness of the\nbenchmark. We envision MMReview as a critical step toward establishing a\nstandardized foundation for the development of automated peer review systems."
                },
                "authors": [
                    {
                        "name": "Xian Gao"
                    },
                    {
                        "name": "Jiacheng Ruan"
                    },
                    {
                        "name": "Zongyun Zhang"
                    },
                    {
                        "name": "Jingsheng Gao"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Yuzhuo Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhuo Fu"
                },
                "author": "Yuzhuo Fu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14146v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14146v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05690v2",
                "updated": "2025-10-07T10:50:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    10,
                    50,
                    33,
                    1,
                    280,
                    0
                ],
                "published": "2025-06-06T02:37:47Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    2,
                    37,
                    47,
                    4,
                    157,
                    0
                ],
                "title": "When to use Graphs in RAG: A Comprehensive Analysis for Graph\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When to use Graphs in RAG: A Comprehensive Analysis for Graph\n  Retrieval-Augmented Generation"
                },
                "summary": "Graph retrieval-augmented generation (GraphRAG) has emerged as a powerful\nparadigm for enhancing large language models (LLMs) with external knowledge. It\nleverages graphs to model the hierarchical structure between specific concepts,\nenabling more coherent and effective knowledge retrieval for accurate\nreasoning.Despite its conceptual promise, recent studies report that GraphRAG\nfrequently underperforms vanilla RAG on many real-world tasks. This raises a\ncritical question: Is GraphRAG really effective, and in which scenarios do\ngraph structures provide measurable benefits for RAG systems? To address this,\nwe propose GraphRAG-Bench, a comprehensive benchmark designed to evaluate\nGraphRAG models onboth hierarchical knowledge retrieval and deep contextual\nreasoning. GraphRAG-Bench features a comprehensive dataset with tasks of\nincreasing difficulty, coveringfact retrieval, complex reasoning, contextual\nsummarization, and creative generation, and a systematic evaluation across the\nentire pipeline, from graph constructionand knowledge retrieval to final\ngeneration. Leveraging this novel benchmark, we systematically investigate the\nconditions when GraphRAG surpasses traditional RAG and the underlying reasons\nfor its success, offering guidelines for its practical application. All related\nresources and analyses are collected for the community at\nhttps://github.com/GraphRAG-Bench/GraphRAG-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph retrieval-augmented generation (GraphRAG) has emerged as a powerful\nparadigm for enhancing large language models (LLMs) with external knowledge. It\nleverages graphs to model the hierarchical structure between specific concepts,\nenabling more coherent and effective knowledge retrieval for accurate\nreasoning.Despite its conceptual promise, recent studies report that GraphRAG\nfrequently underperforms vanilla RAG on many real-world tasks. This raises a\ncritical question: Is GraphRAG really effective, and in which scenarios do\ngraph structures provide measurable benefits for RAG systems? To address this,\nwe propose GraphRAG-Bench, a comprehensive benchmark designed to evaluate\nGraphRAG models onboth hierarchical knowledge retrieval and deep contextual\nreasoning. GraphRAG-Bench features a comprehensive dataset with tasks of\nincreasing difficulty, coveringfact retrieval, complex reasoning, contextual\nsummarization, and creative generation, and a systematic evaluation across the\nentire pipeline, from graph constructionand knowledge retrieval to final\ngeneration. Leveraging this novel benchmark, we systematically investigate the\nconditions when GraphRAG surpasses traditional RAG and the underlying reasons\nfor its success, offering guidelines for its practical application. All related\nresources and analyses are collected for the community at\nhttps://github.com/GraphRAG-Bench/GraphRAG-Benchmark."
                },
                "authors": [
                    {
                        "name": "Zhishang Xiang"
                    },
                    {
                        "name": "Chuanjie Wu"
                    },
                    {
                        "name": "Qinggang Zhang"
                    },
                    {
                        "name": "Shengyuan Chen"
                    },
                    {
                        "name": "Zijin Hong"
                    },
                    {
                        "name": "Xiao Huang"
                    },
                    {
                        "name": "Jinsong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Su"
                },
                "author": "Jinsong Su",
                "arxiv_comment": "All resources and analyses are collected at\n  https://github.com/GraphRAG-Bench/GraphRAG-Benchmark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05774v1",
                "updated": "2025-10-07T10:43:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    10,
                    43,
                    39,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T10:43:39Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    10,
                    43,
                    39,
                    1,
                    280,
                    0
                ],
                "title": "ConstraintLLM: A Neuro-Symbolic Framework for Industrial-Level\n  Constraint Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConstraintLLM: A Neuro-Symbolic Framework for Industrial-Level\n  Constraint Programming"
                },
                "summary": "Constraint programming (CP) is a crucial technology for solving real-world\nconstraint optimization problems (COPs), with the advantages of rich modeling\nsemantics and high solving efficiency. Using large language models (LLMs) to\ngenerate formal modeling automatically for COPs is becoming a promising\napproach, which aims to build trustworthy neuro-symbolic AI with the help of\nsymbolic solvers. However, CP has received less attention compared to works\nbased on operations research (OR) models. We introduce ConstraintLLM, the first\nLLM specifically designed for CP modeling, which is trained on an open-source\nLLM with multi-instruction supervised fine-tuning. We propose the\nConstraint-Aware Retrieval Module (CARM) to increase the in-context learning\ncapabilities, which is integrated in a Tree-of-Thoughts (ToT) framework with\nguided self-correction mechanism. Moreover, we construct and release IndusCP,\nthe first industrial-level benchmark for CP modeling, which contains 140\nchallenging tasks from various domains. Our experiments demonstrate that\nConstraintLLM achieves state-of-the-art solving accuracy across multiple\nbenchmarks and outperforms the baselines by 2x on the new IndusCP benchmark.\nCode and data are available at: https://github.com/william4s/ConstraintLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint programming (CP) is a crucial technology for solving real-world\nconstraint optimization problems (COPs), with the advantages of rich modeling\nsemantics and high solving efficiency. Using large language models (LLMs) to\ngenerate formal modeling automatically for COPs is becoming a promising\napproach, which aims to build trustworthy neuro-symbolic AI with the help of\nsymbolic solvers. However, CP has received less attention compared to works\nbased on operations research (OR) models. We introduce ConstraintLLM, the first\nLLM specifically designed for CP modeling, which is trained on an open-source\nLLM with multi-instruction supervised fine-tuning. We propose the\nConstraint-Aware Retrieval Module (CARM) to increase the in-context learning\ncapabilities, which is integrated in a Tree-of-Thoughts (ToT) framework with\nguided self-correction mechanism. Moreover, we construct and release IndusCP,\nthe first industrial-level benchmark for CP modeling, which contains 140\nchallenging tasks from various domains. Our experiments demonstrate that\nConstraintLLM achieves state-of-the-art solving accuracy across multiple\nbenchmarks and outperforms the baselines by 2x on the new IndusCP benchmark.\nCode and data are available at: https://github.com/william4s/ConstraintLLM."
                },
                "authors": [
                    {
                        "name": "Weichun Shi"
                    },
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Wanting Zhang"
                    },
                    {
                        "name": "Langchen Shi"
                    },
                    {
                        "name": "Fuqi Jia"
                    },
                    {
                        "name": "Feifei Ma"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "arxiv_comment": "Accepted to the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025), Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06573v2",
                "updated": "2025-10-07T10:31:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    10,
                    31,
                    48,
                    1,
                    280,
                    0
                ],
                "published": "2025-03-09T12:06:29Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    6,
                    29,
                    6,
                    68,
                    0
                ],
                "title": "WildIFEval: Instruction Following in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildIFEval: Instruction Following in the Wild"
                },
                "summary": "Recent LLMs have shown remarkable success in following user instructions, yet\nhandling instructions with multiple constraints remains a significant\nchallenge. In this work, we introduce WildIFEval - a large-scale dataset of 7K\nreal user instructions with diverse, multi-constraint conditions. Unlike prior\ndatasets, our collection spans a broad lexical and topical spectrum of\nconstraints, extracted from natural user instructions. We categorize these\nconstraints into eight high-level classes to capture their distribution and\ndynamics in real-world scenarios. Leveraging WildIFEval, we conduct extensive\nexperiments to benchmark the instruction-following capabilities of leading\nLLMs. WildIFEval clearly differentiates between small and large models, and\ndemonstrates that all models have a large room for improvement on such tasks.\nWe analyze the effects of the number and type of constraints on performance,\nrevealing interesting patterns of model constraint-following behavior. We\nrelease our dataset to promote further research on instruction-following under\ncomplex, realistic conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent LLMs have shown remarkable success in following user instructions, yet\nhandling instructions with multiple constraints remains a significant\nchallenge. In this work, we introduce WildIFEval - a large-scale dataset of 7K\nreal user instructions with diverse, multi-constraint conditions. Unlike prior\ndatasets, our collection spans a broad lexical and topical spectrum of\nconstraints, extracted from natural user instructions. We categorize these\nconstraints into eight high-level classes to capture their distribution and\ndynamics in real-world scenarios. Leveraging WildIFEval, we conduct extensive\nexperiments to benchmark the instruction-following capabilities of leading\nLLMs. WildIFEval clearly differentiates between small and large models, and\ndemonstrates that all models have a large room for improvement on such tasks.\nWe analyze the effects of the number and type of constraints on performance,\nrevealing interesting patterns of model constraint-following behavior. We\nrelease our dataset to promote further research on instruction-following under\ncomplex, realistic conditions."
                },
                "authors": [
                    {
                        "name": "Gili Lior"
                    },
                    {
                        "name": "Asaf Yehudai"
                    },
                    {
                        "name": "Ariel Gera"
                    },
                    {
                        "name": "Liat Ein-Dor"
                    }
                ],
                "author_detail": {
                    "name": "Liat Ein-Dor"
                },
                "author": "Liat Ein-Dor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05758v1",
                "updated": "2025-10-07T10:24:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    10,
                    24,
                    12,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T10:24:12Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    10,
                    24,
                    12,
                    1,
                    280,
                    0
                ],
                "title": "EMORL-TTS: Reinforcement Learning for Fine-Grained Emotion Control in\n  LLM-based TTS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMORL-TTS: Reinforcement Learning for Fine-Grained Emotion Control in\n  LLM-based TTS"
                },
                "summary": "Recent LLM-based TTS systems achieve strong quality and zero-shot ability,\nbut lack fine-grained emotional control due to their reliance on discrete\nspeech tokens. Existing approaches either limit emotions to categorical labels\nor cannot generalize to LLM-based architectures. We propose EMORL-TTS\n(Fine-grained Emotion-controllable TTS with Reinforcement Learning), a\nframework that unifies global intensity control in the VAD space with local\nemphasis regulation. Our method combines supervised fine-tuning with\nreinforcement learning guided by task-specific rewards for emotion category,\nintensity, and emphasis. Moreover, we further investigate how emphasis\nplacement modulates fine-grained emotion intensity. Experiments show that\nEMORL-TTS improves emotion accuracy, intensity differentiation, and emphasis\nclarity, while preserving synthesis quality comparable to strong LLM-based\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent LLM-based TTS systems achieve strong quality and zero-shot ability,\nbut lack fine-grained emotional control due to their reliance on discrete\nspeech tokens. Existing approaches either limit emotions to categorical labels\nor cannot generalize to LLM-based architectures. We propose EMORL-TTS\n(Fine-grained Emotion-controllable TTS with Reinforcement Learning), a\nframework that unifies global intensity control in the VAD space with local\nemphasis regulation. Our method combines supervised fine-tuning with\nreinforcement learning guided by task-specific rewards for emotion category,\nintensity, and emphasis. Moreover, we further investigate how emphasis\nplacement modulates fine-grained emotion intensity. Experiments show that\nEMORL-TTS improves emotion accuracy, intensity differentiation, and emphasis\nclarity, while preserving synthesis quality comparable to strong LLM-based\nbaselines."
                },
                "authors": [
                    {
                        "name": "Haoxun Li"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Yuqing Sun"
                    },
                    {
                        "name": "Hanlei Shi"
                    },
                    {
                        "name": "Leyuan Qu"
                    },
                    {
                        "name": "Taihao Li"
                    }
                ],
                "author_detail": {
                    "name": "Taihao Li"
                },
                "author": "Taihao Li",
                "arxiv_comment": "Under review for ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00927v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00927v3",
                "updated": "2025-10-07T10:17:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    10,
                    17,
                    31,
                    1,
                    280,
                    0
                ],
                "published": "2024-09-30T16:57:34Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    57,
                    34,
                    0,
                    274,
                    0
                ],
                "title": "Text Clustering as Classification with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Clustering as Classification with LLMs"
                },
                "summary": "Text clustering serves as a fundamental technique for organizing and\ninterpreting unstructured textual data, particularly in contexts where manual\nannotation is prohibitively costly. With the rapid advancement of Large\nLanguage Models (LLMs) and their demonstrated effectiveness across a broad\nspectrum of NLP tasks, an emerging body of research has begun to explore their\npotential in the domain of text clustering. However, existing LLM-based\napproaches still rely on fine-tuned embedding models and sophisticated\nsimilarity metrics, rendering them computationally intensive and necessitating\ndomain-specific adaptation. To address these limitations, we propose a novel\nframework that reframes text clustering as a classification task by harnessing\nthe in-context learning capabilities of LLMs. Our framework eliminates the need\nfor fine-tuning embedding models or intricate clustering algorithms. It\ncomprises two key steps: first, the LLM is prompted to generate a set of\ncandidate labels based on the dataset and then merges semantically similar\nlabels; second, it assigns the most appropriate label to each text sample. By\nleveraging the advanced natural language understanding and generalization\ncapabilities of LLMs, the proposed approach enables effective clustering with\nminimal human intervention. Experimental results on diverse datasets\ndemonstrate that our framework achieves comparable or superior performance to\nstate-of-the-art embedding-based clustering techniques, while significantly\nreducing computational complexity and resource requirements. These findings\nunderscore the transformative potential of LLMs in simplifying and enhancing\ntext clustering tasks. We make our code available to the public for utilization\nat https://github.com/ECNU-Text-Computing/Text-Clustering-via-LLM. We also\nprovide the supplementary Appendix within the repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text clustering serves as a fundamental technique for organizing and\ninterpreting unstructured textual data, particularly in contexts where manual\nannotation is prohibitively costly. With the rapid advancement of Large\nLanguage Models (LLMs) and their demonstrated effectiveness across a broad\nspectrum of NLP tasks, an emerging body of research has begun to explore their\npotential in the domain of text clustering. However, existing LLM-based\napproaches still rely on fine-tuned embedding models and sophisticated\nsimilarity metrics, rendering them computationally intensive and necessitating\ndomain-specific adaptation. To address these limitations, we propose a novel\nframework that reframes text clustering as a classification task by harnessing\nthe in-context learning capabilities of LLMs. Our framework eliminates the need\nfor fine-tuning embedding models or intricate clustering algorithms. It\ncomprises two key steps: first, the LLM is prompted to generate a set of\ncandidate labels based on the dataset and then merges semantically similar\nlabels; second, it assigns the most appropriate label to each text sample. By\nleveraging the advanced natural language understanding and generalization\ncapabilities of LLMs, the proposed approach enables effective clustering with\nminimal human intervention. Experimental results on diverse datasets\ndemonstrate that our framework achieves comparable or superior performance to\nstate-of-the-art embedding-based clustering techniques, while significantly\nreducing computational complexity and resource requirements. These findings\nunderscore the transformative potential of LLMs in simplifying and enhancing\ntext clustering tasks. We make our code available to the public for utilization\nat https://github.com/ECNU-Text-Computing/Text-Clustering-via-LLM. We also\nprovide the supplementary Appendix within the repository."
                },
                "authors": [
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Guoxiu He"
                    }
                ],
                "author_detail": {
                    "name": "Guoxiu He"
                },
                "author": "Guoxiu He",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00927v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00927v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15295v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15295v4",
                "updated": "2025-10-07T10:09:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    10,
                    9,
                    59,
                    1,
                    280,
                    0
                ],
                "published": "2024-01-27T04:49:37Z",
                "published_parsed": [
                    2024,
                    1,
                    27,
                    4,
                    49,
                    37,
                    5,
                    27,
                    0
                ],
                "title": "Shortcuts Everywhere and Nowhere: Exploring Multi-Trigger Backdoor\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shortcuts Everywhere and Nowhere: Exploring Multi-Trigger Backdoor\n  Attacks"
                },
                "summary": "Backdoor attacks have become a significant threat to the pre-training and\ndeployment of deep neural networks (DNNs). Although numerous methods for\ndetecting and mitigating backdoor attacks have been proposed, most rely on\nidentifying and eliminating the ``shortcut\" created by the backdoor, which\nlinks a specific source class to a target class. However, these approaches can\nbe easily circumvented by designing multiple backdoor triggers that create\nshortcuts everywhere and therefore nowhere specific. In this study, we explore\nthe concept of Multi-Trigger Backdoor Attacks (MTBAs), where multiple\nadversaries leverage different types of triggers to poison the same dataset. By\nproposing and investigating three types of multi-trigger attacks including\n\\textit{parallel}, \\textit{sequential}, and \\textit{hybrid} attacks, we\ndemonstrate that 1) multiple triggers can coexist, overwrite, or cross-activate\none another, and 2) MTBAs easily break the prevalent shortcut assumption\nunderlying most existing backdoor detection/removal methods, rendering them\nineffective. Given the security risk posed by MTBAs, we have created a\nmulti-trigger backdoor poisoning dataset to facilitate future research on\ndetecting and mitigating these attacks, and we also discuss potential defense\nstrategies against MTBAs. Our code is available at\nhttps://github.com/bboylyg/Multi-Trigger-Backdoor-Attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backdoor attacks have become a significant threat to the pre-training and\ndeployment of deep neural networks (DNNs). Although numerous methods for\ndetecting and mitigating backdoor attacks have been proposed, most rely on\nidentifying and eliminating the ``shortcut\" created by the backdoor, which\nlinks a specific source class to a target class. However, these approaches can\nbe easily circumvented by designing multiple backdoor triggers that create\nshortcuts everywhere and therefore nowhere specific. In this study, we explore\nthe concept of Multi-Trigger Backdoor Attacks (MTBAs), where multiple\nadversaries leverage different types of triggers to poison the same dataset. By\nproposing and investigating three types of multi-trigger attacks including\n\\textit{parallel}, \\textit{sequential}, and \\textit{hybrid} attacks, we\ndemonstrate that 1) multiple triggers can coexist, overwrite, or cross-activate\none another, and 2) MTBAs easily break the prevalent shortcut assumption\nunderlying most existing backdoor detection/removal methods, rendering them\nineffective. Given the security risk posed by MTBAs, we have created a\nmulti-trigger backdoor poisoning dataset to facilitate future research on\ndetecting and mitigating these attacks, and we also discuss potential defense\nstrategies against MTBAs. Our code is available at\nhttps://github.com/bboylyg/Multi-Trigger-Backdoor-Attacks."
                },
                "authors": [
                    {
                        "name": "Yige Li"
                    },
                    {
                        "name": "Jiabo He"
                    },
                    {
                        "name": "Hanxun Huang"
                    },
                    {
                        "name": "Jun Sun"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_doi": "10.1109/TDSC.2025.3605597",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TDSC.2025.3605597",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.15295v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15295v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05748v1",
                "updated": "2025-10-07T10:06:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    10,
                    6,
                    29,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T10:06:29Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    10,
                    6,
                    29,
                    1,
                    280,
                    0
                ],
                "title": "Communication Enables Cooperation in LLM Agents: A Comparison with\n  Curriculum-Based Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication Enables Cooperation in LLM Agents: A Comparison with\n  Curriculum-Based Approaches"
                },
                "summary": "Eliciting cooperation in multi-agent LLM systems is critical for AI\nalignment. We investigate two approaches: direct communication and curriculum\nlearning. In a 4-player Stag Hunt, a one-word \"cheap talk\" channel increases\ncooperation from 0% to 48.3%, demonstrating communication as a robust\ncoordination mechanism. In contrast, we find that curriculum learning is highly\nsensitive to design choices: our pedagogical curriculum through progressively\ncomplex games reduced agent payoffs by 27.4% in an Iterated Public Goods Game\nwith Punishment. Qualitative analysis reveals that curricula emphasizing\ndefection-equilibrium games can induce \"learned pessimism\" in agents. These\nfindings suggest that for coordination problems, simple communication protocols\nmay be more reliable than experience-based training, and that curriculum design\nfor social dilemmas requires careful attention to the strategic lessons\nembedded in game sequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliciting cooperation in multi-agent LLM systems is critical for AI\nalignment. We investigate two approaches: direct communication and curriculum\nlearning. In a 4-player Stag Hunt, a one-word \"cheap talk\" channel increases\ncooperation from 0% to 48.3%, demonstrating communication as a robust\ncoordination mechanism. In contrast, we find that curriculum learning is highly\nsensitive to design choices: our pedagogical curriculum through progressively\ncomplex games reduced agent payoffs by 27.4% in an Iterated Public Goods Game\nwith Punishment. Qualitative analysis reveals that curricula emphasizing\ndefection-equilibrium games can induce \"learned pessimism\" in agents. These\nfindings suggest that for coordination problems, simple communication protocols\nmay be more reliable than experience-based training, and that curriculum design\nfor social dilemmas requires careful attention to the strategic lessons\nembedded in game sequences."
                },
                "authors": [
                    {
                        "name": "Hachem Madmoun"
                    },
                    {
                        "name": "Salem Lahlou"
                    }
                ],
                "author_detail": {
                    "name": "Salem Lahlou"
                },
                "author": "Salem Lahlou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]