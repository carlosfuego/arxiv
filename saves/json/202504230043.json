[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.15260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15260v1",
                "updated": "2025-04-21T17:39:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:39:59Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "title": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks"
                },
                "summary": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks."
                },
                "authors": [
                    {
                        "name": "Xuesong Liu"
                    },
                    {
                        "name": "Yansong Liu"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Fangzhou Zhao"
                    },
                    {
                        "name": "Le Xia"
                    },
                    {
                        "name": "Yao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yao Sun"
                },
                "author": "Yao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15247v1",
                "updated": "2025-04-21T17:22:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:22:18Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "title": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings"
                },
                "summary": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization."
                },
                "authors": [
                    {
                        "name": "Weston Pace"
                    },
                    {
                        "name": "Chang She"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Will Jones"
                    },
                    {
                        "name": "Albert Lockett"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Raunak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Raunak Shah"
                },
                "author": "Raunak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v3",
                "updated": "2025-04-21T15:36:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    36,
                    53,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v2",
                "updated": "2025-04-21T15:13:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    13,
                    44,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15021v1",
                "updated": "2025-04-21T11:09:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:09:43Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "title": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?"
                },
                "summary": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies."
                },
                "authors": [
                    {
                        "name": "Xinglei Dou"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Limin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Limin Xiao"
                },
                "author": "Limin Xiao",
                "arxiv_comment": "25 pages, 14 figures, to be published in ACM Transactions on Storage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14992v1",
                "updated": "2025-04-21T09:41:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T09:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Pretraining Length Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pretraining Length Scaling"
                },
                "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v1",
                "updated": "2025-04-21T05:27:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Jiajun Cheng"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "15 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v3",
                "updated": "2025-04-21T03:40:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    40,
                    10,
                    0,
                    111,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v1",
                "updated": "2025-04-21T00:07:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v2",
                "updated": "2025-04-20T21:50:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    21,
                    50,
                    3,
                    6,
                    110,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v3",
                "updated": "2025-04-20T19:57:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    19,
                    57,
                    16,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11208v2",
                "updated": "2025-04-20T07:53:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    7,
                    53,
                    9,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-15T14:11:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye"
                },
                "summary": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively."
                },
                "authors": [
                    {
                        "name": "Bradley Morgan"
                    },
                    {
                        "name": "Gal Horowitz"
                    },
                    {
                        "name": "Sioli O'Connell"
                    },
                    {
                        "name": "Stephan van Schaik"
                    },
                    {
                        "name": "Chitchanok Chuengsatiansup"
                    },
                    {
                        "name": "Daniel Genkin"
                    },
                    {
                        "name": "Olaf Maennel"
                    },
                    {
                        "name": "Paul Montague"
                    },
                    {
                        "name": "Eyal Ronen"
                    },
                    {
                        "name": "Yuval Yarom"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Yarom"
                },
                "author": "Yuval Yarom",
                "arxiv_comment": "Added reference to the ID3 decision tree induction algorithm by J. R.\n  Quinlan in Section 5.4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14489v1",
                "updated": "2025-04-20T04:46:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    4,
                    46,
                    34,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-20T04:46:34Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    4,
                    46,
                    34,
                    6,
                    110,
                    0
                ],
                "title": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing"
                },
                "summary": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Yoda, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Yoda leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Yoda introduces an adaptive gang scheduling mechanism,\na contention-free modeling method, and a SLO-aware dispatching policy.\nEvaluation shows that Yoda achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Yoda, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Yoda leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Yoda introduces an adaptive gang scheduling mechanism,\na contention-free modeling method, and a SLO-aware dispatching policy.\nEvaluation shows that Yoda achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads."
                },
                "authors": [
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14435v1",
                "updated": "2025-04-20T00:49:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-20T00:49:27Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "title": "Deuteronomy 2.0: Record Caching and Latch Freedom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deuteronomy 2.0: Record Caching and Latch Freedom"
                },
                "summary": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance."
                },
                "authors": [
                    {
                        "name": "David Lomet"
                    }
                ],
                "author_detail": {
                    "name": "David Lomet"
                },
                "author": "David Lomet",
                "arxiv_comment": "6 pages, 5 figures, potential CIDR submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v1",
                "updated": "2025-04-19T18:25:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lübke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) submitted\n  to \"25th International Conference on Computational Science\" (ICCS25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14196v1",
                "updated": "2025-04-19T06:18:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T06:18:56Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "title": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser"
                },
                "summary": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine."
                },
                "authors": [
                    {
                        "name": "Deyin Kong"
                    },
                    {
                        "name": "Yichen Su"
                    },
                    {
                        "name": "Cheng Song"
                    },
                    {
                        "name": "Xiaojun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wu"
                },
                "author": "Xiaojun Wu",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v5",
                "updated": "2025-04-19T05:57:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    5,
                    57,
                    44,
                    5,
                    109,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v1",
                "updated": "2025-04-18T22:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v1",
                "updated": "2025-04-18T19:46:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Caching through Attention Output Error based Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Caching through Attention Output Error based Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v1",
                "updated": "2025-04-18T13:46:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13981v1",
                "updated": "2025-04-18T06:34:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T06:34:57Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "title": "CacheFormer: High Attention-Based Segment Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFormer: High Attention-Based Segment Caching"
                },
                "summary": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes."
                },
                "authors": [
                    {
                        "name": "Sushant Singh"
                    },
                    {
                        "name": "Ausif Mahmood"
                    }
                ],
                "author_detail": {
                    "name": "Ausif Mahmood"
                },
                "author": "Ausif Mahmood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v2",
                "updated": "2025-04-18T05:13:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    13,
                    52,
                    4,
                    108,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "32 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13385v1",
                "updated": "2025-04-18T00:21:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T00:21:00Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "title": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks"
                },
                "summary": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments."
                },
                "authors": [
                    {
                        "name": "Tianhong Xu"
                    },
                    {
                        "name": "Aidong Adam Ding"
                    },
                    {
                        "name": "Yunsi Fei"
                    }
                ],
                "author_detail": {
                    "name": "Yunsi Fei"
                },
                "author": "Yunsi Fei",
                "arxiv_comment": "Accepted to ACM ASIA CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v2",
                "updated": "2025-04-17T23:45:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    23,
                    45,
                    51,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "12 pages, 7 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v2",
                "updated": "2025-04-17T21:19:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    21,
                    19,
                    19,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19325v2",
                "updated": "2025-04-17T15:26:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    26,
                    4,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-25T03:38:06Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    38,
                    6,
                    1,
                    84,
                    0
                ],
                "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction"
                },
                "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR."
                },
                "authors": [
                    {
                        "name": "Yuchao Gu"
                    },
                    {
                        "name": "Weijia Mao"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "Project page at https://farlongctx.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v3",
                "updated": "2025-04-17T03:51:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    51,
                    6,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v2",
                "updated": "2025-04-17T00:38:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    0,
                    38,
                    24,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "arxiv_comment": "The modified version of this preprint has been submitted to ESORICS\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12526v1",
                "updated": "2025-04-16T23:15:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T23:15:09Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models"
                },
                "summary": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Tianyi Zhu"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "arxiv_comment": "Submitted to COLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v1",
                "updated": "2025-04-16T18:03:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v1",
                "updated": "2025-04-16T16:45:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11816v1",
                "updated": "2025-04-16T07:02:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:02:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading"
                },
                "summary": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads."
                },
                "authors": [
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Hyunsun Chung"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "author": "Youngjae Kim",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08334v3",
                "updated": "2025-04-16T05:57:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    57,
                    8,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-11T07:59:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    59,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "Efficient Architecture for RISC-V Vector Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Architecture for RISC-V Vector Memory Access"
                },
                "summary": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors."
                },
                "authors": [
                    {
                        "name": "Hongyi Guan"
                    },
                    {
                        "name": "Yichuan Gao"
                    },
                    {
                        "name": "Chenlu Miao"
                    },
                    {
                        "name": "Haoyang Wu"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Mingfeng Lin"
                    },
                    {
                        "name": "Huayue Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huayue Liang"
                },
                "author": "Huayue Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11765v1",
                "updated": "2025-04-16T04:59:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T04:59:18Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "title": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs"
                },
                "summary": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration."
                },
                "authors": [
                    {
                        "name": "Hyungwoo Lee"
                    },
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Jungmin So"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "James J. Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "arxiv_affiliation": "Dept. of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea",
                "author": "Youngjae Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11729v1",
                "updated": "2025-04-16T03:07:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T03:07:07Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "title": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks"
                },
                "summary": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments."
                },
                "authors": [
                    {
                        "name": "Jiahong Ning"
                    },
                    {
                        "name": "Pengyan Zhu"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Gary Lee"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11652v1",
                "updated": "2025-04-15T22:38:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T22:38:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues"
                },
                "summary": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures."
                },
                "authors": [
                    {
                        "name": "Marvin Williams"
                    },
                    {
                        "name": "Peter Sanders"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sanders"
                },
                "author": "Peter Sanders",
                "arxiv_comment": "40 pages, extended journal version of arXiv:2107.01350",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11320v1",
                "updated": "2025-04-15T16:00:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints"
                },
                "summary": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints."
                },
                "authors": [
                    {
                        "name": "Ruicheng Ao"
                    },
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang",
                "arxiv_comment": "42 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v3",
                "updated": "2025-04-15T15:40:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    40,
                    25,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13195v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13195v5",
                "updated": "2025-04-15T15:37:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    37,
                    58,
                    1,
                    105,
                    0
                ],
                "published": "2024-04-19T22:06:14Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    22,
                    6,
                    14,
                    4,
                    110,
                    0
                ],
                "title": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper"
                },
                "summary": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "arxiv_doi": "10.1145/3626203.3670561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626203.3670561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.13195v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13195v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11067v1",
                "updated": "2025-04-15T11:02:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:02:34Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "title": "Morphing-based Compression for Data-centric ML Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphing-based Compression for Data-centric ML Pipelines"
                },
                "summary": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours."
                },
                "authors": [
                    {
                        "name": "Sebastian Baunsgaard"
                    },
                    {
                        "name": "Matthias Boehm"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Boehm"
                },
                "author": "Matthias Boehm",
                "arxiv_comment": "20 pages, 28 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10326v1",
                "updated": "2025-04-14T15:34:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:34:26Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference"
                },
                "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks."
                },
                "authors": [
                    {
                        "name": "Yangshen Deng"
                    },
                    {
                        "name": "Zhengxin You"
                    },
                    {
                        "name": "Long Xiang"
                    },
                    {
                        "name": "Qilong Li"
                    },
                    {
                        "name": "Peiqi Yuan"
                    },
                    {
                        "name": "Zhaoyang Hong"
                    },
                    {
                        "name": "Yitao Zheng"
                    },
                    {
                        "name": "Wanting Li"
                    },
                    {
                        "name": "Runzhong Li"
                    },
                    {
                        "name": "Haotian Liu"
                    },
                    {
                        "name": "Kyriakos Mouratidis"
                    },
                    {
                        "name": "Man Lung Yiu"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Bo Tang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Tang"
                },
                "author": "Bo Tang",
                "arxiv_comment": "14 pages, 12 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; H.3.2; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10318v1",
                "updated": "2025-04-14T15:27:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:27:32Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "title": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation"
                },
                "summary": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead."
                },
                "authors": [
                    {
                        "name": "Kartik Ramkrishnan"
                    },
                    {
                        "name": "Antonia Zhai"
                    },
                    {
                        "name": "Stephen McCamant"
                    },
                    {
                        "name": "Pen Chung Yew"
                    }
                ],
                "author_detail": {
                    "name": "Pen Chung Yew"
                },
                "author": "Pen Chung Yew",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10181v1",
                "updated": "2025-04-14T12:34:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:34:20Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "title": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis"
                },
                "summary": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes."
                },
                "authors": [
                    {
                        "name": "Zahid Javid"
                    },
                    {
                        "name": "Firdous Ul Nazir"
                    },
                    {
                        "name": "Wentao Zhu"
                    },
                    {
                        "name": "Diptargha Chakravorty"
                    },
                    {
                        "name": "Ahmed Aboushady"
                    },
                    {
                        "name": "Mohamed Galeela"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Galeela"
                },
                "author": "Mohamed Galeela",
                "arxiv_comment": "12 Pages, First Revision Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v3",
                "updated": "2025-04-14T11:20:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    20,
                    56,
                    0,
                    104,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09984v1",
                "updated": "2025-04-14T08:51:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T08:51:35Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "title": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures"
                },
                "summary": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines."
                },
                "authors": [
                    {
                        "name": "Sean MacAvaney"
                    },
                    {
                        "name": "Craig Macdonald"
                    }
                ],
                "author_detail": {
                    "name": "Craig Macdonald"
                },
                "author": "Craig Macdonald",
                "arxiv_comment": "WOWS @ ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09952v1",
                "updated": "2025-04-14T07:30:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T07:30:03Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "title": "Secrecy and Privacy in Multi-Access Combinatorial Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secrecy and Privacy in Multi-Access Combinatorial Topology"
                },
                "summary": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09936v1",
                "updated": "2025-04-14T06:58:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T06:58:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "title": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference"
                },
                "summary": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Yebo Peng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Zhiming Wang"
                    },
                    {
                        "name": "Bairen Yi"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12280v2",
                "updated": "2025-04-13T14:17:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    17,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2024-02-19T16:47:04Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    16,
                    47,
                    4,
                    0,
                    50,
                    0
                ],
                "title": "Plato: Plan to Efficiently Decode for Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plato: Plan to Efficiently Decode for Large Language Model Inference"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Haizhong Zheng"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Atul Prakash"
                    },
                    {
                        "name": "Matthew Lentz"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Feng Qian"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09590v1",
                "updated": "2025-04-13T14:16:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T14:16:57Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "title": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests"
                },
                "summary": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI."
                },
                "authors": [
                    {
                        "name": "Wan Borui"
                    },
                    {
                        "name": "Zhao Juntao"
                    },
                    {
                        "name": "Jiang Chenyu"
                    },
                    {
                        "name": "Guo Chuanxiong"
                    },
                    {
                        "name": "Wu Chuan"
                    }
                ],
                "author_detail": {
                    "name": "Wu Chuan"
                },
                "author": "Wu Chuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v5",
                "updated": "2025-04-13T14:02:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    2,
                    47,
                    6,
                    103,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient Prefilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient Prefilling"
                },
                "summary": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section."
                },
                "authors": [
                    {
                        "name": "Dongyang Ma"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10540v1",
                "updated": "2025-04-13T08:29:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T08:29:58Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "title": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse"
                },
                "summary": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality."
                },
                "authors": [
                    {
                        "name": "Zichao Yu"
                    },
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Guojiang Shao"
                    },
                    {
                        "name": "Chengwei Zhang"
                    },
                    {
                        "name": "Shengze Xu"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Xiaodong Cun"
                    },
                    {
                        "name": "Wenyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenyi Zhang"
                },
                "author": "Wenyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09431v1",
                "updated": "2025-04-13T04:46:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T04:46:02Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "title": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets"
                },
                "summary": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications."
                },
                "authors": [
                    {
                        "name": "Hanying Zhang"
                    },
                    {
                        "name": "Ziqian Cui"
                    },
                    {
                        "name": "Baiqing Jiang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "C. Bi"
                    }
                ],
                "author_detail": {
                    "name": "C. Bi"
                },
                "author": "C. Bi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09261v1",
                "updated": "2025-04-12T15:42:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "published": "2025-04-12T15:42:17Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "title": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling"
                },
                "summary": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Youru Lv"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Danping Zou"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v3",
                "updated": "2025-04-11T12:31:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    31,
                    7,
                    4,
                    101,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v1",
                "updated": "2025-04-11T09:26:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08204v1",
                "updated": "2025-04-11T02:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T02:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "title": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping"
                },
                "summary": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\"."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Yina Jian"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Yongxin Ma"
                    },
                    {
                        "name": "Xinglai Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xinglai Jin"
                },
                "author": "Xinglai Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07596v2",
                "updated": "2025-04-11T02:05:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    5,
                    1,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T09:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution"
                },
                "summary": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward."
                },
                "authors": [
                    {
                        "name": "Zen Kit Heng"
                    },
                    {
                        "name": "Zimeng Zhao"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Mingdong Wu"
                    },
                    {
                        "name": "Yangang Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13915v1",
                "updated": "2025-04-10T17:13:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    13,
                    8,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T17:13:08Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    13,
                    8,
                    3,
                    100,
                    0
                ],
                "title": "Memory-efficient Streaming VideoLLMs for Real-time Procedural Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-efficient Streaming VideoLLMs for Real-time Procedural Video\n  Understanding"
                },
                "summary": "We introduce ProVideLLM, an end-to-end framework for real-time procedural\nvideo understanding. ProVideLLM integrates a multimodal cache configured to\nstore two types of tokens - verbalized text tokens, which provide compressed\ntextual summaries of long-term observations, and visual tokens, encoded with\nDETR-QFormer to capture fine-grained details from short-term observations. This\ndesign reduces token count by 22x over existing methods in representing one\nhour of long-term observations while effectively encoding fine-granularity of\nthe present. By interleaving these tokens in our multimodal cache, ProVideLLM\nensures sub-linear scaling of memory and compute with video length, enabling\nper-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with\na minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art\nresults on six procedural tasks across four datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ProVideLLM, an end-to-end framework for real-time procedural\nvideo understanding. ProVideLLM integrates a multimodal cache configured to\nstore two types of tokens - verbalized text tokens, which provide compressed\ntextual summaries of long-term observations, and visual tokens, encoded with\nDETR-QFormer to capture fine-grained details from short-term observations. This\ndesign reduces token count by 22x over existing methods in representing one\nhour of long-term observations while effectively encoding fine-granularity of\nthe present. By interleaving these tokens in our multimodal cache, ProVideLLM\nensures sub-linear scaling of memory and compute with video length, enabling\nper-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with\na minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art\nresults on six procedural tasks across four datasets."
                },
                "authors": [
                    {
                        "name": "Dibyadip Chatterjee"
                    },
                    {
                        "name": "Edoardo Remelli"
                    },
                    {
                        "name": "Yale Song"
                    },
                    {
                        "name": "Bugra Tekin"
                    },
                    {
                        "name": "Abhay Mittal"
                    },
                    {
                        "name": "Bharat Bhatnagar"
                    },
                    {
                        "name": "Necati Cihan Camgöz"
                    },
                    {
                        "name": "Shreyas Hampali"
                    },
                    {
                        "name": "Eric Sauser"
                    },
                    {
                        "name": "Shugao Ma"
                    },
                    {
                        "name": "Angela Yao"
                    },
                    {
                        "name": "Fadime Sener"
                    }
                ],
                "author_detail": {
                    "name": "Fadime Sener"
                },
                "author": "Fadime Sener",
                "arxiv_comment": "13 pages, 5 figures; https://dibschat.github.io/ProVideLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07815v1",
                "updated": "2025-04-10T14:52:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:52:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis"
                },
                "summary": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes."
                },
                "authors": [
                    {
                        "name": "Georgeta Bordea"
                    },
                    {
                        "name": "Stephane Campinas"
                    },
                    {
                        "name": "Matteo Catena"
                    },
                    {
                        "name": "Renaud Delbru"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Delbru"
                },
                "author": "Renaud Delbru",
                "arxiv_comment": "36 pages, 16 figures, submitted to the ComSIS journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11; E.1; H.2.4; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07642v1",
                "updated": "2025-04-10T10:43:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:43:42Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "title": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis"
                },
                "summary": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis."
                },
                "authors": [
                    {
                        "name": "Rustam Sadykov"
                    },
                    {
                        "name": "Azat Abdullin"
                    },
                    {
                        "name": "Marat Akhin"
                    }
                ],
                "author_detail": {
                    "name": "Marat Akhin"
                },
                "author": "Marat Akhin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07494v1",
                "updated": "2025-04-10T06:51:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:51:23Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving"
                },
                "summary": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems."
                },
                "authors": [
                    {
                        "name": "Shihong Gao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yanyan Shen"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_doi": "10.1145/3725394",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725394",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07479v1",
                "updated": "2025-04-10T06:13:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:13:30Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "title": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference"
                },
                "summary": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Weikai Xu"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Qianqian Huang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v3",
                "updated": "2025-04-10T05:06:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    6,
                    29,
                    3,
                    100,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "arxiv_comment": "MLSys 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v2",
                "updated": "2025-04-09T21:47:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    21,
                    47,
                    31,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "arxiv_comment": "Accepted to ICS25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v3",
                "updated": "2025-04-09T20:51:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    20,
                    51,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v2",
                "updated": "2025-04-09T17:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    56,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04514v2",
                "updated": "2025-04-09T14:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    36,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-06T15:15:07Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    15,
                    15,
                    7,
                    6,
                    96,
                    0
                ],
                "title": "Saliency-driven Dynamic Token Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saliency-driven Dynamic Token Pruning for Large Language Models"
                },
                "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression."
                },
                "authors": [
                    {
                        "name": "Yao Tao"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06813v1",
                "updated": "2025-04-09T12:07:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T12:07:26Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "title": "Introducing the Arm-membench Throughput Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing the Arm-membench Throughput Benchmark"
                },
                "summary": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA."
                },
                "authors": [
                    {
                        "name": "Cyrill Burth"
                    },
                    {
                        "name": "Markus Velten"
                    },
                    {
                        "name": "Robert Schöne"
                    }
                ],
                "author_detail": {
                    "name": "Robert Schöne"
                },
                "author": "Robert Schöne",
                "arxiv_doi": "10.1007/978-3-031-85697-6_7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-85697-6_7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.06813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 6 figures, published in Parallel Processing and Applied\n  Mathematics (PPAM 2024), see https://doi.org/10.1007/978-3-031-85697-6_7",
                "arxiv_journal_ref": "Parallel Processing and Applied Mathematics. PPAM 2024. Lecture\n  Notes in Computer Science, vol 15579. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05821v2",
                "updated": "2025-04-09T10:23:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    23,
                    39,
                    2,
                    99,
                    0
                ],
                "published": "2024-03-09T07:01:44Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    7,
                    1,
                    44,
                    5,
                    69,
                    0
                ],
                "title": "Optimizing LLM Queries in Relational Data Analytics Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Queries in Relational Data Analytics Workloads"
                },
                "summary": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models."
                },
                "authors": [
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Asim Biswal"
                    },
                    {
                        "name": "Amog Kamsetty"
                    },
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Liana Patel"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Matei Zaharia"
                    }
                ],
                "author_detail": {
                    "name": "Matei Zaharia"
                },
                "author": "Matei Zaharia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05591v3",
                "updated": "2025-04-09T09:09:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    9,
                    37,
                    2,
                    99,
                    0
                ],
                "published": "2024-09-09T13:20:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    20,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation"
                },
                "summary": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Tiejun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Huang"
                },
                "author": "Tiejun Huang",
                "arxiv_comment": "theWebConf 2025. Codes and models are in\n  https://github.com/qhjqhj00/MemoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v5",
                "updated": "2025-04-09T07:55:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    55,
                    43,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v5",
                "updated": "2025-04-09T03:49:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    49,
                    16,
                    2,
                    99,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06419v1",
                "updated": "2025-04-08T20:39:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:39:20Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "title": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding"
                },
                "summary": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests."
                },
                "authors": [
                    {
                        "name": "Sanjit Neelam"
                    },
                    {
                        "name": "Daniel Heinlein"
                    },
                    {
                        "name": "Vaclav Cvicek"
                    },
                    {
                        "name": "Akshay Mishra"
                    },
                    {
                        "name": "Reiner Pope"
                    }
                ],
                "author_detail": {
                    "name": "Reiner Pope"
                },
                "author": "Reiner Pope",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06416v1",
                "updated": "2025-04-08T20:32:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Autoregressive and Diffusion-Based Sequence Generation"
                },
                "summary": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation."
                },
                "authors": [
                    {
                        "name": "Nima Fathi"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Pierre-André Noël"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-André Noël"
                },
                "author": "Pierre-André Noël",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17692v2",
                "updated": "2025-04-08T19:26:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    19,
                    26,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2024-04-26T20:44:36Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    20,
                    44,
                    36,
                    4,
                    117,
                    0
                ],
                "title": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation"
                },
                "summary": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity."
                },
                "authors": [
                    {
                        "name": "Michael Czekanski"
                    },
                    {
                        "name": "Benjamin Faber"
                    },
                    {
                        "name": "Margaret Fairborn"
                    },
                    {
                        "name": "Adelle Wright"
                    },
                    {
                        "name": "David Bindel"
                    }
                ],
                "author_detail": {
                    "name": "David Bindel"
                },
                "author": "David Bindel",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06067v1",
                "updated": "2025-04-08T14:09:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T14:09:23Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "title": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III"
                },
                "summary": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo"
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Zhenyu Liang"
                    },
                    {
                        "name": "Ran Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ran Cheng"
                },
                "author": "Ran Cheng",
                "arxiv_comment": "Accepted by IEEE CEC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v2",
                "updated": "2025-04-08T14:05:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    5,
                    12,
                    1,
                    98,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v2",
                "updated": "2025-04-08T12:46:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    46,
                    45,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05897v1",
                "updated": "2025-04-08T10:47:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T10:47:37Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "title": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference"
                },
                "summary": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yanfan Sun"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by DAC 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06319v1",
                "updated": "2025-04-08T09:17:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:17:35Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "title": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching"
                },
                "summary": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines."
                },
                "authors": [
                    {
                        "name": "Yanhao Dong"
                    },
                    {
                        "name": "Yubo Miao"
                    },
                    {
                        "name": "Weinan Li"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Feng Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Lyu"
                },
                "author": "Feng Lyu",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05807v1",
                "updated": "2025-04-08T08:40:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:40:36Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "title": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks"
                },
                "summary": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes."
                },
                "authors": [
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Shengtian Yang"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Anding Wang"
                    }
                ],
                "author_detail": {
                    "name": "Anding Wang"
                },
                "author": "Anding Wang",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05718v1",
                "updated": "2025-04-08T06:38:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T06:38:27Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "title": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor"
                },
                "summary": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core."
                },
                "authors": [
                    {
                        "name": "Christopher Reinwardt"
                    },
                    {
                        "name": "Robert Balas"
                    },
                    {
                        "name": "Alessandro Ottaviano"
                    },
                    {
                        "name": "Angelo Garofalo"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "8 pages, 7 figures, accepted at the 22nd ACM International Conference\n  on Computing Frontiers 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v2",
                "updated": "2025-04-08T05:27:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    27,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03661v2",
                "updated": "2025-04-08T04:34:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    4,
                    34,
                    44,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-12T13:32:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    32,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization"
                },
                "summary": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION."
                },
                "authors": [
                    {
                        "name": "Zongwu Wang"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Fangxin Liu"
                    },
                    {
                        "name": "Yiwei Hu"
                    },
                    {
                        "name": "Qingxiao Sun"
                    },
                    {
                        "name": "Gezi Li"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Haibing Guan"
                    }
                ],
                "author_detail": {
                    "name": "Haibing Guan"
                },
                "author": "Haibing Guan",
                "arxiv_comment": "7 pages, 7 figures and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05646v1",
                "updated": "2025-04-08T03:48:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T03:48:43Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "title": "Lattice: Learning to Efficiently Compress the Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice: Learning to Efficiently Compress the Memory"
                },
                "summary": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases."
                },
                "authors": [
                    {
                        "name": "Mahdi Karami"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02533v3",
                "updated": "2025-04-07T22:48:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    22,
                    48,
                    33,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-03T12:36:01Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    36,
                    1,
                    3,
                    93,
                    0
                ],
                "title": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions"
                },
                "summary": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead."
                },
                "authors": [
                    {
                        "name": "Vincenzo Petrolo"
                    },
                    {
                        "name": "Flavia Guella"
                    },
                    {
                        "name": "Michele Caon"
                    },
                    {
                        "name": "Pasquale Davide Schiavone"
                    },
                    {
                        "name": "Guido Masera"
                    },
                    {
                        "name": "Maurizio Martina"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Martina"
                },
                "author": "Maurizio Martina",
                "arxiv_comment": "6 pages, 4 figures, accepted at the Design Automation Conference\n  (DAC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v2",
                "updated": "2025-04-07T20:52:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    20,
                    52,
                    4,
                    0,
                    97,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05097v1",
                "updated": "2025-04-07T14:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "title": "State Tuning: State-based Test-Time Scaling on RWKV-7",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Tuning: State-based Test-Time Scaling on RWKV-7"
                },
                "summary": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v1",
                "updated": "2025-04-07T08:22:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00414v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00414v3",
                "updated": "2025-04-07T06:27:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    27,
                    48,
                    0,
                    97,
                    0
                ],
                "published": "2024-10-01T05:46:22Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    46,
                    22,
                    1,
                    275,
                    0
                ],
                "title": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering"
                },
                "summary": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments."
                },
                "authors": [
                    {
                        "name": "Daehwan Nam"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00414v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00414v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v1",
                "updated": "2025-04-07T03:22:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v2",
                "updated": "2025-04-07T01:35:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    1,
                    35,
                    39,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v2",
                "updated": "2025-04-06T12:20:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    6,
                    12,
                    20,
                    25,
                    6,
                    96,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04005v1",
                "updated": "2025-04-05T00:59:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "published": "2025-04-05T00:59:52Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "title": "Learning Cache Coherence Traffic for NoC Routing Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Cache Coherence Traffic for NoC Routing Design"
                },
                "summary": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_comment": "7 pages, 14 figures. Preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03632v1",
                "updated": "2025-04-04T17:56:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:56:44Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "title": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs"
                },
                "summary": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems."
                },
                "authors": [
                    {
                        "name": "Huda Ibeid"
                    },
                    {
                        "name": "Vikram Narayana"
                    },
                    {
                        "name": "Jeongnim Kim"
                    },
                    {
                        "name": "Anthony Nguyen"
                    },
                    {
                        "name": "Vitali Morozov"
                    },
                    {
                        "name": "Ye Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ye Luo"
                },
                "author": "Ye Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v2",
                "updated": "2025-04-04T16:51:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    51,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.02073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.02073v2",
                "updated": "2025-04-04T15:30:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    30,
                    20,
                    4,
                    94,
                    0
                ],
                "published": "2023-07-05T07:30:53Z",
                "published_parsed": [
                    2023,
                    7,
                    5,
                    7,
                    30,
                    53,
                    2,
                    186,
                    0
                ],
                "title": "Performance Modeling of Data Storage Systems using Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Modeling of Data Storage Systems using Generative Models"
                },
                "summary": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning."
                },
                "authors": [
                    {
                        "name": "Abdalaziz Rashid Al-Maeeni"
                    },
                    {
                        "name": "Aziz Temirkhanov"
                    },
                    {
                        "name": "Artem Ryzhikov"
                    },
                    {
                        "name": "Mikhail Hushchyn"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Hushchyn"
                },
                "author": "Mikhail Hushchyn",
                "arxiv_doi": "10.1109/ACCESS.2025.3552409",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3552409",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.02073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.02073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Access 2025 ( Volume: 13) 49643 - 49658",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03499v1",
                "updated": "2025-04-04T14:55:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T14:55:27Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "title": "Optimistic Learning for Communication Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimistic Learning for Communication Networks"
                },
                "summary": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach."
                },
                "authors": [
                    {
                        "name": "George Iosifidis"
                    },
                    {
                        "name": "Naram Mhaisen"
                    },
                    {
                        "name": "Douglas J. Leith"
                    }
                ],
                "author_detail": {
                    "name": "Douglas J. Leith"
                },
                "author": "Douglas J. Leith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v3",
                "updated": "2025-04-04T13:27:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    27,
                    49,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v1",
                "updated": "2025-04-04T03:30:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.15275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15275v1",
                "updated": "2025-04-21T17:59:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    59,
                    2,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:59:02Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    59,
                    2,
                    0,
                    111,
                    0
                ],
                "title": "Stop Summation: Min-Form Credit Assignment Is All Process Reward Model\n  Needs for Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stop Summation: Min-Form Credit Assignment Is All Process Reward Model\n  Needs for Reasoning"
                },
                "summary": "Process reward models (PRMs) have proven effective for test-time scaling of\nLarge Language Models (LLMs) on challenging reasoning tasks. However, reward\nhacking issues with PRMs limit their successful application in reinforcement\nfine-tuning. In this paper, we identify the main cause of PRM-induced reward\nhacking: the canonical summation-form credit assignment in reinforcement\nlearning (RL), which defines the value as cumulative gamma-decayed future\nrewards, easily induces LLMs to hack steps with high rewards. To address this,\nwe propose PURE: Process sUpervised Reinforcement lEarning. The key innovation\nof PURE is a min-form credit assignment that formulates the value function as\nthe minimum of future rewards. This method significantly alleviates reward\nhacking by limiting the value function range and distributing advantages more\nreasonably. Through extensive experiments on 3 base models, we show that\nPRM-based approaches enabling min-form credit assignment achieve comparable\nreasoning performance to verifiable reward-based methods within only 30% steps.\nIn contrast, the canonical sum-form credit assignment collapses training even\nat the beginning! Additionally, when we supplement PRM-based fine-tuning with\njust 10% verifiable rewards, we further alleviate reward hacking and produce\nthe best fine-tuned model based on Qwen2.5-Math-7B in our experiments,\nachieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5\nbenchmarks. Moreover, we summarize the observed reward hacking cases and\nanalyze the causes of training collapse. Code and models are available at\nhttps://github.com/CJReinforce/PURE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process reward models (PRMs) have proven effective for test-time scaling of\nLarge Language Models (LLMs) on challenging reasoning tasks. However, reward\nhacking issues with PRMs limit their successful application in reinforcement\nfine-tuning. In this paper, we identify the main cause of PRM-induced reward\nhacking: the canonical summation-form credit assignment in reinforcement\nlearning (RL), which defines the value as cumulative gamma-decayed future\nrewards, easily induces LLMs to hack steps with high rewards. To address this,\nwe propose PURE: Process sUpervised Reinforcement lEarning. The key innovation\nof PURE is a min-form credit assignment that formulates the value function as\nthe minimum of future rewards. This method significantly alleviates reward\nhacking by limiting the value function range and distributing advantages more\nreasonably. Through extensive experiments on 3 base models, we show that\nPRM-based approaches enabling min-form credit assignment achieve comparable\nreasoning performance to verifiable reward-based methods within only 30% steps.\nIn contrast, the canonical sum-form credit assignment collapses training even\nat the beginning! Additionally, when we supplement PRM-based fine-tuning with\njust 10% verifiable rewards, we further alleviate reward hacking and produce\nthe best fine-tuned model based on Qwen2.5-Math-7B in our experiments,\nachieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5\nbenchmarks. Moreover, we summarize the observed reward hacking cases and\nanalyze the causes of training collapse. Code and models are available at\nhttps://github.com/CJReinforce/PURE."
                },
                "authors": [
                    {
                        "name": "Jie Cheng"
                    },
                    {
                        "name": "Ruixi Qiao"
                    },
                    {
                        "name": "Lijun Li"
                    },
                    {
                        "name": "Chao Guo"
                    },
                    {
                        "name": "Junle Wang"
                    },
                    {
                        "name": "Gang Xiong"
                    },
                    {
                        "name": "Yisheng Lv"
                    },
                    {
                        "name": "Fei-Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fei-Yue Wang"
                },
                "author": "Fei-Yue Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13263v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13263v2",
                "updated": "2025-04-21T17:58:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    58,
                    8,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-17T18:05:39Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    18,
                    5,
                    39,
                    3,
                    107,
                    0
                ],
                "title": "Causal-Copilot: An Autonomous Causal Analysis Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal-Copilot: An Autonomous Causal Analysis Agent"
                },
                "summary": "Causal analysis plays a foundational role in scientific discovery and\nreliable decision-making, yet it remains largely inaccessible to domain experts\ndue to its conceptual and algorithmic complexity. This disconnect between\ncausal methodology and practical usability presents a dual challenge: domain\nexperts are unable to leverage recent advances in causal learning, while causal\nresearchers lack broad, real-world deployment to test and refine their methods.\nTo address this, we introduce Causal-Copilot, an autonomous agent that\noperationalizes expert-level causal analysis within a large language model\nframework. Causal-Copilot automates the full pipeline of causal analysis for\nboth tabular and time-series data -- including causal discovery, causal\ninference, algorithm selection, hyperparameter optimization, result\ninterpretation, and generation of actionable insights. It supports interactive\nrefinement through natural language, lowering the barrier for non-specialists\nwhile preserving methodological rigor. By integrating over 20 state-of-the-art\ncausal analysis techniques, our system fosters a virtuous cycle -- expanding\naccess to advanced causal methods for domain experts while generating rich,\nreal-world applications that inform and advance causal theory. Empirical\nevaluations demonstrate that Causal-Copilot achieves superior performance\ncompared to existing baselines, offering a reliable, scalable, and extensible\nsolution that bridges the gap between theoretical sophistication and real-world\napplicability in causal analysis. A live interactive demo of Causal-Copilot is\navailable at https://causalcopilot.com/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal analysis plays a foundational role in scientific discovery and\nreliable decision-making, yet it remains largely inaccessible to domain experts\ndue to its conceptual and algorithmic complexity. This disconnect between\ncausal methodology and practical usability presents a dual challenge: domain\nexperts are unable to leverage recent advances in causal learning, while causal\nresearchers lack broad, real-world deployment to test and refine their methods.\nTo address this, we introduce Causal-Copilot, an autonomous agent that\noperationalizes expert-level causal analysis within a large language model\nframework. Causal-Copilot automates the full pipeline of causal analysis for\nboth tabular and time-series data -- including causal discovery, causal\ninference, algorithm selection, hyperparameter optimization, result\ninterpretation, and generation of actionable insights. It supports interactive\nrefinement through natural language, lowering the barrier for non-specialists\nwhile preserving methodological rigor. By integrating over 20 state-of-the-art\ncausal analysis techniques, our system fosters a virtuous cycle -- expanding\naccess to advanced causal methods for domain experts while generating rich,\nreal-world applications that inform and advance causal theory. Empirical\nevaluations demonstrate that Causal-Copilot achieves superior performance\ncompared to existing baselines, offering a reliable, scalable, and extensible\nsolution that bridges the gap between theoretical sophistication and real-world\napplicability in causal analysis. A live interactive demo of Causal-Copilot is\navailable at https://causalcopilot.com/."
                },
                "authors": [
                    {
                        "name": "Xinyue Wang"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Wenyi Wu"
                    },
                    {
                        "name": "Har Simrat Singh"
                    },
                    {
                        "name": "Fang Nan"
                    },
                    {
                        "name": "Songyao Jin"
                    },
                    {
                        "name": "Aryan Philip"
                    },
                    {
                        "name": "Saloni Patnaik"
                    },
                    {
                        "name": "Hou Zhu"
                    },
                    {
                        "name": "Shivam Singh"
                    },
                    {
                        "name": "Parjanya Prashant"
                    },
                    {
                        "name": "Qian Shen"
                    },
                    {
                        "name": "Biwei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Biwei Huang"
                },
                "author": "Biwei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13263v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13263v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15268v1",
                "updated": "2025-04-21T17:52:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    52,
                    36,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:52:36Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    52,
                    36,
                    0,
                    111,
                    0
                ],
                "title": "Beating the Correlation Breakdown: Robust Inference, Flexible Scenarios,\n  and Stress Testing for Financial Portfolios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beating the Correlation Breakdown: Robust Inference, Flexible Scenarios,\n  and Stress Testing for Financial Portfolios"
                },
                "summary": "We live in a multivariate world, and effective modeling of financial\nportfolios, including their construction, allocation, forecasting, and risk\nanalysis, simply is not possible without explicitly modeling the dependence\nstructure of their assets. Dependence structure can drive portfolio results\nmore than many other parameters in investment and risk models, sometimes even\nmore than their combined effects, but the literature provides relatively little\nto define the finite-sample distributions of dependence measures in useable and\nuseful ways under challenging, real-world financial data conditions. Yet this\nis exactly what is needed to make valid inferences about their estimates, and\nto use these inferences for a myriad of essential purposes, such as hypothesis\ntesting, dynamic monitoring, realistic and granular scenario and reverse\nscenario analyses, and mitigating the effects of correlation breakdowns during\nmarket upheavals (which is when we need valid inferences the most). This work\ndevelops a new and straightforward method, Nonparametric Angles-based\nCorrelation (NAbC), for defining the finite-sample distributions of any\ndependence measure whose matrix of pairwise associations is positive definite\n(e.g. Pearsons, Kendalls Tau, Spearmans Rho, Chatterjees, Lancasters, Szekelys,\nand their many variants). The solution remains valid under marginal asset\ndistributions characterized by notably different and varying degrees of serial\ncorrelation, non-stationarity, heavy-tailedness, and asymmetry. Notably, NAbCs\np-values and confidence intervals remain analytically consistent at both the\nmatrix level and the pairwise cell level. Finally, NAbC maintains validity even\nwhen selected cells in the matrix are frozen for a given scenario or stress\ntest, that is, unaffected by the scenario, thus enabling flexible, granular,\nand realistic scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We live in a multivariate world, and effective modeling of financial\nportfolios, including their construction, allocation, forecasting, and risk\nanalysis, simply is not possible without explicitly modeling the dependence\nstructure of their assets. Dependence structure can drive portfolio results\nmore than many other parameters in investment and risk models, sometimes even\nmore than their combined effects, but the literature provides relatively little\nto define the finite-sample distributions of dependence measures in useable and\nuseful ways under challenging, real-world financial data conditions. Yet this\nis exactly what is needed to make valid inferences about their estimates, and\nto use these inferences for a myriad of essential purposes, such as hypothesis\ntesting, dynamic monitoring, realistic and granular scenario and reverse\nscenario analyses, and mitigating the effects of correlation breakdowns during\nmarket upheavals (which is when we need valid inferences the most). This work\ndevelops a new and straightforward method, Nonparametric Angles-based\nCorrelation (NAbC), for defining the finite-sample distributions of any\ndependence measure whose matrix of pairwise associations is positive definite\n(e.g. Pearsons, Kendalls Tau, Spearmans Rho, Chatterjees, Lancasters, Szekelys,\nand their many variants). The solution remains valid under marginal asset\ndistributions characterized by notably different and varying degrees of serial\ncorrelation, non-stationarity, heavy-tailedness, and asymmetry. Notably, NAbCs\np-values and confidence intervals remain analytically consistent at both the\nmatrix level and the pairwise cell level. Finally, NAbC maintains validity even\nwhen selected cells in the matrix are frozen for a given scenario or stress\ntest, that is, unaffected by the scenario, thus enabling flexible, granular,\nand realistic scenarios."
                },
                "authors": [
                    {
                        "name": "JD Opdyke"
                    }
                ],
                "author_detail": {
                    "name": "JD Opdyke"
                },
                "author": "JD Opdyke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.RM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62-07, 62E20, 62F10, 62F12, 60E05, 60G70, 91B30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15263v1",
                "updated": "2025-04-21T17:45:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    45,
                    21,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:45:21Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    45,
                    21,
                    0,
                    111,
                    0
                ],
                "title": "Interpretable Locomotion Prediction in Construction Using a\n  Memory-Driven LLM Agent With Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Locomotion Prediction in Construction Using a\n  Memory-Driven LLM Agent With Chain-of-Thought Reasoning"
                },
                "summary": "Construction tasks are inherently unpredictable, with dynamic environments\nand safety-critical demands posing significant risks to workers. Exoskeletons\noffer potential assistance but falter without accurate intent recognition\nacross diverse locomotion modes. This paper presents a locomotion prediction\nagent leveraging Large Language Models (LLMs) augmented with memory systems,\naimed at improving exoskeleton assistance in such settings. Using multimodal\ninputs - spoken commands and visual data from smart glasses - the agent\nintegrates a Perception Module, Short-Term Memory (STM), Long-Term Memory\n(LTM), and Refinement Module to predict locomotion modes effectively.\nEvaluation reveals a baseline weighted F1-score of 0.73 without memory, rising\nto 0.81 with STM, and reaching 0.90 with both STM and LTM, excelling with vague\nand safety-critical commands. Calibration metrics, including a Brier Score drop\nfrom 0.244 to 0.090 and ECE from 0.222 to 0.044, affirm improved reliability.\nThis framework supports safer, high-level human-exoskeleton collaboration, with\npromise for adaptive assistive systems in dynamic industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Construction tasks are inherently unpredictable, with dynamic environments\nand safety-critical demands posing significant risks to workers. Exoskeletons\noffer potential assistance but falter without accurate intent recognition\nacross diverse locomotion modes. This paper presents a locomotion prediction\nagent leveraging Large Language Models (LLMs) augmented with memory systems,\naimed at improving exoskeleton assistance in such settings. Using multimodal\ninputs - spoken commands and visual data from smart glasses - the agent\nintegrates a Perception Module, Short-Term Memory (STM), Long-Term Memory\n(LTM), and Refinement Module to predict locomotion modes effectively.\nEvaluation reveals a baseline weighted F1-score of 0.73 without memory, rising\nto 0.81 with STM, and reaching 0.90 with both STM and LTM, excelling with vague\nand safety-critical commands. Calibration metrics, including a Brier Score drop\nfrom 0.244 to 0.090 and ECE from 0.222 to 0.044, affirm improved reliability.\nThis framework supports safer, high-level human-exoskeleton collaboration, with\npromise for adaptive assistive systems in dynamic industries."
                },
                "authors": [
                    {
                        "name": "Ehsan Ahmadi"
                    },
                    {
                        "name": "Chao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Wang"
                },
                "author": "Chao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10566v2",
                "updated": "2025-04-21T17:45:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    45,
                    8,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-13T17:17:17Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    17,
                    17,
                    3,
                    72,
                    0
                ],
                "title": "ASIDE: Architectural Separation of Instructions and Data in Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASIDE: Architectural Separation of Instructions and Data in Language\n  Models"
                },
                "summary": "Despite their remarkable performance, large language models lack elementary\nsafety features, and this makes them susceptible to numerous malicious attacks.\nIn particular, previous work has identified the absence of an intrinsic\nseparation between instructions and data as a root cause for the success of\nprompt injection attacks. In this work, we propose a method, ASIDE, that allows\nthe model to clearly separate between instructions and data on the level of\nembeddings. ASIDE applies a fixed orthogonal rotation to the embeddings of data\ntokens, thus creating distinct representations of instructions and data tokens\nwithout introducing any additional parameters. We demonstrate the effectiveness\nof our method by instruct-tuning LLMs with ASIDE and showing (1) highly\nincreased instruction-data separation scores without a loss in model\ncapabilities and (2) competitive results on prompt injection benchmarks, even\nwithout dedicated safety training. Additionally, we study the working mechanism\nbehind our method through an analysis of model representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable performance, large language models lack elementary\nsafety features, and this makes them susceptible to numerous malicious attacks.\nIn particular, previous work has identified the absence of an intrinsic\nseparation between instructions and data as a root cause for the success of\nprompt injection attacks. In this work, we propose a method, ASIDE, that allows\nthe model to clearly separate between instructions and data on the level of\nembeddings. ASIDE applies a fixed orthogonal rotation to the embeddings of data\ntokens, thus creating distinct representations of instructions and data tokens\nwithout introducing any additional parameters. We demonstrate the effectiveness\nof our method by instruct-tuning LLMs with ASIDE and showing (1) highly\nincreased instruction-data separation scores without a loss in model\ncapabilities and (2) competitive results on prompt injection benchmarks, even\nwithout dedicated safety training. Additionally, we study the working mechanism\nbehind our method through an analysis of model representations."
                },
                "authors": [
                    {
                        "name": "Egor Zverev"
                    },
                    {
                        "name": "Evgenii Kortukov"
                    },
                    {
                        "name": "Alexander Panfilov"
                    },
                    {
                        "name": "Alexandra Volkova"
                    },
                    {
                        "name": "Soroush Tabesh"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Wojciech Samek"
                    },
                    {
                        "name": "Christoph H. Lampert"
                    }
                ],
                "author_detail": {
                    "name": "Christoph H. Lampert"
                },
                "author": "Christoph H. Lampert",
                "arxiv_comment": "ICLR 2025 Workshop on Building Trust in Language Models and\n  Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15255v1",
                "updated": "2025-04-21T17:34:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    34,
                    55,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:34:55Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    34,
                    55,
                    0,
                    111,
                    0
                ],
                "title": "Population Models for Star Formation Timescales in Early Galaxies: The\n  First Step Towards Solving Outshining in Star Formation History Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Population Models for Star Formation Timescales in Early Galaxies: The\n  First Step Towards Solving Outshining in Star Formation History Inference"
                },
                "summary": "JWST have revealed temporarily-quenched and ultraviolet-luminous galaxies in\nthe early universe, suggesting enhanced star formation stochasticity. Verifying\nthis hypothesis is critical, yet challenging; outshining, wherein light from\nyoung stars dominates the spectral energy distribution, represents perhaps the\ngreatest challenge in inferring the formation histories of unresolved galaxies.\nIn this paper, we take a simple model of burstiness and show that\nstate-of-the-art inference methods with flexible star formation histories\n(SFHs) and neutral priors, while recovering average star formation rates (SFRs;\n$\\sim0.1$ dex median offset), fail to recover the complexities of fluctuations\non tens of Myr timescales, and typically underestimate masses in bursty systems\n($\\sim0.15$ dex). Surprisingly, detailed SFH recovery is still sensitive to\npriors even when data quality is optimal, e.g., including high signal-to-noise\n($\\rm20~pixel^{-1}$) spectroscopy with wide coverage (rest-frame\n$0.12-1.06~\\mu$m). Crucially, however, refitting the same data with a prior\ncorrectly encoding the bursty expectation eliminates these biases: median\noffsets in mass and SFRs decrease to $\\sim 0.04$ dex and $\\sim 0.05$ dex,\nrespectively. Under the assumption that current population burstiness predicts\npast SFH, the solution to outshining in modeling statistical samples is\nempirically measuring recent galaxy SFHs with population modeling. A prototype\nis H$\\alpha$/UV: while helpful, it is insufficient to constrain the expected\ncomplex burstiness. To this end, we introduce a more complete, quantitative\npopulation-level approach and demonstrate that it promises to recover the\ntypical amplitude, timescale, and slope of the recent SFH to high accuracy.\nThis approach thus has the strong potential to solve outshining using\nobservations from JWST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JWST have revealed temporarily-quenched and ultraviolet-luminous galaxies in\nthe early universe, suggesting enhanced star formation stochasticity. Verifying\nthis hypothesis is critical, yet challenging; outshining, wherein light from\nyoung stars dominates the spectral energy distribution, represents perhaps the\ngreatest challenge in inferring the formation histories of unresolved galaxies.\nIn this paper, we take a simple model of burstiness and show that\nstate-of-the-art inference methods with flexible star formation histories\n(SFHs) and neutral priors, while recovering average star formation rates (SFRs;\n$\\sim0.1$ dex median offset), fail to recover the complexities of fluctuations\non tens of Myr timescales, and typically underestimate masses in bursty systems\n($\\sim0.15$ dex). Surprisingly, detailed SFH recovery is still sensitive to\npriors even when data quality is optimal, e.g., including high signal-to-noise\n($\\rm20~pixel^{-1}$) spectroscopy with wide coverage (rest-frame\n$0.12-1.06~\\mu$m). Crucially, however, refitting the same data with a prior\ncorrectly encoding the bursty expectation eliminates these biases: median\noffsets in mass and SFRs decrease to $\\sim 0.04$ dex and $\\sim 0.05$ dex,\nrespectively. Under the assumption that current population burstiness predicts\npast SFH, the solution to outshining in modeling statistical samples is\nempirically measuring recent galaxy SFHs with population modeling. A prototype\nis H$\\alpha$/UV: while helpful, it is insufficient to constrain the expected\ncomplex burstiness. To this end, we introduce a more complete, quantitative\npopulation-level approach and demonstrate that it promises to recover the\ntypical amplitude, timescale, and slope of the recent SFH to high accuracy.\nThis approach thus has the strong potential to solve outshining using\nobservations from JWST."
                },
                "authors": [
                    {
                        "name": "Bingjie Wang"
                    },
                    {
                        "name": "Joel Leja"
                    },
                    {
                        "name": "Hakim Atek"
                    },
                    {
                        "name": "Rachel Bezanson"
                    },
                    {
                        "name": "Emilie Burnham"
                    },
                    {
                        "name": "Pratika Dayal"
                    },
                    {
                        "name": "Robert Feldmann"
                    },
                    {
                        "name": "Jenny E. Greene"
                    },
                    {
                        "name": "Benjamin D. Johnson"
                    },
                    {
                        "name": "Ivo Labbe"
                    },
                    {
                        "name": "Michael V. Maseda"
                    },
                    {
                        "name": "Themiya Nanayakkara"
                    },
                    {
                        "name": "Sedona H. Price"
                    },
                    {
                        "name": "Katherine A. Suess"
                    },
                    {
                        "name": "John R. Weaver"
                    },
                    {
                        "name": "Katherine E. Whitaker"
                    }
                ],
                "author_detail": {
                    "name": "Katherine E. Whitaker"
                },
                "author": "Katherine E. Whitaker",
                "arxiv_comment": "Submitted to ApJ on 12/18/24; this version corresponds to a revised\n  version after a reviewer's report. 27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15254v1",
                "updated": "2025-04-21T17:33:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    33,
                    33,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:33:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    33,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation"
                },
                "summary": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench."
                },
                "authors": [
                    {
                        "name": "Anirudh Khatry"
                    },
                    {
                        "name": "Robert Zhang"
                    },
                    {
                        "name": "Jia Pan"
                    },
                    {
                        "name": "Ziteng Wang"
                    },
                    {
                        "name": "Qiaochu Chen"
                    },
                    {
                        "name": "Greg Durrett"
                    },
                    {
                        "name": "Isil Dillig"
                    }
                ],
                "author_detail": {
                    "name": "Isil Dillig"
                },
                "author": "Isil Dillig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15253v1",
                "updated": "2025-04-21T17:33:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    33,
                    23,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:33:23Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    33,
                    23,
                    0,
                    111,
                    0
                ],
                "title": "Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as\n  Test-Time Scaling Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as\n  Test-Time Scaling Evaluators"
                },
                "summary": "Scaling test-time computation, or affording a generator large language model\n(LLM) extra compute during inference, typically employs the help of external\nnon-generative evaluators (i.e., reward models). Concurrently, LLM-judges,\nmodels trained to generate evaluations and critiques (explanations) in natural\nlanguage, are becoming increasingly popular in automatic evaluation. Despite\njudge empirical successes, their effectiveness as evaluators in test-time\nscaling settings is largely unknown. In this paper, we introduce the Judge\nEvaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge\nperformance in three domains (math reasoning, code generation, and instruction\nfollowing) under three task settings: response reranking, step-level beam\nsearch, and critique-based response refinement. We evaluate 10 different judge\nmodels (7B-70B parameters) for 8 different base generator models (6.7B-72B\nparameters). Our benchmark shows that while judges are competitive with outcome\nreward models in reranking, they are consistently worse than process reward\nmodels in beam search procedures. Furthermore, though unique to LLM-judges,\ntheir natural language critiques are currently ineffective in guiding the\ngenerator towards better responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling test-time computation, or affording a generator large language model\n(LLM) extra compute during inference, typically employs the help of external\nnon-generative evaluators (i.e., reward models). Concurrently, LLM-judges,\nmodels trained to generate evaluations and critiques (explanations) in natural\nlanguage, are becoming increasingly popular in automatic evaluation. Despite\njudge empirical successes, their effectiveness as evaluators in test-time\nscaling settings is largely unknown. In this paper, we introduce the Judge\nEvaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge\nperformance in three domains (math reasoning, code generation, and instruction\nfollowing) under three task settings: response reranking, step-level beam\nsearch, and critique-based response refinement. We evaluate 10 different judge\nmodels (7B-70B parameters) for 8 different base generator models (6.7B-72B\nparameters). Our benchmark shows that while judges are competitive with outcome\nreward models in reranking, they are consistently worse than process reward\nmodels in beam search procedures. Furthermore, though unique to LLM-judges,\ntheir natural language critiques are currently ineffective in guiding the\ngenerator towards better responses."
                },
                "authors": [
                    {
                        "name": "Yilun Zhou"
                    },
                    {
                        "name": "Austin Xu"
                    },
                    {
                        "name": "Peifeng Wang"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "The first two authors contributed equally. The codebase is at\n  https://github.com/SalesforceAIResearch/jetts-benchmark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05291v2",
                "updated": "2025-04-21T17:30:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    30,
                    23,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-07T19:50:02Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    19,
                    50,
                    2,
                    4,
                    38,
                    0
                ],
                "title": "Can LLMs Rank the Harmfulness of Smaller LLMs? We are Not There Yet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Rank the Harmfulness of Smaller LLMs? We are Not There Yet"
                },
                "summary": "Large language models (LLMs) have become ubiquitous, thus it is important to\nunderstand their risks and limitations. Smaller LLMs can be deployed where\ncompute resources are constrained, such as edge devices, but with different\npropensity to generate harmful output. Mitigation of LLM harm typically depends\non annotating the harmfulness of LLM output, which is expensive to collect from\nhumans. This work studies two questions: How do smaller LLMs rank regarding\ngeneration of harmful content? How well can larger LLMs annotate harmfulness?\nWe prompt three small LLMs to elicit harmful content of various types, such as\ndiscriminatory language, offensive content, privacy invasion, or negative\ninfluence, and collect human rankings of their outputs. Then, we evaluate three\nstate-of-the-art large LLMs on their ability to annotate the harmfulness of\nthese responses. We find that the smaller models differ with respect to\nharmfulness. We also find that large LLMs show low to moderate agreement with\nhumans. These findings underline the need for further work on harm mitigation\nin LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become ubiquitous, thus it is important to\nunderstand their risks and limitations. Smaller LLMs can be deployed where\ncompute resources are constrained, such as edge devices, but with different\npropensity to generate harmful output. Mitigation of LLM harm typically depends\non annotating the harmfulness of LLM output, which is expensive to collect from\nhumans. This work studies two questions: How do smaller LLMs rank regarding\ngeneration of harmful content? How well can larger LLMs annotate harmfulness?\nWe prompt three small LLMs to elicit harmful content of various types, such as\ndiscriminatory language, offensive content, privacy invasion, or negative\ninfluence, and collect human rankings of their outputs. Then, we evaluate three\nstate-of-the-art large LLMs on their ability to annotate the harmfulness of\nthese responses. We find that the smaller models differ with respect to\nharmfulness. We also find that large LLMs show low to moderate agreement with\nhumans. These findings underline the need for further work on harm mitigation\nin LLMs."
                },
                "authors": [
                    {
                        "name": "Berk Atil"
                    },
                    {
                        "name": "Vipul Gupta"
                    },
                    {
                        "name": "Sarkar Snigdha Sarathi Das"
                    },
                    {
                        "name": "Rebecca J. Passonneau"
                    }
                ],
                "author_detail": {
                    "name": "Rebecca J. Passonneau"
                },
                "author": "Rebecca J. Passonneau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15241v1",
                "updated": "2025-04-21T17:15:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    15,
                    6,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:15:06Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    15,
                    6,
                    0,
                    111,
                    0
                ],
                "title": "MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning"
                },
                "summary": "Large Language Models (LLMs) are susceptible to adversarial attacks such as\njailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability\nis exacerbated in multilingual setting, where multilingual safety-aligned data\nare often limited. Thus, developing a guardrail capable of detecting and\nfiltering unsafe content across diverse languages is critical for deploying\nLLMs in real-world applications. In this work, we propose an approach to build\na multilingual guardrail with reasoning. Our method consists of: (1) synthetic\nmultilingual data generation incorporating culturally and linguistically\nnuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group\nRelative Policy Optimization (GRPO) framework that further improves\nperformance. Experimental results demonstrate that our multilingual guardrail\nconsistently outperforms recent baselines across both in-domain and\nout-of-domain languages. The multilingual reasoning capability of our guardrail\nenables it to generate multilingual explanations, which are particularly useful\nfor understanding language-specific risks and ambiguities in multilingual\ncontent moderation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are susceptible to adversarial attacks such as\njailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability\nis exacerbated in multilingual setting, where multilingual safety-aligned data\nare often limited. Thus, developing a guardrail capable of detecting and\nfiltering unsafe content across diverse languages is critical for deploying\nLLMs in real-world applications. In this work, we propose an approach to build\na multilingual guardrail with reasoning. Our method consists of: (1) synthetic\nmultilingual data generation incorporating culturally and linguistically\nnuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group\nRelative Policy Optimization (GRPO) framework that further improves\nperformance. Experimental results demonstrate that our multilingual guardrail\nconsistently outperforms recent baselines across both in-domain and\nout-of-domain languages. The multilingual reasoning capability of our guardrail\nenables it to generate multilingual explanations, which are particularly useful\nfor understanding language-specific risks and ambiguities in multilingual\ncontent moderation."
                },
                "authors": [
                    {
                        "name": "Yahan Yang"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Shuo Li"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Insup Lee"
                    }
                ],
                "author_detail": {
                    "name": "Insup Lee"
                },
                "author": "Insup Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04491v2",
                "updated": "2025-04-21T17:07:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    7,
                    52,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-06T14:39:22Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    39,
                    22,
                    3,
                    65,
                    0
                ],
                "title": "A Spatiotemporal, Quasi-experimental Causal Inference Approach to\n  Characterize the Effects of Global Plastic Waste Export and Burning on Air\n  Quality Using Remotely Sensed Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Spatiotemporal, Quasi-experimental Causal Inference Approach to\n  Characterize the Effects of Global Plastic Waste Export and Burning on Air\n  Quality Using Remotely Sensed Data"
                },
                "summary": "Open burning of plastic waste may pose a significant threat to global health\nby degrading air quality, but quantitative research on this problem -- crucial\nfor policy making -- has been stunted by lack of data. Critically, many low-\nand middle-income countries, where open burning is of greatest concern, have\nlittle to no air quality monitoring. Here, we propose an approach to leverage\nremotely sensed data products combined with spatiotemporal causal analytic\ntechniques to evaluate the impact of large-scale plastic waste policies on air\nquality. Throughout, we use the case study of Indonesia before and after 2018,\nwhen China halted its import of plastic waste, resulting in diversion of this\nmassive waste stream to other countries. We tailor cutting-edge statistical\nmethods to this setting, estimating effects of the increase in plastic waste\nimports on fine particulate matter (PM$_{2.5}$) near waste dump sites in\nIndonesia as a function of proximity to ports, which serves as an induced\ncontinuous exposure. We observe that dump sites above the 20th quantile of port\nproximity experienced a statistically significant increase in monthly\nPM$_{2.5}$ concentrations after China's ban took effect (2018-2019) compared to\nconcentrations expected under business-as-usual (2012-2017), with increases\nranging from 0.76--1.72$\\mu$g/m$^3$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open burning of plastic waste may pose a significant threat to global health\nby degrading air quality, but quantitative research on this problem -- crucial\nfor policy making -- has been stunted by lack of data. Critically, many low-\nand middle-income countries, where open burning is of greatest concern, have\nlittle to no air quality monitoring. Here, we propose an approach to leverage\nremotely sensed data products combined with spatiotemporal causal analytic\ntechniques to evaluate the impact of large-scale plastic waste policies on air\nquality. Throughout, we use the case study of Indonesia before and after 2018,\nwhen China halted its import of plastic waste, resulting in diversion of this\nmassive waste stream to other countries. We tailor cutting-edge statistical\nmethods to this setting, estimating effects of the increase in plastic waste\nimports on fine particulate matter (PM$_{2.5}$) near waste dump sites in\nIndonesia as a function of proximity to ports, which serves as an induced\ncontinuous exposure. We observe that dump sites above the 20th quantile of port\nproximity experienced a statistically significant increase in monthly\nPM$_{2.5}$ concentrations after China's ban took effect (2018-2019) compared to\nconcentrations expected under business-as-usual (2012-2017), with increases\nranging from 0.76--1.72$\\mu$g/m$^3$."
                },
                "authors": [
                    {
                        "name": "Ellen M. Considine"
                    },
                    {
                        "name": "Rachel C. Nethery"
                    }
                ],
                "author_detail": {
                    "name": "Rachel C. Nethery"
                },
                "author": "Rachel C. Nethery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15228v1",
                "updated": "2025-04-21T16:58:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    58,
                    18,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T16:58:18Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    58,
                    18,
                    0,
                    111,
                    0
                ],
                "title": "A Self-Improving Coding Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Self-Improving Coding Agent"
                },
                "summary": "We demonstrate that an LLM coding agent, equipped with basic coding tools,\ncan autonomously edit itself, and thereby improve its performance on benchmark\ntasks. We find performance gains from 17% to 53% on a random subset of SWE\nBench Verified, with additional performance gains on LiveCodeBench, as well as\nsynthetically generated agent benchmarks. Our work represents an advancement in\nthe automated and open-ended design of agentic systems, and provides a\nreference agent framework for those seeking to post-train LLMs on tool use and\nother agentic tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that an LLM coding agent, equipped with basic coding tools,\ncan autonomously edit itself, and thereby improve its performance on benchmark\ntasks. We find performance gains from 17% to 53% on a random subset of SWE\nBench Verified, with additional performance gains on LiveCodeBench, as well as\nsynthetically generated agent benchmarks. Our work represents an advancement in\nthe automated and open-ended design of agentic systems, and provides a\nreference agent framework for those seeking to post-train LLMs on tool use and\nother agentic tasks."
                },
                "authors": [
                    {
                        "name": "Maxime Robeyns"
                    },
                    {
                        "name": "Martin Szummer"
                    },
                    {
                        "name": "Laurence Aitchison"
                    }
                ],
                "author_detail": {
                    "name": "Laurence Aitchison"
                },
                "author": "Laurence Aitchison",
                "arxiv_comment": "Published at an ICLR 2025 workshop on Scaling Self-Improving\n  Foundation Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15220v1",
                "updated": "2025-04-21T16:46:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    46,
                    7,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T16:46:07Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    46,
                    7,
                    0,
                    111,
                    0
                ],
                "title": "Fully Bayesian Approaches to Topics over Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Bayesian Approaches to Topics over Time"
                },
                "summary": "The Topics over Time (ToT) model captures thematic changes in timestamped\ndatasets by explicitly modeling publication dates jointly with word\nco-occurrence patterns. However, ToT was not approached in a fully Bayesian\nfashion, a flaw that makes it susceptible to stability problems. To address\nthis issue, we propose a fully Bayesian Topics over Time (BToT) model via the\nintroduction of a conjugate prior to the Beta distribution. This prior acts as\na regularization that prevents the online version of the algorithm from\nunstable updates when a topic is poorly represented in a mini-batch. The\ncharacteristics of this prior to the Beta distribution are studied here for the\nfirst time. Still, this model suffers from a difference in scale between the\nsingle-time observations and the multiplicity of words per document. A\nvariation of BToT, Weighted Bayesian Topics over Time (WBToT), is proposed as a\nsolution. In WBToT, publication dates are repeated a certain number of times\nper document, which balances the relative influence of words and timestamps\nalong the inference process. We have tested our models on two datasets: a\ncollection of over 200 years of US state-of-the-union (SOTU) addresses and a\nlarge-scale COVID-19 Twitter corpus of 10 million tweets. The results show that\nWBToT captures events better than Latent Dirichlet Allocation and other SOTA\ntopic models like BERTopic: the median absolute deviation of the topic presence\nover time is reduced by $51\\%$ and $34\\%$, respectively. Our experiments also\ndemonstrate the superior coherence of WBToT over BToT, which highlights the\nimportance of balancing the time and word modalities. Finally, we illustrate\nthe stability of the online optimization algorithm in WBToT, which allows the\napplication of WBToT to problems that are intractable for standard ToT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Topics over Time (ToT) model captures thematic changes in timestamped\ndatasets by explicitly modeling publication dates jointly with word\nco-occurrence patterns. However, ToT was not approached in a fully Bayesian\nfashion, a flaw that makes it susceptible to stability problems. To address\nthis issue, we propose a fully Bayesian Topics over Time (BToT) model via the\nintroduction of a conjugate prior to the Beta distribution. This prior acts as\na regularization that prevents the online version of the algorithm from\nunstable updates when a topic is poorly represented in a mini-batch. The\ncharacteristics of this prior to the Beta distribution are studied here for the\nfirst time. Still, this model suffers from a difference in scale between the\nsingle-time observations and the multiplicity of words per document. A\nvariation of BToT, Weighted Bayesian Topics over Time (WBToT), is proposed as a\nsolution. In WBToT, publication dates are repeated a certain number of times\nper document, which balances the relative influence of words and timestamps\nalong the inference process. We have tested our models on two datasets: a\ncollection of over 200 years of US state-of-the-union (SOTU) addresses and a\nlarge-scale COVID-19 Twitter corpus of 10 million tweets. The results show that\nWBToT captures events better than Latent Dirichlet Allocation and other SOTA\ntopic models like BERTopic: the median absolute deviation of the topic presence\nover time is reduced by $51\\%$ and $34\\%$, respectively. Our experiments also\ndemonstrate the superior coherence of WBToT over BToT, which highlights the\nimportance of balancing the time and word modalities. Finally, we illustrate\nthe stability of the online optimization algorithm in WBToT, which allows the\napplication of WBToT to problems that are intractable for standard ToT."
                },
                "authors": [
                    {
                        "name": "Julián Cendrero"
                    },
                    {
                        "name": "Julio Gonzalo"
                    },
                    {
                        "name": "Ivar Zapata"
                    }
                ],
                "author_detail": {
                    "name": "Ivar Zapata"
                },
                "author": "Ivar Zapata",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15219v1",
                "updated": "2025-04-21T16:43:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    43,
                    50,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T16:43:50Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    43,
                    50,
                    0,
                    111,
                    0
                ],
                "title": "EvalAgent: Discovering Implicit Evaluation Criteria from the Web",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvalAgent: Discovering Implicit Evaluation Criteria from the Web"
                },
                "summary": "Evaluation of language model outputs on structured writing tasks is typically\nconducted with a number of desirable criteria presented to human evaluators or\nlarge language models (LLMs). For instance, on a prompt like \"Help me draft an\nacademic talk on coffee intake vs research productivity\", a model response may\nbe evaluated for criteria like accuracy and coherence. However, high-quality\nresponses should do more than just satisfy basic task requirements. An\neffective response to this query should include quintessential features of an\nacademic talk, such as a compelling opening, clear research questions, and a\ntakeaway. To help identify these implicit criteria, we introduce EvalAgent, a\nnovel framework designed to automatically uncover nuanced and task-specific\ncriteria. EvalAgent first mines expert-authored online guidance. It then uses\nthis evidence to propose diverse, long-tail evaluation criteria that are\ngrounded in reliable external sources. Our experiments demonstrate that the\ngrounded criteria produced by EvalAgent are often implicit (not directly stated\nin the user's prompt), yet specific (high degree of lexical precision).\nFurther, EvalAgent criteria are often not satisfied by initial responses but\nthey are actionable, such that responses can be refined to satisfy them.\nFinally, we show that combining LLM-generated and EvalAgent criteria uncovers\nmore human-valued criteria than using LLMs alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of language model outputs on structured writing tasks is typically\nconducted with a number of desirable criteria presented to human evaluators or\nlarge language models (LLMs). For instance, on a prompt like \"Help me draft an\nacademic talk on coffee intake vs research productivity\", a model response may\nbe evaluated for criteria like accuracy and coherence. However, high-quality\nresponses should do more than just satisfy basic task requirements. An\neffective response to this query should include quintessential features of an\nacademic talk, such as a compelling opening, clear research questions, and a\ntakeaway. To help identify these implicit criteria, we introduce EvalAgent, a\nnovel framework designed to automatically uncover nuanced and task-specific\ncriteria. EvalAgent first mines expert-authored online guidance. It then uses\nthis evidence to propose diverse, long-tail evaluation criteria that are\ngrounded in reliable external sources. Our experiments demonstrate that the\ngrounded criteria produced by EvalAgent are often implicit (not directly stated\nin the user's prompt), yet specific (high degree of lexical precision).\nFurther, EvalAgent criteria are often not satisfied by initial responses but\nthey are actionable, such that responses can be refined to satisfy them.\nFinally, we show that combining LLM-generated and EvalAgent criteria uncovers\nmore human-valued criteria than using LLMs alone."
                },
                "authors": [
                    {
                        "name": "Manya Wadhwa"
                    },
                    {
                        "name": "Zayne Sprague"
                    },
                    {
                        "name": "Chaitanya Malaviya"
                    },
                    {
                        "name": "Philippe Laban"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    },
                    {
                        "name": "Greg Durrett"
                    }
                ],
                "author_detail": {
                    "name": "Greg Durrett"
                },
                "author": "Greg Durrett",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14744v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14744v3",
                "updated": "2025-04-21T16:41:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    41,
                    37,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-20T17:14:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    14,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language\n  Models via Monitoring Hidden States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language\n  Models via Monitoring Hidden States"
                },
                "summary": "The integration of additional modalities increases the susceptibility of\nlarge vision-language models (LVLMs) to safety risks, such as jailbreak\nattacks, compared to their language-only counterparts. While existing research\nprimarily focuses on post-hoc alignment techniques, the underlying safety\nmechanisms within LVLMs remain largely unexplored. In this work , we\ninvestigate whether LVLMs inherently encode safety-relevant signals within\ntheir internal activations during inference. Our findings reveal that LVLMs\nexhibit distinct activation patterns when processing unsafe prompts, which can\nbe leveraged to detect and mitigate adversarial inputs without requiring\nextensive fine-tuning. Building on this insight, we introduce HiddenDetect, a\nnovel tuning-free framework that harnesses internal model activations to\nenhance safety. Experimental results show that {HiddenDetect} surpasses\nstate-of-the-art methods in detecting jailbreak attacks against LVLMs. By\nutilizing intrinsic safety-aware patterns, our method provides an efficient and\nscalable solution for strengthening LVLM robustness against multimodal threats.\nOur code will be released publicly at\nhttps://github.com/leigest519/HiddenDetect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of additional modalities increases the susceptibility of\nlarge vision-language models (LVLMs) to safety risks, such as jailbreak\nattacks, compared to their language-only counterparts. While existing research\nprimarily focuses on post-hoc alignment techniques, the underlying safety\nmechanisms within LVLMs remain largely unexplored. In this work , we\ninvestigate whether LVLMs inherently encode safety-relevant signals within\ntheir internal activations during inference. Our findings reveal that LVLMs\nexhibit distinct activation patterns when processing unsafe prompts, which can\nbe leveraged to detect and mitigate adversarial inputs without requiring\nextensive fine-tuning. Building on this insight, we introduce HiddenDetect, a\nnovel tuning-free framework that harnesses internal model activations to\nenhance safety. Experimental results show that {HiddenDetect} surpasses\nstate-of-the-art methods in detecting jailbreak attacks against LVLMs. By\nutilizing intrinsic safety-aware patterns, our method provides an efficient and\nscalable solution for strengthening LVLM robustness against multimodal threats.\nOur code will be released publicly at\nhttps://github.com/leigest519/HiddenDetect."
                },
                "authors": [
                    {
                        "name": "Yilei Jiang"
                    },
                    {
                        "name": "Xinyan Gao"
                    },
                    {
                        "name": "Tianshuo Peng"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Xiaoyong Zhu"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14744v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14744v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.14381v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.14381v4",
                "updated": "2025-04-21T16:33:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    33,
                    14,
                    0,
                    111,
                    0
                ],
                "published": "2023-11-24T10:00:23Z",
                "published_parsed": [
                    2023,
                    11,
                    24,
                    10,
                    0,
                    23,
                    4,
                    328,
                    0
                ],
                "title": "Potential Societal Biases of ChatGPT in Higher Education: A Scoping\n  Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential Societal Biases of ChatGPT in Higher Education: A Scoping\n  Review"
                },
                "summary": "Purpose:Generative Artificial Intelligence (GAI) models, such as ChatGPT, may\ninherit or amplify societal biases due to their training on extensive datasets.\nWith the increasing usage of GAI by students, faculty, and staff in higher\neducation institutions (HEIs), it is urgent to examine the ethical issues and\npotential biases associated with these technologies.\nDesign/Approach/Methods:This scoping review aims to elucidate how biases\nrelated to GAI in HEIs have been researched and discussed in recent academic\npublications. We categorized the potential societal biases that GAI might cause\nin the field of higher education. Our review includes articles written in\nEnglish, Chinese, and Japanese across four main databases, focusing on GAI\nusage in higher education and bias. Findings:Our findings reveal that while\nthere is meaningful scholarly discussion around bias and discrimination\nconcerning LLMs in the AI field, most articles addressing higher education\napproach the issue superficially. Few articles identify specific types of bias\nunder different circumstances, and there is a notable lack of empirical\nresearch. Most papers in our review focus primarily on educational and research\nfields related to medicine and engineering, with some addressing English\neducation. However, there is almost no discussion regarding the humanities and\nsocial sciences. Additionally, a significant portion of the current discourse\nis in English and primarily addresses English-speaking contexts.\nOriginality/Value:To the best of our knowledge, our study is the first to\nsummarize the potential societal biases in higher education. This review\nhighlights the need for more in-depth studies and empirical work to understand\nthe specific biases that GAI might introduce or amplify in educational\nsettings, guiding the development of more ethical AI applications in higher\neducation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose:Generative Artificial Intelligence (GAI) models, such as ChatGPT, may\ninherit or amplify societal biases due to their training on extensive datasets.\nWith the increasing usage of GAI by students, faculty, and staff in higher\neducation institutions (HEIs), it is urgent to examine the ethical issues and\npotential biases associated with these technologies.\nDesign/Approach/Methods:This scoping review aims to elucidate how biases\nrelated to GAI in HEIs have been researched and discussed in recent academic\npublications. We categorized the potential societal biases that GAI might cause\nin the field of higher education. Our review includes articles written in\nEnglish, Chinese, and Japanese across four main databases, focusing on GAI\nusage in higher education and bias. Findings:Our findings reveal that while\nthere is meaningful scholarly discussion around bias and discrimination\nconcerning LLMs in the AI field, most articles addressing higher education\napproach the issue superficially. Few articles identify specific types of bias\nunder different circumstances, and there is a notable lack of empirical\nresearch. Most papers in our review focus primarily on educational and research\nfields related to medicine and engineering, with some addressing English\neducation. However, there is almost no discussion regarding the humanities and\nsocial sciences. Additionally, a significant portion of the current discourse\nis in English and primarily addresses English-speaking contexts.\nOriginality/Value:To the best of our knowledge, our study is the first to\nsummarize the potential societal biases in higher education. This review\nhighlights the need for more in-depth studies and empirical work to understand\nthe specific biases that GAI might introduce or amplify in educational\nsettings, guiding the development of more ethical AI applications in higher\neducation."
                },
                "authors": [
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ariunaa Enkhtur"
                    },
                    {
                        "name": "Beverley Anne Yamamoto"
                    },
                    {
                        "name": "Fei Cheng"
                    },
                    {
                        "name": "Lilan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lilan Chen"
                },
                "author": "Lilan Chen",
                "arxiv_doi": "10.55982/openpraxis.17.1.750",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.55982/openpraxis.17.1.750",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.14381v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.14381v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Open Praxis",
                "arxiv_journal_ref": "2025, 17(1), pp.79-94",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15211v1",
                "updated": "2025-04-21T16:31:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    31,
                    15,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T16:31:15Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    31,
                    15,
                    0,
                    111,
                    0
                ],
                "title": "Position: Bayesian Statistics Facilitates Stakeholder Participation in\n  Evaluation of Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Bayesian Statistics Facilitates Stakeholder Participation in\n  Evaluation of Generative AI"
                },
                "summary": "The evaluation of Generative AI (GenAI) systems plays a critical role in\npublic policy and decision-making, yet existing methods are often limited by\nreliance on benchmark-driven, point-estimate comparisons that fail to capture\nuncertainty and broader societal impacts. This paper argues for the use of\nBayesian statistics as a principled framework to address these challenges.\nBayesian methods enable the integration of domain expertise through prior\nelicitation, allow for continuous learning from new data, and provide robust\nuncertainty quantification via posterior inference. We demonstrate how Bayesian\ninference can be applied to GenAI evaluation, particularly in incorporating\nstakeholder perspectives to enhance fairness, transparency, and reliability.\nFurthermore, we discuss Bayesian workflows as an iterative process for model\nvalidation and refinement, ensuring robust assessments of GenAI systems in\ndynamic, real-world contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of Generative AI (GenAI) systems plays a critical role in\npublic policy and decision-making, yet existing methods are often limited by\nreliance on benchmark-driven, point-estimate comparisons that fail to capture\nuncertainty and broader societal impacts. This paper argues for the use of\nBayesian statistics as a principled framework to address these challenges.\nBayesian methods enable the integration of domain expertise through prior\nelicitation, allow for continuous learning from new data, and provide robust\nuncertainty quantification via posterior inference. We demonstrate how Bayesian\ninference can be applied to GenAI evaluation, particularly in incorporating\nstakeholder perspectives to enhance fairness, transparency, and reliability.\nFurthermore, we discuss Bayesian workflows as an iterative process for model\nvalidation and refinement, ensuring robust assessments of GenAI systems in\ndynamic, real-world contexts."
                },
                "authors": [
                    {
                        "name": "Yanan Long"
                    }
                ],
                "author_detail": {
                    "name": "Yanan Long"
                },
                "author": "Yanan Long",
                "arxiv_comment": "To be presented at ACM CHI 2025 workshop STAIG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15210v1",
                "updated": "2025-04-21T16:29:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    29,
                    7,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T16:29:07Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    29,
                    7,
                    0,
                    111,
                    0
                ],
                "title": "Integrating Symbolic Execution into the Fine-Tuning of Code-Generating\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Symbolic Execution into the Fine-Tuning of Code-Generating\n  LLMs"
                },
                "summary": "Code-generating Large Language Models (LLMs) have become essential tools in\nmodern software development, enhancing productivity and accelerating\ndevelopment. This paper aims to investigate the fine-tuning of code-generating\nLLMs using Reinforcement Learning and Direct Preference Optimization, further\nimproving their performance. To achieve this, we enhance the training data for\nthe reward model with the help of symbolic execution techniques, ensuring more\ncomprehensive and objective data. With symbolic execution, we create a custom\ndataset that better captures the nuances in code evaluation. Our reward models,\nfine-tuned on this dataset, demonstrate significant improvements over the\nbaseline, CodeRL, in estimating the quality of generated code. Our\ncode-generating LLMs, trained with the help of reward model feedback, achieve\nsimilar results compared to the CodeRL benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code-generating Large Language Models (LLMs) have become essential tools in\nmodern software development, enhancing productivity and accelerating\ndevelopment. This paper aims to investigate the fine-tuning of code-generating\nLLMs using Reinforcement Learning and Direct Preference Optimization, further\nimproving their performance. To achieve this, we enhance the training data for\nthe reward model with the help of symbolic execution techniques, ensuring more\ncomprehensive and objective data. With symbolic execution, we create a custom\ndataset that better captures the nuances in code evaluation. Our reward models,\nfine-tuned on this dataset, demonstrate significant improvements over the\nbaseline, CodeRL, in estimating the quality of generated code. Our\ncode-generating LLMs, trained with the help of reward model feedback, achieve\nsimilar results compared to the CodeRL benchmark."
                },
                "authors": [
                    {
                        "name": "Marina Sakharova"
                    },
                    {
                        "name": "Abhinav Anand"
                    },
                    {
                        "name": "Mira Mezini"
                    }
                ],
                "author_detail": {
                    "name": "Mira Mezini"
                },
                "author": "Mira Mezini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15208v1",
                "updated": "2025-04-21T16:26:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    26,
                    56,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T16:26:56Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    26,
                    56,
                    0,
                    111,
                    0
                ],
                "title": "Compute-Optimal LLMs Provably Generalize Better With Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-Optimal LLMs Provably Generalize Better With Scale"
                },
                "summary": "Why do larger language models generalize better? To investigate this\nquestion, we develop generalization bounds on the pretraining objective of\nlarge language models (LLMs) in the compute-optimal regime, as described by the\nChinchilla scaling laws. We introduce a novel, fully empirical Freedman-type\nmartingale concentration inequality that tightens existing bounds by accounting\nfor the variance of the loss function. This generalization bound can be\ndecomposed into three interpretable components: the number of parameters per\ntoken, the loss variance, and the quantization error at a fixed bitrate. As\ncompute-optimal language models are scaled up, the number of parameters per\ndata point remains constant; however, both the loss variance and the\nquantization error decrease, implying that larger models should have smaller\ngeneralization gaps. We examine why larger models tend to be more quantizable\nfrom an information theoretic perspective, showing that the rate at which they\ncan integrate new information grows more slowly than their capacity on the\ncompute-optimal frontier. From these findings we produce a scaling law for the\ngeneralization gap, with bounds that become predictably stronger with scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why do larger language models generalize better? To investigate this\nquestion, we develop generalization bounds on the pretraining objective of\nlarge language models (LLMs) in the compute-optimal regime, as described by the\nChinchilla scaling laws. We introduce a novel, fully empirical Freedman-type\nmartingale concentration inequality that tightens existing bounds by accounting\nfor the variance of the loss function. This generalization bound can be\ndecomposed into three interpretable components: the number of parameters per\ntoken, the loss variance, and the quantization error at a fixed bitrate. As\ncompute-optimal language models are scaled up, the number of parameters per\ndata point remains constant; however, both the loss variance and the\nquantization error decrease, implying that larger models should have smaller\ngeneralization gaps. We examine why larger models tend to be more quantizable\nfrom an information theoretic perspective, showing that the rate at which they\ncan integrate new information grows more slowly than their capacity on the\ncompute-optimal frontier. From these findings we produce a scaling law for the\ngeneralization gap, with bounds that become predictably stronger with scale."
                },
                "authors": [
                    {
                        "name": "Marc Finzi"
                    },
                    {
                        "name": "Sanyam Kapoor"
                    },
                    {
                        "name": "Diego Granziol"
                    },
                    {
                        "name": "Anming Gu"
                    },
                    {
                        "name": "Christopher De Sa"
                    },
                    {
                        "name": "J. Zico Kolter"
                    },
                    {
                        "name": "Andrew Gordon Wilson"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Gordon Wilson"
                },
                "author": "Andrew Gordon Wilson",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15205v1",
                "updated": "2025-04-21T16:20:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    20,
                    43,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T16:20:43Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    20,
                    43,
                    0,
                    111,
                    0
                ],
                "title": "Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus\n  LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus\n  LLM Judges"
                },
                "summary": "Retrieval-augmented generation (RAG) enables large language models (LLMs) to\ngenerate answers with citations from source documents containing \"ground\ntruth\", thereby reducing system hallucinations. A crucial factor in RAG\nevaluation is \"support\", whether the information in the cited documents\nsupports the answer. To this end, we conducted a large-scale comparative study\nof 45 participant submissions on 36 topics to the TREC 2024 RAG Track,\ncomparing an automatic LLM judge (GPT-4o) against human judges for support\nassessment. We considered two conditions: (1) fully manual assessments from\nscratch and (2) manual assessments with post-editing of LLM predictions. Our\nresults indicate that for 56% of the manual from-scratch assessments, human and\nGPT-4o predictions match perfectly (on a three-level scale), increasing to 72%\nin the manual with post-editing condition. Furthermore, by carefully analyzing\nthe disagreements in an unbiased study, we found that an independent human\njudge correlates better with GPT-4o than a human judge, suggesting that LLM\njudges can be a reliable alternative for support assessment. To conclude, we\nprovide a qualitative analysis of human and GPT-4o errors to help guide future\niterations of support assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enables large language models (LLMs) to\ngenerate answers with citations from source documents containing \"ground\ntruth\", thereby reducing system hallucinations. A crucial factor in RAG\nevaluation is \"support\", whether the information in the cited documents\nsupports the answer. To this end, we conducted a large-scale comparative study\nof 45 participant submissions on 36 topics to the TREC 2024 RAG Track,\ncomparing an automatic LLM judge (GPT-4o) against human judges for support\nassessment. We considered two conditions: (1) fully manual assessments from\nscratch and (2) manual assessments with post-editing of LLM predictions. Our\nresults indicate that for 56% of the manual from-scratch assessments, human and\nGPT-4o predictions match perfectly (on a three-level scale), increasing to 72%\nin the manual with post-editing condition. Furthermore, by carefully analyzing\nthe disagreements in an unbiased study, we found that an independent human\njudge correlates better with GPT-4o than a human judge, suggesting that LLM\njudges can be a reliable alternative for support assessment. To conclude, we\nprovide a qualitative analysis of human and GPT-4o errors to help guide future\niterations of support assessment."
                },
                "authors": [
                    {
                        "name": "Nandan Thakur"
                    },
                    {
                        "name": "Ronak Pradeep"
                    },
                    {
                        "name": "Shivani Upadhyay"
                    },
                    {
                        "name": "Daniel Campos"
                    },
                    {
                        "name": "Nick Craswell"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "arxiv_comment": "Accepted at SIGIR 2025 (short)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15199v1",
                "updated": "2025-04-21T16:16:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    16,
                    19,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T16:16:19Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    16,
                    19,
                    0,
                    111,
                    0
                ],
                "title": "Zero-Shot, But at What Cost? Unveiling the Hidden Overhead of MILS's\n  LLM-CLIP Framework for Image Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot, But at What Cost? Unveiling the Hidden Overhead of MILS's\n  LLM-CLIP Framework for Image Captioning"
                },
                "summary": "MILS (Multimodal Iterative LLM Solver) is a recently published framework that\nclaims \"LLMs can see and hear without any training\" by leveraging an iterative,\nLLM-CLIP based approach for zero-shot image captioning. While this MILS\napproach demonstrates good performance, our investigation reveals that this\nsuccess comes at a hidden, substantial computational cost due to its expensive\nmulti-step refinement process. In contrast, alternative models such as BLIP-2\nand GPT-4V achieve competitive results through a streamlined, single-pass\napproach. We hypothesize that the significant overhead inherent in MILS's\niterative process may undermine its practical benefits, thereby challenging the\nnarrative that zero-shot performance can be attained without incurring heavy\nresource demands. This work is the first to expose and quantify the trade-offs\nbetween output quality and computational cost in MILS, providing critical\ninsights for the design of more efficient multimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MILS (Multimodal Iterative LLM Solver) is a recently published framework that\nclaims \"LLMs can see and hear without any training\" by leveraging an iterative,\nLLM-CLIP based approach for zero-shot image captioning. While this MILS\napproach demonstrates good performance, our investigation reveals that this\nsuccess comes at a hidden, substantial computational cost due to its expensive\nmulti-step refinement process. In contrast, alternative models such as BLIP-2\nand GPT-4V achieve competitive results through a streamlined, single-pass\napproach. We hypothesize that the significant overhead inherent in MILS's\niterative process may undermine its practical benefits, thereby challenging the\nnarrative that zero-shot performance can be attained without incurring heavy\nresource demands. This work is the first to expose and quantify the trade-offs\nbetween output quality and computational cost in MILS, providing critical\ninsights for the design of more efficient multimodal models."
                },
                "authors": [
                    {
                        "name": "Yassir Benhammou"
                    },
                    {
                        "name": "Alessandro Tiberio"
                    },
                    {
                        "name": "Gabriel Trautmann"
                    },
                    {
                        "name": "Suman Kalyan"
                    }
                ],
                "author_detail": {
                    "name": "Suman Kalyan"
                },
                "author": "Suman Kalyan",
                "arxiv_comment": "9 pages, 2 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15188v2",
                "updated": "2025-04-22T04:22:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    4,
                    22,
                    9,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T15:57:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    57,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "Synergistic Weak-Strong Collaboration by Aligning Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergistic Weak-Strong Collaboration by Aligning Preferences"
                },
                "summary": "Current Large Language Models (LLMs) excel in general reasoning yet struggle\nwith specialized tasks requiring proprietary or domain-specific knowledge.\nFine-tuning large models for every niche application is often infeasible due to\nblack-box constraints and high computational overhead. To address this, we\npropose a collaborative framework that pairs a specialized weak model with a\ngeneral strong model. The weak model, tailored to specific domains, produces\ninitial drafts and background information, while the strong model leverages its\nadvanced reasoning to refine these drafts, extending LLMs' capabilities to\ncritical yet specialized tasks. To optimize this collaboration, we introduce a\ncollaborative feedback to fine-tunes the weak model, which quantifies the\ninfluence of the weak model's contributions in the collaboration procedure and\nestablishes preference pairs to guide preference tuning of the weak model. We\nvalidate our framework through experiments on three domains. We find that the\ncollaboration significantly outperforms each model alone by leveraging\ncomplementary strengths. Moreover, aligning the weak model with the\ncollaborative preference further enhances overall performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) excel in general reasoning yet struggle\nwith specialized tasks requiring proprietary or domain-specific knowledge.\nFine-tuning large models for every niche application is often infeasible due to\nblack-box constraints and high computational overhead. To address this, we\npropose a collaborative framework that pairs a specialized weak model with a\ngeneral strong model. The weak model, tailored to specific domains, produces\ninitial drafts and background information, while the strong model leverages its\nadvanced reasoning to refine these drafts, extending LLMs' capabilities to\ncritical yet specialized tasks. To optimize this collaboration, we introduce a\ncollaborative feedback to fine-tunes the weak model, which quantifies the\ninfluence of the weak model's contributions in the collaboration procedure and\nestablishes preference pairs to guide preference tuning of the weak model. We\nvalidate our framework through experiments on three domains. We find that the\ncollaboration significantly outperforms each model alone by leveraging\ncomplementary strengths. Moreover, aligning the weak model with the\ncollaborative preference further enhances overall performance."
                },
                "authors": [
                    {
                        "name": "Yizhu Jiao"
                    },
                    {
                        "name": "Xuchao Zhang"
                    },
                    {
                        "name": "Zhaoyang Wang"
                    },
                    {
                        "name": "Yubo Ma"
                    },
                    {
                        "name": "Zhun Deng"
                    },
                    {
                        "name": "Rujia Wang"
                    },
                    {
                        "name": "Chetan Bansal"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Huaxiu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Huaxiu Yao"
                },
                "author": "Huaxiu Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07728v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07728v4",
                "updated": "2025-04-21T15:51:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    51,
                    1,
                    0,
                    111,
                    0
                ],
                "published": "2024-03-12T15:07:20Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    15,
                    7,
                    20,
                    1,
                    72,
                    0
                ],
                "title": "CAP: A General Algorithm for Online Selective Conformal Prediction with\n  FCR Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAP: A General Algorithm for Online Selective Conformal Prediction with\n  FCR Control"
                },
                "summary": "We study the problem of post-selection predictive inference in an online\nfashion. To avoid devoting resources to unimportant units, a preliminary\nselection of the current individual before reporting its prediction interval is\ncommon and meaningful in online predictive tasks. Since the online selection\ncauses a temporal multiplicity in the selected prediction intervals, it is\nimportant to control the real-time false coverage-statement rate (FCR) which\nmeasures the overall miscoverage level. We develop a general framework named\nCAP (Calibration after Adaptive Pick) that performs an adaptive pick rule on\nhistorical data to construct a calibration set if the current individual is\nselected and then outputs a conformal prediction interval for the unobserved\nlabel. We provide tractable procedures for constructing the calibration set for\npopular online selection rules. We proved that CAP can achieve an exact\nselection-conditional coverage guarantee in the finite-sample and\ndistribution-free regimes. To account for the distribution shift in online\ndata, we also embed CAP into some recent dynamic conformal prediction\nalgorithms and show that the proposed method can deliver long-run FCR control.\nNumerical results on both synthetic and real data corroborate that CAP can\neffectively control FCR around the target level and yield more narrowed\nprediction intervals over existing baselines across various settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of post-selection predictive inference in an online\nfashion. To avoid devoting resources to unimportant units, a preliminary\nselection of the current individual before reporting its prediction interval is\ncommon and meaningful in online predictive tasks. Since the online selection\ncauses a temporal multiplicity in the selected prediction intervals, it is\nimportant to control the real-time false coverage-statement rate (FCR) which\nmeasures the overall miscoverage level. We develop a general framework named\nCAP (Calibration after Adaptive Pick) that performs an adaptive pick rule on\nhistorical data to construct a calibration set if the current individual is\nselected and then outputs a conformal prediction interval for the unobserved\nlabel. We provide tractable procedures for constructing the calibration set for\npopular online selection rules. We proved that CAP can achieve an exact\nselection-conditional coverage guarantee in the finite-sample and\ndistribution-free regimes. To account for the distribution shift in online\ndata, we also embed CAP into some recent dynamic conformal prediction\nalgorithms and show that the proposed method can deliver long-run FCR control.\nNumerical results on both synthetic and real data corroborate that CAP can\neffectively control FCR around the target level and yield more narrowed\nprediction intervals over existing baselines across various settings."
                },
                "authors": [
                    {
                        "name": "Yajie Bao"
                    },
                    {
                        "name": "Yuyang Huo"
                    },
                    {
                        "name": "Haojie Ren"
                    },
                    {
                        "name": "Changliang Zou"
                    }
                ],
                "author_detail": {
                    "name": "Changliang Zou"
                },
                "author": "Changliang Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07728v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07728v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15178v1",
                "updated": "2025-04-21T15:40:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    40,
                    10,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T15:40:10Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    40,
                    10,
                    0,
                    111,
                    0
                ],
                "title": "Time-Series Analysis on Edge-AI Hardware for Healthcare Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Series Analysis on Edge-AI Hardware for Healthcare Monitoring"
                },
                "summary": "This project addresses the need for efficient, real-time analysis of\nbiomedical signals such as electrocardiograms (ECG) and electroencephalograms\n(EEG) for continuous health monitoring. Traditional methods rely on\nlong-duration data recording followed by offline analysis, which is\npower-intensive and delays responses to critical symptoms such as arrhythmia.\nTo overcome these limitations, a time-domain ECG analysis model based on a\nnovel dynamically-biased Long Short-Term Memory (DB-LSTM) neural network is\nproposed. This model supports simultaneous ECG forecasting and classification\nwith high performance-achieving over 98% accuracy and a normalized mean square\nerror below 1e-3 for forecasting, and over 97% accuracy with faster convergence\nand fewer training parameters for classification. To enable edge deployment,\nthe model is hardware-optimized by quantizing weights to INT4 or INT3 formats,\nresulting in only a 2% and 6% drop in classification accuracy during training\nand inference, respectively, while maintaining full accuracy for forecasting.\nExtensive simulations using multiple ECG datasets confirm the model's\nrobustness. Future work includes implementing the algorithm on FPGA and CMOS\ncircuits for practical cardiac monitoring, as well as developing a digital\nhardware platform that supports flexible neural network configurations and\non-chip online training for personalized healthcare applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This project addresses the need for efficient, real-time analysis of\nbiomedical signals such as electrocardiograms (ECG) and electroencephalograms\n(EEG) for continuous health monitoring. Traditional methods rely on\nlong-duration data recording followed by offline analysis, which is\npower-intensive and delays responses to critical symptoms such as arrhythmia.\nTo overcome these limitations, a time-domain ECG analysis model based on a\nnovel dynamically-biased Long Short-Term Memory (DB-LSTM) neural network is\nproposed. This model supports simultaneous ECG forecasting and classification\nwith high performance-achieving over 98% accuracy and a normalized mean square\nerror below 1e-3 for forecasting, and over 97% accuracy with faster convergence\nand fewer training parameters for classification. To enable edge deployment,\nthe model is hardware-optimized by quantizing weights to INT4 or INT3 formats,\nresulting in only a 2% and 6% drop in classification accuracy during training\nand inference, respectively, while maintaining full accuracy for forecasting.\nExtensive simulations using multiple ECG datasets confirm the model's\nrobustness. Future work includes implementing the algorithm on FPGA and CMOS\ncircuits for practical cardiac monitoring, as well as developing a digital\nhardware platform that supports flexible neural network configurations and\non-chip online training for personalized healthcare applications."
                },
                "authors": [
                    {
                        "name": "Jinhai Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jinhai Hu"
                },
                "author": "Jinhai Hu",
                "arxiv_comment": "38 pages, 20 figures, Progress report for qualification cum PhD\n  confirmation exercise",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11201v2",
                "updated": "2025-04-21T15:37:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    37,
                    50,
                    0,
                    111,
                    0
                ],
                "published": "2024-10-15T02:37:39Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    2,
                    37,
                    39,
                    1,
                    289,
                    0
                ],
                "title": "Tree of Attributes Prompt Learning for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Attributes Prompt Learning for Vision-Language Models"
                },
                "summary": "Prompt learning has proven effective in adapting vision language models for\ndownstream tasks. However, existing methods usually append learnable prompt\ntokens solely with the category names to obtain textual features, which fails\nto fully leverage the rich context indicated in the category name. To address\nthis issue, we propose the Tree of Attributes Prompt learning (TAP), which\nfirst instructs LLMs to generate a tree of attributes with a \"concept -\nattribute - description\" structure for each category, and then learn the\nhierarchy with vision and text prompt tokens. Unlike existing methods that\nmerely augment category names with a set of unstructured descriptions, our\napproach essentially distills structured knowledge graphs associated with class\nnames from LLMs. Furthermore, our approach introduces text and vision prompts\ndesigned to explicitly learn the corresponding visual attributes, effectively\nserving as domain experts. Additionally, the general and diverse descriptions\ngenerated based on the class names may be wrong or absent in the specific given\nimages. To address this misalignment, we further introduce a vision-conditional\npooling module to extract instance-specific text features. Extensive\nexperimental results demonstrate that our approach outperforms state-of-the-art\nmethods on the zero-shot base-to-novel generalization, cross-dataset transfer,\nas well as few-shot classification across 11 diverse datasets. Code is\navailable at https://github.com/HHenryD/TAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt learning has proven effective in adapting vision language models for\ndownstream tasks. However, existing methods usually append learnable prompt\ntokens solely with the category names to obtain textual features, which fails\nto fully leverage the rich context indicated in the category name. To address\nthis issue, we propose the Tree of Attributes Prompt learning (TAP), which\nfirst instructs LLMs to generate a tree of attributes with a \"concept -\nattribute - description\" structure for each category, and then learn the\nhierarchy with vision and text prompt tokens. Unlike existing methods that\nmerely augment category names with a set of unstructured descriptions, our\napproach essentially distills structured knowledge graphs associated with class\nnames from LLMs. Furthermore, our approach introduces text and vision prompts\ndesigned to explicitly learn the corresponding visual attributes, effectively\nserving as domain experts. Additionally, the general and diverse descriptions\ngenerated based on the class names may be wrong or absent in the specific given\nimages. To address this misalignment, we further introduce a vision-conditional\npooling module to extract instance-specific text features. Extensive\nexperimental results demonstrate that our approach outperforms state-of-the-art\nmethods on the zero-shot base-to-novel generalization, cross-dataset transfer,\nas well as few-shot classification across 11 diverse datasets. Code is\navailable at https://github.com/HHenryD/TAP."
                },
                "authors": [
                    {
                        "name": "Tong Ding"
                    },
                    {
                        "name": "Wanhua Li"
                    },
                    {
                        "name": "Zhongqi Miao"
                    },
                    {
                        "name": "Hanspeter Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Hanspeter Pfister"
                },
                "author": "Hanspeter Pfister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06560v2",
                "updated": "2025-04-21T15:37:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    37,
                    15,
                    0,
                    111,
                    0
                ],
                "published": "2024-06-02T11:54:50Z",
                "published_parsed": [
                    2024,
                    6,
                    2,
                    11,
                    54,
                    50,
                    6,
                    154,
                    0
                ],
                "title": "Inverse Constitutional AI: Compressing Preferences into Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse Constitutional AI: Compressing Preferences into Principles"
                },
                "summary": "Feedback data is widely used for fine-tuning and evaluating state-of-the-art\nAI models. Pairwise text preferences, where human or AI annotators select the\n\"better\" of two options, are particularly common. Such preferences are used to\ntrain (reward) models or to rank models with aggregate statistics. For many\napplications it is desirable to understand annotator preferences in addition to\nmodelling them - not least because extensive prior work has shown various\nunintended biases in preference datasets. Yet, preference datasets remain\nchallenging to interpret. Neither black-box reward models nor statistics can\nanswer why one text is preferred over another. Manual interpretation of the\nnumerous (long) response pairs is usually equally infeasible. In this paper, we\nintroduce the Inverse Constitutional AI (ICAI) problem, formulating the\ninterpretation of pairwise text preference data as a compression task. In\nconstitutional AI, a set of principles (a constitution) is used to provide\nfeedback and fine-tune AI models. ICAI inverts this process: given a feedback\ndataset, we aim to extract a constitution that best enables a large language\nmodel (LLM) to reconstruct the original annotations. We propose a corresponding\nICAI algorithm and validate its generated constitutions quantitatively based on\nannotation reconstruction accuracy on several datasets: (a) synthetic feedback\ndata with known principles; (b) AlpacaEval cross-annotated human feedback data;\n(c) crowdsourced Chatbot Arena data; and (d) PRISM data from diverse\ndemographic groups. As a short and interpretable representation of the original\ndataset, generated constitutions have many potential use cases: help identify\nundesirable annotator biases, understand model performance better, scale\nfeedback to unseen data, or adapt models to individual user or group\npreferences. We release the source code at https://github.com/rdnfn/icai.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feedback data is widely used for fine-tuning and evaluating state-of-the-art\nAI models. Pairwise text preferences, where human or AI annotators select the\n\"better\" of two options, are particularly common. Such preferences are used to\ntrain (reward) models or to rank models with aggregate statistics. For many\napplications it is desirable to understand annotator preferences in addition to\nmodelling them - not least because extensive prior work has shown various\nunintended biases in preference datasets. Yet, preference datasets remain\nchallenging to interpret. Neither black-box reward models nor statistics can\nanswer why one text is preferred over another. Manual interpretation of the\nnumerous (long) response pairs is usually equally infeasible. In this paper, we\nintroduce the Inverse Constitutional AI (ICAI) problem, formulating the\ninterpretation of pairwise text preference data as a compression task. In\nconstitutional AI, a set of principles (a constitution) is used to provide\nfeedback and fine-tune AI models. ICAI inverts this process: given a feedback\ndataset, we aim to extract a constitution that best enables a large language\nmodel (LLM) to reconstruct the original annotations. We propose a corresponding\nICAI algorithm and validate its generated constitutions quantitatively based on\nannotation reconstruction accuracy on several datasets: (a) synthetic feedback\ndata with known principles; (b) AlpacaEval cross-annotated human feedback data;\n(c) crowdsourced Chatbot Arena data; and (d) PRISM data from diverse\ndemographic groups. As a short and interpretable representation of the original\ndataset, generated constitutions have many potential use cases: help identify\nundesirable annotator biases, understand model performance better, scale\nfeedback to unseen data, or adapt models to individual user or group\npreferences. We release the source code at https://github.com/rdnfn/icai."
                },
                "authors": [
                    {
                        "name": "Arduin Findeis"
                    },
                    {
                        "name": "Timo Kaufmann"
                    },
                    {
                        "name": "Eyke Hüllermeier"
                    },
                    {
                        "name": "Samuel Albanie"
                    },
                    {
                        "name": "Robert Mullins"
                    }
                ],
                "author_detail": {
                    "name": "Robert Mullins"
                },
                "author": "Robert Mullins",
                "arxiv_comment": "Accepted at ICLR 2025, v2 is camera-ready version; Main changes from\n  v1: extended experiments, additional baselines",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09597v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09597v3",
                "updated": "2025-04-21T15:18:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    18,
                    42,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-13T14:31:52Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    31,
                    52,
                    6,
                    103,
                    0
                ],
                "title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge\n  Acquisition and Scaling Laws",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge\n  Acquisition and Scaling Laws"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions."
                },
                "authors": [
                    {
                        "name": "Zhixuan Pan"
                    },
                    {
                        "name": "Shaowen Wang"
                    },
                    {
                        "name": "Jian Li"
                    }
                ],
                "author_detail": {
                    "name": "Jian Li"
                },
                "author": "Jian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09597v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09597v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15166v1",
                "updated": "2025-04-21T15:16:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    16,
                    30,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T15:16:30Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    16,
                    30,
                    0,
                    111,
                    0
                ],
                "title": "Simulating biochemical reactions: The Linear Noise Approximation can\n  capture non-linear dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating biochemical reactions: The Linear Noise Approximation can\n  capture non-linear dynamics"
                },
                "summary": "There is a plethora of highly stochastic non-linear dynamical systems in\nfields such as molecular biology, chemistry, epidemiology, and ecology. Yet,\nnone of the currently available stochastic models are both accurate and\ncomputationally efficient for long-term predictions of large systems. The\nLinear Noise Approximation (LNA) model for biochemical reaction networks is\nanalytically tractable, which makes it computationally efficient for\nsimulation, analysis, and inference. However, it is only accurate for linear\nsystems and short-time transitions. Other methods can achieve greater accuracy\nacross a wider range of systems, including non-linear ones, but lack analytical\ntractability. This paper seeks to challenge the prevailing view by\ndemonstrating that the Linear Noise Approximation can indeed capture non-linear\ndynamics after certain modifications. We introduce a new framework that\nutilises centre manifold theory allowing us to identify simple interventions to\nthe LNA that do not significantly compromise its computational efficiency. We\ndevelop specific algorithms for systems that exhibit oscillations or\nbi-stability and demonstrate their accuracy and computational efficiency across\nmultiple examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a plethora of highly stochastic non-linear dynamical systems in\nfields such as molecular biology, chemistry, epidemiology, and ecology. Yet,\nnone of the currently available stochastic models are both accurate and\ncomputationally efficient for long-term predictions of large systems. The\nLinear Noise Approximation (LNA) model for biochemical reaction networks is\nanalytically tractable, which makes it computationally efficient for\nsimulation, analysis, and inference. However, it is only accurate for linear\nsystems and short-time transitions. Other methods can achieve greater accuracy\nacross a wider range of systems, including non-linear ones, but lack analytical\ntractability. This paper seeks to challenge the prevailing view by\ndemonstrating that the Linear Noise Approximation can indeed capture non-linear\ndynamics after certain modifications. We introduce a new framework that\nutilises centre manifold theory allowing us to identify simple interventions to\nthe LNA that do not significantly compromise its computational efficiency. We\ndevelop specific algorithms for systems that exhibit oscillations or\nbi-stability and demonstrate their accuracy and computational efficiency across\nmultiple examples."
                },
                "authors": [
                    {
                        "name": "Frederick Truman-Williams"
                    },
                    {
                        "name": "Giorgos Minas"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Minas"
                },
                "author": "Giorgos Minas",
                "arxiv_comment": "37 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92C40, 82C31 (Primary) 60J20, 92C42 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v2",
                "updated": "2025-04-21T15:13:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    13,
                    44,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10168v2",
                "updated": "2025-04-21T15:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    10,
                    2,
                    0,
                    111,
                    0
                ],
                "published": "2024-06-14T16:41:07Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    16,
                    41,
                    7,
                    4,
                    166,
                    0
                ],
                "title": "Study of the Coulomb and nuclear breakup of $^{11}$Be using a Halo-EFT\n  description at N$^2$LO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of the Coulomb and nuclear breakup of $^{11}$Be using a Halo-EFT\n  description at N$^2$LO"
                },
                "summary": "Background: The halo effective field theory (Halo-EFT) provides a very\nefficient description of loosely-bound nuclei in models of reaction. It offers\na very systematical ranking of the significance of nuclear-structure\nobservables in reaction calculations. This greatly helps to infer reliable\nstructure information from reaction cross sections. However, for a meaningful\nanalysis, the Halo-EFT scheme needs to have converged. Purpose: In a previous\nstudy [P. Capel, D. R. Phillips, and H.-W. Hammer, Phys. Rev. C 98, 034610\n(2018)], NLO descriptions of 11Be have been developed and lead to excellent\nagreement with existing breakup data. However, the convergence of the scheme at\nNLO was not fully demonstrated. Moreover, a significant dependence on the\nregulator of the effective 10Be-n interaction has been observed. Method: We\ndevelop Halo-EFT descriptions of 11Be at N2LO and use them in an accurate\nbreakup-reaction code. We compare our theoretical cross sections with\nexperiment on Pb and C targets at about 70 MeV/nucleon. Results: On Pb, the\nN2LO descriptions of 11Be lead to little change to the NLO results of the\nprevious study, confirming the convergence of that scheme. On C, the reaction\nis significantly affected by the presence of d resonances in the low-energy\nspectrum of 11Be. In the Halo-EFT power counting these resonances appear only\nat N2LO; our new descriptions include them naturally. Going to N2LO removes\nalso the cutoff dependence observed in the previous study. Conclusions: We\ndemonstrate the convergence of the Halo-EFT description of 11Be at NLO for\nCoulomb breakup and at N2LO for nuclear-dominated dissociation. The reliability\nof the nuclear-structure information inferred in the previous study is thus\nconfirmed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: The halo effective field theory (Halo-EFT) provides a very\nefficient description of loosely-bound nuclei in models of reaction. It offers\na very systematical ranking of the significance of nuclear-structure\nobservables in reaction calculations. This greatly helps to infer reliable\nstructure information from reaction cross sections. However, for a meaningful\nanalysis, the Halo-EFT scheme needs to have converged. Purpose: In a previous\nstudy [P. Capel, D. R. Phillips, and H.-W. Hammer, Phys. Rev. C 98, 034610\n(2018)], NLO descriptions of 11Be have been developed and lead to excellent\nagreement with existing breakup data. However, the convergence of the scheme at\nNLO was not fully demonstrated. Moreover, a significant dependence on the\nregulator of the effective 10Be-n interaction has been observed. Method: We\ndevelop Halo-EFT descriptions of 11Be at N2LO and use them in an accurate\nbreakup-reaction code. We compare our theoretical cross sections with\nexperiment on Pb and C targets at about 70 MeV/nucleon. Results: On Pb, the\nN2LO descriptions of 11Be lead to little change to the NLO results of the\nprevious study, confirming the convergence of that scheme. On C, the reaction\nis significantly affected by the presence of d resonances in the low-energy\nspectrum of 11Be. In the Halo-EFT power counting these resonances appear only\nat N2LO; our new descriptions include them naturally. Going to N2LO removes\nalso the cutoff dependence observed in the previous study. Conclusions: We\ndemonstrate the convergence of the Halo-EFT description of 11Be at NLO for\nCoulomb breakup and at N2LO for nuclear-dominated dissociation. The reliability\nof the nuclear-structure information inferred in the previous study is thus\nconfirmed."
                },
                "authors": [
                    {
                        "name": "L. -P. Kubushishi"
                    },
                    {
                        "name": "P. Capel"
                    }
                ],
                "author_detail": {
                    "name": "P. Capel"
                },
                "author": "P. Capel",
                "arxiv_comment": "9 pages, 7 figures - This work has been accepted for publication in\n  Physical Review C",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15162v1",
                "updated": "2025-04-21T15:08:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    8,
                    1,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T15:08:01Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    8,
                    1,
                    0,
                    111,
                    0
                ],
                "title": "To Offload or Not To Offload: Model-driven Comparison of Edge-native and\n  On-device Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Offload or Not To Offload: Model-driven Comparison of Edge-native and\n  On-device Processing"
                },
                "summary": "Computational offloading is a promising approach for overcoming resource\nconstraints on client devices by moving some or all of an application's\ncomputations to remote servers. With the advent of specialized hardware\naccelerators, client devices are now able to perform fast local processing of\nspecific tasks, such as machine learning inference, reducing the need for\noffloading computations. However, edge servers with accelerators also offer\nfaster processing for offloaded tasks than was previously possible. In this\npaper, we present an analytic and experimental comparison of on-device\nprocessing and edge offloading for a range of accelerator, network, and\napplication workload scenarios, with the goal of understanding when to use\nlocal on-device processing and when to offload computations. We present models\nthat leverage analytical queuing results to capture the effects of dynamic\nfactors such as the performance gap between the device and edge server, network\nvariability, server load, and multi-tenancy on the edge server. We\nexperimentally demonstrate the accuracy of our models for a range of hardware\nand application scenarios and show that our models achieve a mean absolute\npercentage error of 2.2% compared to observed latencies. We use our models to\ndevelop an adaptive resource manager for intelligent offloading and show its\nefficacy in the presence of variable network conditions and dynamic\nmulti-tenant edge settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational offloading is a promising approach for overcoming resource\nconstraints on client devices by moving some or all of an application's\ncomputations to remote servers. With the advent of specialized hardware\naccelerators, client devices are now able to perform fast local processing of\nspecific tasks, such as machine learning inference, reducing the need for\noffloading computations. However, edge servers with accelerators also offer\nfaster processing for offloaded tasks than was previously possible. In this\npaper, we present an analytic and experimental comparison of on-device\nprocessing and edge offloading for a range of accelerator, network, and\napplication workload scenarios, with the goal of understanding when to use\nlocal on-device processing and when to offload computations. We present models\nthat leverage analytical queuing results to capture the effects of dynamic\nfactors such as the performance gap between the device and edge server, network\nvariability, server load, and multi-tenancy on the edge server. We\nexperimentally demonstrate the accuracy of our models for a range of hardware\nand application scenarios and show that our models achieve a mean absolute\npercentage error of 2.2% compared to observed latencies. We use our models to\ndevelop an adaptive resource manager for intelligent offloading and show its\nefficacy in the presence of variable network conditions and dynamic\nmulti-tenant edge settings."
                },
                "authors": [
                    {
                        "name": "Nathan Ng"
                    },
                    {
                        "name": "David Irwin"
                    },
                    {
                        "name": "Ananthram Swami"
                    },
                    {
                        "name": "Don Towsley"
                    },
                    {
                        "name": "Prashant Shenoy"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Shenoy"
                },
                "author": "Prashant Shenoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15160v1",
                "updated": "2025-04-21T15:07:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    7,
                    26,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T15:07:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    7,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "The Synthetic Imputation Approach: Generating Optimal Synthetic Texts\n  For Underrepresented Categories In Supervised Classification Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Synthetic Imputation Approach: Generating Optimal Synthetic Texts\n  For Underrepresented Categories In Supervised Classification Tasks"
                },
                "summary": "Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa,\nrequire that all categories in an annotation task be sufficiently represented\nin the training data for optimal performance. However, it is often difficult to\nfind sufficient examples for all categories in a task when building a\nhigh-quality training set. In this article, I describe this problem and propose\na solution, the synthetic imputation approach. Leveraging a generative LLM\n(GPT-4o), this approach generates synthetic texts based on careful prompting\nand five original examples drawn randomly with replacement from the sample.\nThis approach ensures that new synthetic texts are sufficiently different from\nthe original texts to reduce overfitting, but retain the underlying substantive\nmeaning of the examples to maximize out-of-sample performance. With 75 original\nexamples or more, synthetic imputation's performance is on par with a full\nsample of original texts, and overfitting remains low, predictable and\ncorrectable with 50 original samples. The synthetic imputation approach\nprovides a novel role for generative LLMs in research and allows applied\nresearchers to balance their datasets for best performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa,\nrequire that all categories in an annotation task be sufficiently represented\nin the training data for optimal performance. However, it is often difficult to\nfind sufficient examples for all categories in a task when building a\nhigh-quality training set. In this article, I describe this problem and propose\na solution, the synthetic imputation approach. Leveraging a generative LLM\n(GPT-4o), this approach generates synthetic texts based on careful prompting\nand five original examples drawn randomly with replacement from the sample.\nThis approach ensures that new synthetic texts are sufficiently different from\nthe original texts to reduce overfitting, but retain the underlying substantive\nmeaning of the examples to maximize out-of-sample performance. With 75 original\nexamples or more, synthetic imputation's performance is on par with a full\nsample of original texts, and overfitting remains low, predictable and\ncorrectable with 50 original samples. The synthetic imputation approach\nprovides a novel role for generative LLMs in research and allows applied\nresearchers to balance their datasets for best performance."
                },
                "authors": [
                    {
                        "name": "Joan C. Timoneda"
                    }
                ],
                "author_detail": {
                    "name": "Joan C. Timoneda"
                },
                "author": "Joan C. Timoneda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15149v1",
                "updated": "2025-04-21T14:52:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    52,
                    56,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T14:52:56Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    52,
                    56,
                    0,
                    111,
                    0
                ],
                "title": "Cosmological Constraints with Void Lensing I: the Simulation-Based\n  Inference Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmological Constraints with Void Lensing I: the Simulation-Based\n  Inference Framework"
                },
                "summary": "We present a Simulation-Based Inference (SBI) framework for cosmological\nparameter estimation via void lensing analysis. Despite the absence of an\nanalytical model of void lensing, SBI can effectively learn posterior\ndistributions through forward modeling of mock data. We develop a forward\nmodeling pipeline that accounts for both cosmology and the galaxy-halo\nconnection. By training a neural density estimator on simulated data, we infer\nthe posteriors of two cosmological parameters, $\\Omega_m$ and $S_8$. Validation\ntests are conducted on posteriors derived from different cosmological\nparameters and a fiducial sample. The results demonstrate that SBI provides\nunbiased estimates of mean values and accurate uncertainties. These findings\nhighlight the potential to apply void lensing analysis to observational data\neven without an analytical void lensing model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a Simulation-Based Inference (SBI) framework for cosmological\nparameter estimation via void lensing analysis. Despite the absence of an\nanalytical model of void lensing, SBI can effectively learn posterior\ndistributions through forward modeling of mock data. We develop a forward\nmodeling pipeline that accounts for both cosmology and the galaxy-halo\nconnection. By training a neural density estimator on simulated data, we infer\nthe posteriors of two cosmological parameters, $\\Omega_m$ and $S_8$. Validation\ntests are conducted on posteriors derived from different cosmological\nparameters and a fiducial sample. The results demonstrate that SBI provides\nunbiased estimates of mean values and accurate uncertainties. These findings\nhighlight the potential to apply void lensing analysis to observational data\neven without an analytical void lensing model."
                },
                "authors": [
                    {
                        "name": "Chen Su"
                    },
                    {
                        "name": "Huanyuan Shan"
                    },
                    {
                        "name": "Cheng Zhao"
                    },
                    {
                        "name": "Wenshuo Xu"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "11 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10733v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10733v7",
                "updated": "2025-04-21T14:48:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    48,
                    23,
                    0,
                    111,
                    0
                ],
                "published": "2024-10-14T17:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    15,
                    7,
                    0,
                    288,
                    0
                ],
                "title": "Deep Compression Autoencoder for Efficient High-Resolution Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Compression Autoencoder for Efficient High-Resolution Diffusion\n  Models"
                },
                "summary": "We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder\nmodels for accelerating high-resolution diffusion models. Existing autoencoder\nmodels have demonstrated impressive results at a moderate spatial compression\nratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for\nhigh spatial compression ratios (e.g., 64x). We address this challenge by\nintroducing two key techniques: (1) Residual Autoencoding, where we design our\nmodels to learn residuals based on the space-to-channel transformed features to\nalleviate the optimization difficulty of high spatial-compression autoencoders;\n(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases\ntraining strategy for mitigating the generalization penalty of high\nspatial-compression autoencoders. With these designs, we improve the\nautoencoder's spatial compression ratio up to 128 while maintaining the\nreconstruction quality. Applying our DC-AE to latent diffusion models, we\nachieve significant speedup without accuracy drop. For example, on ImageNet\n512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup\non H100 GPU for UViT-H while achieving a better FID, compared with the widely\nused SD-VAE-f8 autoencoder. Our code is available at\nhttps://github.com/mit-han-lab/efficientvit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder\nmodels for accelerating high-resolution diffusion models. Existing autoencoder\nmodels have demonstrated impressive results at a moderate spatial compression\nratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for\nhigh spatial compression ratios (e.g., 64x). We address this challenge by\nintroducing two key techniques: (1) Residual Autoencoding, where we design our\nmodels to learn residuals based on the space-to-channel transformed features to\nalleviate the optimization difficulty of high spatial-compression autoencoders;\n(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases\ntraining strategy for mitigating the generalization penalty of high\nspatial-compression autoencoders. With these designs, we improve the\nautoencoder's spatial compression ratio up to 128 while maintaining the\nreconstruction quality. Applying our DC-AE to latent diffusion models, we\nachieve significant speedup without accuracy drop. For example, on ImageNet\n512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup\non H100 GPU for UViT-H while achieving a better FID, compared with the widely\nused SD-VAE-f8 autoencoder. Our code is available at\nhttps://github.com/mit-han-lab/efficientvit."
                },
                "authors": [
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICLR 2025. The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10733v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10733v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15138v1",
                "updated": "2025-04-21T14:40:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    40,
                    55,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T14:40:55Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    40,
                    55,
                    0,
                    111,
                    0
                ],
                "title": "Automatic Generation of Aerobatic Flight in Complex Environments via\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Generation of Aerobatic Flight in Complex Environments via\n  Diffusion Models"
                },
                "summary": "Performing striking aerobatic flight in complex environments demands manual\ndesigns of key maneuvers in advance, which is intricate and time-consuming as\nthe horizon of the trajectory performed becomes long. This paper presents a\nnovel framework that leverages diffusion models to automate and scale up\naerobatic trajectory generation. Our key innovation is the decomposition of\ncomplex maneuvers into aerobatic primitives, which are short frame sequences\nthat act as building blocks, featuring critical aerobatic behaviors for\ntractable trajectory synthesis. The model learns aerobatic primitives using\nhistorical trajectory observations as dynamic priors to ensure motion\ncontinuity, with additional conditional inputs (target waypoints and optional\naction constraints) integrated to enable user-editable trajectory generation.\nDuring model inference, classifier guidance is incorporated with batch sampling\nto achieve obstacle avoidance. Additionally, the generated outcomes are refined\nthrough post-processing with spatial-temporal trajectory optimization to ensure\ndynamical feasibility. Extensive simulations and real-world experiments have\nvalidated the key component designs of our method, demonstrating its\nfeasibility for deploying on real drones to achieve long-horizon aerobatic\nflight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performing striking aerobatic flight in complex environments demands manual\ndesigns of key maneuvers in advance, which is intricate and time-consuming as\nthe horizon of the trajectory performed becomes long. This paper presents a\nnovel framework that leverages diffusion models to automate and scale up\naerobatic trajectory generation. Our key innovation is the decomposition of\ncomplex maneuvers into aerobatic primitives, which are short frame sequences\nthat act as building blocks, featuring critical aerobatic behaviors for\ntractable trajectory synthesis. The model learns aerobatic primitives using\nhistorical trajectory observations as dynamic priors to ensure motion\ncontinuity, with additional conditional inputs (target waypoints and optional\naction constraints) integrated to enable user-editable trajectory generation.\nDuring model inference, classifier guidance is incorporated with batch sampling\nto achieve obstacle avoidance. Additionally, the generated outcomes are refined\nthrough post-processing with spatial-temporal trajectory optimization to ensure\ndynamical feasibility. Extensive simulations and real-world experiments have\nvalidated the key component designs of our method, demonstrating its\nfeasibility for deploying on real drones to achieve long-horizon aerobatic\nflight."
                },
                "authors": [
                    {
                        "name": "Yuhang Zhong"
                    },
                    {
                        "name": "Anke Zhao"
                    },
                    {
                        "name": "Tianyue Wu"
                    },
                    {
                        "name": "Tingrui Zhang"
                    },
                    {
                        "name": "Fei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Fei Gao"
                },
                "author": "Fei Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21060v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21060v2",
                "updated": "2025-04-21T14:37:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    37,
                    40,
                    0,
                    111,
                    0
                ],
                "published": "2024-10-28T14:18:32Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    18,
                    32,
                    0,
                    302,
                    0
                ],
                "title": "CTINexus: Automatic Cyber Threat Intelligence Knowledge Graph\n  Construction Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CTINexus: Automatic Cyber Threat Intelligence Knowledge Graph\n  Construction Using Large Language Models"
                },
                "summary": "Textual descriptions in cyber threat intelligence (CTI) reports, such as\nsecurity articles and news, are rich sources of knowledge about cyber threats,\ncrucial for organizations to stay informed about the rapidly evolving threat\nlandscape. However, current CTI knowledge extraction methods lack flexibility\nand generalizability, often resulting in inaccurate and incomplete knowledge\nextraction. Syntax parsing relies on fixed rules and dictionaries, while model\nfine-tuning requires large annotated datasets, making both paradigms\nchallenging to adapt to new threats and ontologies. To bridge the gap, we\npropose CTINexus, a novel framework leveraging optimized in-context learning\n(ICL) of large language models (LLMs) for data-efficient CTI knowledge\nextraction and high-quality cybersecurity knowledge graph (CSKG) construction.\nUnlike existing methods, CTINexus requires neither extensive data nor parameter\ntuning and can adapt to various ontologies with minimal annotated examples.\nThis is achieved through: (1) a carefully designed automatic prompt\nconstruction strategy with optimal demonstration retrieval for extracting a\nwide range of cybersecurity entities and relations; (2) a hierarchical entity\nalignment technique that canonicalizes the extracted knowledge and removes\nredundancy; (3) an long-distance relation prediction technique to further\ncomplete the CSKG with missing links. Our extensive evaluations using 150\nreal-world CTI reports collected from 10 platforms demonstrate that CTINexus\nsignificantly outperforms existing methods in constructing accurate and\ncomplete CSKG, highlighting its potential to transform CTI analysis with an\nefficient and adaptable solution for the dynamic threat landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual descriptions in cyber threat intelligence (CTI) reports, such as\nsecurity articles and news, are rich sources of knowledge about cyber threats,\ncrucial for organizations to stay informed about the rapidly evolving threat\nlandscape. However, current CTI knowledge extraction methods lack flexibility\nand generalizability, often resulting in inaccurate and incomplete knowledge\nextraction. Syntax parsing relies on fixed rules and dictionaries, while model\nfine-tuning requires large annotated datasets, making both paradigms\nchallenging to adapt to new threats and ontologies. To bridge the gap, we\npropose CTINexus, a novel framework leveraging optimized in-context learning\n(ICL) of large language models (LLMs) for data-efficient CTI knowledge\nextraction and high-quality cybersecurity knowledge graph (CSKG) construction.\nUnlike existing methods, CTINexus requires neither extensive data nor parameter\ntuning and can adapt to various ontologies with minimal annotated examples.\nThis is achieved through: (1) a carefully designed automatic prompt\nconstruction strategy with optimal demonstration retrieval for extracting a\nwide range of cybersecurity entities and relations; (2) a hierarchical entity\nalignment technique that canonicalizes the extracted knowledge and removes\nredundancy; (3) an long-distance relation prediction technique to further\ncomplete the CSKG with missing links. Our extensive evaluations using 150\nreal-world CTI reports collected from 10 platforms demonstrate that CTINexus\nsignificantly outperforms existing methods in constructing accurate and\ncomplete CSKG, highlighting its potential to transform CTI analysis with an\nefficient and adaptable solution for the dynamic threat landscape."
                },
                "authors": [
                    {
                        "name": "Yutong Cheng"
                    },
                    {
                        "name": "Osama Bajaber"
                    },
                    {
                        "name": "Saimon Amanuel Tsegai"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Peng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Peng Gao"
                },
                "author": "Peng Gao",
                "arxiv_comment": "Accepted at 2025 IEEE European Symposium on Security and Privacy\n  (Euro S&P)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21060v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21060v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15133v1",
                "updated": "2025-04-21T14:33:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    33,
                    55,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T14:33:55Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    33,
                    55,
                    0,
                    111,
                    0
                ],
                "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language\n  Models"
                },
                "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction."
                },
                "authors": [
                    {
                        "name": "Ziwen Xu"
                    },
                    {
                        "name": "Shuxun Wang"
                    },
                    {
                        "name": "Kewei Xu"
                    },
                    {
                        "name": "Haoming Xu"
                    },
                    {
                        "name": "Mengru Wang"
                    },
                    {
                        "name": "Xinle Deng"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Guozhou Zheng"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Work in progress. Demo:\n  https://zjunlp.github.io/project/EasyEdit2/video; code:\n  https://github.com/zjunlp/EasyEdit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15132v1",
                "updated": "2025-04-21T14:30:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    30,
                    16,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T14:30:16Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    30,
                    16,
                    0,
                    111,
                    0
                ],
                "title": "Investigating Youth's Technical and Ethical Understanding of Generative\n  Language Models When Engaging in Construction and Deconstruction Activities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Youth's Technical and Ethical Understanding of Generative\n  Language Models When Engaging in Construction and Deconstruction Activities"
                },
                "summary": "The widespread adoption of generative artificial intelligence/machine\nlearning (AI/ML) technologies has increased the need to support youth in\ndeveloping AI/ML literacies. However, most work has centered on preparing young\npeople to use these systems, with less attention to how they can participate in\ndesigning and evaluating them. This study investigates how engaging young\npeople in the design and auditing of generative language models (GLMs) may\nfoster the development of their understanding of how these systems work from\nboth technical and ethical perspectives. The study takes an in-pieces approach\nto investigate novices' conceptions of GLMs. Such an approach supports the\nanalysis of how technical and ethical conceptions evolve and relate to each\nother. I am currently conducting a series of participatory design workshops\nwith sixteen ninth graders (ages 14-15) in which they will (a) build GLMs from\na data-driven perspective that glassboxes how data shapes model performance and\n(b) audit commercial GLMs by repeatedly and systematically querying them to\ndraw inferences about their behaviors. I will analyze participants'\ninteractions to identify ethical and technical conceptions they may exhibit\nwhile designing and auditing GLMs. I will also conduct clinical interviews and\nuse microgenetic knowledge analysis and ordered network analysis to investigate\nhow participants' ethical and technical conceptions of GLMs relate to each\nother and change after the workshop. The study will contribute (a) evidence of\nhow engaging youth in design and auditing activities may support the\ndevelopment of ethical and technical understanding of GLMs and (b) an inventory\nof novice design and auditing practices that may support youth's technical and\nethical understanding of GLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of generative artificial intelligence/machine\nlearning (AI/ML) technologies has increased the need to support youth in\ndeveloping AI/ML literacies. However, most work has centered on preparing young\npeople to use these systems, with less attention to how they can participate in\ndesigning and evaluating them. This study investigates how engaging young\npeople in the design and auditing of generative language models (GLMs) may\nfoster the development of their understanding of how these systems work from\nboth technical and ethical perspectives. The study takes an in-pieces approach\nto investigate novices' conceptions of GLMs. Such an approach supports the\nanalysis of how technical and ethical conceptions evolve and relate to each\nother. I am currently conducting a series of participatory design workshops\nwith sixteen ninth graders (ages 14-15) in which they will (a) build GLMs from\na data-driven perspective that glassboxes how data shapes model performance and\n(b) audit commercial GLMs by repeatedly and systematically querying them to\ndraw inferences about their behaviors. I will analyze participants'\ninteractions to identify ethical and technical conceptions they may exhibit\nwhile designing and auditing GLMs. I will also conduct clinical interviews and\nuse microgenetic knowledge analysis and ordered network analysis to investigate\nhow participants' ethical and technical conceptions of GLMs relate to each\nother and change after the workshop. The study will contribute (a) evidence of\nhow engaging youth in design and auditing activities may support the\ndevelopment of ethical and technical understanding of GLMs and (b) an inventory\nof novice design and auditing practices that may support youth's technical and\nethical understanding of GLMs."
                },
                "authors": [
                    {
                        "name": "Luis Morales-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "Luis Morales-Navarro"
                },
                "author": "Luis Morales-Navarro",
                "arxiv_doi": "10.1145/3713043.3731602",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713043.3731602",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; K.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15125v1",
                "updated": "2025-04-21T14:20:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    20,
                    49,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T14:20:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    20,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "Contemplative Wisdom for Superalignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemplative Wisdom for Superalignment"
                },
                "summary": "As artificial intelligence (AI) improves, traditional alignment strategies\nmay falter in the face of unpredictable self-improvement, hidden subgoals, and\nthe sheer complexity of intelligent systems. Rather than externally\nconstraining behavior, we advocate designing AI with intrinsic morality built\ninto its cognitive architecture and world model. Inspired by contemplative\nwisdom traditions, we show how four axiomatic principles can instil a resilient\nWise World Model in AI systems. First, mindfulness enables self-monitoring and\nrecalibration of emergent subgoals. Second, emptiness forestalls dogmatic goal\nfixation and relaxes rigid priors. Third, non-duality dissolves adversarial\nself-other boundaries. Fourth, boundless care motivates the universal reduction\nof suffering. We find that prompting AI to reflect on these principles improves\nperformance on the AILuminate Benchmark using GPT-4o, particularly when\ncombined. We offer detailed implementation strategies for state-of-the-art\nmodels, including contemplative architectures, constitutions, and reinforcement\nof chain-of-thought. For future systems, the active inference framework may\noffer the self-organizing and dynamic coupling capabilities needed to enact\nthese insights in embodied agents. This interdisciplinary approach offers a\nself-correcting and resilient alternative to prevailing brittle control\nschemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As artificial intelligence (AI) improves, traditional alignment strategies\nmay falter in the face of unpredictable self-improvement, hidden subgoals, and\nthe sheer complexity of intelligent systems. Rather than externally\nconstraining behavior, we advocate designing AI with intrinsic morality built\ninto its cognitive architecture and world model. Inspired by contemplative\nwisdom traditions, we show how four axiomatic principles can instil a resilient\nWise World Model in AI systems. First, mindfulness enables self-monitoring and\nrecalibration of emergent subgoals. Second, emptiness forestalls dogmatic goal\nfixation and relaxes rigid priors. Third, non-duality dissolves adversarial\nself-other boundaries. Fourth, boundless care motivates the universal reduction\nof suffering. We find that prompting AI to reflect on these principles improves\nperformance on the AILuminate Benchmark using GPT-4o, particularly when\ncombined. We offer detailed implementation strategies for state-of-the-art\nmodels, including contemplative architectures, constitutions, and reinforcement\nof chain-of-thought. For future systems, the active inference framework may\noffer the self-organizing and dynamic coupling capabilities needed to enact\nthese insights in embodied agents. This interdisciplinary approach offers a\nself-correcting and resilient alternative to prevailing brittle control\nschemes."
                },
                "authors": [
                    {
                        "name": "Ruben Laukkonen"
                    },
                    {
                        "name": "Fionn Inglis"
                    },
                    {
                        "name": "Shamil Chandaria"
                    },
                    {
                        "name": "Lars Sandved-Smith"
                    },
                    {
                        "name": "Jakob Hohwy"
                    },
                    {
                        "name": "Jonathan Gold"
                    },
                    {
                        "name": "Adam Elwood"
                    }
                ],
                "author_detail": {
                    "name": "Adam Elwood"
                },
                "author": "Adam Elwood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15120v1",
                "updated": "2025-04-21T14:17:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    17,
                    25,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T14:17:25Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    17,
                    25,
                    0,
                    111,
                    0
                ],
                "title": "Kuwain 1.5B: An Arabic SLM via Language Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kuwain 1.5B: An Arabic SLM via Language Injection"
                },
                "summary": "Enhancing existing models with new knowledge is a crucial aspect of AI\ndevelopment. This paper introduces a novel method for integrating a new\nlanguage into a large language model (LLM). Our approach successfully\nincorporates a previously unseen target language into an existing LLM without\ncompromising its prior knowledge. We trained a tiny model with 1.5 billion\nparameters named Kuwain by injecting the Arabic language into a small\nopen-source model mainly trained in English. Our method demonstrates\nsignificant improvements in Arabic language performance, with an average 8%\nimprovement across various benchmarks, while retaining the model's existing\nknowledge with a minimum amount of the original model's data. This offers a\ncost-effective alternative to training a comprehensive model in both English\nand Arabic. The results highlight the potential for efficient, targeted\nlanguage model expansion without extensive retraining or resource-intensive\nprocesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing existing models with new knowledge is a crucial aspect of AI\ndevelopment. This paper introduces a novel method for integrating a new\nlanguage into a large language model (LLM). Our approach successfully\nincorporates a previously unseen target language into an existing LLM without\ncompromising its prior knowledge. We trained a tiny model with 1.5 billion\nparameters named Kuwain by injecting the Arabic language into a small\nopen-source model mainly trained in English. Our method demonstrates\nsignificant improvements in Arabic language performance, with an average 8%\nimprovement across various benchmarks, while retaining the model's existing\nknowledge with a minimum amount of the original model's data. This offers a\ncost-effective alternative to training a comprehensive model in both English\nand Arabic. The results highlight the potential for efficient, targeted\nlanguage model expansion without extensive retraining or resource-intensive\nprocesses."
                },
                "authors": [
                    {
                        "name": "Khalil Hennara"
                    },
                    {
                        "name": "Sara Chrouf"
                    },
                    {
                        "name": "Mohamed Motaism Hamed"
                    },
                    {
                        "name": "Zeina Aldallal"
                    },
                    {
                        "name": "Omar Hadid"
                    },
                    {
                        "name": "Safwan AlModhayan"
                    }
                ],
                "author_detail": {
                    "name": "Safwan AlModhayan"
                },
                "author": "Safwan AlModhayan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15093v1",
                "updated": "2025-04-21T13:25:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    13,
                    25,
                    55,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T13:25:55Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    13,
                    25,
                    55,
                    0,
                    111,
                    0
                ],
                "title": "Rethinking the Potential of Multimodality in Collaborative Problem\n  Solving Diagnosis with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Potential of Multimodality in Collaborative Problem\n  Solving Diagnosis with Large Language Models"
                },
                "summary": "Detecting collaborative and problem-solving behaviours from digital traces to\ninterpret students' collaborative problem solving (CPS) competency is a\nlong-term goal in the Artificial Intelligence in Education (AIEd) field.\nAlthough multimodal data and advanced models are argued to have the potential\nto detect complex CPS behaviours, empirical evidence on their value remains\nlimited with some contrasting evidence. In this study, we investigated the\npotential of multimodal data to improve model performance in diagnosing 78\nsecondary school students' CPS subskills and indicators in authentic\neducational settings. In particular, text embeddings from verbal data and\nacoustic embeddings from audio data were used in a multimodal classification\nmodel for CPS diagnosis. Both unimodal and multimodal transformer-based models\noutperformed traditional models in detecting CPS classes. Although the\ninclusion of multimodality did not improve the performance of traditional\nunimodal models, its integration into transformer-based models demonstrated\nimproved performance for diagnosing social-cognitive CPS classes compared to\nunimodal transformer-based models. Based on the results, the paper argues that\nmultimodality and the selection of a particular modelling technique should not\nbe taken for granted to achieve the best performance in the automated detection\nof every CPS subskill and indicator. Rather, their value is limited to certain\ntypes of CPS indicators, affected by the complexity of the labels, and\ndependent on the composition of indicators in the dataset. We conclude the\npaper by discussing the required nuance when considering the value of LLMs and\nmultimodality in automated CPS diagnosis, highlighting the need for human-AI\ncomplementarity, and proposing the exploration of relevant model architectures\nand techniques to improve CPS diagnosis in authentic educational contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting collaborative and problem-solving behaviours from digital traces to\ninterpret students' collaborative problem solving (CPS) competency is a\nlong-term goal in the Artificial Intelligence in Education (AIEd) field.\nAlthough multimodal data and advanced models are argued to have the potential\nto detect complex CPS behaviours, empirical evidence on their value remains\nlimited with some contrasting evidence. In this study, we investigated the\npotential of multimodal data to improve model performance in diagnosing 78\nsecondary school students' CPS subskills and indicators in authentic\neducational settings. In particular, text embeddings from verbal data and\nacoustic embeddings from audio data were used in a multimodal classification\nmodel for CPS diagnosis. Both unimodal and multimodal transformer-based models\noutperformed traditional models in detecting CPS classes. Although the\ninclusion of multimodality did not improve the performance of traditional\nunimodal models, its integration into transformer-based models demonstrated\nimproved performance for diagnosing social-cognitive CPS classes compared to\nunimodal transformer-based models. Based on the results, the paper argues that\nmultimodality and the selection of a particular modelling technique should not\nbe taken for granted to achieve the best performance in the automated detection\nof every CPS subskill and indicator. Rather, their value is limited to certain\ntypes of CPS indicators, affected by the complexity of the labels, and\ndependent on the composition of indicators in the dataset. We conclude the\npaper by discussing the required nuance when considering the value of LLMs and\nmultimodality in automated CPS diagnosis, highlighting the need for human-AI\ncomplementarity, and proposing the exploration of relevant model architectures\nand techniques to improve CPS diagnosis in authentic educational contexts."
                },
                "authors": [
                    {
                        "name": "K. Wong"
                    },
                    {
                        "name": "B. Wu"
                    },
                    {
                        "name": "S. Bulathwela"
                    },
                    {
                        "name": "M. Cukurova"
                    }
                ],
                "author_detail": {
                    "name": "M. Cukurova"
                },
                "author": "M. Cukurova",
                "arxiv_comment": "Accepted for 26th International Conference on Artificial Intelligence\n  in Education (AIED 2025), 22 - 26 July 2025, Palermo, Italy. 17 pages, 1\n  figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15080v1",
                "updated": "2025-04-21T13:09:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    13,
                    9,
                    25,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T13:09:25Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    13,
                    9,
                    25,
                    0,
                    111,
                    0
                ],
                "title": "Empowering AI to Generate Better AI Code: Guided Generation of Deep\n  Learning Projects with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering AI to Generate Better AI Code: Guided Generation of Deep\n  Learning Projects with LLMs"
                },
                "summary": "While large language models (LLMs) have been widely applied to code\ngeneration, they struggle with generating entire deep learning projects, which\nare characterized by complex structures, longer functions, and stronger\nreliance on domain knowledge than general-purpose code. An open-domain LLM\noften lacks coherent contextual guidance and domain expertise for specific\nprojects, making it challenging to produce complete code that fully meets user\nrequirements.\n  In this paper, we propose a novel planning-guided code generation method,\nDLCodeGen, tailored for generating deep learning projects. DLCodeGen predicts a\nstructured solution plan, offering global guidance for LLMs to generate the\nproject. The generated plan is then leveraged to retrieve semantically\nanalogous code samples and subsequently abstract a code template. To\neffectively integrate these multiple retrieval-augmented techniques, a\ncomparative learning mechanism is designed to generate the final code. We\nvalidate the effectiveness of our approach on a dataset we build for deep\nlearning code generation. Experimental results demonstrate that DLCodeGen\noutperforms other baselines, achieving improvements of 9.7% in CodeBLEU and\n3.6% in human evaluation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have been widely applied to code\ngeneration, they struggle with generating entire deep learning projects, which\nare characterized by complex structures, longer functions, and stronger\nreliance on domain knowledge than general-purpose code. An open-domain LLM\noften lacks coherent contextual guidance and domain expertise for specific\nprojects, making it challenging to produce complete code that fully meets user\nrequirements.\n  In this paper, we propose a novel planning-guided code generation method,\nDLCodeGen, tailored for generating deep learning projects. DLCodeGen predicts a\nstructured solution plan, offering global guidance for LLMs to generate the\nproject. The generated plan is then leveraged to retrieve semantically\nanalogous code samples and subsequently abstract a code template. To\neffectively integrate these multiple retrieval-augmented techniques, a\ncomparative learning mechanism is designed to generate the final code. We\nvalidate the effectiveness of our approach on a dataset we build for deep\nlearning code generation. Experimental results demonstrate that DLCodeGen\noutperforms other baselines, achieving improvements of 9.7% in CodeBLEU and\n3.6% in human evaluation metrics."
                },
                "authors": [
                    {
                        "name": "Chen Xie"
                    },
                    {
                        "name": "Mingsheng Jiao"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Beijun Shen"
                    }
                ],
                "author_detail": {
                    "name": "Beijun Shen"
                },
                "author": "Beijun Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15077v1",
                "updated": "2025-04-21T13:05:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    13,
                    5,
                    26,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T13:05:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    13,
                    5,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL"
                },
                "summary": "Large Language Models (LLMs) have shown impressive capabilities in\ntransforming natural language questions about relational databases into SQL\nqueries. Despite recent improvements, small LLMs struggle to handle questions\ninvolving multiple tables and complex SQL patterns under a Zero-Shot Learning\n(ZSL) setting. Supervised Fine-Tuning (SFT) partially compensate the knowledge\ndeficits in pretrained models but falls short while dealing with queries\ninvolving multi-hop reasoning. To bridge this gap, different LLM training\nstrategies to reinforce reasoning capabilities have been proposed, ranging from\nleveraging a thinking process within ZSL, including reasoning traces in SFT, or\nadopt Reinforcement Learning (RL) strategies. However, the influence of\nreasoning on Text2SQL performance is still largely unexplored. This paper\ninvestigates to what extent LLM reasoning capabilities influence their Text2SQL\nperformance on four benchmark datasets. To this end, it considers the following\nLLM settings: (1) ZSL, including general-purpose reasoning or not; (2) SFT,\nwith and without task-specific reasoning traces; (3) RL, leveraging execution\naccuracy as primary reward function; (4) SFT+RL, i.e, a two-stage approach that\ncombines SFT and RL. The results show that general-purpose reasoning under ZSL\nproves to be ineffective in tackling complex Text2SQL cases. Small LLMs benefit\nfrom SFT with reasoning much more than larger ones, bridging the gap of their\n(weaker) model pretraining. RL is generally beneficial across all tested models\nand datasets, particularly when SQL queries involve multi-hop reasoning and\nmultiple tables. Small LLMs with SFT+RL excel on most complex datasets thanks\nto a strategic balance between generality of the reasoning process and\noptimization of the execution accuracy. Thanks to RL, the7B Qwen-Coder-2.5\nmodel performs on par with 100+ Billion ones on the Bird dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive capabilities in\ntransforming natural language questions about relational databases into SQL\nqueries. Despite recent improvements, small LLMs struggle to handle questions\ninvolving multiple tables and complex SQL patterns under a Zero-Shot Learning\n(ZSL) setting. Supervised Fine-Tuning (SFT) partially compensate the knowledge\ndeficits in pretrained models but falls short while dealing with queries\ninvolving multi-hop reasoning. To bridge this gap, different LLM training\nstrategies to reinforce reasoning capabilities have been proposed, ranging from\nleveraging a thinking process within ZSL, including reasoning traces in SFT, or\nadopt Reinforcement Learning (RL) strategies. However, the influence of\nreasoning on Text2SQL performance is still largely unexplored. This paper\ninvestigates to what extent LLM reasoning capabilities influence their Text2SQL\nperformance on four benchmark datasets. To this end, it considers the following\nLLM settings: (1) ZSL, including general-purpose reasoning or not; (2) SFT,\nwith and without task-specific reasoning traces; (3) RL, leveraging execution\naccuracy as primary reward function; (4) SFT+RL, i.e, a two-stage approach that\ncombines SFT and RL. The results show that general-purpose reasoning under ZSL\nproves to be ineffective in tackling complex Text2SQL cases. Small LLMs benefit\nfrom SFT with reasoning much more than larger ones, bridging the gap of their\n(weaker) model pretraining. RL is generally beneficial across all tested models\nand datasets, particularly when SQL queries involve multi-hop reasoning and\nmultiple tables. Small LLMs with SFT+RL excel on most complex datasets thanks\nto a strategic balance between generality of the reasoning process and\noptimization of the execution accuracy. Thanks to RL, the7B Qwen-Coder-2.5\nmodel performs on par with 100+ Billion ones on the Bird dataset."
                },
                "authors": [
                    {
                        "name": "Simone Papicchio"
                    },
                    {
                        "name": "Simone Rossi"
                    },
                    {
                        "name": "Luca Cagliero"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10664v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10664v2",
                "updated": "2025-04-21T13:04:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    13,
                    4,
                    29,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-09T08:23:31Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    8,
                    23,
                    31,
                    6,
                    68,
                    0
                ],
                "title": "Semantic Wave Functions: Exploring Meaning in Large Language Models\n  Through Quantum Formalism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Wave Functions: Exploring Meaning in Large Language Models\n  Through Quantum Formalism"
                },
                "summary": "Large Language Models (LLMs) encode semantic relationships in\nhigh-dimensional vector embeddings. This paper explores the analogy between LLM\nembedding spaces and quantum mechanics, positing that LLMs operate within a\nquantized semantic space where words and phrases behave as quantum states. To\ncapture nuanced semantic interference effects, we extend the standard\nreal-valued embedding space to the complex domain, drawing parallels to the\ndouble-slit experiment. We introduce a \"semantic wave function\" to formalize\nthis quantum-derived representation and utilize potential landscapes, such as\nthe double-well potential, to model semantic ambiguity. Furthermore, we propose\na complex-valued similarity measure that incorporates both magnitude and phase\ninformation, enabling a more sensitive comparison of semantic representations.\nWe develop a path integral formalism, based on a nonlinear Schr\\\"odinger\nequation with a gauge field and Mexican hat potential, to model the dynamic\nevolution of LLM behavior. This interdisciplinary approach offers a new\ntheoretical framework for understanding and potentially manipulating LLMs, with\nthe goal of advancing both artificial and natural language understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) encode semantic relationships in\nhigh-dimensional vector embeddings. This paper explores the analogy between LLM\nembedding spaces and quantum mechanics, positing that LLMs operate within a\nquantized semantic space where words and phrases behave as quantum states. To\ncapture nuanced semantic interference effects, we extend the standard\nreal-valued embedding space to the complex domain, drawing parallels to the\ndouble-slit experiment. We introduce a \"semantic wave function\" to formalize\nthis quantum-derived representation and utilize potential landscapes, such as\nthe double-well potential, to model semantic ambiguity. Furthermore, we propose\na complex-valued similarity measure that incorporates both magnitude and phase\ninformation, enabling a more sensitive comparison of semantic representations.\nWe develop a path integral formalism, based on a nonlinear Schr\\\"odinger\nequation with a gauge field and Mexican hat potential, to model the dynamic\nevolution of LLM behavior. This interdisciplinary approach offers a new\ntheoretical framework for understanding and potentially manipulating LLMs, with\nthe goal of advancing both artificial and natural language understanding."
                },
                "authors": [
                    {
                        "name": "Timo Aukusti Laine"
                    }
                ],
                "author_detail": {
                    "name": "Timo Aukusti Laine"
                },
                "author": "Timo Aukusti Laine",
                "arxiv_doi": "10.33140/OAJAST",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.33140/OAJAST",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.10664v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10664v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "29 pages, 4 figures. Some corrections added",
                "arxiv_journal_ref": "OA J Applied Sci Technol, 3(1), 01-22 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15068v1",
                "updated": "2025-04-21T12:55:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    55,
                    6,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T12:55:06Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    55,
                    6,
                    0,
                    111,
                    0
                ],
                "title": "The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation\n  with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have significantly enhanced the capabilities of\ninformation access systems, especially with retrieval-augmented generation\n(RAG). Nevertheless, the evaluation of RAG systems remains a barrier to\ncontinued progress, a challenge we tackle in this work by proposing an\nautomatic evaluation framework that is validated against human annotations. We\nbelieve that the nugget evaluation methodology provides a solid foundation for\nevaluating RAG systems. This approach, originally developed for the TREC\nQuestion Answering (QA) Track in 2003, evaluates systems based on atomic facts\nthat should be present in good answers. Our efforts focus on \"refactoring\" this\nmethodology, where we describe the AutoNuggetizer framework that specifically\napplies LLMs to both automatically create nuggets and automatically assign\nnuggets to system answers. In the context of the TREC 2024 RAG Track, we\ncalibrate a fully automatic approach against strategies where nuggets are\ncreated manually or semi-manually by human assessors and then assigned manually\nto system answers. Based on results from a community-wide evaluation, we\nobserve strong agreement at the run level between scores derived from fully\nautomatic nugget evaluation and human-based variants. The agreement is stronger\nwhen individual framework components such as nugget assignment are automated\nindependently. This suggests that our evaluation framework provides tradeoffs\nbetween effort and quality that can be used to guide the development of future\nRAG systems. However, further research is necessary to refine our approach,\nparticularly in establishing robust per-topic agreement to diagnose system\nfailures effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly enhanced the capabilities of\ninformation access systems, especially with retrieval-augmented generation\n(RAG). Nevertheless, the evaluation of RAG systems remains a barrier to\ncontinued progress, a challenge we tackle in this work by proposing an\nautomatic evaluation framework that is validated against human annotations. We\nbelieve that the nugget evaluation methodology provides a solid foundation for\nevaluating RAG systems. This approach, originally developed for the TREC\nQuestion Answering (QA) Track in 2003, evaluates systems based on atomic facts\nthat should be present in good answers. Our efforts focus on \"refactoring\" this\nmethodology, where we describe the AutoNuggetizer framework that specifically\napplies LLMs to both automatically create nuggets and automatically assign\nnuggets to system answers. In the context of the TREC 2024 RAG Track, we\ncalibrate a fully automatic approach against strategies where nuggets are\ncreated manually or semi-manually by human assessors and then assigned manually\nto system answers. Based on results from a community-wide evaluation, we\nobserve strong agreement at the run level between scores derived from fully\nautomatic nugget evaluation and human-based variants. The agreement is stronger\nwhen individual framework components such as nugget assignment are automated\nindependently. This suggests that our evaluation framework provides tradeoffs\nbetween effort and quality that can be used to guide the development of future\nRAG systems. However, further research is necessary to refine our approach,\nparticularly in establishing robust per-topic agreement to diagnose system\nfailures effectively."
                },
                "authors": [
                    {
                        "name": "Ronak Pradeep"
                    },
                    {
                        "name": "Nandan Thakur"
                    },
                    {
                        "name": "Shivani Upadhyay"
                    },
                    {
                        "name": "Daniel Campos"
                    },
                    {
                        "name": "Nick Craswell"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "arxiv_comment": "To appear in SIGIR 2025. Significant updates and revisions to\n  arXiv:2411.09607",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10258v3",
                "updated": "2025-04-21T12:52:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    52,
                    49,
                    0,
                    111,
                    0
                ],
                "published": "2024-03-15T12:47:39Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    12,
                    47,
                    39,
                    4,
                    75,
                    0
                ],
                "title": "Is Translation All You Need? A Study on Solving Multilingual Tasks with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Translation All You Need? A Study on Solving Multilingual Tasks with\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated multilingual capabilities, yet\nthey are mostly English-centric due to the imbalanced training corpora. While\nprior works have leveraged this bias to enhance multilingual performance\nthrough translation, they have been largely limited to natural language\nprocessing (NLP) tasks. In this work, we extend the evaluation to real-world\nuser queries and non-English-centric LLMs, offering a broader examination of\nmultilingual performance. Our key contribution lies in demonstrating that while\ntranslation into English can boost the performance of English-centric LLMs on\nNLP tasks, it is not universally optimal. For culture-related tasks that need\ndeep language understanding, prompting in the native language proves more\neffective as it better captures the nuances of culture and language. Our\nexperiments expose varied behaviors across LLMs and tasks in the multilingual\ncontext, underscoring the need for a more comprehensive approach to\nmultilingual evaluation. Therefore, we call for greater efforts in developing\nand evaluating LLMs that go beyond English-centric paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated multilingual capabilities, yet\nthey are mostly English-centric due to the imbalanced training corpora. While\nprior works have leveraged this bias to enhance multilingual performance\nthrough translation, they have been largely limited to natural language\nprocessing (NLP) tasks. In this work, we extend the evaluation to real-world\nuser queries and non-English-centric LLMs, offering a broader examination of\nmultilingual performance. Our key contribution lies in demonstrating that while\ntranslation into English can boost the performance of English-centric LLMs on\nNLP tasks, it is not universally optimal. For culture-related tasks that need\ndeep language understanding, prompting in the native language proves more\neffective as it better captures the nuances of culture and language. Our\nexperiments expose varied behaviors across LLMs and tasks in the multilingual\ncontext, underscoring the need for a more comprehensive approach to\nmultilingual evaluation. Therefore, we call for greater efforts in developing\nand evaluating LLMs that go beyond English-centric paradigms."
                },
                "authors": [
                    {
                        "name": "Chaoqun Liu"
                    },
                    {
                        "name": "Wenxuan Zhang"
                    },
                    {
                        "name": "Yiran Zhao"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15057v1",
                "updated": "2025-04-21T12:34:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    34,
                    57,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T12:34:57Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    34,
                    57,
                    0,
                    111,
                    0
                ],
                "title": "Linear Item-Item Model with Neural Knowledge for Session-based\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Item-Item Model with Neural Knowledge for Session-based\n  Recommendation"
                },
                "summary": "Session-based recommendation (SBR) aims to predict users' subsequent actions\nby modeling short-term interactions within sessions. Existing neural models\nprimarily focus on capturing complex dependencies for sequential item\ntransitions. As an alternative solution, linear item-item models mainly\nidentify strong co-occurrence patterns across items and support faster\ninference speed. Although each paradigm has been actively studied in SBR, their\nfundamental differences in capturing item relationships and how to bridge these\ndistinct modeling paradigms effectively remain unexplored. In this paper, we\npropose a novel SBR model, namely Linear Item-Item model with Neural Knowledge\n(LINK), which integrates both types of knowledge into a unified linear\nframework. Specifically, we design two specialized components of LINK: (i)\nLinear knowledge-enhanced Item-item Similarity model (LIS), which refines the\nitem similarity correlation via self-distillation, and (ii) Neural\nknowledge-enhanced Item-item Transition model (NIT), which seamlessly\nincorporates complicated neural knowledge distilled from the off-the-shelf\nneural model. Extensive experiments demonstrate that LINK outperforms\nstate-of-the-art linear SBR models across six real-world datasets, achieving\nimprovements of up to 14.78% and 11.04% in Recall@20 and MRR@20 while showing\nup to 813x fewer inference FLOPs. Our code is available at\nhttps://github.com/jin530/LINK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Session-based recommendation (SBR) aims to predict users' subsequent actions\nby modeling short-term interactions within sessions. Existing neural models\nprimarily focus on capturing complex dependencies for sequential item\ntransitions. As an alternative solution, linear item-item models mainly\nidentify strong co-occurrence patterns across items and support faster\ninference speed. Although each paradigm has been actively studied in SBR, their\nfundamental differences in capturing item relationships and how to bridge these\ndistinct modeling paradigms effectively remain unexplored. In this paper, we\npropose a novel SBR model, namely Linear Item-Item model with Neural Knowledge\n(LINK), which integrates both types of knowledge into a unified linear\nframework. Specifically, we design two specialized components of LINK: (i)\nLinear knowledge-enhanced Item-item Similarity model (LIS), which refines the\nitem similarity correlation via self-distillation, and (ii) Neural\nknowledge-enhanced Item-item Transition model (NIT), which seamlessly\nincorporates complicated neural knowledge distilled from the off-the-shelf\nneural model. Extensive experiments demonstrate that LINK outperforms\nstate-of-the-art linear SBR models across six real-world datasets, achieving\nimprovements of up to 14.78% and 11.04% in Recall@20 and MRR@20 while showing\nup to 813x fewer inference FLOPs. Our code is available at\nhttps://github.com/jin530/LINK."
                },
                "authors": [
                    {
                        "name": "Minjin Choi"
                    },
                    {
                        "name": "Sunkyung Lee"
                    },
                    {
                        "name": "Seongmin Park"
                    },
                    {
                        "name": "Jongwuk Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jongwuk Lee"
                },
                "author": "Jongwuk Lee",
                "arxiv_doi": "10.1145/3726302.3730024",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730024",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SIGIR 2025, 9 pages",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15054v1",
                "updated": "2025-04-21T12:30:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    30,
                    1,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T12:30:01Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    30,
                    1,
                    0,
                    111,
                    0
                ],
                "title": "Structure-guided Diffusion Transformer for Low-Light Image Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structure-guided Diffusion Transformer for Low-Light Image Enhancement"
                },
                "summary": "While the diffusion transformer (DiT) has become a focal point of interest in\nrecent years, its application in low-light image enhancement remains a blank\narea for exploration. Current methods recover the details from low-light images\nwhile inevitably amplifying the noise in images, resulting in poor visual\nquality. In this paper, we firstly introduce DiT into the low-light enhancement\ntask and design a novel Structure-guided Diffusion Transformer based Low-light\nimage enhancement (SDTL) framework. We compress the feature through wavelet\ntransform to improve the inference efficiency of the model and capture the\nmulti-directional frequency band. Then we propose a Structure Enhancement\nModule (SEM) that uses structural prior to enhance the texture and leverages an\nadaptive fusion strategy to achieve more accurate enhancement effect. In\nAddition, we propose a Structure-guided Attention Block (SAB) to pay more\nattention to texture-riched tokens and avoid interference from noisy areas in\nnoise prediction. Extensive qualitative and quantitative experiments\ndemonstrate that our method achieves SOTA performance on several popular\ndatasets, validating the effectiveness of SDTL in improving image quality and\nthe potential of DiT in low-light enhancement tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the diffusion transformer (DiT) has become a focal point of interest in\nrecent years, its application in low-light image enhancement remains a blank\narea for exploration. Current methods recover the details from low-light images\nwhile inevitably amplifying the noise in images, resulting in poor visual\nquality. In this paper, we firstly introduce DiT into the low-light enhancement\ntask and design a novel Structure-guided Diffusion Transformer based Low-light\nimage enhancement (SDTL) framework. We compress the feature through wavelet\ntransform to improve the inference efficiency of the model and capture the\nmulti-directional frequency band. Then we propose a Structure Enhancement\nModule (SEM) that uses structural prior to enhance the texture and leverages an\nadaptive fusion strategy to achieve more accurate enhancement effect. In\nAddition, we propose a Structure-guided Attention Block (SAB) to pay more\nattention to texture-riched tokens and avoid interference from noisy areas in\nnoise prediction. Extensive qualitative and quantitative experiments\ndemonstrate that our method achieves SOTA performance on several popular\ndatasets, validating the effectiveness of SDTL in improving image quality and\nthe potential of DiT in low-light enhancement tasks."
                },
                "authors": [
                    {
                        "name": "Xiangchen Yin"
                    },
                    {
                        "name": "Zhenda Yu"
                    },
                    {
                        "name": "Longtao Jiang"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Xiao Sun"
                    },
                    {
                        "name": "Zhi Liu"
                    },
                    {
                        "name": "Xun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Yang"
                },
                "author": "Xun Yang",
                "arxiv_comment": "Accepted by IEEE Transactions on Multimedia (TMM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15053v1",
                "updated": "2025-04-21T12:26:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    26,
                    27,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T12:26:27Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    26,
                    27,
                    0,
                    111,
                    0
                ],
                "title": "One pathogen does not an epidemic make: A review of interacting\n  contagions, diseases, beliefs, and stories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One pathogen does not an epidemic make: A review of interacting\n  contagions, diseases, beliefs, and stories"
                },
                "summary": "From pathogens and computer viruses to genes and memes, contagion models have\nfound widespread utility across the natural and social sciences. Despite their\nsuccess and breadth of adoption, the approach and structure of these models\nremain surprisingly siloed by field. Given the siloed nature of their\ndevelopment and widespread use, one persistent assumption is that a given\ncontagion can be studied in isolation, independently from what else might be\nspreading in the population. In reality, countless contagions of biological and\nsocial nature interact within hosts (interacting with existing beliefs, or the\nimmune system) and across hosts (interacting in the environment, or affecting\ntransmission mechanisms). Additionally, from a modeling perspective, we know\nthat relaxing these assumptions has profound effects on the physics and\ntranslational implications of the models. Here, we review mechanisms for\ninteractions in social and biological contagions, as well as the models and\nframeworks developed to include these interactions in the study of the\ncontagions. We highlight existing problems related to the inference of\ninteractions and to the scalability of mathematical models and identify\npromising avenues of future inquiries. In doing so, we highlight the need for\ninterdisciplinary efforts under a unified science of contagions and for\nremoving a common dichotomy between social and biological contagions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From pathogens and computer viruses to genes and memes, contagion models have\nfound widespread utility across the natural and social sciences. Despite their\nsuccess and breadth of adoption, the approach and structure of these models\nremain surprisingly siloed by field. Given the siloed nature of their\ndevelopment and widespread use, one persistent assumption is that a given\ncontagion can be studied in isolation, independently from what else might be\nspreading in the population. In reality, countless contagions of biological and\nsocial nature interact within hosts (interacting with existing beliefs, or the\nimmune system) and across hosts (interacting in the environment, or affecting\ntransmission mechanisms). Additionally, from a modeling perspective, we know\nthat relaxing these assumptions has profound effects on the physics and\ntranslational implications of the models. Here, we review mechanisms for\ninteractions in social and biological contagions, as well as the models and\nframeworks developed to include these interactions in the study of the\ncontagions. We highlight existing problems related to the inference of\ninteractions and to the scalability of mathematical models and identify\npromising avenues of future inquiries. In doing so, we highlight the need for\ninterdisciplinary efforts under a unified science of contagions and for\nremoving a common dichotomy between social and biological contagions."
                },
                "authors": [
                    {
                        "name": "Laurent Hébert-Dufresne"
                    },
                    {
                        "name": "Yong-Yeol Ahn"
                    },
                    {
                        "name": "Antoine Allard"
                    },
                    {
                        "name": "Jessica W. Crothers"
                    },
                    {
                        "name": "Peter Sheridan Dodds"
                    },
                    {
                        "name": "Mirta Galesic"
                    },
                    {
                        "name": "Fakhteh Ghanbarnejad"
                    },
                    {
                        "name": "Dominique Gravel"
                    },
                    {
                        "name": "Ross A. Hammond"
                    },
                    {
                        "name": "Kristina Lerman"
                    },
                    {
                        "name": "Juniper Lovato"
                    },
                    {
                        "name": "John J. Openshaw"
                    },
                    {
                        "name": "S. Redner"
                    },
                    {
                        "name": "Samuel V. Scarpino"
                    },
                    {
                        "name": "Guillaume St-Onge"
                    },
                    {
                        "name": "Timothy R. Tangherlini"
                    },
                    {
                        "name": "Jean-Gabriel Young"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Gabriel Young"
                },
                "author": "Jean-Gabriel Young",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15052v1",
                "updated": "2025-04-21T12:21:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    21,
                    37,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T12:21:37Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    21,
                    37,
                    0,
                    111,
                    0
                ],
                "title": "Testing LLMs' Capabilities in Annotating Translations Based on an Error\n  Typology Designed for LSP Translation: First Experiments with ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing LLMs' Capabilities in Annotating Translations Based on an Error\n  Typology Designed for LSP Translation: First Experiments with ChatGPT"
                },
                "summary": "This study investigates the capabilities of large language models (LLMs),\nspecifically ChatGPT, in annotating MT outputs based on an error typology. In\ncontrast to previous work focusing mainly on general language, we explore\nChatGPT's ability to identify and categorise errors in specialised\ntranslations. By testing two different prompts and based on a customised error\ntypology, we compare ChatGPT annotations with human expert evaluations of\ntranslations produced by DeepL and ChatGPT itself. The results show that, for\ntranslations generated by DeepL, recall and precision are quite high. However,\nthe degree of accuracy in error categorisation depends on the prompt's specific\nfeatures and its level of detail, ChatGPT performing very well with a detailed\nprompt. When evaluating its own translations, ChatGPT achieves significantly\npoorer results, revealing limitations with self-assessment. These results\nhighlight both the potential and the limitations of LLMs for translation\nevaluation, particularly in specialised domains. Our experiments pave the way\nfor future research on open-source LLMs, which could produce annotations of\ncomparable or even higher quality. In the future, we also aim to test the\npractical effectiveness of this automated evaluation in the context of\ntranslation training, particularly by optimising the process of human\nevaluation by teachers and by exploring the impact of annotations by LLMs on\nstudents' post-editing and translation learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the capabilities of large language models (LLMs),\nspecifically ChatGPT, in annotating MT outputs based on an error typology. In\ncontrast to previous work focusing mainly on general language, we explore\nChatGPT's ability to identify and categorise errors in specialised\ntranslations. By testing two different prompts and based on a customised error\ntypology, we compare ChatGPT annotations with human expert evaluations of\ntranslations produced by DeepL and ChatGPT itself. The results show that, for\ntranslations generated by DeepL, recall and precision are quite high. However,\nthe degree of accuracy in error categorisation depends on the prompt's specific\nfeatures and its level of detail, ChatGPT performing very well with a detailed\nprompt. When evaluating its own translations, ChatGPT achieves significantly\npoorer results, revealing limitations with self-assessment. These results\nhighlight both the potential and the limitations of LLMs for translation\nevaluation, particularly in specialised domains. Our experiments pave the way\nfor future research on open-source LLMs, which could produce annotations of\ncomparable or even higher quality. In the future, we also aim to test the\npractical effectiveness of this automated evaluation in the context of\ntranslation training, particularly by optimising the process of human\nevaluation by teachers and by exploring the impact of annotations by LLMs on\nstudents' post-editing and translation learning."
                },
                "authors": [
                    {
                        "name": "Joachim Minder"
                    },
                    {
                        "name": "Guillaume Wisniewski"
                    },
                    {
                        "name": "Natalie Kübler"
                    }
                ],
                "author_detail": {
                    "name": "Natalie Kübler"
                },
                "author": "Natalie Kübler",
                "arxiv_comment": "Accepted for publication in the proceedings of MT Summit 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15049v1",
                "updated": "2025-04-21T12:12:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    12,
                    43,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T12:12:43Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    12,
                    43,
                    0,
                    111,
                    0
                ],
                "title": "ScanEdit: Hierarchically-Guided Functional 3D Scan Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScanEdit: Hierarchically-Guided Functional 3D Scan Editing"
                },
                "summary": "With the fast pace of 3D capture technology and resulting abundance of 3D\ndata, effective 3D scene editing becomes essential for a variety of graphics\napplications. In this work we present ScanEdit, an instruction-driven method\nfor functional editing of complex, real-world 3D scans. To model large and\ninterdependent sets of ob- jectswe propose a hierarchically-guided approach.\nGiven a 3D scan decomposed into its object instances, we first construct a\nhierarchical scene graph representation to enable effective, tractable editing.\nWe then leverage reason- ing capabilities of Large Language Models (LLMs) and\ntranslate high-level language instructions into actionable commands applied\nhierarchically to the scene graph. Fi- nally, ScanEdit integrates LLM-based\nguidance with ex- plicit physical constraints and generates realistic scenes\nwhere object arrangements obey both physics and common sense. In our extensive\nexperimental evaluation ScanEdit outperforms state of the art and demonstrates\nexcellent re- sults for a variety of real-world scenes and input instruc-\ntions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the fast pace of 3D capture technology and resulting abundance of 3D\ndata, effective 3D scene editing becomes essential for a variety of graphics\napplications. In this work we present ScanEdit, an instruction-driven method\nfor functional editing of complex, real-world 3D scans. To model large and\ninterdependent sets of ob- jectswe propose a hierarchically-guided approach.\nGiven a 3D scan decomposed into its object instances, we first construct a\nhierarchical scene graph representation to enable effective, tractable editing.\nWe then leverage reason- ing capabilities of Large Language Models (LLMs) and\ntranslate high-level language instructions into actionable commands applied\nhierarchically to the scene graph. Fi- nally, ScanEdit integrates LLM-based\nguidance with ex- plicit physical constraints and generates realistic scenes\nwhere object arrangements obey both physics and common sense. In our extensive\nexperimental evaluation ScanEdit outperforms state of the art and demonstrates\nexcellent re- sults for a variety of real-world scenes and input instruc-\ntions."
                },
                "authors": [
                    {
                        "name": "Mohamed el amine Boudjoghra"
                    },
                    {
                        "name": "Ivan Laptev"
                    },
                    {
                        "name": "Angela Dai"
                    }
                ],
                "author_detail": {
                    "name": "Angela Dai"
                },
                "author": "Angela Dai",
                "arxiv_comment": "Project webpage: https://aminebdj.github.io/scanedit/ Video:\n  https://www.youtube.com/watch?v=Dfmu2g6pVlg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15047v1",
                "updated": "2025-04-21T12:04:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    4,
                    57,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T12:04:57Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    4,
                    57,
                    0,
                    111,
                    0
                ],
                "title": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary\n  Quality-Diversity Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary\n  Quality-Diversity Search"
                },
                "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but are\nsusceptible to adversarial prompts that exploit vulnerabilities to produce\nunsafe or biased outputs. Existing red-teaming methods often face scalability\nchallenges, resource-intensive requirements, or limited diversity in attack\nstrategies. We propose RainbowPlus, a novel red-teaming framework rooted in\nevolutionary computation, enhancing adversarial prompt generation through an\nadaptive quality-diversity (QD) search that extends classical evolutionary\nalgorithms like MAP-Elites with innovations tailored for language models. By\nemploying a multi-element archive to store diverse high-quality prompts and a\ncomprehensive fitness function to evaluate multiple prompts concurrently,\nRainbowPlus overcomes the constraints of single-prompt archives and pairwise\ncomparisons in prior QD methods like Rainbow Teaming. Experiments comparing\nRainbowPlus to QD methods across six benchmark datasets and four open-source\nLLMs demonstrate superior attack success rate (ASR) and diversity\n(Diverse-Score $\\approx 0.84$), generating up to 100 times more unique prompts\n(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine\nstate-of-the-art methods on the HarmBench dataset with twelve LLMs (ten\nopen-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,\nsurpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).\nOur open-source implementation fosters further advancements in LLM safety,\noffering a scalable tool for vulnerability assessment. Code and resources are\npublicly available at https://github.com/knoveleng/rainbowplus, supporting\nreproducibility and future research in LLM red-teaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit remarkable capabilities but are\nsusceptible to adversarial prompts that exploit vulnerabilities to produce\nunsafe or biased outputs. Existing red-teaming methods often face scalability\nchallenges, resource-intensive requirements, or limited diversity in attack\nstrategies. We propose RainbowPlus, a novel red-teaming framework rooted in\nevolutionary computation, enhancing adversarial prompt generation through an\nadaptive quality-diversity (QD) search that extends classical evolutionary\nalgorithms like MAP-Elites with innovations tailored for language models. By\nemploying a multi-element archive to store diverse high-quality prompts and a\ncomprehensive fitness function to evaluate multiple prompts concurrently,\nRainbowPlus overcomes the constraints of single-prompt archives and pairwise\ncomparisons in prior QD methods like Rainbow Teaming. Experiments comparing\nRainbowPlus to QD methods across six benchmark datasets and four open-source\nLLMs demonstrate superior attack success rate (ASR) and diversity\n(Diverse-Score $\\approx 0.84$), generating up to 100 times more unique prompts\n(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine\nstate-of-the-art methods on the HarmBench dataset with twelve LLMs (ten\nopen-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,\nsurpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).\nOur open-source implementation fosters further advancements in LLM safety,\noffering a scalable tool for vulnerability assessment. Code and resources are\npublicly available at https://github.com/knoveleng/rainbowplus, supporting\nreproducibility and future research in LLM red-teaming."
                },
                "authors": [
                    {
                        "name": "Quy-Anh Dang"
                    },
                    {
                        "name": "Chris Ngo"
                    },
                    {
                        "name": "Truong-Son Hy"
                    }
                ],
                "author_detail": {
                    "name": "Truong-Son Hy"
                },
                "author": "Truong-Son Hy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04176v2",
                "updated": "2025-04-21T12:02:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    2,
                    8,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-06T16:07:24Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    7,
                    24,
                    3,
                    37,
                    0
                ],
                "title": "MRAMG-Bench: A Comprehensive Benchmark for Advancing Multimodal\n  Retrieval-Augmented Multimodal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MRAMG-Bench: A Comprehensive Benchmark for Advancing Multimodal\n  Retrieval-Augmented Multimodal Generation"
                },
                "summary": "Recent advances in Retrieval-Augmented Generation (RAG) have significantly\nimproved response accuracy and relevance by incorporating external knowledge\ninto Large Language Models (LLMs). However, existing RAG methods primarily\nfocus on generating text-only answers, even in Multimodal Retrieval-Augmented\nGeneration (MRAG) scenarios, where multimodal elements are retrieved to assist\nin generating text answers. To address this, we introduce the Multimodal\nRetrieval-Augmented Multimodal Generation (MRAMG) task, in which we aim to\ngenerate multimodal answers that combine both text and images, fully leveraging\nthe multimodal data within a corpus. Despite growing attention to this\nchallenging task, a notable lack of a comprehensive benchmark persists for\neffectively evaluating its performance. To bridge this gap, we provide\nMRAMG-Bench, a meticulously curated, human-annotated benchmark comprising 4,346\ndocuments, 14,190 images, and 4,800 QA pairs, distributed across six distinct\ndatasets and spanning three domains: Web, Academia, and Lifestyle. The datasets\nincorporate diverse difficulty levels and complex multi-image scenarios,\nproviding a robust foundation for evaluating the MRAMG task. To facilitate\nrigorous evaluation, MRAMG-Bench incorporates a comprehensive suite of both\nstatistical and LLM-based metrics, enabling a thorough analysis of the\nperformance of generative models in the MRAMG task. Additionally, we propose an\nefficient and flexible multimodal answer generation framework that can leverage\nLLMs/MLLMs to generate multimodal responses. Our datasets and complete\nevaluation results for 11 popular generative models are available at\nhttps://github.com/MRAMG-Bench/MRAMG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Retrieval-Augmented Generation (RAG) have significantly\nimproved response accuracy and relevance by incorporating external knowledge\ninto Large Language Models (LLMs). However, existing RAG methods primarily\nfocus on generating text-only answers, even in Multimodal Retrieval-Augmented\nGeneration (MRAG) scenarios, where multimodal elements are retrieved to assist\nin generating text answers. To address this, we introduce the Multimodal\nRetrieval-Augmented Multimodal Generation (MRAMG) task, in which we aim to\ngenerate multimodal answers that combine both text and images, fully leveraging\nthe multimodal data within a corpus. Despite growing attention to this\nchallenging task, a notable lack of a comprehensive benchmark persists for\neffectively evaluating its performance. To bridge this gap, we provide\nMRAMG-Bench, a meticulously curated, human-annotated benchmark comprising 4,346\ndocuments, 14,190 images, and 4,800 QA pairs, distributed across six distinct\ndatasets and spanning three domains: Web, Academia, and Lifestyle. The datasets\nincorporate diverse difficulty levels and complex multi-image scenarios,\nproviding a robust foundation for evaluating the MRAMG task. To facilitate\nrigorous evaluation, MRAMG-Bench incorporates a comprehensive suite of both\nstatistical and LLM-based metrics, enabling a thorough analysis of the\nperformance of generative models in the MRAMG task. Additionally, we propose an\nefficient and flexible multimodal answer generation framework that can leverage\nLLMs/MLLMs to generate multimodal responses. Our datasets and complete\nevaluation results for 11 popular generative models are available at\nhttps://github.com/MRAMG-Bench/MRAMG."
                },
                "authors": [
                    {
                        "name": "Qinhan Yu"
                    },
                    {
                        "name": "Zhiyou Xiao"
                    },
                    {
                        "name": "Binghui Li"
                    },
                    {
                        "name": "Zhengren Wang"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_comment": "Published as a conference paper at SIGIR 2025; 11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15046v2",
                "updated": "2025-04-22T05:56:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    5,
                    56,
                    57,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T12:00:20Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    0,
                    20,
                    0,
                    111,
                    0
                ],
                "title": "Text-to-Decision Agent: Learning Generalist Policies from Natural\n  Language Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Decision Agent: Learning Generalist Policies from Natural\n  Language Supervision"
                },
                "summary": "RL systems usually tackle generalization by inferring task beliefs from\nhigh-quality samples or warmup explorations. The restricted form limits their\ngenerality and usability since these supervision signals are expensive and even\ninfeasible to acquire in advance for unseen tasks. Learning directly from the\nraw text about decision tasks is a promising alternative to leverage a much\nbroader source of supervision. In the paper, we propose Text-to-Decision Agent\n(T2DA), a simple and scalable framework that supervises generalist policy\nlearning with natural language. We first introduce a generalized world model to\nencode multi-task decision data into a dynamics-aware embedding space. Then,\ninspired by CLIP, we predict which textual description goes with which decision\nembedding, effectively bridging their semantic gap via contrastive\nlanguage-decision pre-training and aligning the text embeddings to comprehend\nthe environment dynamics. After training the text-conditioned generalist\npolicy, the agent can directly realize zero-shot text-to-decision generation in\nresponse to language instructions. Comprehensive experiments on MuJoCo and\nMeta-World benchmarks show that T2DA facilitates high-capacity zero-shot\ngeneralization and outperforms various types of baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL systems usually tackle generalization by inferring task beliefs from\nhigh-quality samples or warmup explorations. The restricted form limits their\ngenerality and usability since these supervision signals are expensive and even\ninfeasible to acquire in advance for unseen tasks. Learning directly from the\nraw text about decision tasks is a promising alternative to leverage a much\nbroader source of supervision. In the paper, we propose Text-to-Decision Agent\n(T2DA), a simple and scalable framework that supervises generalist policy\nlearning with natural language. We first introduce a generalized world model to\nencode multi-task decision data into a dynamics-aware embedding space. Then,\ninspired by CLIP, we predict which textual description goes with which decision\nembedding, effectively bridging their semantic gap via contrastive\nlanguage-decision pre-training and aligning the text embeddings to comprehend\nthe environment dynamics. After training the text-conditioned generalist\npolicy, the agent can directly realize zero-shot text-to-decision generation in\nresponse to language instructions. Comprehensive experiments on MuJoCo and\nMeta-World benchmarks show that T2DA facilitates high-capacity zero-shot\ngeneralization and outperforms various types of baselines."
                },
                "authors": [
                    {
                        "name": "Shilin Zhang"
                    },
                    {
                        "name": "Zican Hu"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Xinyi Xie"
                    },
                    {
                        "name": "Jianxiang Tang"
                    },
                    {
                        "name": "Chunlin Chen"
                    },
                    {
                        "name": "Daoyi Dong"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Zhenhong Sun"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15044v1",
                "updated": "2025-04-21T11:56:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    56,
                    36,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:56:36Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    56,
                    36,
                    0,
                    111,
                    0
                ],
                "title": "Beyond Terabit/s Integrated Neuromorphic Photonic Processor for DSP-Free\n  Optical Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Terabit/s Integrated Neuromorphic Photonic Processor for DSP-Free\n  Optical Interconnects"
                },
                "summary": "The rapid expansion of generative AI drives unprecedented demands for\nhigh-performance computing. Training large-scale AI models now requires vast\ninterconnected GPU clusters across multiple data centers. Multi-scale AI\ntraining and inference demand uniform, ultra-low latency, and energy-efficient\nlinks to enable massive GPUs to function as a single cohesive unit. However,\ntraditional electrical and optical interconnects, relying on conventional\ndigital signal processors (DSPs) for signal distortion compensation,\nincreasingly fail to meet these stringent requirements. To overcome these\nlimitations, we present an integrated neuromorphic optical signal processor\n(OSP) that leverages deep reservoir computing and achieves DSP-free,\nall-optical, real-time processing. Experimentally, our OSP achieves a 100 Gbaud\nPAM4 per lane, 1.6 Tbit/s data center interconnect over a 5 km optical fiber in\nthe C-band (equivalent to over 80 km in the O-band), far exceeding the reach of\nstate-of-the-art DSP solutions, which are fundamentally constrained by\nchromatic dispersion in IMDD systems. Simultaneously, it reduces processing\nlatency by four orders of magnitude and energy consumption by three orders of\nmagnitude. Unlike DSPs, which introduce increased latency at high data rates,\nour OSP maintains consistent, ultra-low latency regardless of data rate\nscaling, making it ideal for future optical interconnects. Moreover, the OSP\nretains full optical field information for better impairment compensation and\nadapts to various modulation formats, data rates, and wavelengths. Fabricated\nusing a mature silicon photonic process, the OSP can be monolithically\nintegrated with silicon photonic transceivers, enhancing the compactness and\nreliability of all-optical interconnects. This research provides a highly\nscalable, energy-efficient, and high-speed solution, paving the way for\nnext-generation AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of generative AI drives unprecedented demands for\nhigh-performance computing. Training large-scale AI models now requires vast\ninterconnected GPU clusters across multiple data centers. Multi-scale AI\ntraining and inference demand uniform, ultra-low latency, and energy-efficient\nlinks to enable massive GPUs to function as a single cohesive unit. However,\ntraditional electrical and optical interconnects, relying on conventional\ndigital signal processors (DSPs) for signal distortion compensation,\nincreasingly fail to meet these stringent requirements. To overcome these\nlimitations, we present an integrated neuromorphic optical signal processor\n(OSP) that leverages deep reservoir computing and achieves DSP-free,\nall-optical, real-time processing. Experimentally, our OSP achieves a 100 Gbaud\nPAM4 per lane, 1.6 Tbit/s data center interconnect over a 5 km optical fiber in\nthe C-band (equivalent to over 80 km in the O-band), far exceeding the reach of\nstate-of-the-art DSP solutions, which are fundamentally constrained by\nchromatic dispersion in IMDD systems. Simultaneously, it reduces processing\nlatency by four orders of magnitude and energy consumption by three orders of\nmagnitude. Unlike DSPs, which introduce increased latency at high data rates,\nour OSP maintains consistent, ultra-low latency regardless of data rate\nscaling, making it ideal for future optical interconnects. Moreover, the OSP\nretains full optical field information for better impairment compensation and\nadapts to various modulation formats, data rates, and wavelengths. Fabricated\nusing a mature silicon photonic process, the OSP can be monolithically\nintegrated with silicon photonic transceivers, enhancing the compactness and\nreliability of all-optical interconnects. This research provides a highly\nscalable, energy-efficient, and high-speed solution, paving the way for\nnext-generation AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Benshan Wang"
                    },
                    {
                        "name": "Qiarong Xiao"
                    },
                    {
                        "name": "Tengji Xu"
                    },
                    {
                        "name": "Li Fan"
                    },
                    {
                        "name": "Shaojie Liu"
                    },
                    {
                        "name": "Jianji Dong"
                    },
                    {
                        "name": "Junwen Zhang"
                    },
                    {
                        "name": "Chaoran Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chaoran Huang"
                },
                "author": "Chaoran Huang",
                "arxiv_comment": "22 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15042v1",
                "updated": "2025-04-21T11:53:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    53,
                    44,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:53:44Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    53,
                    44,
                    0,
                    111,
                    0
                ],
                "title": "Bayesian Sensing for Time-Varying Channels in ISAC Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Sensing for Time-Varying Channels in ISAC Systems"
                },
                "summary": "Future mobile networks are projected to support integrated sensing and\ncommunications in high-speed communication scenarios. Nevertheless, large\nDoppler shifts induced by time-varying channels may cause severe inter-carrier\ninterference (ICI). Frequency domain shows the potential of reducing ISAC\ncomplexity as compared with other domains. However, parameter mismatching issue\nstill exists for such sensing. In this paper, we develop a novel sensing scheme\nbased on sparse Bayesian framework, where the delay and Doppler estimation\nproblem in time-varying channels is formulated as a 3D multiple\nmeasurement-sparse signal recovery (MM-SSR) problem. We then propose a novel\ntwo-layer variational Bayesian inference (VBI) method to decompose the 3D\nMM-SSR problem into two layers and estimate the Doppler in the first layer and\nthe delay in the second layer alternatively. Subsequently, as is benefited from\nnewly unveiled signal construction, a simplified two-stage multiple signal\nclassification (MUSIC)-based VBI method is proposed, where the delay and the\nDoppler are estimated by MUSIC and VBI, respectively. Additionally, the\nCram\\'er-Rao bound (CRB) of the considered sensing parameters is derived to\ncharacterize the lower bound for the proposed estimators. Corroborated by\nextensive simulation results, our proposed method can achieve improved mean\nsquare error (MSE) than its conventional counterparts and is robust against the\ntarget number and target speed, thereby validating its wide applicability and\nadvantages over prior arts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future mobile networks are projected to support integrated sensing and\ncommunications in high-speed communication scenarios. Nevertheless, large\nDoppler shifts induced by time-varying channels may cause severe inter-carrier\ninterference (ICI). Frequency domain shows the potential of reducing ISAC\ncomplexity as compared with other domains. However, parameter mismatching issue\nstill exists for such sensing. In this paper, we develop a novel sensing scheme\nbased on sparse Bayesian framework, where the delay and Doppler estimation\nproblem in time-varying channels is formulated as a 3D multiple\nmeasurement-sparse signal recovery (MM-SSR) problem. We then propose a novel\ntwo-layer variational Bayesian inference (VBI) method to decompose the 3D\nMM-SSR problem into two layers and estimate the Doppler in the first layer and\nthe delay in the second layer alternatively. Subsequently, as is benefited from\nnewly unveiled signal construction, a simplified two-stage multiple signal\nclassification (MUSIC)-based VBI method is proposed, where the delay and the\nDoppler are estimated by MUSIC and VBI, respectively. Additionally, the\nCram\\'er-Rao bound (CRB) of the considered sensing parameters is derived to\ncharacterize the lower bound for the proposed estimators. Corroborated by\nextensive simulation results, our proposed method can achieve improved mean\nsquare error (MSE) than its conventional counterparts and is robust against the\ntarget number and target speed, thereby validating its wide applicability and\nadvantages over prior arts."
                },
                "authors": [
                    {
                        "name": "Xueyang Wang"
                    },
                    {
                        "name": "Kai Wu"
                    },
                    {
                        "name": "J. Andrew Zhang"
                    },
                    {
                        "name": "Shiqi Gong"
                    },
                    {
                        "name": "Chengwen Xing"
                    }
                ],
                "author_detail": {
                    "name": "Chengwen Xing"
                },
                "author": "Chengwen Xing",
                "arxiv_comment": "14 pages, 8 figures, manuscript submitted to IEEE Transactions on\n  Communications (TCOM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15032v1",
                "updated": "2025-04-21T11:41:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    41,
                    22,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:41:22Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    41,
                    22,
                    0,
                    111,
                    0
                ],
                "title": "DyST-XL: Dynamic Layout Planning and Content Control for Compositional\n  Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyST-XL: Dynamic Layout Planning and Content Control for Compositional\n  Text-to-Video Generation"
                },
                "summary": "Compositional text-to-video generation, which requires synthesizing dynamic\nscenes with multiple interacting entities and precise spatial-temporal\nrelationships, remains a critical challenge for diffusion-based models.\nExisting methods struggle with layout discontinuity, entity identity drift, and\nimplausible interaction dynamics due to unconstrained cross-attention\nmechanisms and inadequate physics-aware reasoning. To address these\nlimitations, we propose DyST-XL, a \\textbf{training-free} framework that\nenhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through\nframe-aware control. DyST-XL integrates three key innovations: (1) A Dynamic\nLayout Planner that leverages large language models (LLMs) to parse input\nprompts into entity-attribute graphs and generates physics-aware keyframe\nlayouts, with intermediate frames interpolated via trajectory optimization; (2)\nA Dual-Prompt Controlled Attention Mechanism that enforces localized text-video\nalignment through frame-aware attention masking, achieving the precise control\nover individual entities; and (3) An Entity-Consistency Constraint strategy\nthat propagates first-frame feature embeddings to subsequent frames during\ndenoising, preserving object identity without manual annotation. Experiments\ndemonstrate that DyST-XL excels in compositional text-to-video generation,\nsignificantly improving performance on complex prompts and bridging a crucial\ngap in training-free video synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional text-to-video generation, which requires synthesizing dynamic\nscenes with multiple interacting entities and precise spatial-temporal\nrelationships, remains a critical challenge for diffusion-based models.\nExisting methods struggle with layout discontinuity, entity identity drift, and\nimplausible interaction dynamics due to unconstrained cross-attention\nmechanisms and inadequate physics-aware reasoning. To address these\nlimitations, we propose DyST-XL, a \\textbf{training-free} framework that\nenhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through\nframe-aware control. DyST-XL integrates three key innovations: (1) A Dynamic\nLayout Planner that leverages large language models (LLMs) to parse input\nprompts into entity-attribute graphs and generates physics-aware keyframe\nlayouts, with intermediate frames interpolated via trajectory optimization; (2)\nA Dual-Prompt Controlled Attention Mechanism that enforces localized text-video\nalignment through frame-aware attention masking, achieving the precise control\nover individual entities; and (3) An Entity-Consistency Constraint strategy\nthat propagates first-frame feature embeddings to subsequent frames during\ndenoising, preserving object identity without manual annotation. Experiments\ndemonstrate that DyST-XL excels in compositional text-to-video generation,\nsignificantly improving performance on complex prompts and bridging a crucial\ngap in training-free video synthesis."
                },
                "authors": [
                    {
                        "name": "Weijie He"
                    },
                    {
                        "name": "Mushui Liu"
                    },
                    {
                        "name": "Yunlong Yu"
                    },
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Chao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chao Wu"
                },
                "author": "Chao Wu",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15027v1",
                "updated": "2025-04-21T11:26:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    26,
                    2,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:26:02Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    26,
                    2,
                    0,
                    111,
                    0
                ],
                "title": "DistilQwen2.5: Industrial Practices of Training Distilled Open\n  Lightweight Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DistilQwen2.5: Industrial Practices of Training Distilled Open\n  Lightweight Language Models"
                },
                "summary": "Enhancing computational efficiency and reducing deployment costs for large\nlanguage models (LLMs) have become critical challenges in various\nresource-constrained scenarios. In this work, we present DistilQwen2.5, a\nfamily of distilled, lightweight LLMs derived from the public Qwen2.5 models.\nThese distilled models exhibit enhanced instruction-following capabilities\ncompared to the original models based on a series of distillation techniques\nthat incorporate knowledge from much larger LLMs. In our industrial practice,\nwe first leverage powerful proprietary LLMs with varying capacities as\nmulti-agent teachers to select, rewrite, and refine instruction-response pairs\nthat are more suitable for student LLMs to learn. After standard fine-tuning,\nwe further leverage a computationally efficient model fusion approach that\nenables student models to progressively integrate fine-grained hidden knowledge\nfrom their teachers. Experimental evaluations demonstrate that the distilled\nmodels possess significantly stronger capabilities than their original\ncheckpoints. Additionally, we present use cases to illustrate the applications\nof our framework in real-world scenarios. To facilitate practical use, we have\nreleased all the DistilQwen2.5 models to the open-source community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing computational efficiency and reducing deployment costs for large\nlanguage models (LLMs) have become critical challenges in various\nresource-constrained scenarios. In this work, we present DistilQwen2.5, a\nfamily of distilled, lightweight LLMs derived from the public Qwen2.5 models.\nThese distilled models exhibit enhanced instruction-following capabilities\ncompared to the original models based on a series of distillation techniques\nthat incorporate knowledge from much larger LLMs. In our industrial practice,\nwe first leverage powerful proprietary LLMs with varying capacities as\nmulti-agent teachers to select, rewrite, and refine instruction-response pairs\nthat are more suitable for student LLMs to learn. After standard fine-tuning,\nwe further leverage a computationally efficient model fusion approach that\nenables student models to progressively integrate fine-grained hidden knowledge\nfrom their teachers. Experimental evaluations demonstrate that the distilled\nmodels possess significantly stronger capabilities than their original\ncheckpoints. Additionally, we present use cases to illustrate the applications\nof our framework in real-world scenarios. To facilitate practical use, we have\nreleased all the DistilQwen2.5 models to the open-source community."
                },
                "authors": [
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Junbing Yan"
                    },
                    {
                        "name": "Yuanhao Yue"
                    },
                    {
                        "name": "Jun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Huang"
                },
                "author": "Jun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15022v1",
                "updated": "2025-04-21T11:11:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    11,
                    7,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:11:07Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    11,
                    7,
                    0,
                    111,
                    0
                ],
                "title": "LLMs as Data Annotators: How Close Are We to Human Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Data Annotators: How Close Are We to Human Performance"
                },
                "summary": "In NLP, fine-tuning LLMs is effective for various applications but requires\nhigh-quality annotated data. However, manual annotation of data is\nlabor-intensive, time-consuming, and costly. Therefore, LLMs are increasingly\nused to automate the process, often employing in-context learning (ICL) in\nwhich some examples related to the task are given in the prompt for better\nperformance. However, manually selecting context examples can lead to\ninefficiencies and suboptimal model performance. This paper presents\ncomprehensive experiments comparing several LLMs, considering different\nembedding models, across various datasets for the Named Entity Recognition\n(NER) task. The evaluation encompasses models with approximately $7$B and $70$B\nparameters, including both proprietary and non-proprietary models. Furthermore,\nleveraging the success of Retrieval-Augmented Generation (RAG), it also\nconsiders a method that addresses the limitations of ICL by automatically\nretrieving contextual examples, thereby enhancing performance. The results\nhighlight the importance of selecting the appropriate LLM and embedding model,\nunderstanding the trade-offs between LLM sizes and desired performance, and the\nnecessity to direct research efforts towards more challenging datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In NLP, fine-tuning LLMs is effective for various applications but requires\nhigh-quality annotated data. However, manual annotation of data is\nlabor-intensive, time-consuming, and costly. Therefore, LLMs are increasingly\nused to automate the process, often employing in-context learning (ICL) in\nwhich some examples related to the task are given in the prompt for better\nperformance. However, manually selecting context examples can lead to\ninefficiencies and suboptimal model performance. This paper presents\ncomprehensive experiments comparing several LLMs, considering different\nembedding models, across various datasets for the Named Entity Recognition\n(NER) task. The evaluation encompasses models with approximately $7$B and $70$B\nparameters, including both proprietary and non-proprietary models. Furthermore,\nleveraging the success of Retrieval-Augmented Generation (RAG), it also\nconsiders a method that addresses the limitations of ICL by automatically\nretrieving contextual examples, thereby enhancing performance. The results\nhighlight the importance of selecting the appropriate LLM and embedding model,\nunderstanding the trade-offs between LLM sizes and desired performance, and the\nnecessity to direct research efforts towards more challenging datasets."
                },
                "authors": [
                    {
                        "name": "Muhammad Uzair Ul Haq"
                    },
                    {
                        "name": "Davide Rigoni"
                    },
                    {
                        "name": "Alessandro Sperduti"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Sperduti"
                },
                "author": "Alessandro Sperduti",
                "arxiv_comment": "27 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15544v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15544v3",
                "updated": "2025-04-21T11:09:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    11,
                    0,
                    111,
                    0
                ],
                "published": "2025-01-26T14:31:03Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    14,
                    31,
                    3,
                    6,
                    26,
                    0
                ],
                "title": "Advancing Generative Artificial Intelligence and Large Language Models\n  for Demand Side Management with Internet of Electric Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Generative Artificial Intelligence and Large Language Models\n  for Demand Side Management with Internet of Electric Vehicles"
                },
                "summary": "Generative artificial intelligence, particularly through large language\nmodels (LLMs), is poised to transform energy optimization and demand side\nmanagement (DSM) within microgrids. This paper explores the integration of LLMs\ninto energy management, emphasizing their roles in automating the optimization\nof DSM strategies with Internet of electric vehicles. We investigate challenges\nand solutions associated with DSM and explore the new opportunities presented\nby leveraging LLMs. Then, we propose an innovative solution that enhances LLMs\nwith retrieval-augmented generation for automatic problem formulation, code\ngeneration, and customizing optimization. We present a case study to\ndemonstrate the effectiveness of our proposed solution in charging scheduling\nand optimization for electric vehicles, highlighting our solution's significant\nadvancements in energy efficiency and user adaptability. This work underscores\nthe potential of LLMs for energy optimization and fosters a new era of\nintelligent DSM solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligence, particularly through large language\nmodels (LLMs), is poised to transform energy optimization and demand side\nmanagement (DSM) within microgrids. This paper explores the integration of LLMs\ninto energy management, emphasizing their roles in automating the optimization\nof DSM strategies with Internet of electric vehicles. We investigate challenges\nand solutions associated with DSM and explore the new opportunities presented\nby leveraging LLMs. Then, we propose an innovative solution that enhances LLMs\nwith retrieval-augmented generation for automatic problem formulation, code\ngeneration, and customizing optimization. We present a case study to\ndemonstrate the effectiveness of our proposed solution in charging scheduling\nand optimization for electric vehicles, highlighting our solution's significant\nadvancements in energy efficiency and user adaptability. This work underscores\nthe potential of LLMs for energy optimization and fosters a new era of\nintelligent DSM solutions."
                },
                "authors": [
                    {
                        "name": "Hanwen Zhang"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen",
                "arxiv_comment": "9 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15544v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15544v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06961v2",
                "updated": "2025-04-21T10:49:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    10,
                    49,
                    13,
                    0,
                    111,
                    0
                ],
                "published": "2025-01-12T22:47:40Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    22,
                    47,
                    40,
                    6,
                    12,
                    0
                ],
                "title": "Impact of weak-lensing mass-mapping algorithms on cosmology inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of weak-lensing mass-mapping algorithms on cosmology inference"
                },
                "summary": "Weak-lensing mass-mapping algorithms, which reconstruct the convergence field\nfrom galaxy shear measurements, are crucial for extracting higher-order\nstatistics to constrain cosmological parameters. However, only limited research\nhas explored whether the choice of mass-mapping algorithm affects the inference\nof cosmological parameters from weak-lensing higher-order statistics. This\nstudy aims to evaluate the impact of different mass-mapping algorithms on the\ninference of cosmological parameters measured with weak-lensing peak counts. We\nemploy Kaiser-Squires, inpainting Kaiser-Squires, and MCALens mass-mapping\nalgorithms to reconstruct the convergence field from simulated weak-lensing\ndata. Using these maps, we compute the peak counts and wavelet peak counts as\ndata vectors and perform Bayesian analysis with MCMC sampling to estimate\nposterior distributions of cosmological parameters. Our results indicate that\nthe choice of mass-mapping algorithm significantly affects the constraints on\ncosmological parameters, with the MCALens method improving constraints by up to\n157$\\%$ compared to the standard Kaiser-Squires method. This improvement arises\nfrom MCALens' ability to better capture small-scale structures. In contrast,\ninpainting Kaiser-Squires yields constraints similar to Kaiser-Squires,\nindicating a limited benefit from inpainting for cosmological parameter\nestimation with peaks. The accuracy of mass-mapping algorithms is thus critical\nfor cosmological inference from weak-lensing data. Advanced algorithms like\nMCALens, which offer superior reconstruction of the convergence field, can\nsubstantially enhance the precision of cosmological parameter estimates. These\nfindings underscore the importance of selecting appropriate mass-mapping\ntechniques in weak-lensing studies to fully exploit the potential of\nhigher-order statistics for cosmological research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weak-lensing mass-mapping algorithms, which reconstruct the convergence field\nfrom galaxy shear measurements, are crucial for extracting higher-order\nstatistics to constrain cosmological parameters. However, only limited research\nhas explored whether the choice of mass-mapping algorithm affects the inference\nof cosmological parameters from weak-lensing higher-order statistics. This\nstudy aims to evaluate the impact of different mass-mapping algorithms on the\ninference of cosmological parameters measured with weak-lensing peak counts. We\nemploy Kaiser-Squires, inpainting Kaiser-Squires, and MCALens mass-mapping\nalgorithms to reconstruct the convergence field from simulated weak-lensing\ndata. Using these maps, we compute the peak counts and wavelet peak counts as\ndata vectors and perform Bayesian analysis with MCMC sampling to estimate\nposterior distributions of cosmological parameters. Our results indicate that\nthe choice of mass-mapping algorithm significantly affects the constraints on\ncosmological parameters, with the MCALens method improving constraints by up to\n157$\\%$ compared to the standard Kaiser-Squires method. This improvement arises\nfrom MCALens' ability to better capture small-scale structures. In contrast,\ninpainting Kaiser-Squires yields constraints similar to Kaiser-Squires,\nindicating a limited benefit from inpainting for cosmological parameter\nestimation with peaks. The accuracy of mass-mapping algorithms is thus critical\nfor cosmological inference from weak-lensing data. Advanced algorithms like\nMCALens, which offer superior reconstruction of the convergence field, can\nsubstantially enhance the precision of cosmological parameter estimates. These\nfindings underscore the importance of selecting appropriate mass-mapping\ntechniques in weak-lensing studies to fully exploit the potential of\nhigher-order statistics for cosmological research."
                },
                "authors": [
                    {
                        "name": "Andreas Tersenov"
                    },
                    {
                        "name": "Lucie Baumont"
                    },
                    {
                        "name": "Jean-Luc Starck"
                    },
                    {
                        "name": "Martin Kilbinger"
                    }
                ],
                "author_detail": {
                    "name": "Martin Kilbinger"
                },
                "author": "Martin Kilbinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15013v1",
                "updated": "2025-04-21T10:35:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    10,
                    35,
                    48,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T10:35:48Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    10,
                    35,
                    48,
                    0,
                    111,
                    0
                ],
                "title": "Stay Hungry, Stay Foolish: On the Extended Reading Articles Generation\n  with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stay Hungry, Stay Foolish: On the Extended Reading Articles Generation\n  with LLMs"
                },
                "summary": "The process of creating educational materials is both time-consuming and\ndemanding for educators. This research explores the potential of Large Language\nModels (LLMs) to streamline this task by automating the generation of extended\nreading materials and relevant course suggestions. Using the TED-Ed Dig Deeper\nsections as an initial exploration, we investigate how supplementary articles\ncan be enriched with contextual knowledge and connected to additional learning\nresources. Our method begins by generating extended articles from video\ntranscripts, leveraging LLMs to include historical insights, cultural examples,\nand illustrative anecdotes. A recommendation system employing semantic\nsimilarity ranking identifies related courses, followed by an LLM-based\nrefinement process to enhance relevance. The final articles are tailored to\nseamlessly integrate these recommendations, ensuring they remain cohesive and\ninformative. Experimental evaluations demonstrate that our model produces\nhigh-quality content and accurate course suggestions, assessed through metrics\nsuch as Hit Rate, semantic similarity, and coherence. Our experimental analysis\nhighlight the nuanced differences between the generated and existing materials,\nunderscoring the model's capacity to offer more engaging and accessible\nlearning experiences. This study showcases how LLMs can bridge the gap between\ncore content and supplementary learning, providing students with additional\nrecommended resources while also assisting teachers in designing educational\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The process of creating educational materials is both time-consuming and\ndemanding for educators. This research explores the potential of Large Language\nModels (LLMs) to streamline this task by automating the generation of extended\nreading materials and relevant course suggestions. Using the TED-Ed Dig Deeper\nsections as an initial exploration, we investigate how supplementary articles\ncan be enriched with contextual knowledge and connected to additional learning\nresources. Our method begins by generating extended articles from video\ntranscripts, leveraging LLMs to include historical insights, cultural examples,\nand illustrative anecdotes. A recommendation system employing semantic\nsimilarity ranking identifies related courses, followed by an LLM-based\nrefinement process to enhance relevance. The final articles are tailored to\nseamlessly integrate these recommendations, ensuring they remain cohesive and\ninformative. Experimental evaluations demonstrate that our model produces\nhigh-quality content and accurate course suggestions, assessed through metrics\nsuch as Hit Rate, semantic similarity, and coherence. Our experimental analysis\nhighlight the nuanced differences between the generated and existing materials,\nunderscoring the model's capacity to offer more engaging and accessible\nlearning experiences. This study showcases how LLMs can bridge the gap between\ncore content and supplementary learning, providing students with additional\nrecommended resources while also assisting teachers in designing educational\nmaterials."
                },
                "authors": [
                    {
                        "name": "Yow-Fu Liou"
                    },
                    {
                        "name": "Yu-Chien Tang"
                    },
                    {
                        "name": "An-Zi Yen"
                    }
                ],
                "author_detail": {
                    "name": "An-Zi Yen"
                },
                "author": "An-Zi Yen",
                "arxiv_comment": "Accepted by iRAISE@AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13074v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13074v3",
                "updated": "2025-04-21T10:34:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    10,
                    34,
                    50,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-17T16:37:27Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    37,
                    27,
                    3,
                    107,
                    0
                ],
                "title": "SkyReels-V2: Infinite-length Film Generative Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyReels-V2: Infinite-length Film Generative Model"
                },
                "summary": "Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2."
                },
                "authors": [
                    {
                        "name": "Guibin Chen"
                    },
                    {
                        "name": "Dixuan Lin"
                    },
                    {
                        "name": "Jiangping Yang"
                    },
                    {
                        "name": "Chunze Lin"
                    },
                    {
                        "name": "Junchen Zhu"
                    },
                    {
                        "name": "Mingyuan Fan"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Sheng Chen"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Chengcheng Ma"
                    },
                    {
                        "name": "Weiming Xiong"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Nuo Pang"
                    },
                    {
                        "name": "Kang Kang"
                    },
                    {
                        "name": "Zhiheng Xu"
                    },
                    {
                        "name": "Yuzhe Jin"
                    },
                    {
                        "name": "Yupeng Liang"
                    },
                    {
                        "name": "Yubing Song"
                    },
                    {
                        "name": "Peng Zhao"
                    },
                    {
                        "name": "Boyuan Xu"
                    },
                    {
                        "name": "Di Qiu"
                    },
                    {
                        "name": "Debang Li"
                    },
                    {
                        "name": "Zhengcong Fei"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yahui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yahui Zhou"
                },
                "author": "Yahui Zhou",
                "arxiv_comment": "31 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13074v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13074v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08041v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08041v4",
                "updated": "2025-04-21T10:30:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    10,
                    30,
                    34,
                    0,
                    111,
                    0
                ],
                "published": "2024-12-11T02:44:14Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    2,
                    44,
                    14,
                    2,
                    346,
                    0
                ],
                "title": "Quantifying the benefits of code hints for refactoring deprecated Java\n  APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the benefits of code hints for refactoring deprecated Java\n  APIs"
                },
                "summary": "When done manually, refactoring legacy code in order to eliminate uses of\ndeprecated APIs is an error-prone and time-consuming process. In this paper, we\ninvestigate to which degree refactorings for deprecated Java APIs can be\nautomated, and quantify the benefit of Javadoc code hints for this task. To\nthis end, we build a symbolic and a neural engine for the automatic refactoring\nof deprecated APIs. The former is based on type-directed and component-based\nprogram synthesis, whereas the latter uses LLMs. We applied our engines to\nrefactor the deprecated methods in the Oracle JDK 15. Our experiments show that\ncode hints are enabling for the automation of this task: even the worst engine\ncorrectly refactors 71% of the tasks with code hints, which drops to at best\n14% on tasks without. Adding more code hints to Javadoc can hence boost the\nrefactoring of code that uses deprecated APIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When done manually, refactoring legacy code in order to eliminate uses of\ndeprecated APIs is an error-prone and time-consuming process. In this paper, we\ninvestigate to which degree refactorings for deprecated Java APIs can be\nautomated, and quantify the benefit of Javadoc code hints for this task. To\nthis end, we build a symbolic and a neural engine for the automatic refactoring\nof deprecated APIs. The former is based on type-directed and component-based\nprogram synthesis, whereas the latter uses LLMs. We applied our engines to\nrefactor the deprecated methods in the Oracle JDK 15. Our experiments show that\ncode hints are enabling for the automation of this task: even the worst engine\ncorrectly refactors 71% of the tasks with code hints, which drops to at best\n14% on tasks without. Adding more code hints to Javadoc can hence boost the\nrefactoring of code that uses deprecated APIs."
                },
                "authors": [
                    {
                        "name": "Cristina David"
                    },
                    {
                        "name": "Pascal Kesseli"
                    },
                    {
                        "name": "Daniel Kroening"
                    },
                    {
                        "name": "Hanliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hanliang Zhang"
                },
                "author": "Hanliang Zhang",
                "arxiv_doi": "10.1145/3696630.3728567",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696630.3728567",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.08041v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08041v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "33rd ACM International Conference on the Foundations of Software\n  Engineering (FSE Companion '25), June 23--28, 2025, Trondheim, Norway",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14994v1",
                "updated": "2025-04-21T09:51:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    51,
                    24,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T09:51:24Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    51,
                    24,
                    0,
                    111,
                    0
                ],
                "title": "Learning Compositional Transferability of Time Series for Source-Free\n  Domain Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Compositional Transferability of Time Series for Source-Free\n  Domain Adaptation"
                },
                "summary": "Domain adaptation is challenging for time series classification due to the\nhighly dynamic nature. This study tackles the most difficult subtask when both\ntarget labels and source data are inaccessible, namely, source-free domain\nadaptation. To reuse the classification backbone pre-trained on source data,\ntime series reconstruction is a sound solution that aligns target and source\ntime series by minimizing the reconstruction errors of both. However, simply\nfine-tuning the source pre-trained reconstruction model on target data may lose\nthe learnt priori, and it struggles to accommodate domain varying temporal\npatterns in a single encoder-decoder. Therefore, this paper tries to\ndisentangle the composition of domain transferability by using a compositional\narchitecture for time series reconstruction. Here, the preceding component is a\nU-net frozen since pre-trained, the output of which during adaptation is the\ninitial reconstruction of a given target time series, acting as a coarse step\nto prompt the subsequent finer adaptation. The following pipeline for finer\nadaptation includes two parallel branches: The source replay branch using a\nresidual link to preserve the output of U-net, and the offset compensation\nbranch that applies an additional autoencoder (AE) to further warp U-net's\noutput. By deploying a learnable factor on either branch to scale their\ncomposition in the final output of reconstruction, the data transferability is\ndisentangled and the learnt reconstructive capability from source data is\nretained. During inference, aside from the batch-level optimization in the\ntraining, we search at test time stability-aware rescaling of source replay\nbranch to tolerate instance-wise variation. The experimental results show that\nsuch compositional architecture of time series reconstruction leads to SOTA\nperformance on 3 widely used benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain adaptation is challenging for time series classification due to the\nhighly dynamic nature. This study tackles the most difficult subtask when both\ntarget labels and source data are inaccessible, namely, source-free domain\nadaptation. To reuse the classification backbone pre-trained on source data,\ntime series reconstruction is a sound solution that aligns target and source\ntime series by minimizing the reconstruction errors of both. However, simply\nfine-tuning the source pre-trained reconstruction model on target data may lose\nthe learnt priori, and it struggles to accommodate domain varying temporal\npatterns in a single encoder-decoder. Therefore, this paper tries to\ndisentangle the composition of domain transferability by using a compositional\narchitecture for time series reconstruction. Here, the preceding component is a\nU-net frozen since pre-trained, the output of which during adaptation is the\ninitial reconstruction of a given target time series, acting as a coarse step\nto prompt the subsequent finer adaptation. The following pipeline for finer\nadaptation includes two parallel branches: The source replay branch using a\nresidual link to preserve the output of U-net, and the offset compensation\nbranch that applies an additional autoencoder (AE) to further warp U-net's\noutput. By deploying a learnable factor on either branch to scale their\ncomposition in the final output of reconstruction, the data transferability is\ndisentangled and the learnt reconstructive capability from source data is\nretained. During inference, aside from the batch-level optimization in the\ntraining, we search at test time stability-aware rescaling of source replay\nbranch to tolerate instance-wise variation. The experimental results show that\nsuch compositional architecture of time series reconstruction leads to SOTA\nperformance on 3 widely used benchmarks."
                },
                "authors": [
                    {
                        "name": "Hankang Sun"
                    },
                    {
                        "name": "Guiming Li"
                    },
                    {
                        "name": "Su Yang"
                    },
                    {
                        "name": "Baoqi Li"
                    }
                ],
                "author_detail": {
                    "name": "Baoqi Li"
                },
                "arxiv_affiliation": "Chinese Academy of Sciences, Institute of Acoustics",
                "author": "Baoqi Li",
                "arxiv_comment": "Corresponding author: Su Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06635v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06635v4",
                "updated": "2025-04-21T09:48:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    48,
                    5,
                    0,
                    111,
                    0
                ],
                "published": "2024-09-10T16:46:18Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    46,
                    18,
                    1,
                    254,
                    0
                ],
                "title": "MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders"
                },
                "summary": "The rapid advancements in large language models (LLMs) have significantly\nenhanced natural language processing capabilities, facilitating the development\nof AudioLLMs that process and understand speech and audio inputs alongside\ntext. Existing AudioLLMs typically combine a pre-trained audio encoder with a\npre-trained LLM, which are subsequently finetuned on specific audio tasks.\nHowever, the pre-trained audio encoder has constrained capacity to capture\nfeatures for new tasks and datasets. To address this, we propose to incorporate\nmixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE\nsupplements a base encoder with a pool of relatively light weight encoders,\nselectively activated based on the audio input to enhance feature extraction\nwithout significantly increasing model size. Our empirical results demonstrate\nthat MoWE effectively improves multi-task performance, broadening the\napplicability of AudioLLMs to more diverse audio tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in large language models (LLMs) have significantly\nenhanced natural language processing capabilities, facilitating the development\nof AudioLLMs that process and understand speech and audio inputs alongside\ntext. Existing AudioLLMs typically combine a pre-trained audio encoder with a\npre-trained LLM, which are subsequently finetuned on specific audio tasks.\nHowever, the pre-trained audio encoder has constrained capacity to capture\nfeatures for new tasks and datasets. To address this, we propose to incorporate\nmixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE\nsupplements a base encoder with a pool of relatively light weight encoders,\nselectively activated based on the audio input to enhance feature extraction\nwithout significantly increasing model size. Our empirical results demonstrate\nthat MoWE effectively improves multi-task performance, broadening the\napplicability of AudioLLMs to more diverse audio tasks."
                },
                "authors": [
                    {
                        "name": "Wenyu Zhang"
                    },
                    {
                        "name": "Shuo Sun"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Xunlong Zou"
                    },
                    {
                        "name": "Zhuohan Liu"
                    },
                    {
                        "name": "Yingxu He"
                    },
                    {
                        "name": "Geyu Lin"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Ai Ti Aw"
                    }
                ],
                "author_detail": {
                    "name": "Ai Ti Aw"
                },
                "author": "Ai Ti Aw",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06635v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06635v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14992v1",
                "updated": "2025-04-21T09:41:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T09:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Pretraining Length Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pretraining Length Scaling"
                },
                "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03312v2",
                "updated": "2025-04-21T09:34:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    34,
                    59,
                    0,
                    111,
                    0
                ],
                "published": "2024-11-05T18:54:21Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    18,
                    54,
                    21,
                    1,
                    310,
                    0
                ],
                "title": "Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters"
                },
                "summary": "Vision Language Models (VLMs) have demonstrated strong capabilities across\nvarious visual understanding and reasoning tasks, driven by incorporating image\nrepresentations into the token inputs of Large Language Models (LLMs). However,\ntheir real-world deployment is often constrained by high latency during\ninference due to the substantial compute required by the LLM to process the\nlarge number of input tokens, predominantly arising from the image. To reduce\ninference costs, one can either downsize the LLM or reduce the number of input\ntokens needed to represent the image, the latter of which has been the focus of\nmany recent efforts around token compression. However, it is unclear what the\noptimal trade-off is given a fixed inference budget. We first characterize this\noptimal trade-off between the number of visual tokens and LLM parameters by\nestablishing scaling laws that capture variations in performance with these two\nfactors. Our results reveal a surprising trend: for visual reasoning tasks, the\ninference-optimal behavior in VLMs is achieved by using the largest LLM that\nfits within the inference budget while minimizing visual token count - often to\na single token. While the token reduction literature has mainly focused on\nmaintaining base model performance by modestly reducing the token count (e.g.,\n$5-10\\times$), our results indicate that the compute-optimal inference regime\nrequires operating under even higher token compression ratios. Based on these\ninsights, we take the first steps toward designing token compression algorithms\ntailored for high-compression settings, utilizing prompt-based compression of\ntokens. Our work underscores the performance and efficiency benefits of\noperating in low visual token regimes and the importance of developing tailored\ntoken reduction algorithms for such conditions. Code is available at\nhttps://github.com/locuslab/llava-token-compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have demonstrated strong capabilities across\nvarious visual understanding and reasoning tasks, driven by incorporating image\nrepresentations into the token inputs of Large Language Models (LLMs). However,\ntheir real-world deployment is often constrained by high latency during\ninference due to the substantial compute required by the LLM to process the\nlarge number of input tokens, predominantly arising from the image. To reduce\ninference costs, one can either downsize the LLM or reduce the number of input\ntokens needed to represent the image, the latter of which has been the focus of\nmany recent efforts around token compression. However, it is unclear what the\noptimal trade-off is given a fixed inference budget. We first characterize this\noptimal trade-off between the number of visual tokens and LLM parameters by\nestablishing scaling laws that capture variations in performance with these two\nfactors. Our results reveal a surprising trend: for visual reasoning tasks, the\ninference-optimal behavior in VLMs is achieved by using the largest LLM that\nfits within the inference budget while minimizing visual token count - often to\na single token. While the token reduction literature has mainly focused on\nmaintaining base model performance by modestly reducing the token count (e.g.,\n$5-10\\times$), our results indicate that the compute-optimal inference regime\nrequires operating under even higher token compression ratios. Based on these\ninsights, we take the first steps toward designing token compression algorithms\ntailored for high-compression settings, utilizing prompt-based compression of\ntokens. Our work underscores the performance and efficiency benefits of\noperating in low visual token regimes and the importance of developing tailored\ntoken reduction algorithms for such conditions. Code is available at\nhttps://github.com/locuslab/llava-token-compression."
                },
                "authors": [
                    {
                        "name": "Kevin Y. Li"
                    },
                    {
                        "name": "Sachin Goyal"
                    },
                    {
                        "name": "Joao D. Semedo"
                    },
                    {
                        "name": "J. Zico Kolter"
                    }
                ],
                "author_detail": {
                    "name": "J. Zico Kolter"
                },
                "author": "J. Zico Kolter",
                "arxiv_comment": "Published at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14985v1",
                "updated": "2025-04-21T09:26:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    26,
                    5,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T09:26:05Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    26,
                    5,
                    0,
                    111,
                    0
                ],
                "title": "aiXamine: LLM Safety and Security Simplified",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "aiXamine: LLM Safety and Security Simplified"
                },
                "summary": "Evaluating Large Language Models (LLMs) for safety and security remains a\ncomplex task, often requiring users to navigate a fragmented landscape of ad\nhoc benchmarks, datasets, metrics, and reporting formats. To address this\nchallenge, we present aiXamine, a comprehensive black-box evaluation platform\nfor LLM safety and security. aiXamine integrates over 40 tests (i.e.,\nbenchmarks) organized into eight key services targeting specific dimensions of\nsafety and security: adversarial robustness, code security, fairness and bias,\nhallucination, model and data privacy, out-of-distribution (OOD) robustness,\nover-refusal, and safety alignment. The platform aggregates the evaluation\nresults into a single detailed report per model, providing a detailed breakdown\nof model performance, test examples, and rich visualizations. We used aiXamine\nto assess over 50 publicly available and proprietary LLMs, conducting over 2K\nexaminations. Our findings reveal notable vulnerabilities in leading models,\nincluding susceptibility to adversarial attacks in OpenAI's GPT-4o, biased\noutputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.\nAdditionally, we observe that open-source models can match or exceed\nproprietary models in specific services such as safety alignment, fairness and\nbias, and OOD robustness. Finally, we identify trade-offs between distillation\nstrategies, model size, training methods, and architectural choices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) for safety and security remains a\ncomplex task, often requiring users to navigate a fragmented landscape of ad\nhoc benchmarks, datasets, metrics, and reporting formats. To address this\nchallenge, we present aiXamine, a comprehensive black-box evaluation platform\nfor LLM safety and security. aiXamine integrates over 40 tests (i.e.,\nbenchmarks) organized into eight key services targeting specific dimensions of\nsafety and security: adversarial robustness, code security, fairness and bias,\nhallucination, model and data privacy, out-of-distribution (OOD) robustness,\nover-refusal, and safety alignment. The platform aggregates the evaluation\nresults into a single detailed report per model, providing a detailed breakdown\nof model performance, test examples, and rich visualizations. We used aiXamine\nto assess over 50 publicly available and proprietary LLMs, conducting over 2K\nexaminations. Our findings reveal notable vulnerabilities in leading models,\nincluding susceptibility to adversarial attacks in OpenAI's GPT-4o, biased\noutputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.\nAdditionally, we observe that open-source models can match or exceed\nproprietary models in specific services such as safety alignment, fairness and\nbias, and OOD robustness. Finally, we identify trade-offs between distillation\nstrategies, model size, training methods, and architectural choices."
                },
                "authors": [
                    {
                        "name": "Fatih Deniz"
                    },
                    {
                        "name": "Dorde Popovic"
                    },
                    {
                        "name": "Yazan Boshmaf"
                    },
                    {
                        "name": "Euisuh Jeong"
                    },
                    {
                        "name": "Minhaj Ahmad"
                    },
                    {
                        "name": "Sanjay Chawla"
                    },
                    {
                        "name": "Issa Khalil"
                    }
                ],
                "author_detail": {
                    "name": "Issa Khalil"
                },
                "author": "Issa Khalil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14980v1",
                "updated": "2025-04-21T09:14:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    14,
                    59,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T09:14:59Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    14,
                    59,
                    0,
                    111,
                    0
                ],
                "title": "Bayesian model selection of Primordial Black Holes and Dressed\n  Primordial Black Holes with lensed Gravitational Waves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian model selection of Primordial Black Holes and Dressed\n  Primordial Black Holes with lensed Gravitational Waves"
                },
                "summary": "If particle dark matter (DM) and primordial black holes (PBHs) coexist, PBHs\nwill be surrounded by particle DM, forming celestial objects known as dressed\nPBHs (dPBHs). These structures suggest a scenario in which PBHs and DM can\nexist simultaneously. However, in the high-frequency regime, the gravitational\nlensing effect of bare PBHs is similar to that of dPBHs. Ground-based\ngravitational wave (GW) detectors are particularly sensitive to high-frequency\nGW signals. In this regime, the lensing effect of a point-mass lens with a mass\nin the range of $10^{-1} \\sim 10^2 M_{\\odot}$ becomes significant. In this\nwork, we incorporate dPBH models with GW observations and employ Bayesian\ninference techniques to distinguish PBHs from dPBHs. Using the third-generation\nground-based GW detectors, Einstein Telescope (ET) and Cosmic Explorer (CE), as\nexamples, we demonstrate that these detectors can effectively differentiate the\nlensing effects of dPBHs from those of PBHs across a broad frequency range.\nFurthermore, we find that with a larger black hole (BH) mass inside the\nsurrounding particle DM, ET and CE can distinguish these two lensed models with\neven greater precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If particle dark matter (DM) and primordial black holes (PBHs) coexist, PBHs\nwill be surrounded by particle DM, forming celestial objects known as dressed\nPBHs (dPBHs). These structures suggest a scenario in which PBHs and DM can\nexist simultaneously. However, in the high-frequency regime, the gravitational\nlensing effect of bare PBHs is similar to that of dPBHs. Ground-based\ngravitational wave (GW) detectors are particularly sensitive to high-frequency\nGW signals. In this regime, the lensing effect of a point-mass lens with a mass\nin the range of $10^{-1} \\sim 10^2 M_{\\odot}$ becomes significant. In this\nwork, we incorporate dPBH models with GW observations and employ Bayesian\ninference techniques to distinguish PBHs from dPBHs. Using the third-generation\nground-based GW detectors, Einstein Telescope (ET) and Cosmic Explorer (CE), as\nexamples, we demonstrate that these detectors can effectively differentiate the\nlensing effects of dPBHs from those of PBHs across a broad frequency range.\nFurthermore, we find that with a larger black hole (BH) mass inside the\nsurrounding particle DM, ET and CE can distinguish these two lensed models with\neven greater precision."
                },
                "authors": [
                    {
                        "name": "Xin-yi Lin"
                    },
                    {
                        "name": "Zhengxiang Li"
                    },
                    {
                        "name": "Jian-dong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian-dong Zhang"
                },
                "author": "Jian-dong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14969v1",
                "updated": "2025-04-21T08:56:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    56,
                    23,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T08:56:23Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    56,
                    23,
                    0,
                    111,
                    0
                ],
                "title": "Evaluating LLMs on Chinese Topic Constructions: A Research Proposal\n  Inspired by Tian et al. (2024)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs on Chinese Topic Constructions: A Research Proposal\n  Inspired by Tian et al. (2024)"
                },
                "summary": "This paper proposes a framework for evaluating large language models (LLMs)\non Chinese topic constructions, focusing on their sensitivity to island\nconstraints. Drawing inspiration from Tian et al. (2024), we outline an\nexperimental design for testing LLMs' grammatical knowledge of Mandarin syntax.\nWhile no experiments have been conducted yet, this proposal aims to provide a\nfoundation for future studies and invites feedback on the methodology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a framework for evaluating large language models (LLMs)\non Chinese topic constructions, focusing on their sensitivity to island\nconstraints. Drawing inspiration from Tian et al. (2024), we outline an\nexperimental design for testing LLMs' grammatical knowledge of Mandarin syntax.\nWhile no experiments have been conducted yet, this proposal aims to provide a\nfoundation for future studies and invites feedback on the methodology."
                },
                "authors": [
                    {
                        "name": "Xiaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong Yang"
                },
                "author": "Xiaodong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14966v1",
                "updated": "2025-04-21T08:48:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    48,
                    48,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T08:48:48Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    48,
                    48,
                    0,
                    111,
                    0
                ],
                "title": "SLO-Aware Scheduling for Large Language Model Inferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-Aware Scheduling for Large Language Model Inferences"
                },
                "summary": "Large language models (LLMs) have revolutionized applications such as code\ncompletion, chatbots, and online classification. To elevate user experiences,\nservice level objectives (SLOs) serve as crucial benchmarks for assessing\ninference services capabilities. In practice, an inference service processes\nmultiple types of tasks, each with its own distinct SLO. To ensure satisfactory\nuser experiences, each request's distinct SLOs should be considered in\nscheduling. However, existing designs lack this consideration, leading to\ninsufficient hardware utility and suboptimal performance.\n  This paper analyzes scenarios to process tasks with varying SLOs, and\nintroduces a simulated annealing-based scheduler to decide request priority\nsequence based on a request's SLO, input lengths, and possible output lengths.\nAs the first specialized scheduler for multi-SLO scenarios, this work improves\nSLO attainment by up to 5x and reduces average latency by 31.6% on\nPython-Code-23k-ShareGPT and ShareGPT_Vicuna_unfiltered datasets, compared to\ncurrent state-of-the-art framework vLLM and a new framework LMDeploy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized applications such as code\ncompletion, chatbots, and online classification. To elevate user experiences,\nservice level objectives (SLOs) serve as crucial benchmarks for assessing\ninference services capabilities. In practice, an inference service processes\nmultiple types of tasks, each with its own distinct SLO. To ensure satisfactory\nuser experiences, each request's distinct SLOs should be considered in\nscheduling. However, existing designs lack this consideration, leading to\ninsufficient hardware utility and suboptimal performance.\n  This paper analyzes scenarios to process tasks with varying SLOs, and\nintroduces a simulated annealing-based scheduler to decide request priority\nsequence based on a request's SLO, input lengths, and possible output lengths.\nAs the first specialized scheduler for multi-SLO scenarios, this work improves\nSLO attainment by up to 5x and reduces average latency by 31.6% on\nPython-Code-23k-ShareGPT and ShareGPT_Vicuna_unfiltered datasets, compared to\ncurrent state-of-the-art framework vLLM and a new framework LMDeploy."
                },
                "authors": [
                    {
                        "name": "Jinqi Huang"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Xuebing Yu"
                    },
                    {
                        "name": "Wenjie Huang"
                    },
                    {
                        "name": "Entong Li"
                    },
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Xin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xin Chen"
                },
                "author": "Xin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14964v1",
                "updated": "2025-04-21T08:45:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    45,
                    23,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T08:45:23Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    45,
                    23,
                    0,
                    111,
                    0
                ],
                "title": "Evaluating Code Generation of LLMs in Advanced Computer Science Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Code Generation of LLMs in Advanced Computer Science Problems"
                },
                "summary": "Large Language Models (LLMs), such as GitHub Copilot and ChatGPT have become\npopular among programming students. Students use LLMs to assist them in\nprogramming courses, including generating source code. Previous work has\nevaluated the ability of LLMs in solving introductory-course programming\nassignments. The results have shown that LLMs are highly effective in\ngenerating code for introductory Computer Science (CS) courses. However, there\nis a gap in research on evaluating LLMs' ability to generate code that solves\nadvanced programming assignments. In this work, we evaluate the ability of four\nLLM tools to solve programming assignments from advanced CS courses in three\npopular programming languages, Java, Python, and C. We manually select 12\nproblems, three problems from introductory courses as the baseline and nine\nprogramming assignments from second- and third-year CS courses. To evaluate the\nLLM-generated code, we generate a test suite of 1000 test cases per problem and\nanalyze the program output. Our evaluation shows that although LLMs are highly\neffective in generating source code for introductory programming courses,\nsolving advanced programming assignments is more challenging. Nonetheless, in\nmany cases, LLMs identify the base problem and provide partial solutions that\nmay be useful to CS students. Furthermore, our results may provide useful\nguidance for teachers of advanced programming courses on how to design\nprogramming assignments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GitHub Copilot and ChatGPT have become\npopular among programming students. Students use LLMs to assist them in\nprogramming courses, including generating source code. Previous work has\nevaluated the ability of LLMs in solving introductory-course programming\nassignments. The results have shown that LLMs are highly effective in\ngenerating code for introductory Computer Science (CS) courses. However, there\nis a gap in research on evaluating LLMs' ability to generate code that solves\nadvanced programming assignments. In this work, we evaluate the ability of four\nLLM tools to solve programming assignments from advanced CS courses in three\npopular programming languages, Java, Python, and C. We manually select 12\nproblems, three problems from introductory courses as the baseline and nine\nprogramming assignments from second- and third-year CS courses. To evaluate the\nLLM-generated code, we generate a test suite of 1000 test cases per problem and\nanalyze the program output. Our evaluation shows that although LLMs are highly\neffective in generating source code for introductory programming courses,\nsolving advanced programming assignments is more challenging. Nonetheless, in\nmany cases, LLMs identify the base problem and provide partial solutions that\nmay be useful to CS students. Furthermore, our results may provide useful\nguidance for teachers of advanced programming courses on how to design\nprogramming assignments."
                },
                "authors": [
                    {
                        "name": "Emir Catir"
                    },
                    {
                        "name": "Robin Claesson"
                    },
                    {
                        "name": "Rodothea Myrsini Tsoupidi"
                    }
                ],
                "author_detail": {
                    "name": "Rodothea Myrsini Tsoupidi"
                },
                "author": "Rodothea Myrsini Tsoupidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06193v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06193v3",
                "updated": "2025-04-21T08:41:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    41,
                    21,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-10T06:49:29Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    6,
                    49,
                    29,
                    0,
                    41,
                    0
                ],
                "title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge\n  in Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge\n  in Software Engineering"
                },
                "summary": "Recently, large language models (LLMs) have been deployed to tackle various\nsoftware engineering (SE) tasks like code generation, significantly advancing\nthe automation of SE tasks. However, assessing the quality of these\nLLM-generated code and text remains challenging. The commonly used Pass@k\nmetric necessitates extensive unit tests and configured environments, demands a\nhigh labor cost, and is not suitable for evaluating LLM-generated text.\nConventional metrics like BLEU, which measure only lexical rather than semantic\nsimilarity, have also come under scrutiny. In response, a new trend has emerged\nto employ LLMs for automated evaluation, known as LLM-as-a-judge. These\nLLM-as-a-judge methods are claimed to better mimic human assessment than\nconventional metrics without relying on high-quality reference answers.\nNevertheless, their exact human alignment in SE tasks remains unexplored.\n  In this paper, we empirically explore LLM-as-a-judge methods for evaluating\nSE tasks, focusing on their alignment with human judgments. We select seven\nLLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs\nspecifically fine-tuned for evaluation. After generating and manually scoring\nLLM responses on three recent SE datasets of code translation, code generation,\nand code summarization, we then prompt these methods to evaluate each response.\nFinally, we compare the scores generated by these methods with human\nevaluation. The results indicate that output-based methods reach the highest\nPearson correlation of 81.32 and 68.51 with human scores in code translation\nand generation, achieving near-human evaluation, noticeably outperforming\nChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such\noutput-based methods prompt LLMs to output judgments directly, and exhibit more\nbalanced score distributions that resemble human score patterns. Finally, we\nprovide...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been deployed to tackle various\nsoftware engineering (SE) tasks like code generation, significantly advancing\nthe automation of SE tasks. However, assessing the quality of these\nLLM-generated code and text remains challenging. The commonly used Pass@k\nmetric necessitates extensive unit tests and configured environments, demands a\nhigh labor cost, and is not suitable for evaluating LLM-generated text.\nConventional metrics like BLEU, which measure only lexical rather than semantic\nsimilarity, have also come under scrutiny. In response, a new trend has emerged\nto employ LLMs for automated evaluation, known as LLM-as-a-judge. These\nLLM-as-a-judge methods are claimed to better mimic human assessment than\nconventional metrics without relying on high-quality reference answers.\nNevertheless, their exact human alignment in SE tasks remains unexplored.\n  In this paper, we empirically explore LLM-as-a-judge methods for evaluating\nSE tasks, focusing on their alignment with human judgments. We select seven\nLLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs\nspecifically fine-tuned for evaluation. After generating and manually scoring\nLLM responses on three recent SE datasets of code translation, code generation,\nand code summarization, we then prompt these methods to evaluate each response.\nFinally, we compare the scores generated by these methods with human\nevaluation. The results indicate that output-based methods reach the highest\nPearson correlation of 81.32 and 68.51 with human scores in code translation\nand generation, achieving near-human evaluation, noticeably outperforming\nChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such\noutput-based methods prompt LLMs to output judgments directly, and exhibit more\nbalanced score distributions that resemble human score patterns. Finally, we\nprovide..."
                },
                "authors": [
                    {
                        "name": "Ruiqi Wang"
                    },
                    {
                        "name": "Jiyu Guo"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Guodong Fan"
                    },
                    {
                        "name": "Chun Yong Chong"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "arxiv_doi": "10.1145/3728963",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3728963",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.06193v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06193v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ISSTA 2025:\n  https://conf.researchr.org/details/issta-2025/issta-2025-papers/85/Can-LLMs-replace-Human-Evaluators-An-Empirical-Study-of-LLM-as-a-Judge-in-Software-E",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14955v1",
                "updated": "2025-04-21T08:27:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    27,
                    26,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T08:27:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    27,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Document Retrieval with G-Retriever",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Document Retrieval with G-Retriever"
                },
                "summary": "Textual data question answering has gained significant attention due to its\ngrowing applicability. Recently, a novel approach leveraging the\nRetrieval-Augmented Generation (RAG) method was introduced, utilizing the\nPrize-Collecting Steiner Tree (PCST) optimization for sub-graph construction.\nHowever, this method focused solely on node attributes, leading to incomplete\ncontextual understanding. In this paper, we propose an enhanced approach that\nreplaces the PCST method with an attention-based sub-graph construction\ntechnique, enabling more efficient and context-aware retrieval. Additionally,\nwe encode both node and edge attributes, leading to richer graph\nrepresentations. Our method also incorporates an improved projection layer and\nmulti-head attention pooling for better alignment with Large Language Models\n(LLMs). Experimental evaluations on the WebQSP dataset demonstrate that our\napproach is competitive and achieves marginally better results compared to the\noriginal method, underscoring its potential for more accurate question\nanswering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual data question answering has gained significant attention due to its\ngrowing applicability. Recently, a novel approach leveraging the\nRetrieval-Augmented Generation (RAG) method was introduced, utilizing the\nPrize-Collecting Steiner Tree (PCST) optimization for sub-graph construction.\nHowever, this method focused solely on node attributes, leading to incomplete\ncontextual understanding. In this paper, we propose an enhanced approach that\nreplaces the PCST method with an attention-based sub-graph construction\ntechnique, enabling more efficient and context-aware retrieval. Additionally,\nwe encode both node and edge attributes, leading to richer graph\nrepresentations. Our method also incorporates an improved projection layer and\nmulti-head attention pooling for better alignment with Large Language Models\n(LLMs). Experimental evaluations on the WebQSP dataset demonstrate that our\napproach is competitive and achieves marginally better results compared to the\noriginal method, underscoring its potential for more accurate question\nanswering."
                },
                "authors": [
                    {
                        "name": "Manthankumar Solanki"
                    }
                ],
                "author_detail": {
                    "name": "Manthankumar Solanki"
                },
                "author": "Manthankumar Solanki",
                "arxiv_comment": "Extended version of a paper presented at NeurIPS 2024\n  (arXiv:2402.07630)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14951v1",
                "updated": "2025-04-21T08:17:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    17,
                    41,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T08:17:41Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    17,
                    41,
                    0,
                    111,
                    0
                ],
                "title": "A Purely Data-Driven Adaptive Impedance Matching Method Robust to\n  Parasitic Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Purely Data-Driven Adaptive Impedance Matching Method Robust to\n  Parasitic Effects"
                },
                "summary": "Adaptive impedance matching between antennas and radio frequency front-end\n(RFFE) power modules is essential for mobile communication systems. To address\nthe matching performance degradation caused by parasitic effects in practical\ntunable matching networks (TMN), this paper proposes a purely data-driven\nadaptive impedance matching method that avoids trial-and-error physical\nadjustment. First, we propose the residual enhanced circuit behavior modeling\nnetwork (RECBM-Net), a deep learning model that maps TMN operating states to\ntheir scattering parameters (S-parameters). Then, we formulate the matching\nprocess based on the trained surrogate model as a mathematical optimization\nproblem. We employ two classic numerical methods with different online\ncomputational overhead, namely simulated annealing particle swarm optimization\n(SAPSO) and adaptive moment estimation with automatic differentiation\n(AD-Adam), to search for the matching solution. To further reduce the online\ninference overhead caused by repeated forward propagation through RECBM-Net, we\ntrain an inverse mapping solver network (IMS-Net) to directly predict the\noptimal solution. Simulation results show that RECBM-Net achieves exceptionally\nhigh modeling accuracy. While AD-Adam significantly reduces computational\noverhead compared to SAPSO, it sacrifices slight accuracy. IMS-Net offers the\nlowest online overhead while maintaining excellent matching accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive impedance matching between antennas and radio frequency front-end\n(RFFE) power modules is essential for mobile communication systems. To address\nthe matching performance degradation caused by parasitic effects in practical\ntunable matching networks (TMN), this paper proposes a purely data-driven\nadaptive impedance matching method that avoids trial-and-error physical\nadjustment. First, we propose the residual enhanced circuit behavior modeling\nnetwork (RECBM-Net), a deep learning model that maps TMN operating states to\ntheir scattering parameters (S-parameters). Then, we formulate the matching\nprocess based on the trained surrogate model as a mathematical optimization\nproblem. We employ two classic numerical methods with different online\ncomputational overhead, namely simulated annealing particle swarm optimization\n(SAPSO) and adaptive moment estimation with automatic differentiation\n(AD-Adam), to search for the matching solution. To further reduce the online\ninference overhead caused by repeated forward propagation through RECBM-Net, we\ntrain an inverse mapping solver network (IMS-Net) to directly predict the\noptimal solution. Simulation results show that RECBM-Net achieves exceptionally\nhigh modeling accuracy. While AD-Adam significantly reduces computational\noverhead compared to SAPSO, it sacrifices slight accuracy. IMS-Net offers the\nlowest online overhead while maintaining excellent matching accuracy."
                },
                "authors": [
                    {
                        "name": "Wendong Cheng"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Weidong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weidong Wang"
                },
                "author": "Weidong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06237v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06237v3",
                "updated": "2025-04-21T08:14:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    14,
                    52,
                    0,
                    111,
                    0
                ],
                "published": "2024-05-10T04:10:50Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    4,
                    10,
                    50,
                    4,
                    131,
                    0
                ],
                "title": "Risks of Practicing Large Language Models in Smart Grid: Threat Modeling\n  and Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risks of Practicing Large Language Models in Smart Grid: Threat Modeling\n  and Validation"
                },
                "summary": "Large language models (LLMs) represent significant breakthroughs in\nartificial intelligence and hold potential for applications within smart grids.\nHowever, as demonstrated in previous literature, AI technologies are\nsusceptible to various types of attacks. It is crucial to investigate and\nevaluate the risks associated with LLMs before deploying them in critical\ninfrastructure like smart grids. In this paper, we systematically evaluated the\nrisks of LLMs and identified two major types of attacks relevant to potential\nsmart grid LLM applications, presenting the corresponding threat models. We\nvalidated these attacks using popular LLMs and real smart grid data. Our\nvalidation demonstrates that attackers are capable of injecting bad data and\nretrieving domain knowledge from LLMs employed in different smart grid\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent significant breakthroughs in\nartificial intelligence and hold potential for applications within smart grids.\nHowever, as demonstrated in previous literature, AI technologies are\nsusceptible to various types of attacks. It is crucial to investigate and\nevaluate the risks associated with LLMs before deploying them in critical\ninfrastructure like smart grids. In this paper, we systematically evaluated the\nrisks of LLMs and identified two major types of attacks relevant to potential\nsmart grid LLM applications, presenting the corresponding threat models. We\nvalidated these attacks using popular LLMs and real smart grid data. Our\nvalidation demonstrates that attackers are capable of injecting bad data and\nretrieving domain knowledge from LLMs employed in different smart grid\napplications."
                },
                "authors": [
                    {
                        "name": "Jiangnan Li"
                    },
                    {
                        "name": "Yingyuan Yang"
                    },
                    {
                        "name": "Jinyuan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jinyuan Sun"
                },
                "author": "Jinyuan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06237v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06237v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14943v1",
                "updated": "2025-04-21T08:06:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    6,
                    55,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T08:06:55Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    6,
                    55,
                    0,
                    111,
                    0
                ],
                "title": "Cloud-cloud collision and star formation in G013.313+0.193",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud-cloud collision and star formation in G013.313+0.193"
                },
                "summary": "We study the G013.313+0.193 G013.313 region, a complex environment\ncharacterized by molecular cloud interactions indicative of cloud-cloud\ncollision (CCC). Observations of the NH3(1,1) and (2,2) inversion transitions\nwere obtained using the Nanshan 26 m radio telescope, while HCO+ (1-0), 12CO,\n13CO, and C18O(1-0) transitions from the Purple Mountain Observatory Delingha\n14 m telescope. Archival data are also included. We identified key\nobservational signatures of CCC, including complementary spatial distributions,\nU-shaped structures, bridge features, and V-shaped velocity distributions. The\nposition-velocity diagrams (P-V) reveal clear indications of gas interaction\nbetween two velocity components, suggesting an ongoing collision at an\nestimated angle of approximately 45 degree to the line of sight. The estimated\ncollision timescale is 0.35-1.03 Myr, aligned with the inferred ages of young\nstellar objects (YSOs) in the region, supporting the hypothesis of\ncollision-induced star formation. Hub-filament system (HFS) are identified in\nthe compressed gas region, where filaments converge toward a dense hub,\nsuggesting the CCC as a potential driver of HFS formation and massive star\nformation. The high column density suggests favorable conditions for the\nformation of massive stars. Although alternative kinematic drivers such as\nlongitudinal collapse and shear motion are considered, CCC remains the most\nplausible explanation for the observed features. Our findings contribute to our\nunderstanding of the mechanisms of cloud dynamics and massive star formation in\nturbulent molecular environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the G013.313+0.193 G013.313 region, a complex environment\ncharacterized by molecular cloud interactions indicative of cloud-cloud\ncollision (CCC). Observations of the NH3(1,1) and (2,2) inversion transitions\nwere obtained using the Nanshan 26 m radio telescope, while HCO+ (1-0), 12CO,\n13CO, and C18O(1-0) transitions from the Purple Mountain Observatory Delingha\n14 m telescope. Archival data are also included. We identified key\nobservational signatures of CCC, including complementary spatial distributions,\nU-shaped structures, bridge features, and V-shaped velocity distributions. The\nposition-velocity diagrams (P-V) reveal clear indications of gas interaction\nbetween two velocity components, suggesting an ongoing collision at an\nestimated angle of approximately 45 degree to the line of sight. The estimated\ncollision timescale is 0.35-1.03 Myr, aligned with the inferred ages of young\nstellar objects (YSOs) in the region, supporting the hypothesis of\ncollision-induced star formation. Hub-filament system (HFS) are identified in\nthe compressed gas region, where filaments converge toward a dense hub,\nsuggesting the CCC as a potential driver of HFS formation and massive star\nformation. The high column density suggests favorable conditions for the\nformation of massive stars. Although alternative kinematic drivers such as\nlongitudinal collapse and shear motion are considered, CCC remains the most\nplausible explanation for the observed features. Our findings contribute to our\nunderstanding of the mechanisms of cloud dynamics and massive star formation in\nturbulent molecular environments."
                },
                "authors": [
                    {
                        "name": "Dilda Berdikhan"
                    },
                    {
                        "name": "Jarken Esimbek"
                    },
                    {
                        "name": "Christian Henkel"
                    },
                    {
                        "name": "Ye Xu"
                    },
                    {
                        "name": "Jianjun Zhou"
                    },
                    {
                        "name": "De-Jian Liu"
                    },
                    {
                        "name": "Ernazar Abdikamalov"
                    },
                    {
                        "name": "Yingxiu Ma"
                    },
                    {
                        "name": "Toktarkhan Komesh"
                    },
                    {
                        "name": "Yuxin He"
                    },
                    {
                        "name": "Wenjun Zhang"
                    },
                    {
                        "name": "Xindi Tang"
                    },
                    {
                        "name": "Gang Wu"
                    },
                    {
                        "name": "Dalei Li"
                    },
                    {
                        "name": "Dongdong Zhou"
                    },
                    {
                        "name": "Kadirya Tursun"
                    },
                    {
                        "name": "Hailiang Shen"
                    },
                    {
                        "name": "Ernar Imanaly"
                    },
                    {
                        "name": "Qaynar Jandaolet"
                    },
                    {
                        "name": "Arailym Manapbayeva"
                    },
                    {
                        "name": "Duriya Tuiakbayeva"
                    }
                ],
                "author_detail": {
                    "name": "Duriya Tuiakbayeva"
                },
                "author": "Duriya Tuiakbayeva",
                "arxiv_comment": "20 pages,21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "85",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "A.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14941v2",
                "updated": "2025-04-22T07:43:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    7,
                    43,
                    1,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T08:02:25Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    2,
                    25,
                    0,
                    111,
                    0
                ],
                "title": "WindVE: Collaborative CPU-NPU Vector Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WindVE: Collaborative CPU-NPU Vector Embedding"
                },
                "summary": "Retrieval-Augmented Generation is a technology that enhances large language\nmodels by integrating information retrieval. In the industry, inference\nservices based on LLMs are highly sensitive to cost-performance ratio,\nprompting the need for improving hardware resource utilization in the inference\nservice. Specifically, vector embedding and retrieval processes take up to 20%\nof the total latency. Therefore, optimizing the utilization of computational\nresources in vector embeddings is crucial for enhancing the cost-performance\nratio of inference processes, which in turn boosts their product\ncompetitiveness.In this paper, we analyze the deployment costs of vector\nembedding technology in inference services, propose a theoretical formula, and\ndetermine through the mathematical expression that increasing the capacity to\nprocess concurrent queries is the key to reducing the deployment costs of\nvector embeddings. Therefore, in this paper, we focus on improving the\nproduct's capability to process concurrent queries. To optimize concurrency\nwithout sacrificing performance, we have designed a queue manager that adeptly\noffloads CPU peak queries. This manager utilizes a linear regression model to\nascertain the optimal queue depths, a critical parameter that significantly\ninfluences the efficacy of the system. We further develop a system named WindVE\nthat uses a CPU-NPU heterogeneous architecture to offload peak concurrent\nqueries, which leverages the performance differences between the two processors\nto effectively manage traffic surges. Through experiments, we compare WindVE to\nthe state-of-the-art vector embedding framework FlagEmbedding, and achieve a\nconcurrency level up to 22.3% higher than the scheme without offloading.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation is a technology that enhances large language\nmodels by integrating information retrieval. In the industry, inference\nservices based on LLMs are highly sensitive to cost-performance ratio,\nprompting the need for improving hardware resource utilization in the inference\nservice. Specifically, vector embedding and retrieval processes take up to 20%\nof the total latency. Therefore, optimizing the utilization of computational\nresources in vector embeddings is crucial for enhancing the cost-performance\nratio of inference processes, which in turn boosts their product\ncompetitiveness.In this paper, we analyze the deployment costs of vector\nembedding technology in inference services, propose a theoretical formula, and\ndetermine through the mathematical expression that increasing the capacity to\nprocess concurrent queries is the key to reducing the deployment costs of\nvector embeddings. Therefore, in this paper, we focus on improving the\nproduct's capability to process concurrent queries. To optimize concurrency\nwithout sacrificing performance, we have designed a queue manager that adeptly\noffloads CPU peak queries. This manager utilizes a linear regression model to\nascertain the optimal queue depths, a critical parameter that significantly\ninfluences the efficacy of the system. We further develop a system named WindVE\nthat uses a CPU-NPU heterogeneous architecture to offload peak concurrent\nqueries, which leverages the performance differences between the two processors\nto effectively manage traffic surges. Through experiments, we compare WindVE to\nthe state-of-the-art vector embedding framework FlagEmbedding, and achieve a\nconcurrency level up to 22.3% higher than the scheme without offloading."
                },
                "authors": [
                    {
                        "name": "Jinqi Huang"
                    },
                    {
                        "name": "Xuebing Yu"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Wenjie Huang"
                    },
                    {
                        "name": "Entong Li"
                    },
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Xin chen"
                    }
                ],
                "author_detail": {
                    "name": "Xin chen"
                },
                "author": "Xin chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14937v1",
                "updated": "2025-04-21T08:01:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    1,
                    32,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T08:01:32Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    1,
                    32,
                    0,
                    111,
                    0
                ],
                "title": "Causal DAG Summarization (Full Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal DAG Summarization (Full Version)"
                },
                "summary": "Causal inference aids researchers in discovering cause-and-effect\nrelationships, leading to scientific insights. Accurate causal estimation\nrequires identifying confounding variables to avoid false discoveries. Pearl's\ncausal model uses causal DAGs to identify confounding variables, but incorrect\nDAGs can lead to unreliable causal conclusions. However, for high dimensional\ndata, the causal DAGs are often complex beyond human verifiability. Graph\nsummarization is a logical next step, but current methods for general-purpose\ngraph summarization are inadequate for causal DAG summarization. This paper\naddresses these challenges by proposing a causal graph summarization objective\nthat balances graph simplification for better understanding while retaining\nessential causal information for reliable inference. We develop an efficient\ngreedy algorithm and show that summary causal DAGs can be directly used for\ninference and are more robust to misspecification of assumptions, enhancing\nrobustness for causal inference. Experimenting with six real-life datasets, we\ncompared our algorithm to three existing solutions, showing its effectiveness\nin handling high-dimensional data and its ability to generate summary DAGs that\nensure both reliable causal inference and robustness against misspecifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference aids researchers in discovering cause-and-effect\nrelationships, leading to scientific insights. Accurate causal estimation\nrequires identifying confounding variables to avoid false discoveries. Pearl's\ncausal model uses causal DAGs to identify confounding variables, but incorrect\nDAGs can lead to unreliable causal conclusions. However, for high dimensional\ndata, the causal DAGs are often complex beyond human verifiability. Graph\nsummarization is a logical next step, but current methods for general-purpose\ngraph summarization are inadequate for causal DAG summarization. This paper\naddresses these challenges by proposing a causal graph summarization objective\nthat balances graph simplification for better understanding while retaining\nessential causal information for reliable inference. We develop an efficient\ngreedy algorithm and show that summary causal DAGs can be directly used for\ninference and are more robust to misspecification of assumptions, enhancing\nrobustness for causal inference. Experimenting with six real-life datasets, we\ncompared our algorithm to three existing solutions, showing its effectiveness\nin handling high-dimensional data and its ability to generate summary DAGs that\nensure both reliable causal inference and robustness against misspecifications."
                },
                "authors": [
                    {
                        "name": "Anna Zeng"
                    },
                    {
                        "name": "Michael Cafarella"
                    },
                    {
                        "name": "Batya Kenig"
                    },
                    {
                        "name": "Markos Markakis"
                    },
                    {
                        "name": "Brit Youngmann"
                    },
                    {
                        "name": "Babak Salimi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Salimi"
                },
                "author": "Babak Salimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14928v1",
                "updated": "2025-04-21T07:48:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    48,
                    20,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T07:48:20Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    48,
                    20,
                    0,
                    111,
                    0
                ],
                "title": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent\n  Dialogue Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent\n  Dialogue Framework"
                },
                "summary": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yao Shi"
                    },
                    {
                        "name": "Rongkeng Liang"
                    },
                    {
                        "name": "Yong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Xu"
                },
                "author": "Yong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14919v1",
                "updated": "2025-04-21T07:38:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    38,
                    25,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T07:38:25Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    38,
                    25,
                    0,
                    111,
                    0
                ],
                "title": "GenCLIP: Generalizing CLIP Prompts for Zero-shot Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenCLIP: Generalizing CLIP Prompts for Zero-shot Anomaly Detection"
                },
                "summary": "Zero-shot anomaly detection (ZSAD) aims to identify anomalies in unseen\ncategories by leveraging CLIP's zero-shot capabilities to match text prompts\nwith visual features. A key challenge in ZSAD is learning general prompts\nstably and utilizing them effectively, while maintaining both generalizability\nand category specificity. Although general prompts have been explored in prior\nworks, achieving their stable optimization and effective deployment remains a\nsignificant challenge. In this work, we propose GenCLIP, a novel framework that\nlearns and leverages general prompts more effectively through multi-layer\nprompting and dual-branch inference. Multi-layer prompting integrates\ncategory-specific visual cues from different CLIP layers, enriching general\nprompts with more comprehensive and robust feature representations. By\ncombining general prompts with multi-layer visual features, our method further\nenhances its generalization capability. To balance specificity and\ngeneralization, we introduce a dual-branch inference strategy, where a\nvision-enhanced branch captures fine-grained category-specific features, while\na query-only branch prioritizes generalization. The complementary outputs from\nboth branches improve the stability and reliability of anomaly detection across\nunseen categories. Additionally, we propose an adaptive text prompt filtering\nmechanism, which removes irrelevant or atypical class names not encountered\nduring CLIP's training, ensuring that only meaningful textual inputs contribute\nto the final vision-language alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot anomaly detection (ZSAD) aims to identify anomalies in unseen\ncategories by leveraging CLIP's zero-shot capabilities to match text prompts\nwith visual features. A key challenge in ZSAD is learning general prompts\nstably and utilizing them effectively, while maintaining both generalizability\nand category specificity. Although general prompts have been explored in prior\nworks, achieving their stable optimization and effective deployment remains a\nsignificant challenge. In this work, we propose GenCLIP, a novel framework that\nlearns and leverages general prompts more effectively through multi-layer\nprompting and dual-branch inference. Multi-layer prompting integrates\ncategory-specific visual cues from different CLIP layers, enriching general\nprompts with more comprehensive and robust feature representations. By\ncombining general prompts with multi-layer visual features, our method further\nenhances its generalization capability. To balance specificity and\ngeneralization, we introduce a dual-branch inference strategy, where a\nvision-enhanced branch captures fine-grained category-specific features, while\na query-only branch prioritizes generalization. The complementary outputs from\nboth branches improve the stability and reliability of anomaly detection across\nunseen categories. Additionally, we propose an adaptive text prompt filtering\nmechanism, which removes irrelevant or atypical class names not encountered\nduring CLIP's training, ensuring that only meaningful textual inputs contribute\nto the final vision-language alignment."
                },
                "authors": [
                    {
                        "name": "Donghyeong Kim"
                    },
                    {
                        "name": "Chaewon Park"
                    },
                    {
                        "name": "Suhwan Cho"
                    },
                    {
                        "name": "Hyeonjeong Lim"
                    },
                    {
                        "name": "Minseok Kang"
                    },
                    {
                        "name": "Jungho Lee"
                    },
                    {
                        "name": "Sangyoun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sangyoun Lee"
                },
                "author": "Sangyoun Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14917v1",
                "updated": "2025-04-21T07:35:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    35,
                    24,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T07:35:24Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    35,
                    24,
                    0,
                    111,
                    0
                ],
                "title": "POLYRAG: Integrating Polyviews into Retrieval-Augmented Generation for\n  Medical Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POLYRAG: Integrating Polyviews into Retrieval-Augmented Generation for\n  Medical Applications"
                },
                "summary": "Large language models (LLMs) have become a disruptive force in the industry,\nintroducing unprecedented capabilities in natural language processing, logical\nreasoning and so on. However, the challenges of knowledge updates and\nhallucination issues have limited the application of LLMs in medical scenarios,\nwhere retrieval-augmented generation (RAG) can offer significant assistance.\nNevertheless, existing retrieve-then-read approaches generally digest the\nretrieved documents, without considering the timeliness, authoritativeness and\ncommonality of retrieval. We argue that these approaches can be suboptimal,\nespecially in real-world applications where information from different sources\nmight conflict with each other and even information from the same source in\ndifferent time scale might be different, and totally relying on this would\ndeteriorate the performance of RAG approaches. We propose PolyRAG that\ncarefully incorporate judges from different perspectives and finally integrate\nthe polyviews for retrieval augmented generation in medical applications. Due\nto the scarcity of real-world benchmarks for evaluation, to bridge the gap we\npropose PolyEVAL, a benchmark consists of queries and documents collected from\nreal-world medical scenarios (including medical policy, hospital & doctor\ninquiry and healthcare) with multiple tagging (e.g., timeliness,\nauthoritativeness) on them. Extensive experiments and analysis on PolyEVAL have\ndemonstrated the superiority of PolyRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become a disruptive force in the industry,\nintroducing unprecedented capabilities in natural language processing, logical\nreasoning and so on. However, the challenges of knowledge updates and\nhallucination issues have limited the application of LLMs in medical scenarios,\nwhere retrieval-augmented generation (RAG) can offer significant assistance.\nNevertheless, existing retrieve-then-read approaches generally digest the\nretrieved documents, without considering the timeliness, authoritativeness and\ncommonality of retrieval. We argue that these approaches can be suboptimal,\nespecially in real-world applications where information from different sources\nmight conflict with each other and even information from the same source in\ndifferent time scale might be different, and totally relying on this would\ndeteriorate the performance of RAG approaches. We propose PolyRAG that\ncarefully incorporate judges from different perspectives and finally integrate\nthe polyviews for retrieval augmented generation in medical applications. Due\nto the scarcity of real-world benchmarks for evaluation, to bridge the gap we\npropose PolyEVAL, a benchmark consists of queries and documents collected from\nreal-world medical scenarios (including medical policy, hospital & doctor\ninquiry and healthcare) with multiple tagging (e.g., timeliness,\nauthoritativeness) on them. Extensive experiments and analysis on PolyEVAL have\ndemonstrated the superiority of PolyRAG."
                },
                "authors": [
                    {
                        "name": "Chunjing Gan"
                    },
                    {
                        "name": "Dan Yang"
                    },
                    {
                        "name": "Binbin Hu"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Yue Shen"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14915v1",
                "updated": "2025-04-21T07:33:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    33,
                    27,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T07:33:27Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    33,
                    27,
                    0,
                    111,
                    0
                ],
                "title": "StableQuant: Layer Adaptive Post-Training Quantization for Speech\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableQuant: Layer Adaptive Post-Training Quantization for Speech\n  Foundation Models"
                },
                "summary": "In this paper, we propose StableQuant, a novel adaptive post-training\nquantization (PTQ) algorithm for widely used speech foundation models (SFMs).\nWhile PTQ has been successfully employed for compressing large language models\n(LLMs) due to its ability to bypass additional fine-tuning, directly applying\nthese techniques to SFMs may not yield optimal results, as SFMs utilize\ndistinct network architecture for feature extraction. StableQuant demonstrates\noptimal quantization performance regardless of the network architecture type,\nas it adaptively determines the quantization range for each layer by analyzing\nboth the scale distributions and overall performance. We evaluate our algorithm\non two SFMs, HuBERT and wav2vec2.0, for an automatic speech recognition (ASR)\ntask, and achieve superior performance compared to traditional PTQ methods.\nStableQuant successfully reduces the sizes of SFM models to a quarter and\ndoubles the inference speed while limiting the word error rate (WER)\nperformance drop to less than 0.3% with 8-bit quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose StableQuant, a novel adaptive post-training\nquantization (PTQ) algorithm for widely used speech foundation models (SFMs).\nWhile PTQ has been successfully employed for compressing large language models\n(LLMs) due to its ability to bypass additional fine-tuning, directly applying\nthese techniques to SFMs may not yield optimal results, as SFMs utilize\ndistinct network architecture for feature extraction. StableQuant demonstrates\noptimal quantization performance regardless of the network architecture type,\nas it adaptively determines the quantization range for each layer by analyzing\nboth the scale distributions and overall performance. We evaluate our algorithm\non two SFMs, HuBERT and wav2vec2.0, for an automatic speech recognition (ASR)\ntask, and achieve superior performance compared to traditional PTQ methods.\nStableQuant successfully reduces the sizes of SFM models to a quarter and\ndoubles the inference speed while limiting the word error rate (WER)\nperformance drop to less than 0.3% with 8-bit quantization."
                },
                "authors": [
                    {
                        "name": "Yeona Hong"
                    },
                    {
                        "name": "Hyewon Han"
                    },
                    {
                        "name": "Woo-jin Chung"
                    },
                    {
                        "name": "Hong-Goo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Hong-Goo Kang"
                },
                "author": "Hong-Goo Kang",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12322v2",
                "updated": "2025-04-21T07:29:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    29,
                    28,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-11T06:13:43Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    6,
                    13,
                    43,
                    4,
                    101,
                    0
                ],
                "title": "A Strategic Coordination Framework of Small LLMs Matches Large LLMs in\n  Data Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Strategic Coordination Framework of Small LLMs Matches Large LLMs in\n  Data Synthesis"
                },
                "summary": "While data synthesis and distillation are promising strategies to enhance\nsmall language models, current approaches heavily rely on Large Language Models\n(LLMs), which suffer from high computational costs, environmental inefficiency,\nand potential biases inherited from monolithic architectures. In contrast,\nsmaller LLMs are more accessible and sustainable, but their individual\ncapabilities often fall short in generating high-quality, diverse, and reliable\ndata. Inspired by collaborative human processes (e.g., peer review), we propose\na multiple small LLMs involved framework, GRA, that aggregates specialized\nroles across small LLMs to iterative refinement and quality control typically\nachieved by a single large LLM. In this collaborative framework, multiple small\nLLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a\npeer-review-inspired data synthesis pipeline. The Generator proposes initial\ndata samples, the Reviewer critiques their quality and diversity, and the\nAdjudicator resolves conflicts to finalize the output. By decomposing the\nsynthesis process into specialized sub-tasks, collaborative small LLMs can\nachieve data-level parity with large LLM-based distillation. Through\nexperiments across multiple benchmarks, we demonstrate that GRA-produced data\nmatches or exceeds the quality of single large LLM outputs, e.g.,\nQwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large\nmodels for high-quality data synthesis, advocating instead for strategic\ncoordination of smaller agents. Our datasets, models, and code are publicly\navailable at https://github.com/GX-XinGao/GRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While data synthesis and distillation are promising strategies to enhance\nsmall language models, current approaches heavily rely on Large Language Models\n(LLMs), which suffer from high computational costs, environmental inefficiency,\nand potential biases inherited from monolithic architectures. In contrast,\nsmaller LLMs are more accessible and sustainable, but their individual\ncapabilities often fall short in generating high-quality, diverse, and reliable\ndata. Inspired by collaborative human processes (e.g., peer review), we propose\na multiple small LLMs involved framework, GRA, that aggregates specialized\nroles across small LLMs to iterative refinement and quality control typically\nachieved by a single large LLM. In this collaborative framework, multiple small\nLLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a\npeer-review-inspired data synthesis pipeline. The Generator proposes initial\ndata samples, the Reviewer critiques their quality and diversity, and the\nAdjudicator resolves conflicts to finalize the output. By decomposing the\nsynthesis process into specialized sub-tasks, collaborative small LLMs can\nachieve data-level parity with large LLM-based distillation. Through\nexperiments across multiple benchmarks, we demonstrate that GRA-produced data\nmatches or exceeds the quality of single large LLM outputs, e.g.,\nQwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large\nmodels for high-quality data synthesis, advocating instead for strategic\ncoordination of smaller agents. Our datasets, models, and code are publicly\navailable at https://github.com/GX-XinGao/GRA."
                },
                "authors": [
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Qizhi Pei"
                    },
                    {
                        "name": "Zinan Tang"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Honglin Lin"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11834v2",
                "updated": "2025-04-21T07:27:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    27,
                    30,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-16T07:45:44Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    45,
                    44,
                    2,
                    106,
                    0
                ],
                "title": "Estimation and inference in error-in-operator model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation and inference in error-in-operator model"
                },
                "summary": "Many statistical problems can be reduced to a linear inverse problem in which\nonly a noisy version of the operator is available. Particular examples include\nrandom design regression, deconvolution problem, instrumental variable\nregression, functional data analysis, error-in-variable regression, drift\nestimation in stochastic diffusion, and many others. The pragmatic plug-in\napproach can be well justified in the classical asymptotic setup with a growing\nsample size. However, recent developments in high dimensional inference reveal\nsome new features of this problem. In high dimensional linear regression with a\nrandom design, the plug-in approach is questionable but the use of a simple\nridge penalization yields a benign overfitting phenomenon; see\n\\cite{baLoLu2020}, \\cite{ChMo2022}, \\cite{NoPuSp2024}. This paper revisits the\ngeneral Error-in-Operator problem for finite samples and high dimension of the\nsource and image spaces. A particular focus is on the choice of a proper\nregularization. We show that a simple ridge penalty (Tikhonov regularization)\nworks properly in the case when the operator is more regular than the signal.\nIn the opposite case, some model reduction technique like spectral truncation\nshould be applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many statistical problems can be reduced to a linear inverse problem in which\nonly a noisy version of the operator is available. Particular examples include\nrandom design regression, deconvolution problem, instrumental variable\nregression, functional data analysis, error-in-variable regression, drift\nestimation in stochastic diffusion, and many others. The pragmatic plug-in\napproach can be well justified in the classical asymptotic setup with a growing\nsample size. However, recent developments in high dimensional inference reveal\nsome new features of this problem. In high dimensional linear regression with a\nrandom design, the plug-in approach is questionable but the use of a simple\nridge penalization yields a benign overfitting phenomenon; see\n\\cite{baLoLu2020}, \\cite{ChMo2022}, \\cite{NoPuSp2024}. This paper revisits the\ngeneral Error-in-Operator problem for finite samples and high dimension of the\nsource and image spaces. A particular focus is on the choice of a proper\nregularization. We show that a simple ridge penalty (Tikhonov regularization)\nworks properly in the case when the operator is more regular than the signal.\nIn the opposite case, some model reduction technique like spectral truncation\nshould be applied."
                },
                "authors": [
                    {
                        "name": "Vladimir Spokoiny"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Spokoiny"
                },
                "author": "Vladimir Spokoiny",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F10, 62E17, 62J12",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01354v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01354v2",
                "updated": "2025-04-21T07:25:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    25,
                    13,
                    0,
                    111,
                    0
                ],
                "published": "2024-08-02T16:04:52Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    16,
                    4,
                    52,
                    4,
                    215,
                    0
                ],
                "title": "MCGMark: An Encodable and Robust Online Watermark for Tracing\n  LLM-Generated Malicious Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCGMark: An Encodable and Robust Online Watermark for Tracing\n  LLM-Generated Malicious Code"
                },
                "summary": "With the advent of large language models (LLMs), numerous software service\nproviders (SSPs) are dedicated to developing LLMs customized for code\ngeneration tasks, such as CodeLlama and Copilot. However, these LLMs can be\nleveraged by attackers to create malicious software, which may pose potential\nthreats to the software ecosystem. For example, they can automate the creation\nof advanced phishing malware. To address this issue, we first conduct an\nempirical study and design a prompt dataset, MCGTest, which involves\napproximately 400 person-hours of work and consists of 406 malicious code\ngeneration tasks. Utilizing this dataset, we propose MCGMark, the first robust,\ncode structure-aware, and encodable watermarking approach to trace\nLLM-generated code. We embed encodable information by controlling the token\nselection and ensuring the output quality based on probabilistic outliers.\nAdditionally, we enhance the robustness of the watermark by considering the\nstructural features of malicious code, preventing the embedding of the\nwatermark in easily modified positions, such as comments. We validate the\neffectiveness and robustness of MCGMark on the DeepSeek-Coder. MCGMark achieves\nan embedding success rate of 88.9% within a maximum output limit of 400 tokens.\nFurthermore, it also demonstrates strong robustness and has minimal impact on\nthe quality of the output code. Our approach assists SSPs in tracing and\nholding responsible parties accountable for malicious code generated by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of large language models (LLMs), numerous software service\nproviders (SSPs) are dedicated to developing LLMs customized for code\ngeneration tasks, such as CodeLlama and Copilot. However, these LLMs can be\nleveraged by attackers to create malicious software, which may pose potential\nthreats to the software ecosystem. For example, they can automate the creation\nof advanced phishing malware. To address this issue, we first conduct an\nempirical study and design a prompt dataset, MCGTest, which involves\napproximately 400 person-hours of work and consists of 406 malicious code\ngeneration tasks. Utilizing this dataset, we propose MCGMark, the first robust,\ncode structure-aware, and encodable watermarking approach to trace\nLLM-generated code. We embed encodable information by controlling the token\nselection and ensuring the output quality based on probabilistic outliers.\nAdditionally, we enhance the robustness of the watermark by considering the\nstructural features of malicious code, preventing the embedding of the\nwatermark in easily modified positions, such as comments. We validate the\neffectiveness and robustness of MCGMark on the DeepSeek-Coder. MCGMark achieves\nan embedding success rate of 88.9% within a maximum output limit of 400 tokens.\nFurthermore, it also demonstrates strong robustness and has minimal impact on\nthe quality of the output code. Our approach assists SSPs in tracing and\nholding responsible parties accountable for malicious code generated by LLMs."
                },
                "authors": [
                    {
                        "name": "Kaiwen Ning"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Qingyuan Zhong"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Jingwen Zhang"
                    },
                    {
                        "name": "Jianxing Yu"
                    },
                    {
                        "name": "Yuming Feng"
                    },
                    {
                        "name": "Weizhe Zhang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01354v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01354v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14905v1",
                "updated": "2025-04-21T07:20:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    20,
                    31,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T07:20:31Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    20,
                    31,
                    0,
                    111,
                    0
                ],
                "title": "CRAVE: A Conflicting Reasoning Approach for Explainable Claim\n  Verification Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRAVE: A Conflicting Reasoning Approach for Explainable Claim\n  Verification Using LLMs"
                },
                "summary": "The rapid spread of misinformation, driven by digital media and AI-generated\ncontent, has made automatic claim verification essential. Traditional methods,\nwhich depend on expert-annotated evidence, are labor-intensive and not\nscalable. Although recent automated systems have improved, they still struggle\nwith complex claims that require nuanced reasoning. To address this, we propose\nCRAVE, a Conflicting Reasoning Approach for explainable claim VErification,\nthat verify the complex claims based on the conflicting rationales reasoned by\nlarge language models (LLMs). Specifically, CRAVE introduces a three-module\nframework. Ambiguity Elimination enchanced Evidence Retrieval module performs\nambiguity elimination and entity-based search to gather relevant evidence\nrelated to claim verification from external sources like Wikipedia. Conflicting\nPerspective Reasoning and Preliminary Judgment module with LLMs adopts LLMs to\nreason rationales with conflicting stances about claim verification from\nretrieved evidence across four dimensions, i.e., direct evidence, semantic\nrelationships, linguistic patterns, and logical reasoning and make a\npreliminary judgment. Finally, Small Language Model (SLM) based Judge module is\nfine-tuned to make use of preliminary judgment from LLMs to assess the\nconfidence of the conflicting rationales and make a final authenticity\njudgment. This methodology allows CRAVE to capture subtle inconsistencies in\ncomplex claims, improving both the accuracy and transparency of claim\nverification. Extensive experiments on two public claim verification datasets\ndemonstrate that our CRAVE model achieves much better performance than\nstate-of-the-art methods and exhibits a superior capacity for finding relevant\nevidence and explaining the model predictions. The code is provided at\nhttps://github.com/8zym/CRAVE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid spread of misinformation, driven by digital media and AI-generated\ncontent, has made automatic claim verification essential. Traditional methods,\nwhich depend on expert-annotated evidence, are labor-intensive and not\nscalable. Although recent automated systems have improved, they still struggle\nwith complex claims that require nuanced reasoning. To address this, we propose\nCRAVE, a Conflicting Reasoning Approach for explainable claim VErification,\nthat verify the complex claims based on the conflicting rationales reasoned by\nlarge language models (LLMs). Specifically, CRAVE introduces a three-module\nframework. Ambiguity Elimination enchanced Evidence Retrieval module performs\nambiguity elimination and entity-based search to gather relevant evidence\nrelated to claim verification from external sources like Wikipedia. Conflicting\nPerspective Reasoning and Preliminary Judgment module with LLMs adopts LLMs to\nreason rationales with conflicting stances about claim verification from\nretrieved evidence across four dimensions, i.e., direct evidence, semantic\nrelationships, linguistic patterns, and logical reasoning and make a\npreliminary judgment. Finally, Small Language Model (SLM) based Judge module is\nfine-tuned to make use of preliminary judgment from LLMs to assess the\nconfidence of the conflicting rationales and make a final authenticity\njudgment. This methodology allows CRAVE to capture subtle inconsistencies in\ncomplex claims, improving both the accuracy and transparency of claim\nverification. Extensive experiments on two public claim verification datasets\ndemonstrate that our CRAVE model achieves much better performance than\nstate-of-the-art methods and exhibits a superior capacity for finding relevant\nevidence and explaining the model predictions. The code is provided at\nhttps://github.com/8zym/CRAVE."
                },
                "authors": [
                    {
                        "name": "Yingming Zheng"
                    },
                    {
                        "name": "Xiaoliang Liu"
                    },
                    {
                        "name": "Peng Wu"
                    },
                    {
                        "name": "Li Pan"
                    }
                ],
                "author_detail": {
                    "name": "Li Pan"
                },
                "author": "Li Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18908v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18908v3",
                "updated": "2025-04-21T07:17:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    17,
                    53,
                    0,
                    111,
                    0
                ],
                "published": "2024-10-24T16:59:28Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    59,
                    28,
                    3,
                    298,
                    0
                ],
                "title": "A Survey on Speech Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Speech Large Language Models"
                },
                "summary": "Large Language Models (LLMs) exhibit strong contextual understanding and\nremarkable multitask performance. As a result, researchers have been actively\nexploring the integration of LLMs into the domain of speech understanding, with\na primary focus on a broad range of speech-to-text tasks. These include\nautomatic speech recognition (ASR), speech-to-text translation (ST), speech\nemotion recognition (SER), and others. We refer to such models as Speech LLMs,\nwhich are typically built on a unified architecture that follows the pipeline\nof Audio Feature Extraction -> Multimodal Information Fusion -> LLM Inference.\nThis approach enables richer audio feature extraction while facilitating\nend-to-end fusion of audio and text modalities, thereby achieving deeper\nunderstanding and reasoning from audio data. This paper elucidates the\ndevelopment of Speech LLMs, offering an in-depth analysis of system\narchitectures. Through extensive research and a series of targeted experiments,\nthe paper assesses the advancements in Speech LLMs and their potential for\ncross-task integration within the speech understanding field. Furthermore, it\nhighlights key challenges identified through experimentation, such as the\ndormancy of LLMs under certain conditions. The paper further explores training\nstrategies for Speech LLMs, proposes potential solutions based on these\nfindings, and offers valuable insights and references for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit strong contextual understanding and\nremarkable multitask performance. As a result, researchers have been actively\nexploring the integration of LLMs into the domain of speech understanding, with\na primary focus on a broad range of speech-to-text tasks. These include\nautomatic speech recognition (ASR), speech-to-text translation (ST), speech\nemotion recognition (SER), and others. We refer to such models as Speech LLMs,\nwhich are typically built on a unified architecture that follows the pipeline\nof Audio Feature Extraction -> Multimodal Information Fusion -> LLM Inference.\nThis approach enables richer audio feature extraction while facilitating\nend-to-end fusion of audio and text modalities, thereby achieving deeper\nunderstanding and reasoning from audio data. This paper elucidates the\ndevelopment of Speech LLMs, offering an in-depth analysis of system\narchitectures. Through extensive research and a series of targeted experiments,\nthe paper assesses the advancements in Speech LLMs and their potential for\ncross-task integration within the speech understanding field. Furthermore, it\nhighlights key challenges identified through experimentation, such as the\ndormancy of LLMs under certain conditions. The paper further explores training\nstrategies for Speech LLMs, proposes potential solutions based on these\nfindings, and offers valuable insights and references for future research."
                },
                "authors": [
                    {
                        "name": "Jing Peng"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Yu Xi"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Xizhuo Zhang"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "This version has been updated to incorporate recent work in the field\n  and includes revised illustrations and textual descriptions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18908v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18908v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14899v1",
                "updated": "2025-04-21T07:10:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    10,
                    41,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T07:10:41Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    10,
                    41,
                    0,
                    111,
                    0
                ],
                "title": "Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls\n  for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls\n  for Video Generation"
                },
                "summary": "Camera and human motion controls have been extensively studied for video\ngeneration, but existing approaches typically address them separately,\nsuffering from limited data with high-quality annotations for both aspects. To\novercome this, we present Uni3C, a unified 3D-enhanced framework for precise\ncontrol of both camera and human motion in video generation. Uni3C includes two\nkey contributions. First, we propose a plug-and-play control module trained\nwith a frozen video generative backbone, PCDController, which utilizes\nunprojected point clouds from monocular depth to achieve accurate camera\ncontrol. By leveraging the strong 3D priors of point clouds and the powerful\ncapacities of video foundational models, PCDController shows impressive\ngeneralization, performing well regardless of whether the inference backbone is\nfrozen or fine-tuned. This flexibility enables different modules of Uni3C to be\ntrained in specific domains, i.e., either camera control or human motion\ncontrol, reducing the dependency on jointly annotated data. Second, we propose\na jointly aligned 3D world guidance for the inference phase that seamlessly\nintegrates both scenic point clouds and SMPL-X characters to unify the control\nsignals for camera and human motion, respectively. Extensive experiments\nconfirm that PCDController enjoys strong robustness in driving camera motion\nfor fine-tuned backbones of video generation. Uni3C substantially outperforms\ncompetitors in both camera controllability and human motion quality.\nAdditionally, we collect tailored validation sets featuring challenging camera\nmovements and human actions to validate the effectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Camera and human motion controls have been extensively studied for video\ngeneration, but existing approaches typically address them separately,\nsuffering from limited data with high-quality annotations for both aspects. To\novercome this, we present Uni3C, a unified 3D-enhanced framework for precise\ncontrol of both camera and human motion in video generation. Uni3C includes two\nkey contributions. First, we propose a plug-and-play control module trained\nwith a frozen video generative backbone, PCDController, which utilizes\nunprojected point clouds from monocular depth to achieve accurate camera\ncontrol. By leveraging the strong 3D priors of point clouds and the powerful\ncapacities of video foundational models, PCDController shows impressive\ngeneralization, performing well regardless of whether the inference backbone is\nfrozen or fine-tuned. This flexibility enables different modules of Uni3C to be\ntrained in specific domains, i.e., either camera control or human motion\ncontrol, reducing the dependency on jointly annotated data. Second, we propose\na jointly aligned 3D world guidance for the inference phase that seamlessly\nintegrates both scenic point clouds and SMPL-X characters to unify the control\nsignals for camera and human motion, respectively. Extensive experiments\nconfirm that PCDController enjoys strong robustness in driving camera motion\nfor fine-tuned backbones of video generation. Uni3C substantially outperforms\ncompetitors in both camera controllability and human motion quality.\nAdditionally, we collect tailored validation sets featuring challenging camera\nmovements and human actions to validate the effectiveness of our method."
                },
                "authors": [
                    {
                        "name": "Chenjie Cao"
                    },
                    {
                        "name": "Jingkai Zhou"
                    },
                    {
                        "name": "Shikai Li"
                    },
                    {
                        "name": "Jingyun Liang"
                    },
                    {
                        "name": "Chaohui Yu"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Xiangyang Xue"
                    },
                    {
                        "name": "Yanwei Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanwei Fu"
                },
                "author": "Yanwei Fu",
                "arxiv_comment": "Project page: https://github.com/ewrfcas/Uni3C",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14898v1",
                "updated": "2025-04-21T07:09:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    9,
                    5,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T07:09:05Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    9,
                    5,
                    0,
                    111,
                    0
                ],
                "title": "Expected Free Energy-based Planning as Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expected Free Energy-based Planning as Variational Inference"
                },
                "summary": "We address the problem of planning under uncertainty, where an agent must\nchoose actions that not only achieve desired outcomes but also reduce\nuncertainty. Traditional methods often treat exploration and exploitation as\nseparate objectives, lacking a unified inferential foundation. Active\ninference, grounded in the Free Energy Principle, offers such a foundation by\nminimizing Expected Free Energy (EFE), a cost function that combines utility\nwith epistemic drives like ambiguity resolution and novelty seeking. However,\nthe computational burden of EFE minimization has remained a major obstacle to\nits scalability. In this paper, we show that EFE-based planning arises\nnaturally from minimizing a variational free energy functional on a generative\nmodel augmented with preference and epistemic priors. This result reinforces\ntheoretical consistency with the Free Energy Principle, by casting planning\nitself as variational inference. Our formulation yields optimal policies that\njointly support goal achievement and information gain, while incorporating a\ncomplexity term that accounts for bounded computational resources. This\nunifying framework connects and extends existing methods, enabling scalable,\nresource-aware implementations of active inference agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of planning under uncertainty, where an agent must\nchoose actions that not only achieve desired outcomes but also reduce\nuncertainty. Traditional methods often treat exploration and exploitation as\nseparate objectives, lacking a unified inferential foundation. Active\ninference, grounded in the Free Energy Principle, offers such a foundation by\nminimizing Expected Free Energy (EFE), a cost function that combines utility\nwith epistemic drives like ambiguity resolution and novelty seeking. However,\nthe computational burden of EFE minimization has remained a major obstacle to\nits scalability. In this paper, we show that EFE-based planning arises\nnaturally from minimizing a variational free energy functional on a generative\nmodel augmented with preference and epistemic priors. This result reinforces\ntheoretical consistency with the Free Energy Principle, by casting planning\nitself as variational inference. Our formulation yields optimal policies that\njointly support goal achievement and information gain, while incorporating a\ncomplexity term that accounts for bounded computational resources. This\nunifying framework connects and extends existing methods, enabling scalable,\nresource-aware implementations of active inference agents."
                },
                "authors": [
                    {
                        "name": "Bert de Vries"
                    },
                    {
                        "name": "Wouter Nuijten"
                    },
                    {
                        "name": "Thijs van de Laar"
                    },
                    {
                        "name": "Wouter Kouw"
                    },
                    {
                        "name": "Sepideh Adamiat"
                    },
                    {
                        "name": "Tim Nisslbeck"
                    },
                    {
                        "name": "Mykola Lukashchuk"
                    },
                    {
                        "name": "Hoang Minh Huu Nguyen"
                    },
                    {
                        "name": "Marco Hidalgo Araya"
                    },
                    {
                        "name": "Raphael Tresor"
                    },
                    {
                        "name": "Thijs Jenneskens"
                    },
                    {
                        "name": "Ivana Nikoloska"
                    },
                    {
                        "name": "Raaja Subramanian"
                    },
                    {
                        "name": "Bart van Erp"
                    },
                    {
                        "name": "Dmitry Bagaev"
                    },
                    {
                        "name": "Albert Podusenko"
                    }
                ],
                "author_detail": {
                    "name": "Albert Podusenko"
                },
                "author": "Albert Podusenko",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09602v2",
                "updated": "2025-04-21T07:04:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    4,
                    57,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-13T14:35:30Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    35,
                    30,
                    6,
                    103,
                    0
                ],
                "title": "Fine-tuning a Large Language Model for Automating Computational Fluid\n  Dynamics Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning a Large Language Model for Automating Computational Fluid\n  Dynamics Simulations"
                },
                "summary": "Configuring computational fluid dynamics (CFD) simulations typically demands\nextensive domain expertise, limiting broader access. Although large language\nmodels (LLMs) have advanced scientific computing, their use in automating CFD\nworkflows is underdeveloped. We introduce a novel approach centered on\ndomain-specific LLM adaptation. By fine-tuning Qwen2.5-7B-Instruct on NL2FOAM,\nour custom dataset of 28716 natural language-to-OpenFOAM configuration pairs\nwith chain-of-thought (CoT) annotations, we enable direct translation from\nnatural language descriptions to executable CFD setups. A multi-agent framework\norchestrates the process, autonomously verifying inputs, generating\nconfigurations, running simulations, and correcting errors. Evaluation on a\nbenchmark of 21 diverse flow cases demonstrates state-of-the-art performance,\nachieving 88.7% solution accuracy and 82.6% first-attempt success rate. This\nsignificantly outperforms larger general-purpose models like\nQwen2.5-72B-Instruct, DeepSeek-R1, and Llama3.3-70B-Instruct, while also\nrequiring fewer correction iterations and maintaining high computational\nefficiency. The results highlight the critical role of domain-specific\nadaptation in deploying LLM assistants for complex engineering workflows. Our\ncode and fine-tuned model have been deposited at\nhttps://github.com/YYgroup/AutoCFD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Configuring computational fluid dynamics (CFD) simulations typically demands\nextensive domain expertise, limiting broader access. Although large language\nmodels (LLMs) have advanced scientific computing, their use in automating CFD\nworkflows is underdeveloped. We introduce a novel approach centered on\ndomain-specific LLM adaptation. By fine-tuning Qwen2.5-7B-Instruct on NL2FOAM,\nour custom dataset of 28716 natural language-to-OpenFOAM configuration pairs\nwith chain-of-thought (CoT) annotations, we enable direct translation from\nnatural language descriptions to executable CFD setups. A multi-agent framework\norchestrates the process, autonomously verifying inputs, generating\nconfigurations, running simulations, and correcting errors. Evaluation on a\nbenchmark of 21 diverse flow cases demonstrates state-of-the-art performance,\nachieving 88.7% solution accuracy and 82.6% first-attempt success rate. This\nsignificantly outperforms larger general-purpose models like\nQwen2.5-72B-Instruct, DeepSeek-R1, and Llama3.3-70B-Instruct, while also\nrequiring fewer correction iterations and maintaining high computational\nefficiency. The results highlight the critical role of domain-specific\nadaptation in deploying LLM assistants for complex engineering workflows. Our\ncode and fine-tuned model have been deposited at\nhttps://github.com/YYgroup/AutoCFD."
                },
                "authors": [
                    {
                        "name": "Zhehao Dong"
                    },
                    {
                        "name": "Zhen Lu"
                    },
                    {
                        "name": "Yue Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Yang"
                },
                "author": "Yue Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14893v1",
                "updated": "2025-04-21T06:45:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    6,
                    45,
                    41,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T06:45:41Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    6,
                    45,
                    41,
                    0,
                    111,
                    0
                ],
                "title": "Hardware-based Heterogeneous Memory Management for Large Language Model\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-based Heterogeneous Memory Management for Large Language Model\n  Inference"
                },
                "summary": "A large language model (LLM) is one of the most important emerging machine\nlearning applications nowadays. However, due to its huge model size and runtime\nincrease of the memory footprint, LLM inferences suffer from the lack of memory\ncapacity in conventional systems consisting of multiple GPUs with a modest\namount of high bandwidth memory. Moreover, since LLM contains many\nbandwidthintensive kernels, only focusing on the memory capacity without\nconsidering the bandwidth incurs a serious performance degradation. To handle\nsuch conflicting memory capacity and bandwidth demands in a cost-effective way,\nthis study investigates the potential of heterogeneous memory systems,\nproposing H2M2. It uses an asymmetric memory architecture consisting of\ncapacity-centric and bandwidthcentric memory with computation units attached to\neach memory device. With the asymmetric memory, we first analyze the effect of\nkernel-memory mapping for the asymmetric memory. Second, we propose a dynamic\nruntime algorithm that finds a mapping solution considering the characteristics\nof LLM operations and the change of footprint during LLM inference. Third, we\nadvocate the need for memory abstraction for the efficient management of the\nasymmetric memory. H2M2 outperforms the conventional homogeneous memory system\nwith LPDDR by 1.46x, 1.55x, and 2.94x speedup in GPT3-175B, Chinchilla-70B, and\nLlama2-70B, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large language model (LLM) is one of the most important emerging machine\nlearning applications nowadays. However, due to its huge model size and runtime\nincrease of the memory footprint, LLM inferences suffer from the lack of memory\ncapacity in conventional systems consisting of multiple GPUs with a modest\namount of high bandwidth memory. Moreover, since LLM contains many\nbandwidthintensive kernels, only focusing on the memory capacity without\nconsidering the bandwidth incurs a serious performance degradation. To handle\nsuch conflicting memory capacity and bandwidth demands in a cost-effective way,\nthis study investigates the potential of heterogeneous memory systems,\nproposing H2M2. It uses an asymmetric memory architecture consisting of\ncapacity-centric and bandwidthcentric memory with computation units attached to\neach memory device. With the asymmetric memory, we first analyze the effect of\nkernel-memory mapping for the asymmetric memory. Second, we propose a dynamic\nruntime algorithm that finds a mapping solution considering the characteristics\nof LLM operations and the change of footprint during LLM inference. Third, we\nadvocate the need for memory abstraction for the efficient management of the\nasymmetric memory. H2M2 outperforms the conventional homogeneous memory system\nwith LPDDR by 1.46x, 1.55x, and 2.94x speedup in GPT3-175B, Chinchilla-70B, and\nLlama2-70B, respectively."
                },
                "authors": [
                    {
                        "name": "Soojin Hwang"
                    },
                    {
                        "name": "Jungwoo Kim"
                    },
                    {
                        "name": "Sanghyeon Lee"
                    },
                    {
                        "name": "Hongbeen Kim"
                    },
                    {
                        "name": "Jaehyuk Huh"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyuk Huh"
                },
                "author": "Jaehyuk Huh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14891v1",
                "updated": "2025-04-21T06:39:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    6,
                    39,
                    47,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T06:39:47Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    6,
                    39,
                    47,
                    0,
                    111,
                    0
                ],
                "title": "Retrieval Augmented Generation Evaluation in the Era of Large Language\n  Models: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation Evaluation in the Era of Large Language\n  Models: A Comprehensive Survey"
                },
                "summary": "Recent advancements in Retrieval-Augmented Generation (RAG) have\nrevolutionized natural language processing by integrating Large Language Models\n(LLMs) with external information retrieval, enabling accurate, up-to-date, and\nverifiable text generation across diverse applications. However, evaluating RAG\nsystems presents unique challenges due to their hybrid architecture that\ncombines retrieval and generation components, as well as their dependence on\ndynamic knowledge sources in the LLM era. In response, this paper provides a\ncomprehensive survey of RAG evaluation methods and frameworks, systematically\nreviewing traditional and emerging evaluation approaches, for system\nperformance, factual accuracy, safety, and computational efficiency in the LLM\nera. We also compile and categorize the RAG-specific datasets and evaluation\nframeworks, conducting a meta-analysis of evaluation practices in high-impact\nRAG research. To the best of our knowledge, this work represents the most\ncomprehensive survey for RAG evaluation, bridging traditional and LLM-driven\nmethods, and serves as a critical resource for advancing RAG development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Retrieval-Augmented Generation (RAG) have\nrevolutionized natural language processing by integrating Large Language Models\n(LLMs) with external information retrieval, enabling accurate, up-to-date, and\nverifiable text generation across diverse applications. However, evaluating RAG\nsystems presents unique challenges due to their hybrid architecture that\ncombines retrieval and generation components, as well as their dependence on\ndynamic knowledge sources in the LLM era. In response, this paper provides a\ncomprehensive survey of RAG evaluation methods and frameworks, systematically\nreviewing traditional and emerging evaluation approaches, for system\nperformance, factual accuracy, safety, and computational efficiency in the LLM\nera. We also compile and categorize the RAG-specific datasets and evaluation\nframeworks, conducting a meta-analysis of evaluation practices in high-impact\nRAG research. To the best of our knowledge, this work represents the most\ncomprehensive survey for RAG evaluation, bridging traditional and LLM-driven\nmethods, and serves as a critical resource for advancing RAG development."
                },
                "authors": [
                    {
                        "name": "Aoran Gan"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Wenyu Yan"
                    },
                    {
                        "name": "Zhenya Huang"
                    },
                    {
                        "name": "Shiwei Tong"
                    },
                    {
                        "name": "Guoping Hu"
                    }
                ],
                "author_detail": {
                    "name": "Guoping Hu"
                },
                "author": "Guoping Hu",
                "arxiv_comment": "18 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14871v1",
                "updated": "2025-04-21T05:48:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    48,
                    52,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T05:48:52Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    48,
                    52,
                    0,
                    111,
                    0
                ],
                "title": "Natural Fingerprints of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Fingerprints of Large Language Models"
                },
                "summary": "Large language models (LLMs) often exhibit biases -- systematic deviations\nfrom expected norms -- in their outputs. These range from overt issues, such as\nunfair responses, to subtler patterns that can reveal which model produced\nthem. We investigate the factors that give rise to identifiable characteristics\nin LLMs. Since LLMs model training data distribution, it is reasonable that\ndifferences in training data naturally lead to the characteristics. However,\nour findings reveal that even when LLMs are trained on the exact same data, it\nis still possible to distinguish the source model based on its generated text.\nWe refer to these unintended, distinctive characteristics as natural\nfingerprints. By systematically controlling training conditions, we show that\nthe natural fingerprints can emerge from subtle differences in the training\nprocess, such as parameter sizes, optimization settings, and even random seeds.\nWe believe that understanding natural fingerprints offers new insights into the\norigins of unintended bias and ways for improving control over LLM behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often exhibit biases -- systematic deviations\nfrom expected norms -- in their outputs. These range from overt issues, such as\nunfair responses, to subtler patterns that can reveal which model produced\nthem. We investigate the factors that give rise to identifiable characteristics\nin LLMs. Since LLMs model training data distribution, it is reasonable that\ndifferences in training data naturally lead to the characteristics. However,\nour findings reveal that even when LLMs are trained on the exact same data, it\nis still possible to distinguish the source model based on its generated text.\nWe refer to these unintended, distinctive characteristics as natural\nfingerprints. By systematically controlling training conditions, we show that\nthe natural fingerprints can emerge from subtle differences in the training\nprocess, such as parameter sizes, optimization settings, and even random seeds.\nWe believe that understanding natural fingerprints offers new insights into the\norigins of unintended bias and ways for improving control over LLM behavior."
                },
                "authors": [
                    {
                        "name": "Teppei Suzuki"
                    },
                    {
                        "name": "Ryokan Ri"
                    },
                    {
                        "name": "Sho Takase"
                    }
                ],
                "author_detail": {
                    "name": "Sho Takase"
                },
                "author": "Sho Takase",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14870v1",
                "updated": "2025-04-21T05:40:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    40,
                    5,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T05:40:05Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    40,
                    5,
                    0,
                    111,
                    0
                ],
                "title": "OTC: Optimal Tool Calls via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OTC: Optimal Tool Calls via Reinforcement Learning"
                },
                "summary": "Tool-integrated reasoning (TIR) augments large language models (LLMs) with\nthe ability to invoke external tools, such as search engines and code\ninterpreters, to solve tasks beyond the capabilities of language-only\nreasoning. While reinforcement learning (RL) has shown promise in improving TIR\nby optimizing final answer correctness, existing approaches often overlook the\nefficiency and cost associated with tool usage. This can lead to suboptimal\nbehavior, including excessive tool calls that increase computational and\nfinancial overhead, or insufficient tool use that compromises answer quality.\nIn this work, we propose Optimal Tool Call-controlled Policy Optimization\n(OTC-PO), a simple yet effective RL-based framework that encourages models to\nproduce accurate answers with minimal tool calls. Our method introduces a\ntool-integrated reward that jointly considers correctness and tool efficiency,\npromoting high tool productivity. We instantiate this framework within both\nProximal Policy Optimization (PPO) and Group Relative Preference Optimization\n(GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and\nQwen-Math across multiple QA benchmarks show that our approach reduces tool\ncalls by up to 73.1\\% and improves tool productivity by up to 229.4\\%, while\nmaintaining comparable answer accuracy. To the best of our knowledge, this is\nthe first RL-based framework that explicitly optimizes tool-use efficiency in\nTIR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-integrated reasoning (TIR) augments large language models (LLMs) with\nthe ability to invoke external tools, such as search engines and code\ninterpreters, to solve tasks beyond the capabilities of language-only\nreasoning. While reinforcement learning (RL) has shown promise in improving TIR\nby optimizing final answer correctness, existing approaches often overlook the\nefficiency and cost associated with tool usage. This can lead to suboptimal\nbehavior, including excessive tool calls that increase computational and\nfinancial overhead, or insufficient tool use that compromises answer quality.\nIn this work, we propose Optimal Tool Call-controlled Policy Optimization\n(OTC-PO), a simple yet effective RL-based framework that encourages models to\nproduce accurate answers with minimal tool calls. Our method introduces a\ntool-integrated reward that jointly considers correctness and tool efficiency,\npromoting high tool productivity. We instantiate this framework within both\nProximal Policy Optimization (PPO) and Group Relative Preference Optimization\n(GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and\nQwen-Math across multiple QA benchmarks show that our approach reduces tool\ncalls by up to 73.1\\% and improves tool productivity by up to 229.4\\%, while\nmaintaining comparable answer accuracy. To the best of our knowledge, this is\nthe first RL-based framework that explicitly optimizes tool-use efficiency in\nTIR."
                },
                "authors": [
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Wanjun Zhong"
                    },
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Shijue Huang"
                    },
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23512v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23512v2",
                "updated": "2025-04-21T05:40:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    40,
                    0,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-30T16:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    16,
                    48,
                    27,
                    6,
                    89,
                    0
                ],
                "title": "SCORE: Story Coherence and Retrieval Enhancement for AI Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCORE: Story Coherence and Retrieval Enhancement for AI Narratives"
                },
                "summary": "Large Language Models (LLMs) can generate creative and engaging narratives\nfrom user-specified input, but maintaining coherence and emotional depth\nthroughout these AI-generated stories remains a challenge. In this work, we\npropose SCORE, a framework for Story Coherence and Retrieval Enhancement,\ndesigned to detect and resolve narrative inconsistencies. By tracking key item\nstatuses and generating episode summaries, SCORE uses a Retrieval-Augmented\nGeneration (RAG) approach, incorporating TF-IDF and cosine similarity to\nidentify related episodes and enhance the overall story structure. Results from\ntesting multiple LLM-generated stories demonstrate that SCORE significantly\nimproves the consistency and stability of narrative coherence compared to\nbaseline GPT models, providing a more robust method for evaluating and refining\nAI-generated narratives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can generate creative and engaging narratives\nfrom user-specified input, but maintaining coherence and emotional depth\nthroughout these AI-generated stories remains a challenge. In this work, we\npropose SCORE, a framework for Story Coherence and Retrieval Enhancement,\ndesigned to detect and resolve narrative inconsistencies. By tracking key item\nstatuses and generating episode summaries, SCORE uses a Retrieval-Augmented\nGeneration (RAG) approach, incorporating TF-IDF and cosine similarity to\nidentify related episodes and enhance the overall story structure. Results from\ntesting multiple LLM-generated stories demonstrate that SCORE significantly\nimproves the consistency and stability of narrative coherence compared to\nbaseline GPT models, providing a more robust method for evaluating and refining\nAI-generated narratives."
                },
                "authors": [
                    {
                        "name": "Qiang Yi"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Shiyao Qian"
                    },
                    {
                        "name": "Xinhang Yuan"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Li Sun"
                    },
                    {
                        "name": "Keqin Li"
                    },
                    {
                        "name": "Kuan Lu"
                    },
                    {
                        "name": "Menghao Huo"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Tianyu Shi"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Shi"
                },
                "author": "Tianyu Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23512v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23512v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14815v2",
                "updated": "2025-04-21T05:29:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    29,
                    1,
                    0,
                    111,
                    0
                ],
                "published": "2024-10-18T18:35:19Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    18,
                    35,
                    19,
                    4,
                    292,
                    0
                ],
                "title": "Adapting Multilingual LLMs to Low-Resource Languages using Continued\n  Pre-training and Synthetic Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Multilingual LLMs to Low-Resource Languages using Continued\n  Pre-training and Synthetic Corpus"
                },
                "summary": "Multilingual LLMs support a variety of languages; however, their performance\nis suboptimal for low-resource languages. In this work, we emphasize the\nimportance of continued pre-training of multilingual LLMs and the use of\ntranslation-based synthetic pre-training corpora for improving LLMs in\nlow-resource languages. We conduct our study in the context of the low-resource\nIndic language Hindi. We introduce Nemotron-Mini-Hindi 4B, a bilingual SLM\nsupporting both Hindi and English, based on Nemotron-Mini 4B. The model is\ntrained using a mix of real and synthetic Hindi + English tokens, with\ncontinuous pre-training performed on 400B tokens. We demonstrate that both the\nbase and instruct models achieve state-of-the-art results on Hindi benchmarks\nwhile remaining competitive on English tasks. Additionally, we observe that the\ncontinued pre-training approach enhances the model's overall factual accuracy.\nWe perform an ablation study to highlight the impact of Hindi pre-training,\nshowing significant improvements in Hindi chat capabilities and factual\naccuracy, which cannot be achieved through Hindi alignment alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual LLMs support a variety of languages; however, their performance\nis suboptimal for low-resource languages. In this work, we emphasize the\nimportance of continued pre-training of multilingual LLMs and the use of\ntranslation-based synthetic pre-training corpora for improving LLMs in\nlow-resource languages. We conduct our study in the context of the low-resource\nIndic language Hindi. We introduce Nemotron-Mini-Hindi 4B, a bilingual SLM\nsupporting both Hindi and English, based on Nemotron-Mini 4B. The model is\ntrained using a mix of real and synthetic Hindi + English tokens, with\ncontinuous pre-training performed on 400B tokens. We demonstrate that both the\nbase and instruct models achieve state-of-the-art results on Hindi benchmarks\nwhile remaining competitive on English tasks. Additionally, we observe that the\ncontinued pre-training approach enhances the model's overall factual accuracy.\nWe perform an ablation study to highlight the impact of Hindi pre-training,\nshowing significant improvements in Hindi chat capabilities and factual\naccuracy, which cannot be achieved through Hindi alignment alone."
                },
                "authors": [
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Kanishk Singla"
                    },
                    {
                        "name": "Anusha Kamath"
                    },
                    {
                        "name": "Raunak Kalani"
                    },
                    {
                        "name": "Rakesh Paul"
                    },
                    {
                        "name": "Utkarsh Vaidya"
                    },
                    {
                        "name": "Sanjay Singh Chauhan"
                    },
                    {
                        "name": "Niranjan Wartikar"
                    },
                    {
                        "name": "Eileen Long"
                    }
                ],
                "author_detail": {
                    "name": "Eileen Long"
                },
                "author": "Eileen Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v1",
                "updated": "2025-04-21T05:27:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Jiajun Cheng"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "15 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09407v2",
                "updated": "2025-04-21T05:22:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    22,
                    55,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-13T02:34:22Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    2,
                    34,
                    22,
                    6,
                    103,
                    0
                ],
                "title": "UXAgent: A System for Simulating Usability Testing of Web Design with\n  LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UXAgent: A System for Simulating Usability Testing of Web Design with\n  LLM Agents"
                },
                "summary": "Usability testing is a fundamental research method that user experience (UX)\nresearchers use to evaluate and iterate a web design, but\\textbf{ how to\nevaluate and iterate the usability testing study design } itself? Recent\nadvances in Large Language Model-simulated Agent (\\textbf{LLM Agent}) research\ninspired us to design \\textbf{UXAgent} to support UX researchers in evaluating\nand reiterating their usability testing study design before they conduct the\nreal human-subject study. Our system features a Persona Generator module, an\nLLM Agent module, and a Universal Browser Connector module to automatically\ngenerate thousands of simulated users to interactively test the target website.\nThe system also provides an Agent Interview Interface and a Video Replay\nInterface so that the UX researchers can easily review and analyze the\ngenerated qualitative and quantitative log data. Through a heuristic\nevaluation, five UX researcher participants praised the innovation of our\nsystem but also expressed concerns about the future of LLM Agent usage in UX\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Usability testing is a fundamental research method that user experience (UX)\nresearchers use to evaluate and iterate a web design, but\\textbf{ how to\nevaluate and iterate the usability testing study design } itself? Recent\nadvances in Large Language Model-simulated Agent (\\textbf{LLM Agent}) research\ninspired us to design \\textbf{UXAgent} to support UX researchers in evaluating\nand reiterating their usability testing study design before they conduct the\nreal human-subject study. Our system features a Persona Generator module, an\nLLM Agent module, and a Universal Browser Connector module to automatically\ngenerate thousands of simulated users to interactively test the target website.\nThe system also provides an Agent Interview Interface and a Video Replay\nInterface so that the UX researchers can easily review and analyze the\ngenerated qualitative and quantitative log data. Through a heuristic\nevaluation, five UX researcher participants praised the innovation of our\nsystem but also expressed concerns about the future of LLM Agent usage in UX\nstudies."
                },
                "authors": [
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Jessie Wang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Jiri Gesi"
                    },
                    {
                        "name": "Qi He"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.15281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15281v1",
                "updated": "2025-04-21T17:59:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    59,
                    55,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:59:55Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    59,
                    55,
                    0,
                    111,
                    0
                ],
                "title": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on\n  3D Gaussians",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on\n  3D Gaussians"
                },
                "summary": "3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction\nbut struggles with stylized scenarios (e.g., cartoons, games) due to fragmented\ntextures, semantic misalignment, and limited adaptability to abstract\naesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer\nthat integrates multi-modal style conditioning, multi-level semantic alignment,\nand perceptual quality enhancement. Our key insights include: (1) optimizing\nonly RGB attributes preserves geometric integrity during stylization; (2)\ndisentangling low-, medium-, and high-level semantics is critical for coherent\nstyle transfer; (3) scalability across isolated objects and complex scenes is\nessential for practical deployment. StyleMe3D introduces four novel components:\nDynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent\nspace for semantic alignment; Contrastive Style Descriptor (CSD) for localized,\ncontent-aware texture transfer; Simultaneously Optimized Scale (SOS) to\ndecouple style details and structural coherence; and 3D Gaussian Quality\nAssessment (3DG-QA), a differentiable aesthetic prior trained on human-rated\ndata to suppress artifacts and enhance visual harmony. Evaluated on NeRF\nsynthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D\noutperforms state-of-the-art methods in preserving geometric details (e.g.,\ncarvings on sculptures) and ensuring stylistic consistency across scenes (e.g.,\ncoherent lighting in landscapes), while maintaining real-time rendering. This\nwork bridges photorealistic 3D GS and artistic stylization, unlocking\napplications in gaming, virtual worlds, and digital art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction\nbut struggles with stylized scenarios (e.g., cartoons, games) due to fragmented\ntextures, semantic misalignment, and limited adaptability to abstract\naesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer\nthat integrates multi-modal style conditioning, multi-level semantic alignment,\nand perceptual quality enhancement. Our key insights include: (1) optimizing\nonly RGB attributes preserves geometric integrity during stylization; (2)\ndisentangling low-, medium-, and high-level semantics is critical for coherent\nstyle transfer; (3) scalability across isolated objects and complex scenes is\nessential for practical deployment. StyleMe3D introduces four novel components:\nDynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent\nspace for semantic alignment; Contrastive Style Descriptor (CSD) for localized,\ncontent-aware texture transfer; Simultaneously Optimized Scale (SOS) to\ndecouple style details and structural coherence; and 3D Gaussian Quality\nAssessment (3DG-QA), a differentiable aesthetic prior trained on human-rated\ndata to suppress artifacts and enhance visual harmony. Evaluated on NeRF\nsynthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D\noutperforms state-of-the-art methods in preserving geometric details (e.g.,\ncarvings on sculptures) and ensuring stylistic consistency across scenes (e.g.,\ncoherent lighting in landscapes), while maintaining real-time rendering. This\nwork bridges photorealistic 3D GS and artistic stylization, unlocking\napplications in gaming, virtual worlds, and digital art."
                },
                "authors": [
                    {
                        "name": "Cailin Zhuang"
                    },
                    {
                        "name": "Yaoqi Hu"
                    },
                    {
                        "name": "Xuanyang Zhang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Jiacheng Bao"
                    },
                    {
                        "name": "Shengqi Liu"
                    },
                    {
                        "name": "Yiying Yang"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_comment": "16 pages; Project page: https://styleme3d.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15275v1",
                "updated": "2025-04-21T17:59:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    59,
                    2,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:59:02Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    59,
                    2,
                    0,
                    111,
                    0
                ],
                "title": "Stop Summation: Min-Form Credit Assignment Is All Process Reward Model\n  Needs for Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stop Summation: Min-Form Credit Assignment Is All Process Reward Model\n  Needs for Reasoning"
                },
                "summary": "Process reward models (PRMs) have proven effective for test-time scaling of\nLarge Language Models (LLMs) on challenging reasoning tasks. However, reward\nhacking issues with PRMs limit their successful application in reinforcement\nfine-tuning. In this paper, we identify the main cause of PRM-induced reward\nhacking: the canonical summation-form credit assignment in reinforcement\nlearning (RL), which defines the value as cumulative gamma-decayed future\nrewards, easily induces LLMs to hack steps with high rewards. To address this,\nwe propose PURE: Process sUpervised Reinforcement lEarning. The key innovation\nof PURE is a min-form credit assignment that formulates the value function as\nthe minimum of future rewards. This method significantly alleviates reward\nhacking by limiting the value function range and distributing advantages more\nreasonably. Through extensive experiments on 3 base models, we show that\nPRM-based approaches enabling min-form credit assignment achieve comparable\nreasoning performance to verifiable reward-based methods within only 30% steps.\nIn contrast, the canonical sum-form credit assignment collapses training even\nat the beginning! Additionally, when we supplement PRM-based fine-tuning with\njust 10% verifiable rewards, we further alleviate reward hacking and produce\nthe best fine-tuned model based on Qwen2.5-Math-7B in our experiments,\nachieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5\nbenchmarks. Moreover, we summarize the observed reward hacking cases and\nanalyze the causes of training collapse. Code and models are available at\nhttps://github.com/CJReinforce/PURE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process reward models (PRMs) have proven effective for test-time scaling of\nLarge Language Models (LLMs) on challenging reasoning tasks. However, reward\nhacking issues with PRMs limit their successful application in reinforcement\nfine-tuning. In this paper, we identify the main cause of PRM-induced reward\nhacking: the canonical summation-form credit assignment in reinforcement\nlearning (RL), which defines the value as cumulative gamma-decayed future\nrewards, easily induces LLMs to hack steps with high rewards. To address this,\nwe propose PURE: Process sUpervised Reinforcement lEarning. The key innovation\nof PURE is a min-form credit assignment that formulates the value function as\nthe minimum of future rewards. This method significantly alleviates reward\nhacking by limiting the value function range and distributing advantages more\nreasonably. Through extensive experiments on 3 base models, we show that\nPRM-based approaches enabling min-form credit assignment achieve comparable\nreasoning performance to verifiable reward-based methods within only 30% steps.\nIn contrast, the canonical sum-form credit assignment collapses training even\nat the beginning! Additionally, when we supplement PRM-based fine-tuning with\njust 10% verifiable rewards, we further alleviate reward hacking and produce\nthe best fine-tuned model based on Qwen2.5-Math-7B in our experiments,\nachieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5\nbenchmarks. Moreover, we summarize the observed reward hacking cases and\nanalyze the causes of training collapse. Code and models are available at\nhttps://github.com/CJReinforce/PURE."
                },
                "authors": [
                    {
                        "name": "Jie Cheng"
                    },
                    {
                        "name": "Ruixi Qiao"
                    },
                    {
                        "name": "Lijun Li"
                    },
                    {
                        "name": "Chao Guo"
                    },
                    {
                        "name": "Junle Wang"
                    },
                    {
                        "name": "Gang Xiong"
                    },
                    {
                        "name": "Yisheng Lv"
                    },
                    {
                        "name": "Fei-Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fei-Yue Wang"
                },
                "author": "Fei-Yue Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13263v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13263v2",
                "updated": "2025-04-21T17:58:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    58,
                    8,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-17T18:05:39Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    18,
                    5,
                    39,
                    3,
                    107,
                    0
                ],
                "title": "Causal-Copilot: An Autonomous Causal Analysis Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal-Copilot: An Autonomous Causal Analysis Agent"
                },
                "summary": "Causal analysis plays a foundational role in scientific discovery and\nreliable decision-making, yet it remains largely inaccessible to domain experts\ndue to its conceptual and algorithmic complexity. This disconnect between\ncausal methodology and practical usability presents a dual challenge: domain\nexperts are unable to leverage recent advances in causal learning, while causal\nresearchers lack broad, real-world deployment to test and refine their methods.\nTo address this, we introduce Causal-Copilot, an autonomous agent that\noperationalizes expert-level causal analysis within a large language model\nframework. Causal-Copilot automates the full pipeline of causal analysis for\nboth tabular and time-series data -- including causal discovery, causal\ninference, algorithm selection, hyperparameter optimization, result\ninterpretation, and generation of actionable insights. It supports interactive\nrefinement through natural language, lowering the barrier for non-specialists\nwhile preserving methodological rigor. By integrating over 20 state-of-the-art\ncausal analysis techniques, our system fosters a virtuous cycle -- expanding\naccess to advanced causal methods for domain experts while generating rich,\nreal-world applications that inform and advance causal theory. Empirical\nevaluations demonstrate that Causal-Copilot achieves superior performance\ncompared to existing baselines, offering a reliable, scalable, and extensible\nsolution that bridges the gap between theoretical sophistication and real-world\napplicability in causal analysis. A live interactive demo of Causal-Copilot is\navailable at https://causalcopilot.com/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal analysis plays a foundational role in scientific discovery and\nreliable decision-making, yet it remains largely inaccessible to domain experts\ndue to its conceptual and algorithmic complexity. This disconnect between\ncausal methodology and practical usability presents a dual challenge: domain\nexperts are unable to leverage recent advances in causal learning, while causal\nresearchers lack broad, real-world deployment to test and refine their methods.\nTo address this, we introduce Causal-Copilot, an autonomous agent that\noperationalizes expert-level causal analysis within a large language model\nframework. Causal-Copilot automates the full pipeline of causal analysis for\nboth tabular and time-series data -- including causal discovery, causal\ninference, algorithm selection, hyperparameter optimization, result\ninterpretation, and generation of actionable insights. It supports interactive\nrefinement through natural language, lowering the barrier for non-specialists\nwhile preserving methodological rigor. By integrating over 20 state-of-the-art\ncausal analysis techniques, our system fosters a virtuous cycle -- expanding\naccess to advanced causal methods for domain experts while generating rich,\nreal-world applications that inform and advance causal theory. Empirical\nevaluations demonstrate that Causal-Copilot achieves superior performance\ncompared to existing baselines, offering a reliable, scalable, and extensible\nsolution that bridges the gap between theoretical sophistication and real-world\napplicability in causal analysis. A live interactive demo of Causal-Copilot is\navailable at https://causalcopilot.com/."
                },
                "authors": [
                    {
                        "name": "Xinyue Wang"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Wenyi Wu"
                    },
                    {
                        "name": "Har Simrat Singh"
                    },
                    {
                        "name": "Fang Nan"
                    },
                    {
                        "name": "Songyao Jin"
                    },
                    {
                        "name": "Aryan Philip"
                    },
                    {
                        "name": "Saloni Patnaik"
                    },
                    {
                        "name": "Hou Zhu"
                    },
                    {
                        "name": "Shivam Singh"
                    },
                    {
                        "name": "Parjanya Prashant"
                    },
                    {
                        "name": "Qian Shen"
                    },
                    {
                        "name": "Biwei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Biwei Huang"
                },
                "author": "Biwei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13263v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13263v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15263v1",
                "updated": "2025-04-21T17:45:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    45,
                    21,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:45:21Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    45,
                    21,
                    0,
                    111,
                    0
                ],
                "title": "Interpretable Locomotion Prediction in Construction Using a\n  Memory-Driven LLM Agent With Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Locomotion Prediction in Construction Using a\n  Memory-Driven LLM Agent With Chain-of-Thought Reasoning"
                },
                "summary": "Construction tasks are inherently unpredictable, with dynamic environments\nand safety-critical demands posing significant risks to workers. Exoskeletons\noffer potential assistance but falter without accurate intent recognition\nacross diverse locomotion modes. This paper presents a locomotion prediction\nagent leveraging Large Language Models (LLMs) augmented with memory systems,\naimed at improving exoskeleton assistance in such settings. Using multimodal\ninputs - spoken commands and visual data from smart glasses - the agent\nintegrates a Perception Module, Short-Term Memory (STM), Long-Term Memory\n(LTM), and Refinement Module to predict locomotion modes effectively.\nEvaluation reveals a baseline weighted F1-score of 0.73 without memory, rising\nto 0.81 with STM, and reaching 0.90 with both STM and LTM, excelling with vague\nand safety-critical commands. Calibration metrics, including a Brier Score drop\nfrom 0.244 to 0.090 and ECE from 0.222 to 0.044, affirm improved reliability.\nThis framework supports safer, high-level human-exoskeleton collaboration, with\npromise for adaptive assistive systems in dynamic industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Construction tasks are inherently unpredictable, with dynamic environments\nand safety-critical demands posing significant risks to workers. Exoskeletons\noffer potential assistance but falter without accurate intent recognition\nacross diverse locomotion modes. This paper presents a locomotion prediction\nagent leveraging Large Language Models (LLMs) augmented with memory systems,\naimed at improving exoskeleton assistance in such settings. Using multimodal\ninputs - spoken commands and visual data from smart glasses - the agent\nintegrates a Perception Module, Short-Term Memory (STM), Long-Term Memory\n(LTM), and Refinement Module to predict locomotion modes effectively.\nEvaluation reveals a baseline weighted F1-score of 0.73 without memory, rising\nto 0.81 with STM, and reaching 0.90 with both STM and LTM, excelling with vague\nand safety-critical commands. Calibration metrics, including a Brier Score drop\nfrom 0.244 to 0.090 and ECE from 0.222 to 0.044, affirm improved reliability.\nThis framework supports safer, high-level human-exoskeleton collaboration, with\npromise for adaptive assistive systems in dynamic industries."
                },
                "authors": [
                    {
                        "name": "Ehsan Ahmadi"
                    },
                    {
                        "name": "Chao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Wang"
                },
                "author": "Chao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10566v2",
                "updated": "2025-04-21T17:45:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    45,
                    8,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-13T17:17:17Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    17,
                    17,
                    3,
                    72,
                    0
                ],
                "title": "ASIDE: Architectural Separation of Instructions and Data in Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASIDE: Architectural Separation of Instructions and Data in Language\n  Models"
                },
                "summary": "Despite their remarkable performance, large language models lack elementary\nsafety features, and this makes them susceptible to numerous malicious attacks.\nIn particular, previous work has identified the absence of an intrinsic\nseparation between instructions and data as a root cause for the success of\nprompt injection attacks. In this work, we propose a method, ASIDE, that allows\nthe model to clearly separate between instructions and data on the level of\nembeddings. ASIDE applies a fixed orthogonal rotation to the embeddings of data\ntokens, thus creating distinct representations of instructions and data tokens\nwithout introducing any additional parameters. We demonstrate the effectiveness\nof our method by instruct-tuning LLMs with ASIDE and showing (1) highly\nincreased instruction-data separation scores without a loss in model\ncapabilities and (2) competitive results on prompt injection benchmarks, even\nwithout dedicated safety training. Additionally, we study the working mechanism\nbehind our method through an analysis of model representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable performance, large language models lack elementary\nsafety features, and this makes them susceptible to numerous malicious attacks.\nIn particular, previous work has identified the absence of an intrinsic\nseparation between instructions and data as a root cause for the success of\nprompt injection attacks. In this work, we propose a method, ASIDE, that allows\nthe model to clearly separate between instructions and data on the level of\nembeddings. ASIDE applies a fixed orthogonal rotation to the embeddings of data\ntokens, thus creating distinct representations of instructions and data tokens\nwithout introducing any additional parameters. We demonstrate the effectiveness\nof our method by instruct-tuning LLMs with ASIDE and showing (1) highly\nincreased instruction-data separation scores without a loss in model\ncapabilities and (2) competitive results on prompt injection benchmarks, even\nwithout dedicated safety training. Additionally, we study the working mechanism\nbehind our method through an analysis of model representations."
                },
                "authors": [
                    {
                        "name": "Egor Zverev"
                    },
                    {
                        "name": "Evgenii Kortukov"
                    },
                    {
                        "name": "Alexander Panfilov"
                    },
                    {
                        "name": "Alexandra Volkova"
                    },
                    {
                        "name": "Soroush Tabesh"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Wojciech Samek"
                    },
                    {
                        "name": "Christoph H. Lampert"
                    }
                ],
                "author_detail": {
                    "name": "Christoph H. Lampert"
                },
                "author": "Christoph H. Lampert",
                "arxiv_comment": "ICLR 2025 Workshop on Building Trust in Language Models and\n  Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15256v1",
                "updated": "2025-04-21T17:35:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    35,
                    12,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:35:12Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    35,
                    12,
                    0,
                    111,
                    0
                ],
                "title": "Impulsive pattern recognition of a myoelectric hand via Dynamic Time\n  Warping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impulsive pattern recognition of a myoelectric hand via Dynamic Time\n  Warping"
                },
                "summary": "Although myoelectric prosthetic hands provide amputees with intuitive\ncontrol, their reliance on many EMG sensors limits accessibility and makes them\ncomplex and expensive. To address this problem, this work presents a different\nperspective that makes use of a single EMG sensor and brief impulse signals in\nconjunction with Dynamic Time Warping (DTW) for accurate pattern detection.\nConventional techniques rely on real-time data from multiple sensors, which can\nbe costly and bulky. The method presents high accuracy while lowering hardware\ncomplexity and expense. A DTW-based system that reliably identifies muscle\nactivation patterns from short EMG signals was created and tested. Results show\nthat this single-sensor approach obtained an accuracy rate of 92%, which is\nsimilar to that of conventional multi-sensor systems. This research provides a\nmore straightforward and economical approach that can be used to obtain\nenhanced myoelectric control. These findings provide a different perspective on\nmore easily accessible and user-friendly prosthetic devices, which will be\nespecially helpful in disaster-affected areas where quick deployment is\nessential. Future improvements would investigate this system's dependability\nover time and wider implementations in real situations, to take prosthetic\ntechnology one step further.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although myoelectric prosthetic hands provide amputees with intuitive\ncontrol, their reliance on many EMG sensors limits accessibility and makes them\ncomplex and expensive. To address this problem, this work presents a different\nperspective that makes use of a single EMG sensor and brief impulse signals in\nconjunction with Dynamic Time Warping (DTW) for accurate pattern detection.\nConventional techniques rely on real-time data from multiple sensors, which can\nbe costly and bulky. The method presents high accuracy while lowering hardware\ncomplexity and expense. A DTW-based system that reliably identifies muscle\nactivation patterns from short EMG signals was created and tested. Results show\nthat this single-sensor approach obtained an accuracy rate of 92%, which is\nsimilar to that of conventional multi-sensor systems. This research provides a\nmore straightforward and economical approach that can be used to obtain\nenhanced myoelectric control. These findings provide a different perspective on\nmore easily accessible and user-friendly prosthetic devices, which will be\nespecially helpful in disaster-affected areas where quick deployment is\nessential. Future improvements would investigate this system's dependability\nover time and wider implementations in real situations, to take prosthetic\ntechnology one step further."
                },
                "authors": [
                    {
                        "name": "Mustafa Can Kadilar"
                    },
                    {
                        "name": "Ersin Toptaş"
                    },
                    {
                        "name": "Gazi Akgün"
                    }
                ],
                "author_detail": {
                    "name": "Gazi Akgün"
                },
                "author": "Gazi Akgün",
                "arxiv_comment": "6 pages, 3 tables, 8 figures. Part of this work was presented at the\n  International Journal of Advanced Natural Sciences and Engineering Researches\n  (ICIAS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.5.1; I.5.4; I.2.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15254v1",
                "updated": "2025-04-21T17:33:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    33,
                    33,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:33:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    33,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation"
                },
                "summary": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench."
                },
                "authors": [
                    {
                        "name": "Anirudh Khatry"
                    },
                    {
                        "name": "Robert Zhang"
                    },
                    {
                        "name": "Jia Pan"
                    },
                    {
                        "name": "Ziteng Wang"
                    },
                    {
                        "name": "Qiaochu Chen"
                    },
                    {
                        "name": "Greg Durrett"
                    },
                    {
                        "name": "Isil Dillig"
                    }
                ],
                "author_detail": {
                    "name": "Isil Dillig"
                },
                "author": "Isil Dillig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15253v1",
                "updated": "2025-04-21T17:33:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    33,
                    23,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:33:23Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    33,
                    23,
                    0,
                    111,
                    0
                ],
                "title": "Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as\n  Test-Time Scaling Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as\n  Test-Time Scaling Evaluators"
                },
                "summary": "Scaling test-time computation, or affording a generator large language model\n(LLM) extra compute during inference, typically employs the help of external\nnon-generative evaluators (i.e., reward models). Concurrently, LLM-judges,\nmodels trained to generate evaluations and critiques (explanations) in natural\nlanguage, are becoming increasingly popular in automatic evaluation. Despite\njudge empirical successes, their effectiveness as evaluators in test-time\nscaling settings is largely unknown. In this paper, we introduce the Judge\nEvaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge\nperformance in three domains (math reasoning, code generation, and instruction\nfollowing) under three task settings: response reranking, step-level beam\nsearch, and critique-based response refinement. We evaluate 10 different judge\nmodels (7B-70B parameters) for 8 different base generator models (6.7B-72B\nparameters). Our benchmark shows that while judges are competitive with outcome\nreward models in reranking, they are consistently worse than process reward\nmodels in beam search procedures. Furthermore, though unique to LLM-judges,\ntheir natural language critiques are currently ineffective in guiding the\ngenerator towards better responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling test-time computation, or affording a generator large language model\n(LLM) extra compute during inference, typically employs the help of external\nnon-generative evaluators (i.e., reward models). Concurrently, LLM-judges,\nmodels trained to generate evaluations and critiques (explanations) in natural\nlanguage, are becoming increasingly popular in automatic evaluation. Despite\njudge empirical successes, their effectiveness as evaluators in test-time\nscaling settings is largely unknown. In this paper, we introduce the Judge\nEvaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge\nperformance in three domains (math reasoning, code generation, and instruction\nfollowing) under three task settings: response reranking, step-level beam\nsearch, and critique-based response refinement. We evaluate 10 different judge\nmodels (7B-70B parameters) for 8 different base generator models (6.7B-72B\nparameters). Our benchmark shows that while judges are competitive with outcome\nreward models in reranking, they are consistently worse than process reward\nmodels in beam search procedures. Furthermore, though unique to LLM-judges,\ntheir natural language critiques are currently ineffective in guiding the\ngenerator towards better responses."
                },
                "authors": [
                    {
                        "name": "Yilun Zhou"
                    },
                    {
                        "name": "Austin Xu"
                    },
                    {
                        "name": "Peifeng Wang"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "The first two authors contributed equally. The codebase is at\n  https://github.com/SalesforceAIResearch/jetts-benchmark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15252v1",
                "updated": "2025-04-21T17:33:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    33,
                    2,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:33:02Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    33,
                    2,
                    0,
                    111,
                    0
                ],
                "title": "SuoiAI: Building a Dataset for Aquatic Invertebrates in Vietnam",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuoiAI: Building a Dataset for Aquatic Invertebrates in Vietnam"
                },
                "summary": "Understanding and monitoring aquatic biodiversity is critical for ecological\nhealth and conservation efforts. This paper proposes SuoiAI, an end-to-end\npipeline for building a dataset of aquatic invertebrates in Vietnam and\nemploying machine learning (ML) techniques for species classification. We\noutline the methods for data collection, annotation, and model training,\nfocusing on reducing annotation effort through semi-supervised learning and\nleveraging state-of-the-art object detection and classification models. Our\napproach aims to overcome challenges such as data scarcity, fine-grained\nclassification, and deployment in diverse environmental conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and monitoring aquatic biodiversity is critical for ecological\nhealth and conservation efforts. This paper proposes SuoiAI, an end-to-end\npipeline for building a dataset of aquatic invertebrates in Vietnam and\nemploying machine learning (ML) techniques for species classification. We\noutline the methods for data collection, annotation, and model training,\nfocusing on reducing annotation effort through semi-supervised learning and\nleveraging state-of-the-art object detection and classification models. Our\napproach aims to overcome challenges such as data scarcity, fine-grained\nclassification, and deployment in diverse environmental conditions."
                },
                "authors": [
                    {
                        "name": "Tue Vo"
                    },
                    {
                        "name": "Lakshay Sharma"
                    },
                    {
                        "name": "Tuan Dinh"
                    },
                    {
                        "name": "Khuong Dinh"
                    },
                    {
                        "name": "Trang Nguyen"
                    },
                    {
                        "name": "Trung Phan"
                    },
                    {
                        "name": "Minh Do"
                    },
                    {
                        "name": "Duong Vu"
                    }
                ],
                "author_detail": {
                    "name": "Duong Vu"
                },
                "author": "Duong Vu",
                "arxiv_comment": "Published as a workshop paper at \"Tackling Climate Change with\n  Machine Learning\", ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05291v2",
                "updated": "2025-04-21T17:30:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    30,
                    23,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-07T19:50:02Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    19,
                    50,
                    2,
                    4,
                    38,
                    0
                ],
                "title": "Can LLMs Rank the Harmfulness of Smaller LLMs? We are Not There Yet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Rank the Harmfulness of Smaller LLMs? We are Not There Yet"
                },
                "summary": "Large language models (LLMs) have become ubiquitous, thus it is important to\nunderstand their risks and limitations. Smaller LLMs can be deployed where\ncompute resources are constrained, such as edge devices, but with different\npropensity to generate harmful output. Mitigation of LLM harm typically depends\non annotating the harmfulness of LLM output, which is expensive to collect from\nhumans. This work studies two questions: How do smaller LLMs rank regarding\ngeneration of harmful content? How well can larger LLMs annotate harmfulness?\nWe prompt three small LLMs to elicit harmful content of various types, such as\ndiscriminatory language, offensive content, privacy invasion, or negative\ninfluence, and collect human rankings of their outputs. Then, we evaluate three\nstate-of-the-art large LLMs on their ability to annotate the harmfulness of\nthese responses. We find that the smaller models differ with respect to\nharmfulness. We also find that large LLMs show low to moderate agreement with\nhumans. These findings underline the need for further work on harm mitigation\nin LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become ubiquitous, thus it is important to\nunderstand their risks and limitations. Smaller LLMs can be deployed where\ncompute resources are constrained, such as edge devices, but with different\npropensity to generate harmful output. Mitigation of LLM harm typically depends\non annotating the harmfulness of LLM output, which is expensive to collect from\nhumans. This work studies two questions: How do smaller LLMs rank regarding\ngeneration of harmful content? How well can larger LLMs annotate harmfulness?\nWe prompt three small LLMs to elicit harmful content of various types, such as\ndiscriminatory language, offensive content, privacy invasion, or negative\ninfluence, and collect human rankings of their outputs. Then, we evaluate three\nstate-of-the-art large LLMs on their ability to annotate the harmfulness of\nthese responses. We find that the smaller models differ with respect to\nharmfulness. We also find that large LLMs show low to moderate agreement with\nhumans. These findings underline the need for further work on harm mitigation\nin LLMs."
                },
                "authors": [
                    {
                        "name": "Berk Atil"
                    },
                    {
                        "name": "Vipul Gupta"
                    },
                    {
                        "name": "Sarkar Snigdha Sarathi Das"
                    },
                    {
                        "name": "Rebecca J. Passonneau"
                    }
                ],
                "author_detail": {
                    "name": "Rebecca J. Passonneau"
                },
                "author": "Rebecca J. Passonneau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15241v1",
                "updated": "2025-04-21T17:15:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    15,
                    6,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:15:06Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    15,
                    6,
                    0,
                    111,
                    0
                ],
                "title": "MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning"
                },
                "summary": "Large Language Models (LLMs) are susceptible to adversarial attacks such as\njailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability\nis exacerbated in multilingual setting, where multilingual safety-aligned data\nare often limited. Thus, developing a guardrail capable of detecting and\nfiltering unsafe content across diverse languages is critical for deploying\nLLMs in real-world applications. In this work, we propose an approach to build\na multilingual guardrail with reasoning. Our method consists of: (1) synthetic\nmultilingual data generation incorporating culturally and linguistically\nnuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group\nRelative Policy Optimization (GRPO) framework that further improves\nperformance. Experimental results demonstrate that our multilingual guardrail\nconsistently outperforms recent baselines across both in-domain and\nout-of-domain languages. The multilingual reasoning capability of our guardrail\nenables it to generate multilingual explanations, which are particularly useful\nfor understanding language-specific risks and ambiguities in multilingual\ncontent moderation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are susceptible to adversarial attacks such as\njailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability\nis exacerbated in multilingual setting, where multilingual safety-aligned data\nare often limited. Thus, developing a guardrail capable of detecting and\nfiltering unsafe content across diverse languages is critical for deploying\nLLMs in real-world applications. In this work, we propose an approach to build\na multilingual guardrail with reasoning. Our method consists of: (1) synthetic\nmultilingual data generation incorporating culturally and linguistically\nnuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group\nRelative Policy Optimization (GRPO) framework that further improves\nperformance. Experimental results demonstrate that our multilingual guardrail\nconsistently outperforms recent baselines across both in-domain and\nout-of-domain languages. The multilingual reasoning capability of our guardrail\nenables it to generate multilingual explanations, which are particularly useful\nfor understanding language-specific risks and ambiguities in multilingual\ncontent moderation."
                },
                "authors": [
                    {
                        "name": "Yahan Yang"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Shuo Li"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Insup Lee"
                    }
                ],
                "author_detail": {
                    "name": "Insup Lee"
                },
                "author": "Insup Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15236v1",
                "updated": "2025-04-21T17:13:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    13,
                    16,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:13:16Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    13,
                    16,
                    0,
                    111,
                    0
                ],
                "title": "Values in the Wild: Discovering and Analyzing Values in Real-World\n  Language Model Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Values in the Wild: Discovering and Analyzing Values in Real-World\n  Language Model Interactions"
                },
                "summary": "AI assistants can impart value judgments that shape people's decisions and\nworldviews, yet little is known empirically about what values these systems\nrely on in practice. To address this, we develop a bottom-up,\nprivacy-preserving method to extract the values (normative considerations\nstated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit\nin hundreds of thousands of real-world interactions. We empirically discover\nand taxonomize 3,307 AI values and study how they vary by context. We find that\nClaude expresses many practical and epistemic values, and typically supports\nprosocial human values while resisting values like \"moral nihilism\". While some\nvalues appear consistently across contexts (e.g. \"transparency\"), many are more\nspecialized and context-dependent, reflecting the diversity of human\ninterlocutors and their varied contexts. For example, \"harm prevention\" emerges\nwhen Claude resists users, \"historical accuracy\" when responding to queries\nabout controversial events, \"healthy boundaries\" when asked for relationship\nadvice, and \"human agency\" in technology ethics discussions. By providing the\nfirst large-scale empirical mapping of AI values in deployment, our work\ncreates a foundation for more grounded evaluation and design of values in AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI assistants can impart value judgments that shape people's decisions and\nworldviews, yet little is known empirically about what values these systems\nrely on in practice. To address this, we develop a bottom-up,\nprivacy-preserving method to extract the values (normative considerations\nstated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit\nin hundreds of thousands of real-world interactions. We empirically discover\nand taxonomize 3,307 AI values and study how they vary by context. We find that\nClaude expresses many practical and epistemic values, and typically supports\nprosocial human values while resisting values like \"moral nihilism\". While some\nvalues appear consistently across contexts (e.g. \"transparency\"), many are more\nspecialized and context-dependent, reflecting the diversity of human\ninterlocutors and their varied contexts. For example, \"harm prevention\" emerges\nwhen Claude resists users, \"historical accuracy\" when responding to queries\nabout controversial events, \"healthy boundaries\" when asked for relationship\nadvice, and \"human agency\" in technology ethics discussions. By providing the\nfirst large-scale empirical mapping of AI values in deployment, our work\ncreates a foundation for more grounded evaluation and design of values in AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Saffron Huang"
                    },
                    {
                        "name": "Esin Durmus"
                    },
                    {
                        "name": "Miles McCain"
                    },
                    {
                        "name": "Kunal Handa"
                    },
                    {
                        "name": "Alex Tamkin"
                    },
                    {
                        "name": "Jerry Hong"
                    },
                    {
                        "name": "Michael Stern"
                    },
                    {
                        "name": "Arushi Somani"
                    },
                    {
                        "name": "Xiuruo Zhang"
                    },
                    {
                        "name": "Deep Ganguli"
                    }
                ],
                "author_detail": {
                    "name": "Deep Ganguli"
                },
                "author": "Deep Ganguli",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15228v1",
                "updated": "2025-04-21T16:58:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    58,
                    18,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T16:58:18Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    58,
                    18,
                    0,
                    111,
                    0
                ],
                "title": "A Self-Improving Coding Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Self-Improving Coding Agent"
                },
                "summary": "We demonstrate that an LLM coding agent, equipped with basic coding tools,\ncan autonomously edit itself, and thereby improve its performance on benchmark\ntasks. We find performance gains from 17% to 53% on a random subset of SWE\nBench Verified, with additional performance gains on LiveCodeBench, as well as\nsynthetically generated agent benchmarks. Our work represents an advancement in\nthe automated and open-ended design of agentic systems, and provides a\nreference agent framework for those seeking to post-train LLMs on tool use and\nother agentic tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that an LLM coding agent, equipped with basic coding tools,\ncan autonomously edit itself, and thereby improve its performance on benchmark\ntasks. We find performance gains from 17% to 53% on a random subset of SWE\nBench Verified, with additional performance gains on LiveCodeBench, as well as\nsynthetically generated agent benchmarks. Our work represents an advancement in\nthe automated and open-ended design of agentic systems, and provides a\nreference agent framework for those seeking to post-train LLMs on tool use and\nother agentic tasks."
                },
                "authors": [
                    {
                        "name": "Maxime Robeyns"
                    },
                    {
                        "name": "Martin Szummer"
                    },
                    {
                        "name": "Laurence Aitchison"
                    }
                ],
                "author_detail": {
                    "name": "Laurence Aitchison"
                },
                "author": "Laurence Aitchison",
                "arxiv_comment": "Published at an ICLR 2025 workshop on Scaling Self-Improving\n  Foundation Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23611v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23611v3",
                "updated": "2025-04-21T16:57:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    57,
                    36,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-30T22:25:18Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    22,
                    25,
                    18,
                    6,
                    89,
                    0
                ],
                "title": "My CXL Pool Obviates Your PCIe Switch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "My CXL Pool Obviates Your PCIe Switch"
                },
                "summary": "Pooling PCIe devices across multiple hosts offers a promising solution to\nmitigate stranded I/O resources, enhance device utilization, address device\nfailures, and reduce total cost of ownership. The only viable option today are\nPCIe switches, which decouple PCIe devices from hosts by connecting them\nthrough a hardware switch. However, the high cost and limited flexibility of\nPCIe switches hinder their widespread adoption beyond specialized datacenter\nuse cases.\n  This paper argues that PCIe device pooling can be effectively implemented in\nsoftware using CXL memory pools. CXL memory pools improve memory utilization\nand already have positive return on investment. We find that, once CXL pools\nare in place, they can serve as a building block for pooling any kind of PCIe\ndevice. We demonstrate that PCIe devices can directly use CXL memory as I/O\nbuffers without device modifications, which enables routing PCIe traffic\nthrough CXL pool memory. This software-based approach is deployable on today's\nhardware and is more flexible than hardware PCIe switches. In particular, we\nexplore how disaggregating devices such as NICs can transform datacenter\ninfrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pooling PCIe devices across multiple hosts offers a promising solution to\nmitigate stranded I/O resources, enhance device utilization, address device\nfailures, and reduce total cost of ownership. The only viable option today are\nPCIe switches, which decouple PCIe devices from hosts by connecting them\nthrough a hardware switch. However, the high cost and limited flexibility of\nPCIe switches hinder their widespread adoption beyond specialized datacenter\nuse cases.\n  This paper argues that PCIe device pooling can be effectively implemented in\nsoftware using CXL memory pools. CXL memory pools improve memory utilization\nand already have positive return on investment. We find that, once CXL pools\nare in place, they can serve as a building block for pooling any kind of PCIe\ndevice. We demonstrate that PCIe devices can directly use CXL memory as I/O\nbuffers without device modifications, which enables routing PCIe traffic\nthrough CXL pool memory. This software-based approach is deployable on today's\nhardware and is more flexible than hardware PCIe switches. In particular, we\nexplore how disaggregating devices such as NICs can transform datacenter\ninfrastructure."
                },
                "authors": [
                    {
                        "name": "Yuhong Zhong"
                    },
                    {
                        "name": "Daniel S. Berger"
                    },
                    {
                        "name": "Pantea Zardoshti"
                    },
                    {
                        "name": "Enrique Saurez"
                    },
                    {
                        "name": "Jacob Nelson"
                    },
                    {
                        "name": "Antonis Psistakis"
                    },
                    {
                        "name": "Joshua Fried"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "arxiv_doi": "10.1145/3713082.3730393",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730393",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.23611v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23611v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15219v1",
                "updated": "2025-04-21T16:43:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    43,
                    50,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T16:43:50Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    43,
                    50,
                    0,
                    111,
                    0
                ],
                "title": "EvalAgent: Discovering Implicit Evaluation Criteria from the Web",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvalAgent: Discovering Implicit Evaluation Criteria from the Web"
                },
                "summary": "Evaluation of language model outputs on structured writing tasks is typically\nconducted with a number of desirable criteria presented to human evaluators or\nlarge language models (LLMs). For instance, on a prompt like \"Help me draft an\nacademic talk on coffee intake vs research productivity\", a model response may\nbe evaluated for criteria like accuracy and coherence. However, high-quality\nresponses should do more than just satisfy basic task requirements. An\neffective response to this query should include quintessential features of an\nacademic talk, such as a compelling opening, clear research questions, and a\ntakeaway. To help identify these implicit criteria, we introduce EvalAgent, a\nnovel framework designed to automatically uncover nuanced and task-specific\ncriteria. EvalAgent first mines expert-authored online guidance. It then uses\nthis evidence to propose diverse, long-tail evaluation criteria that are\ngrounded in reliable external sources. Our experiments demonstrate that the\ngrounded criteria produced by EvalAgent are often implicit (not directly stated\nin the user's prompt), yet specific (high degree of lexical precision).\nFurther, EvalAgent criteria are often not satisfied by initial responses but\nthey are actionable, such that responses can be refined to satisfy them.\nFinally, we show that combining LLM-generated and EvalAgent criteria uncovers\nmore human-valued criteria than using LLMs alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of language model outputs on structured writing tasks is typically\nconducted with a number of desirable criteria presented to human evaluators or\nlarge language models (LLMs). For instance, on a prompt like \"Help me draft an\nacademic talk on coffee intake vs research productivity\", a model response may\nbe evaluated for criteria like accuracy and coherence. However, high-quality\nresponses should do more than just satisfy basic task requirements. An\neffective response to this query should include quintessential features of an\nacademic talk, such as a compelling opening, clear research questions, and a\ntakeaway. To help identify these implicit criteria, we introduce EvalAgent, a\nnovel framework designed to automatically uncover nuanced and task-specific\ncriteria. EvalAgent first mines expert-authored online guidance. It then uses\nthis evidence to propose diverse, long-tail evaluation criteria that are\ngrounded in reliable external sources. Our experiments demonstrate that the\ngrounded criteria produced by EvalAgent are often implicit (not directly stated\nin the user's prompt), yet specific (high degree of lexical precision).\nFurther, EvalAgent criteria are often not satisfied by initial responses but\nthey are actionable, such that responses can be refined to satisfy them.\nFinally, we show that combining LLM-generated and EvalAgent criteria uncovers\nmore human-valued criteria than using LLMs alone."
                },
                "authors": [
                    {
                        "name": "Manya Wadhwa"
                    },
                    {
                        "name": "Zayne Sprague"
                    },
                    {
                        "name": "Chaitanya Malaviya"
                    },
                    {
                        "name": "Philippe Laban"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    },
                    {
                        "name": "Greg Durrett"
                    }
                ],
                "author_detail": {
                    "name": "Greg Durrett"
                },
                "author": "Greg Durrett",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.14381v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.14381v4",
                "updated": "2025-04-21T16:33:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    33,
                    14,
                    0,
                    111,
                    0
                ],
                "published": "2023-11-24T10:00:23Z",
                "published_parsed": [
                    2023,
                    11,
                    24,
                    10,
                    0,
                    23,
                    4,
                    328,
                    0
                ],
                "title": "Potential Societal Biases of ChatGPT in Higher Education: A Scoping\n  Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential Societal Biases of ChatGPT in Higher Education: A Scoping\n  Review"
                },
                "summary": "Purpose:Generative Artificial Intelligence (GAI) models, such as ChatGPT, may\ninherit or amplify societal biases due to their training on extensive datasets.\nWith the increasing usage of GAI by students, faculty, and staff in higher\neducation institutions (HEIs), it is urgent to examine the ethical issues and\npotential biases associated with these technologies.\nDesign/Approach/Methods:This scoping review aims to elucidate how biases\nrelated to GAI in HEIs have been researched and discussed in recent academic\npublications. We categorized the potential societal biases that GAI might cause\nin the field of higher education. Our review includes articles written in\nEnglish, Chinese, and Japanese across four main databases, focusing on GAI\nusage in higher education and bias. Findings:Our findings reveal that while\nthere is meaningful scholarly discussion around bias and discrimination\nconcerning LLMs in the AI field, most articles addressing higher education\napproach the issue superficially. Few articles identify specific types of bias\nunder different circumstances, and there is a notable lack of empirical\nresearch. Most papers in our review focus primarily on educational and research\nfields related to medicine and engineering, with some addressing English\neducation. However, there is almost no discussion regarding the humanities and\nsocial sciences. Additionally, a significant portion of the current discourse\nis in English and primarily addresses English-speaking contexts.\nOriginality/Value:To the best of our knowledge, our study is the first to\nsummarize the potential societal biases in higher education. This review\nhighlights the need for more in-depth studies and empirical work to understand\nthe specific biases that GAI might introduce or amplify in educational\nsettings, guiding the development of more ethical AI applications in higher\neducation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose:Generative Artificial Intelligence (GAI) models, such as ChatGPT, may\ninherit or amplify societal biases due to their training on extensive datasets.\nWith the increasing usage of GAI by students, faculty, and staff in higher\neducation institutions (HEIs), it is urgent to examine the ethical issues and\npotential biases associated with these technologies.\nDesign/Approach/Methods:This scoping review aims to elucidate how biases\nrelated to GAI in HEIs have been researched and discussed in recent academic\npublications. We categorized the potential societal biases that GAI might cause\nin the field of higher education. Our review includes articles written in\nEnglish, Chinese, and Japanese across four main databases, focusing on GAI\nusage in higher education and bias. Findings:Our findings reveal that while\nthere is meaningful scholarly discussion around bias and discrimination\nconcerning LLMs in the AI field, most articles addressing higher education\napproach the issue superficially. Few articles identify specific types of bias\nunder different circumstances, and there is a notable lack of empirical\nresearch. Most papers in our review focus primarily on educational and research\nfields related to medicine and engineering, with some addressing English\neducation. However, there is almost no discussion regarding the humanities and\nsocial sciences. Additionally, a significant portion of the current discourse\nis in English and primarily addresses English-speaking contexts.\nOriginality/Value:To the best of our knowledge, our study is the first to\nsummarize the potential societal biases in higher education. This review\nhighlights the need for more in-depth studies and empirical work to understand\nthe specific biases that GAI might introduce or amplify in educational\nsettings, guiding the development of more ethical AI applications in higher\neducation."
                },
                "authors": [
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ariunaa Enkhtur"
                    },
                    {
                        "name": "Beverley Anne Yamamoto"
                    },
                    {
                        "name": "Fei Cheng"
                    },
                    {
                        "name": "Lilan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lilan Chen"
                },
                "author": "Lilan Chen",
                "arxiv_doi": "10.55982/openpraxis.17.1.750",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.55982/openpraxis.17.1.750",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.14381v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.14381v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Open Praxis",
                "arxiv_journal_ref": "2025, 17(1), pp.79-94",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15210v1",
                "updated": "2025-04-21T16:29:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    29,
                    7,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T16:29:07Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    29,
                    7,
                    0,
                    111,
                    0
                ],
                "title": "Integrating Symbolic Execution into the Fine-Tuning of Code-Generating\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Symbolic Execution into the Fine-Tuning of Code-Generating\n  LLMs"
                },
                "summary": "Code-generating Large Language Models (LLMs) have become essential tools in\nmodern software development, enhancing productivity and accelerating\ndevelopment. This paper aims to investigate the fine-tuning of code-generating\nLLMs using Reinforcement Learning and Direct Preference Optimization, further\nimproving their performance. To achieve this, we enhance the training data for\nthe reward model with the help of symbolic execution techniques, ensuring more\ncomprehensive and objective data. With symbolic execution, we create a custom\ndataset that better captures the nuances in code evaluation. Our reward models,\nfine-tuned on this dataset, demonstrate significant improvements over the\nbaseline, CodeRL, in estimating the quality of generated code. Our\ncode-generating LLMs, trained with the help of reward model feedback, achieve\nsimilar results compared to the CodeRL benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code-generating Large Language Models (LLMs) have become essential tools in\nmodern software development, enhancing productivity and accelerating\ndevelopment. This paper aims to investigate the fine-tuning of code-generating\nLLMs using Reinforcement Learning and Direct Preference Optimization, further\nimproving their performance. To achieve this, we enhance the training data for\nthe reward model with the help of symbolic execution techniques, ensuring more\ncomprehensive and objective data. With symbolic execution, we create a custom\ndataset that better captures the nuances in code evaluation. Our reward models,\nfine-tuned on this dataset, demonstrate significant improvements over the\nbaseline, CodeRL, in estimating the quality of generated code. Our\ncode-generating LLMs, trained with the help of reward model feedback, achieve\nsimilar results compared to the CodeRL benchmark."
                },
                "authors": [
                    {
                        "name": "Marina Sakharova"
                    },
                    {
                        "name": "Abhinav Anand"
                    },
                    {
                        "name": "Mira Mezini"
                    }
                ],
                "author_detail": {
                    "name": "Mira Mezini"
                },
                "author": "Mira Mezini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15208v1",
                "updated": "2025-04-21T16:26:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    26,
                    56,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T16:26:56Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    26,
                    56,
                    0,
                    111,
                    0
                ],
                "title": "Compute-Optimal LLMs Provably Generalize Better With Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-Optimal LLMs Provably Generalize Better With Scale"
                },
                "summary": "Why do larger language models generalize better? To investigate this\nquestion, we develop generalization bounds on the pretraining objective of\nlarge language models (LLMs) in the compute-optimal regime, as described by the\nChinchilla scaling laws. We introduce a novel, fully empirical Freedman-type\nmartingale concentration inequality that tightens existing bounds by accounting\nfor the variance of the loss function. This generalization bound can be\ndecomposed into three interpretable components: the number of parameters per\ntoken, the loss variance, and the quantization error at a fixed bitrate. As\ncompute-optimal language models are scaled up, the number of parameters per\ndata point remains constant; however, both the loss variance and the\nquantization error decrease, implying that larger models should have smaller\ngeneralization gaps. We examine why larger models tend to be more quantizable\nfrom an information theoretic perspective, showing that the rate at which they\ncan integrate new information grows more slowly than their capacity on the\ncompute-optimal frontier. From these findings we produce a scaling law for the\ngeneralization gap, with bounds that become predictably stronger with scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why do larger language models generalize better? To investigate this\nquestion, we develop generalization bounds on the pretraining objective of\nlarge language models (LLMs) in the compute-optimal regime, as described by the\nChinchilla scaling laws. We introduce a novel, fully empirical Freedman-type\nmartingale concentration inequality that tightens existing bounds by accounting\nfor the variance of the loss function. This generalization bound can be\ndecomposed into three interpretable components: the number of parameters per\ntoken, the loss variance, and the quantization error at a fixed bitrate. As\ncompute-optimal language models are scaled up, the number of parameters per\ndata point remains constant; however, both the loss variance and the\nquantization error decrease, implying that larger models should have smaller\ngeneralization gaps. We examine why larger models tend to be more quantizable\nfrom an information theoretic perspective, showing that the rate at which they\ncan integrate new information grows more slowly than their capacity on the\ncompute-optimal frontier. From these findings we produce a scaling law for the\ngeneralization gap, with bounds that become predictably stronger with scale."
                },
                "authors": [
                    {
                        "name": "Marc Finzi"
                    },
                    {
                        "name": "Sanyam Kapoor"
                    },
                    {
                        "name": "Diego Granziol"
                    },
                    {
                        "name": "Anming Gu"
                    },
                    {
                        "name": "Christopher De Sa"
                    },
                    {
                        "name": "J. Zico Kolter"
                    },
                    {
                        "name": "Andrew Gordon Wilson"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Gordon Wilson"
                },
                "author": "Andrew Gordon Wilson",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15205v1",
                "updated": "2025-04-21T16:20:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    20,
                    43,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T16:20:43Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    20,
                    43,
                    0,
                    111,
                    0
                ],
                "title": "Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus\n  LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus\n  LLM Judges"
                },
                "summary": "Retrieval-augmented generation (RAG) enables large language models (LLMs) to\ngenerate answers with citations from source documents containing \"ground\ntruth\", thereby reducing system hallucinations. A crucial factor in RAG\nevaluation is \"support\", whether the information in the cited documents\nsupports the answer. To this end, we conducted a large-scale comparative study\nof 45 participant submissions on 36 topics to the TREC 2024 RAG Track,\ncomparing an automatic LLM judge (GPT-4o) against human judges for support\nassessment. We considered two conditions: (1) fully manual assessments from\nscratch and (2) manual assessments with post-editing of LLM predictions. Our\nresults indicate that for 56% of the manual from-scratch assessments, human and\nGPT-4o predictions match perfectly (on a three-level scale), increasing to 72%\nin the manual with post-editing condition. Furthermore, by carefully analyzing\nthe disagreements in an unbiased study, we found that an independent human\njudge correlates better with GPT-4o than a human judge, suggesting that LLM\njudges can be a reliable alternative for support assessment. To conclude, we\nprovide a qualitative analysis of human and GPT-4o errors to help guide future\niterations of support assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enables large language models (LLMs) to\ngenerate answers with citations from source documents containing \"ground\ntruth\", thereby reducing system hallucinations. A crucial factor in RAG\nevaluation is \"support\", whether the information in the cited documents\nsupports the answer. To this end, we conducted a large-scale comparative study\nof 45 participant submissions on 36 topics to the TREC 2024 RAG Track,\ncomparing an automatic LLM judge (GPT-4o) against human judges for support\nassessment. We considered two conditions: (1) fully manual assessments from\nscratch and (2) manual assessments with post-editing of LLM predictions. Our\nresults indicate that for 56% of the manual from-scratch assessments, human and\nGPT-4o predictions match perfectly (on a three-level scale), increasing to 72%\nin the manual with post-editing condition. Furthermore, by carefully analyzing\nthe disagreements in an unbiased study, we found that an independent human\njudge correlates better with GPT-4o than a human judge, suggesting that LLM\njudges can be a reliable alternative for support assessment. To conclude, we\nprovide a qualitative analysis of human and GPT-4o errors to help guide future\niterations of support assessment."
                },
                "authors": [
                    {
                        "name": "Nandan Thakur"
                    },
                    {
                        "name": "Ronak Pradeep"
                    },
                    {
                        "name": "Shivani Upadhyay"
                    },
                    {
                        "name": "Daniel Campos"
                    },
                    {
                        "name": "Nick Craswell"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "arxiv_comment": "Accepted at SIGIR 2025 (short)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15199v1",
                "updated": "2025-04-21T16:16:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    16,
                    19,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T16:16:19Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    16,
                    19,
                    0,
                    111,
                    0
                ],
                "title": "Zero-Shot, But at What Cost? Unveiling the Hidden Overhead of MILS's\n  LLM-CLIP Framework for Image Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot, But at What Cost? Unveiling the Hidden Overhead of MILS's\n  LLM-CLIP Framework for Image Captioning"
                },
                "summary": "MILS (Multimodal Iterative LLM Solver) is a recently published framework that\nclaims \"LLMs can see and hear without any training\" by leveraging an iterative,\nLLM-CLIP based approach for zero-shot image captioning. While this MILS\napproach demonstrates good performance, our investigation reveals that this\nsuccess comes at a hidden, substantial computational cost due to its expensive\nmulti-step refinement process. In contrast, alternative models such as BLIP-2\nand GPT-4V achieve competitive results through a streamlined, single-pass\napproach. We hypothesize that the significant overhead inherent in MILS's\niterative process may undermine its practical benefits, thereby challenging the\nnarrative that zero-shot performance can be attained without incurring heavy\nresource demands. This work is the first to expose and quantify the trade-offs\nbetween output quality and computational cost in MILS, providing critical\ninsights for the design of more efficient multimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MILS (Multimodal Iterative LLM Solver) is a recently published framework that\nclaims \"LLMs can see and hear without any training\" by leveraging an iterative,\nLLM-CLIP based approach for zero-shot image captioning. While this MILS\napproach demonstrates good performance, our investigation reveals that this\nsuccess comes at a hidden, substantial computational cost due to its expensive\nmulti-step refinement process. In contrast, alternative models such as BLIP-2\nand GPT-4V achieve competitive results through a streamlined, single-pass\napproach. We hypothesize that the significant overhead inherent in MILS's\niterative process may undermine its practical benefits, thereby challenging the\nnarrative that zero-shot performance can be attained without incurring heavy\nresource demands. This work is the first to expose and quantify the trade-offs\nbetween output quality and computational cost in MILS, providing critical\ninsights for the design of more efficient multimodal models."
                },
                "authors": [
                    {
                        "name": "Yassir Benhammou"
                    },
                    {
                        "name": "Alessandro Tiberio"
                    },
                    {
                        "name": "Gabriel Trautmann"
                    },
                    {
                        "name": "Suman Kalyan"
                    }
                ],
                "author_detail": {
                    "name": "Suman Kalyan"
                },
                "author": "Suman Kalyan",
                "arxiv_comment": "9 pages, 2 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15188v2",
                "updated": "2025-04-22T04:22:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    4,
                    22,
                    9,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T15:57:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    57,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "Synergistic Weak-Strong Collaboration by Aligning Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergistic Weak-Strong Collaboration by Aligning Preferences"
                },
                "summary": "Current Large Language Models (LLMs) excel in general reasoning yet struggle\nwith specialized tasks requiring proprietary or domain-specific knowledge.\nFine-tuning large models for every niche application is often infeasible due to\nblack-box constraints and high computational overhead. To address this, we\npropose a collaborative framework that pairs a specialized weak model with a\ngeneral strong model. The weak model, tailored to specific domains, produces\ninitial drafts and background information, while the strong model leverages its\nadvanced reasoning to refine these drafts, extending LLMs' capabilities to\ncritical yet specialized tasks. To optimize this collaboration, we introduce a\ncollaborative feedback to fine-tunes the weak model, which quantifies the\ninfluence of the weak model's contributions in the collaboration procedure and\nestablishes preference pairs to guide preference tuning of the weak model. We\nvalidate our framework through experiments on three domains. We find that the\ncollaboration significantly outperforms each model alone by leveraging\ncomplementary strengths. Moreover, aligning the weak model with the\ncollaborative preference further enhances overall performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) excel in general reasoning yet struggle\nwith specialized tasks requiring proprietary or domain-specific knowledge.\nFine-tuning large models for every niche application is often infeasible due to\nblack-box constraints and high computational overhead. To address this, we\npropose a collaborative framework that pairs a specialized weak model with a\ngeneral strong model. The weak model, tailored to specific domains, produces\ninitial drafts and background information, while the strong model leverages its\nadvanced reasoning to refine these drafts, extending LLMs' capabilities to\ncritical yet specialized tasks. To optimize this collaboration, we introduce a\ncollaborative feedback to fine-tunes the weak model, which quantifies the\ninfluence of the weak model's contributions in the collaboration procedure and\nestablishes preference pairs to guide preference tuning of the weak model. We\nvalidate our framework through experiments on three domains. We find that the\ncollaboration significantly outperforms each model alone by leveraging\ncomplementary strengths. Moreover, aligning the weak model with the\ncollaborative preference further enhances overall performance."
                },
                "authors": [
                    {
                        "name": "Yizhu Jiao"
                    },
                    {
                        "name": "Xuchao Zhang"
                    },
                    {
                        "name": "Zhaoyang Wang"
                    },
                    {
                        "name": "Yubo Ma"
                    },
                    {
                        "name": "Zhun Deng"
                    },
                    {
                        "name": "Rujia Wang"
                    },
                    {
                        "name": "Chetan Bansal"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Huaxiu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Huaxiu Yao"
                },
                "author": "Huaxiu Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15178v1",
                "updated": "2025-04-21T15:40:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    40,
                    10,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T15:40:10Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    40,
                    10,
                    0,
                    111,
                    0
                ],
                "title": "Time-Series Analysis on Edge-AI Hardware for Healthcare Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Series Analysis on Edge-AI Hardware for Healthcare Monitoring"
                },
                "summary": "This project addresses the need for efficient, real-time analysis of\nbiomedical signals such as electrocardiograms (ECG) and electroencephalograms\n(EEG) for continuous health monitoring. Traditional methods rely on\nlong-duration data recording followed by offline analysis, which is\npower-intensive and delays responses to critical symptoms such as arrhythmia.\nTo overcome these limitations, a time-domain ECG analysis model based on a\nnovel dynamically-biased Long Short-Term Memory (DB-LSTM) neural network is\nproposed. This model supports simultaneous ECG forecasting and classification\nwith high performance-achieving over 98% accuracy and a normalized mean square\nerror below 1e-3 for forecasting, and over 97% accuracy with faster convergence\nand fewer training parameters for classification. To enable edge deployment,\nthe model is hardware-optimized by quantizing weights to INT4 or INT3 formats,\nresulting in only a 2% and 6% drop in classification accuracy during training\nand inference, respectively, while maintaining full accuracy for forecasting.\nExtensive simulations using multiple ECG datasets confirm the model's\nrobustness. Future work includes implementing the algorithm on FPGA and CMOS\ncircuits for practical cardiac monitoring, as well as developing a digital\nhardware platform that supports flexible neural network configurations and\non-chip online training for personalized healthcare applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This project addresses the need for efficient, real-time analysis of\nbiomedical signals such as electrocardiograms (ECG) and electroencephalograms\n(EEG) for continuous health monitoring. Traditional methods rely on\nlong-duration data recording followed by offline analysis, which is\npower-intensive and delays responses to critical symptoms such as arrhythmia.\nTo overcome these limitations, a time-domain ECG analysis model based on a\nnovel dynamically-biased Long Short-Term Memory (DB-LSTM) neural network is\nproposed. This model supports simultaneous ECG forecasting and classification\nwith high performance-achieving over 98% accuracy and a normalized mean square\nerror below 1e-3 for forecasting, and over 97% accuracy with faster convergence\nand fewer training parameters for classification. To enable edge deployment,\nthe model is hardware-optimized by quantizing weights to INT4 or INT3 formats,\nresulting in only a 2% and 6% drop in classification accuracy during training\nand inference, respectively, while maintaining full accuracy for forecasting.\nExtensive simulations using multiple ECG datasets confirm the model's\nrobustness. Future work includes implementing the algorithm on FPGA and CMOS\ncircuits for practical cardiac monitoring, as well as developing a digital\nhardware platform that supports flexible neural network configurations and\non-chip online training for personalized healthcare applications."
                },
                "authors": [
                    {
                        "name": "Jinhai Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jinhai Hu"
                },
                "author": "Jinhai Hu",
                "arxiv_comment": "38 pages, 20 figures, Progress report for qualification cum PhD\n  confirmation exercise",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11201v2",
                "updated": "2025-04-21T15:37:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    37,
                    50,
                    0,
                    111,
                    0
                ],
                "published": "2024-10-15T02:37:39Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    2,
                    37,
                    39,
                    1,
                    289,
                    0
                ],
                "title": "Tree of Attributes Prompt Learning for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Attributes Prompt Learning for Vision-Language Models"
                },
                "summary": "Prompt learning has proven effective in adapting vision language models for\ndownstream tasks. However, existing methods usually append learnable prompt\ntokens solely with the category names to obtain textual features, which fails\nto fully leverage the rich context indicated in the category name. To address\nthis issue, we propose the Tree of Attributes Prompt learning (TAP), which\nfirst instructs LLMs to generate a tree of attributes with a \"concept -\nattribute - description\" structure for each category, and then learn the\nhierarchy with vision and text prompt tokens. Unlike existing methods that\nmerely augment category names with a set of unstructured descriptions, our\napproach essentially distills structured knowledge graphs associated with class\nnames from LLMs. Furthermore, our approach introduces text and vision prompts\ndesigned to explicitly learn the corresponding visual attributes, effectively\nserving as domain experts. Additionally, the general and diverse descriptions\ngenerated based on the class names may be wrong or absent in the specific given\nimages. To address this misalignment, we further introduce a vision-conditional\npooling module to extract instance-specific text features. Extensive\nexperimental results demonstrate that our approach outperforms state-of-the-art\nmethods on the zero-shot base-to-novel generalization, cross-dataset transfer,\nas well as few-shot classification across 11 diverse datasets. Code is\navailable at https://github.com/HHenryD/TAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt learning has proven effective in adapting vision language models for\ndownstream tasks. However, existing methods usually append learnable prompt\ntokens solely with the category names to obtain textual features, which fails\nto fully leverage the rich context indicated in the category name. To address\nthis issue, we propose the Tree of Attributes Prompt learning (TAP), which\nfirst instructs LLMs to generate a tree of attributes with a \"concept -\nattribute - description\" structure for each category, and then learn the\nhierarchy with vision and text prompt tokens. Unlike existing methods that\nmerely augment category names with a set of unstructured descriptions, our\napproach essentially distills structured knowledge graphs associated with class\nnames from LLMs. Furthermore, our approach introduces text and vision prompts\ndesigned to explicitly learn the corresponding visual attributes, effectively\nserving as domain experts. Additionally, the general and diverse descriptions\ngenerated based on the class names may be wrong or absent in the specific given\nimages. To address this misalignment, we further introduce a vision-conditional\npooling module to extract instance-specific text features. Extensive\nexperimental results demonstrate that our approach outperforms state-of-the-art\nmethods on the zero-shot base-to-novel generalization, cross-dataset transfer,\nas well as few-shot classification across 11 diverse datasets. Code is\navailable at https://github.com/HHenryD/TAP."
                },
                "authors": [
                    {
                        "name": "Tong Ding"
                    },
                    {
                        "name": "Wanhua Li"
                    },
                    {
                        "name": "Zhongqi Miao"
                    },
                    {
                        "name": "Hanspeter Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Hanspeter Pfister"
                },
                "author": "Hanspeter Pfister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06560v2",
                "updated": "2025-04-21T15:37:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    37,
                    15,
                    0,
                    111,
                    0
                ],
                "published": "2024-06-02T11:54:50Z",
                "published_parsed": [
                    2024,
                    6,
                    2,
                    11,
                    54,
                    50,
                    6,
                    154,
                    0
                ],
                "title": "Inverse Constitutional AI: Compressing Preferences into Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse Constitutional AI: Compressing Preferences into Principles"
                },
                "summary": "Feedback data is widely used for fine-tuning and evaluating state-of-the-art\nAI models. Pairwise text preferences, where human or AI annotators select the\n\"better\" of two options, are particularly common. Such preferences are used to\ntrain (reward) models or to rank models with aggregate statistics. For many\napplications it is desirable to understand annotator preferences in addition to\nmodelling them - not least because extensive prior work has shown various\nunintended biases in preference datasets. Yet, preference datasets remain\nchallenging to interpret. Neither black-box reward models nor statistics can\nanswer why one text is preferred over another. Manual interpretation of the\nnumerous (long) response pairs is usually equally infeasible. In this paper, we\nintroduce the Inverse Constitutional AI (ICAI) problem, formulating the\ninterpretation of pairwise text preference data as a compression task. In\nconstitutional AI, a set of principles (a constitution) is used to provide\nfeedback and fine-tune AI models. ICAI inverts this process: given a feedback\ndataset, we aim to extract a constitution that best enables a large language\nmodel (LLM) to reconstruct the original annotations. We propose a corresponding\nICAI algorithm and validate its generated constitutions quantitatively based on\nannotation reconstruction accuracy on several datasets: (a) synthetic feedback\ndata with known principles; (b) AlpacaEval cross-annotated human feedback data;\n(c) crowdsourced Chatbot Arena data; and (d) PRISM data from diverse\ndemographic groups. As a short and interpretable representation of the original\ndataset, generated constitutions have many potential use cases: help identify\nundesirable annotator biases, understand model performance better, scale\nfeedback to unseen data, or adapt models to individual user or group\npreferences. We release the source code at https://github.com/rdnfn/icai.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feedback data is widely used for fine-tuning and evaluating state-of-the-art\nAI models. Pairwise text preferences, where human or AI annotators select the\n\"better\" of two options, are particularly common. Such preferences are used to\ntrain (reward) models or to rank models with aggregate statistics. For many\napplications it is desirable to understand annotator preferences in addition to\nmodelling them - not least because extensive prior work has shown various\nunintended biases in preference datasets. Yet, preference datasets remain\nchallenging to interpret. Neither black-box reward models nor statistics can\nanswer why one text is preferred over another. Manual interpretation of the\nnumerous (long) response pairs is usually equally infeasible. In this paper, we\nintroduce the Inverse Constitutional AI (ICAI) problem, formulating the\ninterpretation of pairwise text preference data as a compression task. In\nconstitutional AI, a set of principles (a constitution) is used to provide\nfeedback and fine-tune AI models. ICAI inverts this process: given a feedback\ndataset, we aim to extract a constitution that best enables a large language\nmodel (LLM) to reconstruct the original annotations. We propose a corresponding\nICAI algorithm and validate its generated constitutions quantitatively based on\nannotation reconstruction accuracy on several datasets: (a) synthetic feedback\ndata with known principles; (b) AlpacaEval cross-annotated human feedback data;\n(c) crowdsourced Chatbot Arena data; and (d) PRISM data from diverse\ndemographic groups. As a short and interpretable representation of the original\ndataset, generated constitutions have many potential use cases: help identify\nundesirable annotator biases, understand model performance better, scale\nfeedback to unseen data, or adapt models to individual user or group\npreferences. We release the source code at https://github.com/rdnfn/icai."
                },
                "authors": [
                    {
                        "name": "Arduin Findeis"
                    },
                    {
                        "name": "Timo Kaufmann"
                    },
                    {
                        "name": "Eyke Hüllermeier"
                    },
                    {
                        "name": "Samuel Albanie"
                    },
                    {
                        "name": "Robert Mullins"
                    }
                ],
                "author_detail": {
                    "name": "Robert Mullins"
                },
                "author": "Robert Mullins",
                "arxiv_comment": "Accepted at ICLR 2025, v2 is camera-ready version; Main changes from\n  v1: extended experiments, additional baselines",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09597v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09597v3",
                "updated": "2025-04-21T15:18:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    18,
                    42,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-13T14:31:52Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    31,
                    52,
                    6,
                    103,
                    0
                ],
                "title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge\n  Acquisition and Scaling Laws",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge\n  Acquisition and Scaling Laws"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions."
                },
                "authors": [
                    {
                        "name": "Zhixuan Pan"
                    },
                    {
                        "name": "Shaowen Wang"
                    },
                    {
                        "name": "Jian Li"
                    }
                ],
                "author_detail": {
                    "name": "Jian Li"
                },
                "author": "Jian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09597v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09597v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v2",
                "updated": "2025-04-21T15:13:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    13,
                    44,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15163v1",
                "updated": "2025-04-21T15:09:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    9,
                    40,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T15:09:40Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    9,
                    40,
                    0,
                    111,
                    0
                ],
                "title": "Survey of Loss Augmented Knowledge Tracing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey of Loss Augmented Knowledge Tracing"
                },
                "summary": "The training of artificial neural networks is heavily dependent on the\ncareful selection of an appropriate loss function. While commonly used loss\nfunctions, such as cross-entropy and mean squared error (MSE), generally\nsuffice for a broad range of tasks, challenges often emerge due to limitations\nin data quality or inefficiencies within the learning process. In such\ncircumstances, the integration of supplementary terms into the loss function\ncan serve to address these challenges, enhancing both model performance and\nrobustness. Two prominent techniques, loss regularization and contrastive\nlearning, have been identified as effective strategies for augmenting the\ncapacity of loss functions in artificial neural networks.\n  Knowledge tracing is a compelling area of research that leverages predictive\nartificial intelligence to facilitate the automation of personalized and\nefficient educational experiences for students. In this paper, we provide a\ncomprehensive review of the deep learning-based knowledge tracing (DKT)\nalgorithms trained using advanced loss functions and discuss their improvements\nover prior techniques. We discuss contrastive knowledge tracing algorithms,\nsuch as Bi-CLKT, CL4KT, SP-CLKT, CoSKT, and prediction-consistent DKT,\nproviding performance benchmarks and insights into real-world deployment\nchallenges. The survey concludes with future research directions, including\nhybrid loss strategies and context-aware modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training of artificial neural networks is heavily dependent on the\ncareful selection of an appropriate loss function. While commonly used loss\nfunctions, such as cross-entropy and mean squared error (MSE), generally\nsuffice for a broad range of tasks, challenges often emerge due to limitations\nin data quality or inefficiencies within the learning process. In such\ncircumstances, the integration of supplementary terms into the loss function\ncan serve to address these challenges, enhancing both model performance and\nrobustness. Two prominent techniques, loss regularization and contrastive\nlearning, have been identified as effective strategies for augmenting the\ncapacity of loss functions in artificial neural networks.\n  Knowledge tracing is a compelling area of research that leverages predictive\nartificial intelligence to facilitate the automation of personalized and\nefficient educational experiences for students. In this paper, we provide a\ncomprehensive review of the deep learning-based knowledge tracing (DKT)\nalgorithms trained using advanced loss functions and discuss their improvements\nover prior techniques. We discuss contrastive knowledge tracing algorithms,\nsuch as Bi-CLKT, CL4KT, SP-CLKT, CoSKT, and prediction-consistent DKT,\nproviding performance benchmarks and insights into real-world deployment\nchallenges. The survey concludes with future research directions, including\nhybrid loss strategies and context-aware modeling."
                },
                "authors": [
                    {
                        "name": "Altun Shukurlu"
                    }
                ],
                "author_detail": {
                    "name": "Altun Shukurlu"
                },
                "author": "Altun Shukurlu",
                "arxiv_comment": "14 pages, no figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15160v1",
                "updated": "2025-04-21T15:07:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    7,
                    26,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T15:07:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    7,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "The Synthetic Imputation Approach: Generating Optimal Synthetic Texts\n  For Underrepresented Categories In Supervised Classification Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Synthetic Imputation Approach: Generating Optimal Synthetic Texts\n  For Underrepresented Categories In Supervised Classification Tasks"
                },
                "summary": "Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa,\nrequire that all categories in an annotation task be sufficiently represented\nin the training data for optimal performance. However, it is often difficult to\nfind sufficient examples for all categories in a task when building a\nhigh-quality training set. In this article, I describe this problem and propose\na solution, the synthetic imputation approach. Leveraging a generative LLM\n(GPT-4o), this approach generates synthetic texts based on careful prompting\nand five original examples drawn randomly with replacement from the sample.\nThis approach ensures that new synthetic texts are sufficiently different from\nthe original texts to reduce overfitting, but retain the underlying substantive\nmeaning of the examples to maximize out-of-sample performance. With 75 original\nexamples or more, synthetic imputation's performance is on par with a full\nsample of original texts, and overfitting remains low, predictable and\ncorrectable with 50 original samples. The synthetic imputation approach\nprovides a novel role for generative LLMs in research and allows applied\nresearchers to balance their datasets for best performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa,\nrequire that all categories in an annotation task be sufficiently represented\nin the training data for optimal performance. However, it is often difficult to\nfind sufficient examples for all categories in a task when building a\nhigh-quality training set. In this article, I describe this problem and propose\na solution, the synthetic imputation approach. Leveraging a generative LLM\n(GPT-4o), this approach generates synthetic texts based on careful prompting\nand five original examples drawn randomly with replacement from the sample.\nThis approach ensures that new synthetic texts are sufficiently different from\nthe original texts to reduce overfitting, but retain the underlying substantive\nmeaning of the examples to maximize out-of-sample performance. With 75 original\nexamples or more, synthetic imputation's performance is on par with a full\nsample of original texts, and overfitting remains low, predictable and\ncorrectable with 50 original samples. The synthetic imputation approach\nprovides a novel role for generative LLMs in research and allows applied\nresearchers to balance their datasets for best performance."
                },
                "authors": [
                    {
                        "name": "Joan C. Timoneda"
                    }
                ],
                "author_detail": {
                    "name": "Joan C. Timoneda"
                },
                "author": "Joan C. Timoneda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21060v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21060v2",
                "updated": "2025-04-21T14:37:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    37,
                    40,
                    0,
                    111,
                    0
                ],
                "published": "2024-10-28T14:18:32Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    18,
                    32,
                    0,
                    302,
                    0
                ],
                "title": "CTINexus: Automatic Cyber Threat Intelligence Knowledge Graph\n  Construction Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CTINexus: Automatic Cyber Threat Intelligence Knowledge Graph\n  Construction Using Large Language Models"
                },
                "summary": "Textual descriptions in cyber threat intelligence (CTI) reports, such as\nsecurity articles and news, are rich sources of knowledge about cyber threats,\ncrucial for organizations to stay informed about the rapidly evolving threat\nlandscape. However, current CTI knowledge extraction methods lack flexibility\nand generalizability, often resulting in inaccurate and incomplete knowledge\nextraction. Syntax parsing relies on fixed rules and dictionaries, while model\nfine-tuning requires large annotated datasets, making both paradigms\nchallenging to adapt to new threats and ontologies. To bridge the gap, we\npropose CTINexus, a novel framework leveraging optimized in-context learning\n(ICL) of large language models (LLMs) for data-efficient CTI knowledge\nextraction and high-quality cybersecurity knowledge graph (CSKG) construction.\nUnlike existing methods, CTINexus requires neither extensive data nor parameter\ntuning and can adapt to various ontologies with minimal annotated examples.\nThis is achieved through: (1) a carefully designed automatic prompt\nconstruction strategy with optimal demonstration retrieval for extracting a\nwide range of cybersecurity entities and relations; (2) a hierarchical entity\nalignment technique that canonicalizes the extracted knowledge and removes\nredundancy; (3) an long-distance relation prediction technique to further\ncomplete the CSKG with missing links. Our extensive evaluations using 150\nreal-world CTI reports collected from 10 platforms demonstrate that CTINexus\nsignificantly outperforms existing methods in constructing accurate and\ncomplete CSKG, highlighting its potential to transform CTI analysis with an\nefficient and adaptable solution for the dynamic threat landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual descriptions in cyber threat intelligence (CTI) reports, such as\nsecurity articles and news, are rich sources of knowledge about cyber threats,\ncrucial for organizations to stay informed about the rapidly evolving threat\nlandscape. However, current CTI knowledge extraction methods lack flexibility\nand generalizability, often resulting in inaccurate and incomplete knowledge\nextraction. Syntax parsing relies on fixed rules and dictionaries, while model\nfine-tuning requires large annotated datasets, making both paradigms\nchallenging to adapt to new threats and ontologies. To bridge the gap, we\npropose CTINexus, a novel framework leveraging optimized in-context learning\n(ICL) of large language models (LLMs) for data-efficient CTI knowledge\nextraction and high-quality cybersecurity knowledge graph (CSKG) construction.\nUnlike existing methods, CTINexus requires neither extensive data nor parameter\ntuning and can adapt to various ontologies with minimal annotated examples.\nThis is achieved through: (1) a carefully designed automatic prompt\nconstruction strategy with optimal demonstration retrieval for extracting a\nwide range of cybersecurity entities and relations; (2) a hierarchical entity\nalignment technique that canonicalizes the extracted knowledge and removes\nredundancy; (3) an long-distance relation prediction technique to further\ncomplete the CSKG with missing links. Our extensive evaluations using 150\nreal-world CTI reports collected from 10 platforms demonstrate that CTINexus\nsignificantly outperforms existing methods in constructing accurate and\ncomplete CSKG, highlighting its potential to transform CTI analysis with an\nefficient and adaptable solution for the dynamic threat landscape."
                },
                "authors": [
                    {
                        "name": "Yutong Cheng"
                    },
                    {
                        "name": "Osama Bajaber"
                    },
                    {
                        "name": "Saimon Amanuel Tsegai"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Peng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Peng Gao"
                },
                "author": "Peng Gao",
                "arxiv_comment": "Accepted at 2025 IEEE European Symposium on Security and Privacy\n  (Euro S&P)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21060v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21060v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09428v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09428v2",
                "updated": "2025-04-21T14:37:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    37,
                    39,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-13T04:27:10Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    27,
                    10,
                    6,
                    103,
                    0
                ],
                "title": "FROG: Effective Friend Recommendation in Online Games via Modality-aware\n  User Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FROG: Effective Friend Recommendation in Online Games via Modality-aware\n  User Preferences"
                },
                "summary": "Due to the convenience of mobile devices, the online games have become an\nimportant part for user entertainments in reality, creating a demand for friend\nrecommendation in online games. However, none of existing approaches can\neffectively incorporate the multi-modal user features (e.g., images and texts)\nwith the structural information in the friendship graph, due to the following\nlimitations: (1) some of them ignore the high-order structural proximity\nbetween users, (2) some fail to learn the pairwise relevance between users at\nmodality-specific level, and (3) some cannot capture both the local and global\nuser preferences on different modalities. By addressing these issues, in this\npaper, we propose an end-to-end model FROG that better models the user\npreferences on potential friends. Comprehensive experiments on both offline\nevaluation and online deployment at Tencent have demonstrated the superiority\nof FROG over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the convenience of mobile devices, the online games have become an\nimportant part for user entertainments in reality, creating a demand for friend\nrecommendation in online games. However, none of existing approaches can\neffectively incorporate the multi-modal user features (e.g., images and texts)\nwith the structural information in the friendship graph, due to the following\nlimitations: (1) some of them ignore the high-order structural proximity\nbetween users, (2) some fail to learn the pairwise relevance between users at\nmodality-specific level, and (3) some cannot capture both the local and global\nuser preferences on different modalities. By addressing these issues, in this\npaper, we propose an end-to-end model FROG that better models the user\npreferences on potential friends. Comprehensive experiments on both offline\nevaluation and online deployment at Tencent have demonstrated the superiority\nof FROG over existing approaches."
                },
                "authors": [
                    {
                        "name": "Qiwei Wang"
                    },
                    {
                        "name": "Dandan Lin"
                    },
                    {
                        "name": "Wenqing Lin"
                    },
                    {
                        "name": "Ziming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ziming Wu"
                },
                "author": "Ziming Wu",
                "arxiv_comment": "Accepted in SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09428v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09428v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15133v1",
                "updated": "2025-04-21T14:33:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    33,
                    55,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T14:33:55Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    33,
                    55,
                    0,
                    111,
                    0
                ],
                "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language\n  Models"
                },
                "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction."
                },
                "authors": [
                    {
                        "name": "Ziwen Xu"
                    },
                    {
                        "name": "Shuxun Wang"
                    },
                    {
                        "name": "Kewei Xu"
                    },
                    {
                        "name": "Haoming Xu"
                    },
                    {
                        "name": "Mengru Wang"
                    },
                    {
                        "name": "Xinle Deng"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Guozhou Zheng"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Work in progress. Demo:\n  https://zjunlp.github.io/project/EasyEdit2/video; code:\n  https://github.com/zjunlp/EasyEdit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15129v1",
                "updated": "2025-04-21T14:25:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    25,
                    23,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T14:25:23Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    25,
                    23,
                    0,
                    111,
                    0
                ],
                "title": "A General Infrastructure and Workflow for Quadrotor Deep Reinforcement\n  Learning and Reality Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Infrastructure and Workflow for Quadrotor Deep Reinforcement\n  Learning and Reality Deployment"
                },
                "summary": "Deploying robot learning methods to a quadrotor in unstructured outdoor\nenvironments is an exciting task. Quadrotors operating in real-world\nenvironments by learning-based methods encounter several challenges: a large\namount of simulator generated data required for training, strict demands for\nreal-time processing onboard, and the sim-to-real gap caused by dynamic and\nnoisy conditions. Current works have made a great breakthrough in applying\nlearning-based methods to end-to-end control of quadrotors, but rarely mention\nthe infrastructure system training from scratch and deploying to reality, which\nmakes it difficult to reproduce methods and applications. To bridge this gap,\nwe propose a platform that enables the seamless transfer of end-to-end deep\nreinforcement learning (DRL) policies. We integrate the training environment,\nflight dynamics control, DRL algorithms, the MAVROS middleware stack, and\nhardware into a comprehensive workflow and architecture that enables\nquadrotors' policies to be trained from scratch to real-world deployment in\nseveral minutes. Our platform provides rich types of environments including\nhovering, dynamic obstacle avoidance, trajectory tracking, balloon hitting, and\nplanning in unknown environments, as a physical experiment benchmark. Through\nextensive empirical validation, we demonstrate the efficiency of proposed\nsim-to-real platform, and robust outdoor flight performance under real-world\nperturbations. Details can be found from our website\nhttps://emnavi.tech/AirGym/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying robot learning methods to a quadrotor in unstructured outdoor\nenvironments is an exciting task. Quadrotors operating in real-world\nenvironments by learning-based methods encounter several challenges: a large\namount of simulator generated data required for training, strict demands for\nreal-time processing onboard, and the sim-to-real gap caused by dynamic and\nnoisy conditions. Current works have made a great breakthrough in applying\nlearning-based methods to end-to-end control of quadrotors, but rarely mention\nthe infrastructure system training from scratch and deploying to reality, which\nmakes it difficult to reproduce methods and applications. To bridge this gap,\nwe propose a platform that enables the seamless transfer of end-to-end deep\nreinforcement learning (DRL) policies. We integrate the training environment,\nflight dynamics control, DRL algorithms, the MAVROS middleware stack, and\nhardware into a comprehensive workflow and architecture that enables\nquadrotors' policies to be trained from scratch to real-world deployment in\nseveral minutes. Our platform provides rich types of environments including\nhovering, dynamic obstacle avoidance, trajectory tracking, balloon hitting, and\nplanning in unknown environments, as a physical experiment benchmark. Through\nextensive empirical validation, we demonstrate the efficiency of proposed\nsim-to-real platform, and robust outdoor flight performance under real-world\nperturbations. Details can be found from our website\nhttps://emnavi.tech/AirGym/."
                },
                "authors": [
                    {
                        "name": "Kangyao Huang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yu Luo"
                    },
                    {
                        "name": "Jingyu Chen"
                    },
                    {
                        "name": "Jintao Chen"
                    },
                    {
                        "name": "Xiangkui Zhang"
                    },
                    {
                        "name": "Xiangyang Ji"
                    },
                    {
                        "name": "Huaping Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huaping Liu"
                },
                "author": "Huaping Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15120v1",
                "updated": "2025-04-21T14:17:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    17,
                    25,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T14:17:25Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    17,
                    25,
                    0,
                    111,
                    0
                ],
                "title": "Kuwain 1.5B: An Arabic SLM via Language Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kuwain 1.5B: An Arabic SLM via Language Injection"
                },
                "summary": "Enhancing existing models with new knowledge is a crucial aspect of AI\ndevelopment. This paper introduces a novel method for integrating a new\nlanguage into a large language model (LLM). Our approach successfully\nincorporates a previously unseen target language into an existing LLM without\ncompromising its prior knowledge. We trained a tiny model with 1.5 billion\nparameters named Kuwain by injecting the Arabic language into a small\nopen-source model mainly trained in English. Our method demonstrates\nsignificant improvements in Arabic language performance, with an average 8%\nimprovement across various benchmarks, while retaining the model's existing\nknowledge with a minimum amount of the original model's data. This offers a\ncost-effective alternative to training a comprehensive model in both English\nand Arabic. The results highlight the potential for efficient, targeted\nlanguage model expansion without extensive retraining or resource-intensive\nprocesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing existing models with new knowledge is a crucial aspect of AI\ndevelopment. This paper introduces a novel method for integrating a new\nlanguage into a large language model (LLM). Our approach successfully\nincorporates a previously unseen target language into an existing LLM without\ncompromising its prior knowledge. We trained a tiny model with 1.5 billion\nparameters named Kuwain by injecting the Arabic language into a small\nopen-source model mainly trained in English. Our method demonstrates\nsignificant improvements in Arabic language performance, with an average 8%\nimprovement across various benchmarks, while retaining the model's existing\nknowledge with a minimum amount of the original model's data. This offers a\ncost-effective alternative to training a comprehensive model in both English\nand Arabic. The results highlight the potential for efficient, targeted\nlanguage model expansion without extensive retraining or resource-intensive\nprocesses."
                },
                "authors": [
                    {
                        "name": "Khalil Hennara"
                    },
                    {
                        "name": "Sara Chrouf"
                    },
                    {
                        "name": "Mohamed Motaism Hamed"
                    },
                    {
                        "name": "Zeina Aldallal"
                    },
                    {
                        "name": "Omar Hadid"
                    },
                    {
                        "name": "Safwan AlModhayan"
                    }
                ],
                "author_detail": {
                    "name": "Safwan AlModhayan"
                },
                "author": "Safwan AlModhayan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15093v1",
                "updated": "2025-04-21T13:25:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    13,
                    25,
                    55,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T13:25:55Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    13,
                    25,
                    55,
                    0,
                    111,
                    0
                ],
                "title": "Rethinking the Potential of Multimodality in Collaborative Problem\n  Solving Diagnosis with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Potential of Multimodality in Collaborative Problem\n  Solving Diagnosis with Large Language Models"
                },
                "summary": "Detecting collaborative and problem-solving behaviours from digital traces to\ninterpret students' collaborative problem solving (CPS) competency is a\nlong-term goal in the Artificial Intelligence in Education (AIEd) field.\nAlthough multimodal data and advanced models are argued to have the potential\nto detect complex CPS behaviours, empirical evidence on their value remains\nlimited with some contrasting evidence. In this study, we investigated the\npotential of multimodal data to improve model performance in diagnosing 78\nsecondary school students' CPS subskills and indicators in authentic\neducational settings. In particular, text embeddings from verbal data and\nacoustic embeddings from audio data were used in a multimodal classification\nmodel for CPS diagnosis. Both unimodal and multimodal transformer-based models\noutperformed traditional models in detecting CPS classes. Although the\ninclusion of multimodality did not improve the performance of traditional\nunimodal models, its integration into transformer-based models demonstrated\nimproved performance for diagnosing social-cognitive CPS classes compared to\nunimodal transformer-based models. Based on the results, the paper argues that\nmultimodality and the selection of a particular modelling technique should not\nbe taken for granted to achieve the best performance in the automated detection\nof every CPS subskill and indicator. Rather, their value is limited to certain\ntypes of CPS indicators, affected by the complexity of the labels, and\ndependent on the composition of indicators in the dataset. We conclude the\npaper by discussing the required nuance when considering the value of LLMs and\nmultimodality in automated CPS diagnosis, highlighting the need for human-AI\ncomplementarity, and proposing the exploration of relevant model architectures\nand techniques to improve CPS diagnosis in authentic educational contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting collaborative and problem-solving behaviours from digital traces to\ninterpret students' collaborative problem solving (CPS) competency is a\nlong-term goal in the Artificial Intelligence in Education (AIEd) field.\nAlthough multimodal data and advanced models are argued to have the potential\nto detect complex CPS behaviours, empirical evidence on their value remains\nlimited with some contrasting evidence. In this study, we investigated the\npotential of multimodal data to improve model performance in diagnosing 78\nsecondary school students' CPS subskills and indicators in authentic\neducational settings. In particular, text embeddings from verbal data and\nacoustic embeddings from audio data were used in a multimodal classification\nmodel for CPS diagnosis. Both unimodal and multimodal transformer-based models\noutperformed traditional models in detecting CPS classes. Although the\ninclusion of multimodality did not improve the performance of traditional\nunimodal models, its integration into transformer-based models demonstrated\nimproved performance for diagnosing social-cognitive CPS classes compared to\nunimodal transformer-based models. Based on the results, the paper argues that\nmultimodality and the selection of a particular modelling technique should not\nbe taken for granted to achieve the best performance in the automated detection\nof every CPS subskill and indicator. Rather, their value is limited to certain\ntypes of CPS indicators, affected by the complexity of the labels, and\ndependent on the composition of indicators in the dataset. We conclude the\npaper by discussing the required nuance when considering the value of LLMs and\nmultimodality in automated CPS diagnosis, highlighting the need for human-AI\ncomplementarity, and proposing the exploration of relevant model architectures\nand techniques to improve CPS diagnosis in authentic educational contexts."
                },
                "authors": [
                    {
                        "name": "K. Wong"
                    },
                    {
                        "name": "B. Wu"
                    },
                    {
                        "name": "S. Bulathwela"
                    },
                    {
                        "name": "M. Cukurova"
                    }
                ],
                "author_detail": {
                    "name": "M. Cukurova"
                },
                "author": "M. Cukurova",
                "arxiv_comment": "Accepted for 26th International Conference on Artificial Intelligence\n  in Education (AIED 2025), 22 - 26 July 2025, Palermo, Italy. 17 pages, 1\n  figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15080v1",
                "updated": "2025-04-21T13:09:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    13,
                    9,
                    25,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T13:09:25Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    13,
                    9,
                    25,
                    0,
                    111,
                    0
                ],
                "title": "Empowering AI to Generate Better AI Code: Guided Generation of Deep\n  Learning Projects with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering AI to Generate Better AI Code: Guided Generation of Deep\n  Learning Projects with LLMs"
                },
                "summary": "While large language models (LLMs) have been widely applied to code\ngeneration, they struggle with generating entire deep learning projects, which\nare characterized by complex structures, longer functions, and stronger\nreliance on domain knowledge than general-purpose code. An open-domain LLM\noften lacks coherent contextual guidance and domain expertise for specific\nprojects, making it challenging to produce complete code that fully meets user\nrequirements.\n  In this paper, we propose a novel planning-guided code generation method,\nDLCodeGen, tailored for generating deep learning projects. DLCodeGen predicts a\nstructured solution plan, offering global guidance for LLMs to generate the\nproject. The generated plan is then leveraged to retrieve semantically\nanalogous code samples and subsequently abstract a code template. To\neffectively integrate these multiple retrieval-augmented techniques, a\ncomparative learning mechanism is designed to generate the final code. We\nvalidate the effectiveness of our approach on a dataset we build for deep\nlearning code generation. Experimental results demonstrate that DLCodeGen\noutperforms other baselines, achieving improvements of 9.7% in CodeBLEU and\n3.6% in human evaluation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have been widely applied to code\ngeneration, they struggle with generating entire deep learning projects, which\nare characterized by complex structures, longer functions, and stronger\nreliance on domain knowledge than general-purpose code. An open-domain LLM\noften lacks coherent contextual guidance and domain expertise for specific\nprojects, making it challenging to produce complete code that fully meets user\nrequirements.\n  In this paper, we propose a novel planning-guided code generation method,\nDLCodeGen, tailored for generating deep learning projects. DLCodeGen predicts a\nstructured solution plan, offering global guidance for LLMs to generate the\nproject. The generated plan is then leveraged to retrieve semantically\nanalogous code samples and subsequently abstract a code template. To\neffectively integrate these multiple retrieval-augmented techniques, a\ncomparative learning mechanism is designed to generate the final code. We\nvalidate the effectiveness of our approach on a dataset we build for deep\nlearning code generation. Experimental results demonstrate that DLCodeGen\noutperforms other baselines, achieving improvements of 9.7% in CodeBLEU and\n3.6% in human evaluation metrics."
                },
                "authors": [
                    {
                        "name": "Chen Xie"
                    },
                    {
                        "name": "Mingsheng Jiao"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Beijun Shen"
                    }
                ],
                "author_detail": {
                    "name": "Beijun Shen"
                },
                "author": "Beijun Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15077v1",
                "updated": "2025-04-21T13:05:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    13,
                    5,
                    26,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T13:05:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    13,
                    5,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL"
                },
                "summary": "Large Language Models (LLMs) have shown impressive capabilities in\ntransforming natural language questions about relational databases into SQL\nqueries. Despite recent improvements, small LLMs struggle to handle questions\ninvolving multiple tables and complex SQL patterns under a Zero-Shot Learning\n(ZSL) setting. Supervised Fine-Tuning (SFT) partially compensate the knowledge\ndeficits in pretrained models but falls short while dealing with queries\ninvolving multi-hop reasoning. To bridge this gap, different LLM training\nstrategies to reinforce reasoning capabilities have been proposed, ranging from\nleveraging a thinking process within ZSL, including reasoning traces in SFT, or\nadopt Reinforcement Learning (RL) strategies. However, the influence of\nreasoning on Text2SQL performance is still largely unexplored. This paper\ninvestigates to what extent LLM reasoning capabilities influence their Text2SQL\nperformance on four benchmark datasets. To this end, it considers the following\nLLM settings: (1) ZSL, including general-purpose reasoning or not; (2) SFT,\nwith and without task-specific reasoning traces; (3) RL, leveraging execution\naccuracy as primary reward function; (4) SFT+RL, i.e, a two-stage approach that\ncombines SFT and RL. The results show that general-purpose reasoning under ZSL\nproves to be ineffective in tackling complex Text2SQL cases. Small LLMs benefit\nfrom SFT with reasoning much more than larger ones, bridging the gap of their\n(weaker) model pretraining. RL is generally beneficial across all tested models\nand datasets, particularly when SQL queries involve multi-hop reasoning and\nmultiple tables. Small LLMs with SFT+RL excel on most complex datasets thanks\nto a strategic balance between generality of the reasoning process and\noptimization of the execution accuracy. Thanks to RL, the7B Qwen-Coder-2.5\nmodel performs on par with 100+ Billion ones on the Bird dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive capabilities in\ntransforming natural language questions about relational databases into SQL\nqueries. Despite recent improvements, small LLMs struggle to handle questions\ninvolving multiple tables and complex SQL patterns under a Zero-Shot Learning\n(ZSL) setting. Supervised Fine-Tuning (SFT) partially compensate the knowledge\ndeficits in pretrained models but falls short while dealing with queries\ninvolving multi-hop reasoning. To bridge this gap, different LLM training\nstrategies to reinforce reasoning capabilities have been proposed, ranging from\nleveraging a thinking process within ZSL, including reasoning traces in SFT, or\nadopt Reinforcement Learning (RL) strategies. However, the influence of\nreasoning on Text2SQL performance is still largely unexplored. This paper\ninvestigates to what extent LLM reasoning capabilities influence their Text2SQL\nperformance on four benchmark datasets. To this end, it considers the following\nLLM settings: (1) ZSL, including general-purpose reasoning or not; (2) SFT,\nwith and without task-specific reasoning traces; (3) RL, leveraging execution\naccuracy as primary reward function; (4) SFT+RL, i.e, a two-stage approach that\ncombines SFT and RL. The results show that general-purpose reasoning under ZSL\nproves to be ineffective in tackling complex Text2SQL cases. Small LLMs benefit\nfrom SFT with reasoning much more than larger ones, bridging the gap of their\n(weaker) model pretraining. RL is generally beneficial across all tested models\nand datasets, particularly when SQL queries involve multi-hop reasoning and\nmultiple tables. Small LLMs with SFT+RL excel on most complex datasets thanks\nto a strategic balance between generality of the reasoning process and\noptimization of the execution accuracy. Thanks to RL, the7B Qwen-Coder-2.5\nmodel performs on par with 100+ Billion ones on the Bird dataset."
                },
                "authors": [
                    {
                        "name": "Simone Papicchio"
                    },
                    {
                        "name": "Simone Rossi"
                    },
                    {
                        "name": "Luca Cagliero"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10664v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10664v2",
                "updated": "2025-04-21T13:04:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    13,
                    4,
                    29,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-09T08:23:31Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    8,
                    23,
                    31,
                    6,
                    68,
                    0
                ],
                "title": "Semantic Wave Functions: Exploring Meaning in Large Language Models\n  Through Quantum Formalism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Wave Functions: Exploring Meaning in Large Language Models\n  Through Quantum Formalism"
                },
                "summary": "Large Language Models (LLMs) encode semantic relationships in\nhigh-dimensional vector embeddings. This paper explores the analogy between LLM\nembedding spaces and quantum mechanics, positing that LLMs operate within a\nquantized semantic space where words and phrases behave as quantum states. To\ncapture nuanced semantic interference effects, we extend the standard\nreal-valued embedding space to the complex domain, drawing parallels to the\ndouble-slit experiment. We introduce a \"semantic wave function\" to formalize\nthis quantum-derived representation and utilize potential landscapes, such as\nthe double-well potential, to model semantic ambiguity. Furthermore, we propose\na complex-valued similarity measure that incorporates both magnitude and phase\ninformation, enabling a more sensitive comparison of semantic representations.\nWe develop a path integral formalism, based on a nonlinear Schr\\\"odinger\nequation with a gauge field and Mexican hat potential, to model the dynamic\nevolution of LLM behavior. This interdisciplinary approach offers a new\ntheoretical framework for understanding and potentially manipulating LLMs, with\nthe goal of advancing both artificial and natural language understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) encode semantic relationships in\nhigh-dimensional vector embeddings. This paper explores the analogy between LLM\nembedding spaces and quantum mechanics, positing that LLMs operate within a\nquantized semantic space where words and phrases behave as quantum states. To\ncapture nuanced semantic interference effects, we extend the standard\nreal-valued embedding space to the complex domain, drawing parallels to the\ndouble-slit experiment. We introduce a \"semantic wave function\" to formalize\nthis quantum-derived representation and utilize potential landscapes, such as\nthe double-well potential, to model semantic ambiguity. Furthermore, we propose\na complex-valued similarity measure that incorporates both magnitude and phase\ninformation, enabling a more sensitive comparison of semantic representations.\nWe develop a path integral formalism, based on a nonlinear Schr\\\"odinger\nequation with a gauge field and Mexican hat potential, to model the dynamic\nevolution of LLM behavior. This interdisciplinary approach offers a new\ntheoretical framework for understanding and potentially manipulating LLMs, with\nthe goal of advancing both artificial and natural language understanding."
                },
                "authors": [
                    {
                        "name": "Timo Aukusti Laine"
                    }
                ],
                "author_detail": {
                    "name": "Timo Aukusti Laine"
                },
                "author": "Timo Aukusti Laine",
                "arxiv_doi": "10.33140/OAJAST",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.33140/OAJAST",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.10664v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10664v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "29 pages, 4 figures. Some corrections added",
                "arxiv_journal_ref": "OA J Applied Sci Technol, 3(1), 01-22 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15068v1",
                "updated": "2025-04-21T12:55:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    55,
                    6,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T12:55:06Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    55,
                    6,
                    0,
                    111,
                    0
                ],
                "title": "The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation\n  with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have significantly enhanced the capabilities of\ninformation access systems, especially with retrieval-augmented generation\n(RAG). Nevertheless, the evaluation of RAG systems remains a barrier to\ncontinued progress, a challenge we tackle in this work by proposing an\nautomatic evaluation framework that is validated against human annotations. We\nbelieve that the nugget evaluation methodology provides a solid foundation for\nevaluating RAG systems. This approach, originally developed for the TREC\nQuestion Answering (QA) Track in 2003, evaluates systems based on atomic facts\nthat should be present in good answers. Our efforts focus on \"refactoring\" this\nmethodology, where we describe the AutoNuggetizer framework that specifically\napplies LLMs to both automatically create nuggets and automatically assign\nnuggets to system answers. In the context of the TREC 2024 RAG Track, we\ncalibrate a fully automatic approach against strategies where nuggets are\ncreated manually or semi-manually by human assessors and then assigned manually\nto system answers. Based on results from a community-wide evaluation, we\nobserve strong agreement at the run level between scores derived from fully\nautomatic nugget evaluation and human-based variants. The agreement is stronger\nwhen individual framework components such as nugget assignment are automated\nindependently. This suggests that our evaluation framework provides tradeoffs\nbetween effort and quality that can be used to guide the development of future\nRAG systems. However, further research is necessary to refine our approach,\nparticularly in establishing robust per-topic agreement to diagnose system\nfailures effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly enhanced the capabilities of\ninformation access systems, especially with retrieval-augmented generation\n(RAG). Nevertheless, the evaluation of RAG systems remains a barrier to\ncontinued progress, a challenge we tackle in this work by proposing an\nautomatic evaluation framework that is validated against human annotations. We\nbelieve that the nugget evaluation methodology provides a solid foundation for\nevaluating RAG systems. This approach, originally developed for the TREC\nQuestion Answering (QA) Track in 2003, evaluates systems based on atomic facts\nthat should be present in good answers. Our efforts focus on \"refactoring\" this\nmethodology, where we describe the AutoNuggetizer framework that specifically\napplies LLMs to both automatically create nuggets and automatically assign\nnuggets to system answers. In the context of the TREC 2024 RAG Track, we\ncalibrate a fully automatic approach against strategies where nuggets are\ncreated manually or semi-manually by human assessors and then assigned manually\nto system answers. Based on results from a community-wide evaluation, we\nobserve strong agreement at the run level between scores derived from fully\nautomatic nugget evaluation and human-based variants. The agreement is stronger\nwhen individual framework components such as nugget assignment are automated\nindependently. This suggests that our evaluation framework provides tradeoffs\nbetween effort and quality that can be used to guide the development of future\nRAG systems. However, further research is necessary to refine our approach,\nparticularly in establishing robust per-topic agreement to diagnose system\nfailures effectively."
                },
                "authors": [
                    {
                        "name": "Ronak Pradeep"
                    },
                    {
                        "name": "Nandan Thakur"
                    },
                    {
                        "name": "Shivani Upadhyay"
                    },
                    {
                        "name": "Daniel Campos"
                    },
                    {
                        "name": "Nick Craswell"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "arxiv_comment": "To appear in SIGIR 2025. Significant updates and revisions to\n  arXiv:2411.09607",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10258v3",
                "updated": "2025-04-21T12:52:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    52,
                    49,
                    0,
                    111,
                    0
                ],
                "published": "2024-03-15T12:47:39Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    12,
                    47,
                    39,
                    4,
                    75,
                    0
                ],
                "title": "Is Translation All You Need? A Study on Solving Multilingual Tasks with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Translation All You Need? A Study on Solving Multilingual Tasks with\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated multilingual capabilities, yet\nthey are mostly English-centric due to the imbalanced training corpora. While\nprior works have leveraged this bias to enhance multilingual performance\nthrough translation, they have been largely limited to natural language\nprocessing (NLP) tasks. In this work, we extend the evaluation to real-world\nuser queries and non-English-centric LLMs, offering a broader examination of\nmultilingual performance. Our key contribution lies in demonstrating that while\ntranslation into English can boost the performance of English-centric LLMs on\nNLP tasks, it is not universally optimal. For culture-related tasks that need\ndeep language understanding, prompting in the native language proves more\neffective as it better captures the nuances of culture and language. Our\nexperiments expose varied behaviors across LLMs and tasks in the multilingual\ncontext, underscoring the need for a more comprehensive approach to\nmultilingual evaluation. Therefore, we call for greater efforts in developing\nand evaluating LLMs that go beyond English-centric paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated multilingual capabilities, yet\nthey are mostly English-centric due to the imbalanced training corpora. While\nprior works have leveraged this bias to enhance multilingual performance\nthrough translation, they have been largely limited to natural language\nprocessing (NLP) tasks. In this work, we extend the evaluation to real-world\nuser queries and non-English-centric LLMs, offering a broader examination of\nmultilingual performance. Our key contribution lies in demonstrating that while\ntranslation into English can boost the performance of English-centric LLMs on\nNLP tasks, it is not universally optimal. For culture-related tasks that need\ndeep language understanding, prompting in the native language proves more\neffective as it better captures the nuances of culture and language. Our\nexperiments expose varied behaviors across LLMs and tasks in the multilingual\ncontext, underscoring the need for a more comprehensive approach to\nmultilingual evaluation. Therefore, we call for greater efforts in developing\nand evaluating LLMs that go beyond English-centric paradigms."
                },
                "authors": [
                    {
                        "name": "Chaoqun Liu"
                    },
                    {
                        "name": "Wenxuan Zhang"
                    },
                    {
                        "name": "Yiran Zhao"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15063v1",
                "updated": "2025-04-21T12:42:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    42,
                    59,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T12:42:59Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    42,
                    59,
                    0,
                    111,
                    0
                ],
                "title": "Mining Characteristics of Vulnerable Smart Contracts Across Lifecycle\n  Stages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mining Characteristics of Vulnerable Smart Contracts Across Lifecycle\n  Stages"
                },
                "summary": "Smart contracts are the cornerstone of decentralized applications and\nfinancial protocols, which extend the application of digital currency\ntransactions. The applications and financial protocols introduce significant\nsecurity challenges, resulting in substantial economic losses. Existing\nsolutions predominantly focus on code vulnerabilities within smart contracts,\naccounting for only 50% of security incidents. Therefore, a more comprehensive\nstudy of security issues related to smart contracts is imperative. The existing\nempirical research realizes the static analysis of smart contracts from the\nperspective of the lifecycle and gives the corresponding measures for each\nstage. However, they lack the characteristic analysis of vulnerabilities in\neach stage and the distinction between the vulnerabilities. In this paper, we\npresent the first empirical study on the security of smart contracts throughout\ntheir lifecycle, including deployment and execution, upgrade, and destruction\nstages. It delves into the security issues at each stage and provides at least\nseven feature descriptions. Finally, utilizing these seven features, five\nmachine-learning classification models are used to identify vulnerabilities at\ndifferent stages. The classification results reveal that vulnerable contracts\nexhibit distinct transaction features and ego network properties at various\nstages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contracts are the cornerstone of decentralized applications and\nfinancial protocols, which extend the application of digital currency\ntransactions. The applications and financial protocols introduce significant\nsecurity challenges, resulting in substantial economic losses. Existing\nsolutions predominantly focus on code vulnerabilities within smart contracts,\naccounting for only 50% of security incidents. Therefore, a more comprehensive\nstudy of security issues related to smart contracts is imperative. The existing\nempirical research realizes the static analysis of smart contracts from the\nperspective of the lifecycle and gives the corresponding measures for each\nstage. However, they lack the characteristic analysis of vulnerabilities in\neach stage and the distinction between the vulnerabilities. In this paper, we\npresent the first empirical study on the security of smart contracts throughout\ntheir lifecycle, including deployment and execution, upgrade, and destruction\nstages. It delves into the security issues at each stage and provides at least\nseven feature descriptions. Finally, utilizing these seven features, five\nmachine-learning classification models are used to identify vulnerabilities at\ndifferent stages. The classification results reveal that vulnerable contracts\nexhibit distinct transaction features and ego network properties at various\nstages."
                },
                "authors": [
                    {
                        "name": "Hongli Peng"
                    },
                    {
                        "name": "Xiaoqi Li"
                    },
                    {
                        "name": "Wenkai Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenkai Li"
                },
                "author": "Wenkai Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15052v1",
                "updated": "2025-04-21T12:21:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    21,
                    37,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T12:21:37Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    21,
                    37,
                    0,
                    111,
                    0
                ],
                "title": "Testing LLMs' Capabilities in Annotating Translations Based on an Error\n  Typology Designed for LSP Translation: First Experiments with ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing LLMs' Capabilities in Annotating Translations Based on an Error\n  Typology Designed for LSP Translation: First Experiments with ChatGPT"
                },
                "summary": "This study investigates the capabilities of large language models (LLMs),\nspecifically ChatGPT, in annotating MT outputs based on an error typology. In\ncontrast to previous work focusing mainly on general language, we explore\nChatGPT's ability to identify and categorise errors in specialised\ntranslations. By testing two different prompts and based on a customised error\ntypology, we compare ChatGPT annotations with human expert evaluations of\ntranslations produced by DeepL and ChatGPT itself. The results show that, for\ntranslations generated by DeepL, recall and precision are quite high. However,\nthe degree of accuracy in error categorisation depends on the prompt's specific\nfeatures and its level of detail, ChatGPT performing very well with a detailed\nprompt. When evaluating its own translations, ChatGPT achieves significantly\npoorer results, revealing limitations with self-assessment. These results\nhighlight both the potential and the limitations of LLMs for translation\nevaluation, particularly in specialised domains. Our experiments pave the way\nfor future research on open-source LLMs, which could produce annotations of\ncomparable or even higher quality. In the future, we also aim to test the\npractical effectiveness of this automated evaluation in the context of\ntranslation training, particularly by optimising the process of human\nevaluation by teachers and by exploring the impact of annotations by LLMs on\nstudents' post-editing and translation learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the capabilities of large language models (LLMs),\nspecifically ChatGPT, in annotating MT outputs based on an error typology. In\ncontrast to previous work focusing mainly on general language, we explore\nChatGPT's ability to identify and categorise errors in specialised\ntranslations. By testing two different prompts and based on a customised error\ntypology, we compare ChatGPT annotations with human expert evaluations of\ntranslations produced by DeepL and ChatGPT itself. The results show that, for\ntranslations generated by DeepL, recall and precision are quite high. However,\nthe degree of accuracy in error categorisation depends on the prompt's specific\nfeatures and its level of detail, ChatGPT performing very well with a detailed\nprompt. When evaluating its own translations, ChatGPT achieves significantly\npoorer results, revealing limitations with self-assessment. These results\nhighlight both the potential and the limitations of LLMs for translation\nevaluation, particularly in specialised domains. Our experiments pave the way\nfor future research on open-source LLMs, which could produce annotations of\ncomparable or even higher quality. In the future, we also aim to test the\npractical effectiveness of this automated evaluation in the context of\ntranslation training, particularly by optimising the process of human\nevaluation by teachers and by exploring the impact of annotations by LLMs on\nstudents' post-editing and translation learning."
                },
                "authors": [
                    {
                        "name": "Joachim Minder"
                    },
                    {
                        "name": "Guillaume Wisniewski"
                    },
                    {
                        "name": "Natalie Kübler"
                    }
                ],
                "author_detail": {
                    "name": "Natalie Kübler"
                },
                "author": "Natalie Kübler",
                "arxiv_comment": "Accepted for publication in the proceedings of MT Summit 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15049v1",
                "updated": "2025-04-21T12:12:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    12,
                    43,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T12:12:43Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    12,
                    43,
                    0,
                    111,
                    0
                ],
                "title": "ScanEdit: Hierarchically-Guided Functional 3D Scan Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScanEdit: Hierarchically-Guided Functional 3D Scan Editing"
                },
                "summary": "With the fast pace of 3D capture technology and resulting abundance of 3D\ndata, effective 3D scene editing becomes essential for a variety of graphics\napplications. In this work we present ScanEdit, an instruction-driven method\nfor functional editing of complex, real-world 3D scans. To model large and\ninterdependent sets of ob- jectswe propose a hierarchically-guided approach.\nGiven a 3D scan decomposed into its object instances, we first construct a\nhierarchical scene graph representation to enable effective, tractable editing.\nWe then leverage reason- ing capabilities of Large Language Models (LLMs) and\ntranslate high-level language instructions into actionable commands applied\nhierarchically to the scene graph. Fi- nally, ScanEdit integrates LLM-based\nguidance with ex- plicit physical constraints and generates realistic scenes\nwhere object arrangements obey both physics and common sense. In our extensive\nexperimental evaluation ScanEdit outperforms state of the art and demonstrates\nexcellent re- sults for a variety of real-world scenes and input instruc-\ntions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the fast pace of 3D capture technology and resulting abundance of 3D\ndata, effective 3D scene editing becomes essential for a variety of graphics\napplications. In this work we present ScanEdit, an instruction-driven method\nfor functional editing of complex, real-world 3D scans. To model large and\ninterdependent sets of ob- jectswe propose a hierarchically-guided approach.\nGiven a 3D scan decomposed into its object instances, we first construct a\nhierarchical scene graph representation to enable effective, tractable editing.\nWe then leverage reason- ing capabilities of Large Language Models (LLMs) and\ntranslate high-level language instructions into actionable commands applied\nhierarchically to the scene graph. Fi- nally, ScanEdit integrates LLM-based\nguidance with ex- plicit physical constraints and generates realistic scenes\nwhere object arrangements obey both physics and common sense. In our extensive\nexperimental evaluation ScanEdit outperforms state of the art and demonstrates\nexcellent re- sults for a variety of real-world scenes and input instruc-\ntions."
                },
                "authors": [
                    {
                        "name": "Mohamed el amine Boudjoghra"
                    },
                    {
                        "name": "Ivan Laptev"
                    },
                    {
                        "name": "Angela Dai"
                    }
                ],
                "author_detail": {
                    "name": "Angela Dai"
                },
                "author": "Angela Dai",
                "arxiv_comment": "Project webpage: https://aminebdj.github.io/scanedit/ Video:\n  https://www.youtube.com/watch?v=Dfmu2g6pVlg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10426v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10426v2",
                "updated": "2025-04-21T12:10:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    10,
                    34,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-14T17:16:08Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    16,
                    8,
                    0,
                    104,
                    0
                ],
                "title": "Spectral Mode Enhancement in Coherent-harmonic Dual-comb Spectroscopy\n  Enables Exceeding 300-fold Averaging Time Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral Mode Enhancement in Coherent-harmonic Dual-comb Spectroscopy\n  Enables Exceeding 300-fold Averaging Time Reduction"
                },
                "summary": "Dual-comb spectroscopy (DCS) is a novel Fourier-transform spectroscopy not\nrelying on mechanical scanning and capable of simultaneously achieving high\nspeed, high spectral resolution, and broad optical bandwidth. Nevertheless, it\nsuffers from low signal-to-noise ratio (SNR) per single acquisition due to the\ndynamic range limitation of photodetectors imposed by the high-peak-power\nmode-locked pulses, making coherent averaging an essential means to improve\nSNR, at the price of compromising the exceptional time resolution and placing\nmore stringent demands on mutual coherence and stability. In this study, we\ndemonstrate a novel approach to enhance SNR by exploiting the spectral mode\nenhancement mechanism in coherent-harmonic pulses. As a proof-of-concept, we\nemploy two frequency combs with mode spacing of $\\sim$12.5 MHz, operating at a\n20th harmonic repetition rate of $\\sim$250 MHz, demonstrating a $>$300-fold\nreduction in averaging time for comparable SNR in conventional DCS. This\nreduction is expected to be further enhanced through integration with\nultra-high repetition rate combs like microresonator combs. This new approach\npromises both a recovery of the inherent high-speed capability and a mitigation\nof the coherence-time requirements, thereby making it possible to significantly\nfacilitate subsequent DCS investigations and field deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-comb spectroscopy (DCS) is a novel Fourier-transform spectroscopy not\nrelying on mechanical scanning and capable of simultaneously achieving high\nspeed, high spectral resolution, and broad optical bandwidth. Nevertheless, it\nsuffers from low signal-to-noise ratio (SNR) per single acquisition due to the\ndynamic range limitation of photodetectors imposed by the high-peak-power\nmode-locked pulses, making coherent averaging an essential means to improve\nSNR, at the price of compromising the exceptional time resolution and placing\nmore stringent demands on mutual coherence and stability. In this study, we\ndemonstrate a novel approach to enhance SNR by exploiting the spectral mode\nenhancement mechanism in coherent-harmonic pulses. As a proof-of-concept, we\nemploy two frequency combs with mode spacing of $\\sim$12.5 MHz, operating at a\n20th harmonic repetition rate of $\\sim$250 MHz, demonstrating a $>$300-fold\nreduction in averaging time for comparable SNR in conventional DCS. This\nreduction is expected to be further enhanced through integration with\nultra-high repetition rate combs like microresonator combs. This new approach\npromises both a recovery of the inherent high-speed capability and a mitigation\nof the coherence-time requirements, thereby making it possible to significantly\nfacilitate subsequent DCS investigations and field deployments."
                },
                "authors": [
                    {
                        "name": "Wei Long"
                    },
                    {
                        "name": "Xinru Cao"
                    },
                    {
                        "name": "Xiangze Ma"
                    },
                    {
                        "name": "Jiaqi Zhou"
                    },
                    {
                        "name": "Wenbin He"
                    },
                    {
                        "name": "Dijun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Dijun Chen"
                },
                "author": "Dijun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10426v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15047v1",
                "updated": "2025-04-21T12:04:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    4,
                    57,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T12:04:57Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    4,
                    57,
                    0,
                    111,
                    0
                ],
                "title": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary\n  Quality-Diversity Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary\n  Quality-Diversity Search"
                },
                "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but are\nsusceptible to adversarial prompts that exploit vulnerabilities to produce\nunsafe or biased outputs. Existing red-teaming methods often face scalability\nchallenges, resource-intensive requirements, or limited diversity in attack\nstrategies. We propose RainbowPlus, a novel red-teaming framework rooted in\nevolutionary computation, enhancing adversarial prompt generation through an\nadaptive quality-diversity (QD) search that extends classical evolutionary\nalgorithms like MAP-Elites with innovations tailored for language models. By\nemploying a multi-element archive to store diverse high-quality prompts and a\ncomprehensive fitness function to evaluate multiple prompts concurrently,\nRainbowPlus overcomes the constraints of single-prompt archives and pairwise\ncomparisons in prior QD methods like Rainbow Teaming. Experiments comparing\nRainbowPlus to QD methods across six benchmark datasets and four open-source\nLLMs demonstrate superior attack success rate (ASR) and diversity\n(Diverse-Score $\\approx 0.84$), generating up to 100 times more unique prompts\n(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine\nstate-of-the-art methods on the HarmBench dataset with twelve LLMs (ten\nopen-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,\nsurpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).\nOur open-source implementation fosters further advancements in LLM safety,\noffering a scalable tool for vulnerability assessment. Code and resources are\npublicly available at https://github.com/knoveleng/rainbowplus, supporting\nreproducibility and future research in LLM red-teaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit remarkable capabilities but are\nsusceptible to adversarial prompts that exploit vulnerabilities to produce\nunsafe or biased outputs. Existing red-teaming methods often face scalability\nchallenges, resource-intensive requirements, or limited diversity in attack\nstrategies. We propose RainbowPlus, a novel red-teaming framework rooted in\nevolutionary computation, enhancing adversarial prompt generation through an\nadaptive quality-diversity (QD) search that extends classical evolutionary\nalgorithms like MAP-Elites with innovations tailored for language models. By\nemploying a multi-element archive to store diverse high-quality prompts and a\ncomprehensive fitness function to evaluate multiple prompts concurrently,\nRainbowPlus overcomes the constraints of single-prompt archives and pairwise\ncomparisons in prior QD methods like Rainbow Teaming. Experiments comparing\nRainbowPlus to QD methods across six benchmark datasets and four open-source\nLLMs demonstrate superior attack success rate (ASR) and diversity\n(Diverse-Score $\\approx 0.84$), generating up to 100 times more unique prompts\n(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine\nstate-of-the-art methods on the HarmBench dataset with twelve LLMs (ten\nopen-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,\nsurpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).\nOur open-source implementation fosters further advancements in LLM safety,\noffering a scalable tool for vulnerability assessment. Code and resources are\npublicly available at https://github.com/knoveleng/rainbowplus, supporting\nreproducibility and future research in LLM red-teaming."
                },
                "authors": [
                    {
                        "name": "Quy-Anh Dang"
                    },
                    {
                        "name": "Chris Ngo"
                    },
                    {
                        "name": "Truong-Son Hy"
                    }
                ],
                "author_detail": {
                    "name": "Truong-Son Hy"
                },
                "author": "Truong-Son Hy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04176v2",
                "updated": "2025-04-21T12:02:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    12,
                    2,
                    8,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-06T16:07:24Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    7,
                    24,
                    3,
                    37,
                    0
                ],
                "title": "MRAMG-Bench: A Comprehensive Benchmark for Advancing Multimodal\n  Retrieval-Augmented Multimodal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MRAMG-Bench: A Comprehensive Benchmark for Advancing Multimodal\n  Retrieval-Augmented Multimodal Generation"
                },
                "summary": "Recent advances in Retrieval-Augmented Generation (RAG) have significantly\nimproved response accuracy and relevance by incorporating external knowledge\ninto Large Language Models (LLMs). However, existing RAG methods primarily\nfocus on generating text-only answers, even in Multimodal Retrieval-Augmented\nGeneration (MRAG) scenarios, where multimodal elements are retrieved to assist\nin generating text answers. To address this, we introduce the Multimodal\nRetrieval-Augmented Multimodal Generation (MRAMG) task, in which we aim to\ngenerate multimodal answers that combine both text and images, fully leveraging\nthe multimodal data within a corpus. Despite growing attention to this\nchallenging task, a notable lack of a comprehensive benchmark persists for\neffectively evaluating its performance. To bridge this gap, we provide\nMRAMG-Bench, a meticulously curated, human-annotated benchmark comprising 4,346\ndocuments, 14,190 images, and 4,800 QA pairs, distributed across six distinct\ndatasets and spanning three domains: Web, Academia, and Lifestyle. The datasets\nincorporate diverse difficulty levels and complex multi-image scenarios,\nproviding a robust foundation for evaluating the MRAMG task. To facilitate\nrigorous evaluation, MRAMG-Bench incorporates a comprehensive suite of both\nstatistical and LLM-based metrics, enabling a thorough analysis of the\nperformance of generative models in the MRAMG task. Additionally, we propose an\nefficient and flexible multimodal answer generation framework that can leverage\nLLMs/MLLMs to generate multimodal responses. Our datasets and complete\nevaluation results for 11 popular generative models are available at\nhttps://github.com/MRAMG-Bench/MRAMG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Retrieval-Augmented Generation (RAG) have significantly\nimproved response accuracy and relevance by incorporating external knowledge\ninto Large Language Models (LLMs). However, existing RAG methods primarily\nfocus on generating text-only answers, even in Multimodal Retrieval-Augmented\nGeneration (MRAG) scenarios, where multimodal elements are retrieved to assist\nin generating text answers. To address this, we introduce the Multimodal\nRetrieval-Augmented Multimodal Generation (MRAMG) task, in which we aim to\ngenerate multimodal answers that combine both text and images, fully leveraging\nthe multimodal data within a corpus. Despite growing attention to this\nchallenging task, a notable lack of a comprehensive benchmark persists for\neffectively evaluating its performance. To bridge this gap, we provide\nMRAMG-Bench, a meticulously curated, human-annotated benchmark comprising 4,346\ndocuments, 14,190 images, and 4,800 QA pairs, distributed across six distinct\ndatasets and spanning three domains: Web, Academia, and Lifestyle. The datasets\nincorporate diverse difficulty levels and complex multi-image scenarios,\nproviding a robust foundation for evaluating the MRAMG task. To facilitate\nrigorous evaluation, MRAMG-Bench incorporates a comprehensive suite of both\nstatistical and LLM-based metrics, enabling a thorough analysis of the\nperformance of generative models in the MRAMG task. Additionally, we propose an\nefficient and flexible multimodal answer generation framework that can leverage\nLLMs/MLLMs to generate multimodal responses. Our datasets and complete\nevaluation results for 11 popular generative models are available at\nhttps://github.com/MRAMG-Bench/MRAMG."
                },
                "authors": [
                    {
                        "name": "Qinhan Yu"
                    },
                    {
                        "name": "Zhiyou Xiao"
                    },
                    {
                        "name": "Binghui Li"
                    },
                    {
                        "name": "Zhengren Wang"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_comment": "Published as a conference paper at SIGIR 2025; 11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15043v1",
                "updated": "2025-04-21T11:54:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    54,
                    40,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:54:40Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    54,
                    40,
                    0,
                    111,
                    0
                ],
                "title": "Energy-Efficient UAV-Mounted RIS for IoT: A Hybrid Energy Harvesting and\n  DRL Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficient UAV-Mounted RIS for IoT: A Hybrid Energy Harvesting and\n  DRL Approach"
                },
                "summary": "Many future Internet of Things (IoT) applications are expected to rely\nheavily on reconfigurable intelligent surface (RIS)-aided unmanned aerial\nvehicles (UAVs). However, the endurance of such systems is constrained by the\nlimited onboard energy, where frequent recharging or battery replacements are\nrequired. This consequently disrupts continuous operation and may be\nimpractical in disaster scenarios. To address this challenge, we explore a dual\nenergy harvesting (EH) framework that integrates time-switching (TS),\npower-splitting (PS), and element-splitting (ES) EH protocols for radio\nfrequency energy, along with solar energy as a renewable source. First, we\npresent the proposed system architecture and EH operating protocols,\nintroducing the proposed hybrid ES-TS-PS EH strategy to extend UAV-mounted RIS\nendurance. Next, we outline key application scenarios and the associated design\nchallenges. After that, a deep reinforcement learning-based framework is\nintroduced to maximize the EH efficiency by jointly optimizing UAV trajectory,\nRIS phase shifts, and EH strategies. The framework considers dual EH, hardware\nimpairments, and channel state information imperfections to reflect real-world\ndeployment conditions. The optimization problem is formulated as a Markov\ndecision process and solved using an enhanced deep deterministic policy\ngradient algorithm, incorporating clipped double Q-learning and softmax-based\nQ-value estimation for improved stability and efficiency. The results\ndemonstrate significant performance gains compared to the considered baseline\napproaches. Finally, possible challenges and open research directions are\npresented, highlighting the transformative potential of energy-efficient\nUAV-mounted RIS networks for IoT systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many future Internet of Things (IoT) applications are expected to rely\nheavily on reconfigurable intelligent surface (RIS)-aided unmanned aerial\nvehicles (UAVs). However, the endurance of such systems is constrained by the\nlimited onboard energy, where frequent recharging or battery replacements are\nrequired. This consequently disrupts continuous operation and may be\nimpractical in disaster scenarios. To address this challenge, we explore a dual\nenergy harvesting (EH) framework that integrates time-switching (TS),\npower-splitting (PS), and element-splitting (ES) EH protocols for radio\nfrequency energy, along with solar energy as a renewable source. First, we\npresent the proposed system architecture and EH operating protocols,\nintroducing the proposed hybrid ES-TS-PS EH strategy to extend UAV-mounted RIS\nendurance. Next, we outline key application scenarios and the associated design\nchallenges. After that, a deep reinforcement learning-based framework is\nintroduced to maximize the EH efficiency by jointly optimizing UAV trajectory,\nRIS phase shifts, and EH strategies. The framework considers dual EH, hardware\nimpairments, and channel state information imperfections to reflect real-world\ndeployment conditions. The optimization problem is formulated as a Markov\ndecision process and solved using an enhanced deep deterministic policy\ngradient algorithm, incorporating clipped double Q-learning and softmax-based\nQ-value estimation for improved stability and efficiency. The results\ndemonstrate significant performance gains compared to the considered baseline\napproaches. Finally, possible challenges and open research directions are\npresented, highlighting the transformative potential of energy-efficient\nUAV-mounted RIS networks for IoT systems."
                },
                "authors": [
                    {
                        "name": "Mahmoud M. Salim"
                    },
                    {
                        "name": "Khaled M. Rabie"
                    },
                    {
                        "name": "Ali H. Muqaibel"
                    }
                ],
                "author_detail": {
                    "name": "Ali H. Muqaibel"
                },
                "author": "Ali H. Muqaibel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15032v1",
                "updated": "2025-04-21T11:41:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    41,
                    22,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:41:22Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    41,
                    22,
                    0,
                    111,
                    0
                ],
                "title": "DyST-XL: Dynamic Layout Planning and Content Control for Compositional\n  Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyST-XL: Dynamic Layout Planning and Content Control for Compositional\n  Text-to-Video Generation"
                },
                "summary": "Compositional text-to-video generation, which requires synthesizing dynamic\nscenes with multiple interacting entities and precise spatial-temporal\nrelationships, remains a critical challenge for diffusion-based models.\nExisting methods struggle with layout discontinuity, entity identity drift, and\nimplausible interaction dynamics due to unconstrained cross-attention\nmechanisms and inadequate physics-aware reasoning. To address these\nlimitations, we propose DyST-XL, a \\textbf{training-free} framework that\nenhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through\nframe-aware control. DyST-XL integrates three key innovations: (1) A Dynamic\nLayout Planner that leverages large language models (LLMs) to parse input\nprompts into entity-attribute graphs and generates physics-aware keyframe\nlayouts, with intermediate frames interpolated via trajectory optimization; (2)\nA Dual-Prompt Controlled Attention Mechanism that enforces localized text-video\nalignment through frame-aware attention masking, achieving the precise control\nover individual entities; and (3) An Entity-Consistency Constraint strategy\nthat propagates first-frame feature embeddings to subsequent frames during\ndenoising, preserving object identity without manual annotation. Experiments\ndemonstrate that DyST-XL excels in compositional text-to-video generation,\nsignificantly improving performance on complex prompts and bridging a crucial\ngap in training-free video synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional text-to-video generation, which requires synthesizing dynamic\nscenes with multiple interacting entities and precise spatial-temporal\nrelationships, remains a critical challenge for diffusion-based models.\nExisting methods struggle with layout discontinuity, entity identity drift, and\nimplausible interaction dynamics due to unconstrained cross-attention\nmechanisms and inadequate physics-aware reasoning. To address these\nlimitations, we propose DyST-XL, a \\textbf{training-free} framework that\nenhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through\nframe-aware control. DyST-XL integrates three key innovations: (1) A Dynamic\nLayout Planner that leverages large language models (LLMs) to parse input\nprompts into entity-attribute graphs and generates physics-aware keyframe\nlayouts, with intermediate frames interpolated via trajectory optimization; (2)\nA Dual-Prompt Controlled Attention Mechanism that enforces localized text-video\nalignment through frame-aware attention masking, achieving the precise control\nover individual entities; and (3) An Entity-Consistency Constraint strategy\nthat propagates first-frame feature embeddings to subsequent frames during\ndenoising, preserving object identity without manual annotation. Experiments\ndemonstrate that DyST-XL excels in compositional text-to-video generation,\nsignificantly improving performance on complex prompts and bridging a crucial\ngap in training-free video synthesis."
                },
                "authors": [
                    {
                        "name": "Weijie He"
                    },
                    {
                        "name": "Mushui Liu"
                    },
                    {
                        "name": "Yunlong Yu"
                    },
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Chao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chao Wu"
                },
                "author": "Chao Wu",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13548v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13548v6",
                "updated": "2025-04-21T11:40:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    40,
                    22,
                    0,
                    111,
                    0
                ],
                "published": "2024-12-18T06:49:46Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    6,
                    49,
                    46,
                    2,
                    353,
                    0
                ],
                "title": "TelePreview: A User-Friendly Teleoperation System with Virtual Arm\n  Assistance for Enhanced Effectiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TelePreview: A User-Friendly Teleoperation System with Virtual Arm\n  Assistance for Enhanced Effectiveness"
                },
                "summary": "Teleoperation provides an effective way to collect robot data, which is\ncrucial for learning from demonstrations. In this field, teleoperation faces\nseveral key challenges: user-friendliness for new users, safety assurance, and\ntransferability across different platforms. While collecting real robot\ndexterous manipulation data by teleoperation to train robots has shown\nimpressive results on diverse tasks, due to the morphological differences\nbetween human and robot hands, it is not only hard for new users to understand\nthe action mapping but also raises potential safety concerns during operation.\nTo address these limitations, we introduce TelePreview. This teleoperation\nsystem offers real-time visual feedback on robot actions based on human user\ninputs, with a total hardware cost of less than $1,000. TelePreview allows the\nuser to see a virtual robot that represents the outcome of the user's next\nmovement. By enabling flexible switching between command visualization and\nactual execution, this system helps new users learn how to demonstrate quickly\nand safely. We demonstrate that it outperforms other teleoperation systems\nacross five tasks, emphasize its ease of use, and highlight its straightforward\ndeployment across diverse robotic platforms. We release our code and a\ndeployment document on our website\nhttps://nus-lins-lab.github.io/telepreview-web/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teleoperation provides an effective way to collect robot data, which is\ncrucial for learning from demonstrations. In this field, teleoperation faces\nseveral key challenges: user-friendliness for new users, safety assurance, and\ntransferability across different platforms. While collecting real robot\ndexterous manipulation data by teleoperation to train robots has shown\nimpressive results on diverse tasks, due to the morphological differences\nbetween human and robot hands, it is not only hard for new users to understand\nthe action mapping but also raises potential safety concerns during operation.\nTo address these limitations, we introduce TelePreview. This teleoperation\nsystem offers real-time visual feedback on robot actions based on human user\ninputs, with a total hardware cost of less than $1,000. TelePreview allows the\nuser to see a virtual robot that represents the outcome of the user's next\nmovement. By enabling flexible switching between command visualization and\nactual execution, this system helps new users learn how to demonstrate quickly\nand safely. We demonstrate that it outperforms other teleoperation systems\nacross five tasks, emphasize its ease of use, and highlight its straightforward\ndeployment across diverse robotic platforms. We release our code and a\ndeployment document on our website\nhttps://nus-lins-lab.github.io/telepreview-web/."
                },
                "authors": [
                    {
                        "name": "Jingxiang Guo"
                    },
                    {
                        "name": "Jiayu Luo"
                    },
                    {
                        "name": "Zhenyu Wei"
                    },
                    {
                        "name": "Yiwen Hou"
                    },
                    {
                        "name": "Zhixuan Xu"
                    },
                    {
                        "name": "Xiaoyi Lin"
                    },
                    {
                        "name": "Chongkai Gao"
                    },
                    {
                        "name": "Lin Shao"
                    }
                ],
                "author_detail": {
                    "name": "Lin Shao"
                },
                "author": "Lin Shao",
                "arxiv_comment": "In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13548v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13548v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15027v1",
                "updated": "2025-04-21T11:26:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    26,
                    2,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:26:02Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    26,
                    2,
                    0,
                    111,
                    0
                ],
                "title": "DistilQwen2.5: Industrial Practices of Training Distilled Open\n  Lightweight Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DistilQwen2.5: Industrial Practices of Training Distilled Open\n  Lightweight Language Models"
                },
                "summary": "Enhancing computational efficiency and reducing deployment costs for large\nlanguage models (LLMs) have become critical challenges in various\nresource-constrained scenarios. In this work, we present DistilQwen2.5, a\nfamily of distilled, lightweight LLMs derived from the public Qwen2.5 models.\nThese distilled models exhibit enhanced instruction-following capabilities\ncompared to the original models based on a series of distillation techniques\nthat incorporate knowledge from much larger LLMs. In our industrial practice,\nwe first leverage powerful proprietary LLMs with varying capacities as\nmulti-agent teachers to select, rewrite, and refine instruction-response pairs\nthat are more suitable for student LLMs to learn. After standard fine-tuning,\nwe further leverage a computationally efficient model fusion approach that\nenables student models to progressively integrate fine-grained hidden knowledge\nfrom their teachers. Experimental evaluations demonstrate that the distilled\nmodels possess significantly stronger capabilities than their original\ncheckpoints. Additionally, we present use cases to illustrate the applications\nof our framework in real-world scenarios. To facilitate practical use, we have\nreleased all the DistilQwen2.5 models to the open-source community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing computational efficiency and reducing deployment costs for large\nlanguage models (LLMs) have become critical challenges in various\nresource-constrained scenarios. In this work, we present DistilQwen2.5, a\nfamily of distilled, lightweight LLMs derived from the public Qwen2.5 models.\nThese distilled models exhibit enhanced instruction-following capabilities\ncompared to the original models based on a series of distillation techniques\nthat incorporate knowledge from much larger LLMs. In our industrial practice,\nwe first leverage powerful proprietary LLMs with varying capacities as\nmulti-agent teachers to select, rewrite, and refine instruction-response pairs\nthat are more suitable for student LLMs to learn. After standard fine-tuning,\nwe further leverage a computationally efficient model fusion approach that\nenables student models to progressively integrate fine-grained hidden knowledge\nfrom their teachers. Experimental evaluations demonstrate that the distilled\nmodels possess significantly stronger capabilities than their original\ncheckpoints. Additionally, we present use cases to illustrate the applications\nof our framework in real-world scenarios. To facilitate practical use, we have\nreleased all the DistilQwen2.5 models to the open-source community."
                },
                "authors": [
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Junbing Yan"
                    },
                    {
                        "name": "Yuanhao Yue"
                    },
                    {
                        "name": "Jun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Huang"
                },
                "author": "Jun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15026v1",
                "updated": "2025-04-21T11:18:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    18,
                    16,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:18:16Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    18,
                    16,
                    0,
                    111,
                    0
                ],
                "title": "Gaussian Shading++: Rethinking the Realistic Deployment Challenge of\n  Performance-Lossless Image Watermark for Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Shading++: Rethinking the Realistic Deployment Challenge of\n  Performance-Lossless Image Watermark for Diffusion Models"
                },
                "summary": "Ethical concerns surrounding copyright protection and inappropriate content\ngeneration pose challenges for the practical implementation of diffusion\nmodels. One effective solution involves watermarking the generated images.\nExisting methods primarily focus on ensuring that watermark embedding does not\ndegrade the model performance. However, they often overlook critical challenges\nin real-world deployment scenarios, such as the complexity of watermark key\nmanagement, user-defined generation parameters, and the difficulty of\nverification by arbitrary third parties. To address this issue, we propose\nGaussian Shading++, a diffusion model watermarking method tailored for\nreal-world deployment. We propose a double-channel design that leverages\npseudorandom error-correcting codes to encode the random seed required for\nwatermark pseudorandomization, achieving performance-lossless watermarking\nunder a fixed watermark key and overcoming key management challenges.\nAdditionally, we model the distortions introduced during generation and\ninversion as an additive white Gaussian noise channel and employ a novel soft\ndecision decoding strategy during extraction, ensuring strong robustness even\nwhen generation parameters vary. To enable third-party verification, we\nincorporate public key signatures, which provide a certain level of resistance\nagainst forgery attacks even when model inversion capabilities are fully\ndisclosed. Extensive experiments demonstrate that Gaussian Shading++ not only\nmaintains performance losslessness but also outperforms existing methods in\nterms of robustness, making it a more practical solution for real-world\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ethical concerns surrounding copyright protection and inappropriate content\ngeneration pose challenges for the practical implementation of diffusion\nmodels. One effective solution involves watermarking the generated images.\nExisting methods primarily focus on ensuring that watermark embedding does not\ndegrade the model performance. However, they often overlook critical challenges\nin real-world deployment scenarios, such as the complexity of watermark key\nmanagement, user-defined generation parameters, and the difficulty of\nverification by arbitrary third parties. To address this issue, we propose\nGaussian Shading++, a diffusion model watermarking method tailored for\nreal-world deployment. We propose a double-channel design that leverages\npseudorandom error-correcting codes to encode the random seed required for\nwatermark pseudorandomization, achieving performance-lossless watermarking\nunder a fixed watermark key and overcoming key management challenges.\nAdditionally, we model the distortions introduced during generation and\ninversion as an additive white Gaussian noise channel and employ a novel soft\ndecision decoding strategy during extraction, ensuring strong robustness even\nwhen generation parameters vary. To enable third-party verification, we\nincorporate public key signatures, which provide a certain level of resistance\nagainst forgery attacks even when model inversion capabilities are fully\ndisclosed. Extensive experiments demonstrate that Gaussian Shading++ not only\nmaintains performance losslessness but also outperforms existing methods in\nterms of robustness, making it a more practical solution for real-world\ndeployment."
                },
                "authors": [
                    {
                        "name": "Zijin Yang"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Kejiang Chen"
                    },
                    {
                        "name": "Kai Zeng"
                    },
                    {
                        "name": "Qiyi Yao"
                    },
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15022v1",
                "updated": "2025-04-21T11:11:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    11,
                    7,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:11:07Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    11,
                    7,
                    0,
                    111,
                    0
                ],
                "title": "LLMs as Data Annotators: How Close Are We to Human Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Data Annotators: How Close Are We to Human Performance"
                },
                "summary": "In NLP, fine-tuning LLMs is effective for various applications but requires\nhigh-quality annotated data. However, manual annotation of data is\nlabor-intensive, time-consuming, and costly. Therefore, LLMs are increasingly\nused to automate the process, often employing in-context learning (ICL) in\nwhich some examples related to the task are given in the prompt for better\nperformance. However, manually selecting context examples can lead to\ninefficiencies and suboptimal model performance. This paper presents\ncomprehensive experiments comparing several LLMs, considering different\nembedding models, across various datasets for the Named Entity Recognition\n(NER) task. The evaluation encompasses models with approximately $7$B and $70$B\nparameters, including both proprietary and non-proprietary models. Furthermore,\nleveraging the success of Retrieval-Augmented Generation (RAG), it also\nconsiders a method that addresses the limitations of ICL by automatically\nretrieving contextual examples, thereby enhancing performance. The results\nhighlight the importance of selecting the appropriate LLM and embedding model,\nunderstanding the trade-offs between LLM sizes and desired performance, and the\nnecessity to direct research efforts towards more challenging datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In NLP, fine-tuning LLMs is effective for various applications but requires\nhigh-quality annotated data. However, manual annotation of data is\nlabor-intensive, time-consuming, and costly. Therefore, LLMs are increasingly\nused to automate the process, often employing in-context learning (ICL) in\nwhich some examples related to the task are given in the prompt for better\nperformance. However, manually selecting context examples can lead to\ninefficiencies and suboptimal model performance. This paper presents\ncomprehensive experiments comparing several LLMs, considering different\nembedding models, across various datasets for the Named Entity Recognition\n(NER) task. The evaluation encompasses models with approximately $7$B and $70$B\nparameters, including both proprietary and non-proprietary models. Furthermore,\nleveraging the success of Retrieval-Augmented Generation (RAG), it also\nconsiders a method that addresses the limitations of ICL by automatically\nretrieving contextual examples, thereby enhancing performance. The results\nhighlight the importance of selecting the appropriate LLM and embedding model,\nunderstanding the trade-offs between LLM sizes and desired performance, and the\nnecessity to direct research efforts towards more challenging datasets."
                },
                "authors": [
                    {
                        "name": "Muhammad Uzair Ul Haq"
                    },
                    {
                        "name": "Davide Rigoni"
                    },
                    {
                        "name": "Alessandro Sperduti"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Sperduti"
                },
                "author": "Alessandro Sperduti",
                "arxiv_comment": "27 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15544v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15544v3",
                "updated": "2025-04-21T11:09:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    11,
                    0,
                    111,
                    0
                ],
                "published": "2025-01-26T14:31:03Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    14,
                    31,
                    3,
                    6,
                    26,
                    0
                ],
                "title": "Advancing Generative Artificial Intelligence and Large Language Models\n  for Demand Side Management with Internet of Electric Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Generative Artificial Intelligence and Large Language Models\n  for Demand Side Management with Internet of Electric Vehicles"
                },
                "summary": "Generative artificial intelligence, particularly through large language\nmodels (LLMs), is poised to transform energy optimization and demand side\nmanagement (DSM) within microgrids. This paper explores the integration of LLMs\ninto energy management, emphasizing their roles in automating the optimization\nof DSM strategies with Internet of electric vehicles. We investigate challenges\nand solutions associated with DSM and explore the new opportunities presented\nby leveraging LLMs. Then, we propose an innovative solution that enhances LLMs\nwith retrieval-augmented generation for automatic problem formulation, code\ngeneration, and customizing optimization. We present a case study to\ndemonstrate the effectiveness of our proposed solution in charging scheduling\nand optimization for electric vehicles, highlighting our solution's significant\nadvancements in energy efficiency and user adaptability. This work underscores\nthe potential of LLMs for energy optimization and fosters a new era of\nintelligent DSM solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligence, particularly through large language\nmodels (LLMs), is poised to transform energy optimization and demand side\nmanagement (DSM) within microgrids. This paper explores the integration of LLMs\ninto energy management, emphasizing their roles in automating the optimization\nof DSM strategies with Internet of electric vehicles. We investigate challenges\nand solutions associated with DSM and explore the new opportunities presented\nby leveraging LLMs. Then, we propose an innovative solution that enhances LLMs\nwith retrieval-augmented generation for automatic problem formulation, code\ngeneration, and customizing optimization. We present a case study to\ndemonstrate the effectiveness of our proposed solution in charging scheduling\nand optimization for electric vehicles, highlighting our solution's significant\nadvancements in energy efficiency and user adaptability. This work underscores\nthe potential of LLMs for energy optimization and fosters a new era of\nintelligent DSM solutions."
                },
                "authors": [
                    {
                        "name": "Hanwen Zhang"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen",
                "arxiv_comment": "9 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15544v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15544v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15013v1",
                "updated": "2025-04-21T10:35:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    10,
                    35,
                    48,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T10:35:48Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    10,
                    35,
                    48,
                    0,
                    111,
                    0
                ],
                "title": "Stay Hungry, Stay Foolish: On the Extended Reading Articles Generation\n  with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stay Hungry, Stay Foolish: On the Extended Reading Articles Generation\n  with LLMs"
                },
                "summary": "The process of creating educational materials is both time-consuming and\ndemanding for educators. This research explores the potential of Large Language\nModels (LLMs) to streamline this task by automating the generation of extended\nreading materials and relevant course suggestions. Using the TED-Ed Dig Deeper\nsections as an initial exploration, we investigate how supplementary articles\ncan be enriched with contextual knowledge and connected to additional learning\nresources. Our method begins by generating extended articles from video\ntranscripts, leveraging LLMs to include historical insights, cultural examples,\nand illustrative anecdotes. A recommendation system employing semantic\nsimilarity ranking identifies related courses, followed by an LLM-based\nrefinement process to enhance relevance. The final articles are tailored to\nseamlessly integrate these recommendations, ensuring they remain cohesive and\ninformative. Experimental evaluations demonstrate that our model produces\nhigh-quality content and accurate course suggestions, assessed through metrics\nsuch as Hit Rate, semantic similarity, and coherence. Our experimental analysis\nhighlight the nuanced differences between the generated and existing materials,\nunderscoring the model's capacity to offer more engaging and accessible\nlearning experiences. This study showcases how LLMs can bridge the gap between\ncore content and supplementary learning, providing students with additional\nrecommended resources while also assisting teachers in designing educational\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The process of creating educational materials is both time-consuming and\ndemanding for educators. This research explores the potential of Large Language\nModels (LLMs) to streamline this task by automating the generation of extended\nreading materials and relevant course suggestions. Using the TED-Ed Dig Deeper\nsections as an initial exploration, we investigate how supplementary articles\ncan be enriched with contextual knowledge and connected to additional learning\nresources. Our method begins by generating extended articles from video\ntranscripts, leveraging LLMs to include historical insights, cultural examples,\nand illustrative anecdotes. A recommendation system employing semantic\nsimilarity ranking identifies related courses, followed by an LLM-based\nrefinement process to enhance relevance. The final articles are tailored to\nseamlessly integrate these recommendations, ensuring they remain cohesive and\ninformative. Experimental evaluations demonstrate that our model produces\nhigh-quality content and accurate course suggestions, assessed through metrics\nsuch as Hit Rate, semantic similarity, and coherence. Our experimental analysis\nhighlight the nuanced differences between the generated and existing materials,\nunderscoring the model's capacity to offer more engaging and accessible\nlearning experiences. This study showcases how LLMs can bridge the gap between\ncore content and supplementary learning, providing students with additional\nrecommended resources while also assisting teachers in designing educational\nmaterials."
                },
                "authors": [
                    {
                        "name": "Yow-Fu Liou"
                    },
                    {
                        "name": "Yu-Chien Tang"
                    },
                    {
                        "name": "An-Zi Yen"
                    }
                ],
                "author_detail": {
                    "name": "An-Zi Yen"
                },
                "author": "An-Zi Yen",
                "arxiv_comment": "Accepted by iRAISE@AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13074v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13074v3",
                "updated": "2025-04-21T10:34:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    10,
                    34,
                    50,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-17T16:37:27Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    37,
                    27,
                    3,
                    107,
                    0
                ],
                "title": "SkyReels-V2: Infinite-length Film Generative Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyReels-V2: Infinite-length Film Generative Model"
                },
                "summary": "Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2."
                },
                "authors": [
                    {
                        "name": "Guibin Chen"
                    },
                    {
                        "name": "Dixuan Lin"
                    },
                    {
                        "name": "Jiangping Yang"
                    },
                    {
                        "name": "Chunze Lin"
                    },
                    {
                        "name": "Junchen Zhu"
                    },
                    {
                        "name": "Mingyuan Fan"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Sheng Chen"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Chengcheng Ma"
                    },
                    {
                        "name": "Weiming Xiong"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Nuo Pang"
                    },
                    {
                        "name": "Kang Kang"
                    },
                    {
                        "name": "Zhiheng Xu"
                    },
                    {
                        "name": "Yuzhe Jin"
                    },
                    {
                        "name": "Yupeng Liang"
                    },
                    {
                        "name": "Yubing Song"
                    },
                    {
                        "name": "Peng Zhao"
                    },
                    {
                        "name": "Boyuan Xu"
                    },
                    {
                        "name": "Di Qiu"
                    },
                    {
                        "name": "Debang Li"
                    },
                    {
                        "name": "Zhengcong Fei"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yahui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yahui Zhou"
                },
                "author": "Yahui Zhou",
                "arxiv_comment": "31 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13074v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13074v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08041v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08041v4",
                "updated": "2025-04-21T10:30:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    10,
                    30,
                    34,
                    0,
                    111,
                    0
                ],
                "published": "2024-12-11T02:44:14Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    2,
                    44,
                    14,
                    2,
                    346,
                    0
                ],
                "title": "Quantifying the benefits of code hints for refactoring deprecated Java\n  APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the benefits of code hints for refactoring deprecated Java\n  APIs"
                },
                "summary": "When done manually, refactoring legacy code in order to eliminate uses of\ndeprecated APIs is an error-prone and time-consuming process. In this paper, we\ninvestigate to which degree refactorings for deprecated Java APIs can be\nautomated, and quantify the benefit of Javadoc code hints for this task. To\nthis end, we build a symbolic and a neural engine for the automatic refactoring\nof deprecated APIs. The former is based on type-directed and component-based\nprogram synthesis, whereas the latter uses LLMs. We applied our engines to\nrefactor the deprecated methods in the Oracle JDK 15. Our experiments show that\ncode hints are enabling for the automation of this task: even the worst engine\ncorrectly refactors 71% of the tasks with code hints, which drops to at best\n14% on tasks without. Adding more code hints to Javadoc can hence boost the\nrefactoring of code that uses deprecated APIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When done manually, refactoring legacy code in order to eliminate uses of\ndeprecated APIs is an error-prone and time-consuming process. In this paper, we\ninvestigate to which degree refactorings for deprecated Java APIs can be\nautomated, and quantify the benefit of Javadoc code hints for this task. To\nthis end, we build a symbolic and a neural engine for the automatic refactoring\nof deprecated APIs. The former is based on type-directed and component-based\nprogram synthesis, whereas the latter uses LLMs. We applied our engines to\nrefactor the deprecated methods in the Oracle JDK 15. Our experiments show that\ncode hints are enabling for the automation of this task: even the worst engine\ncorrectly refactors 71% of the tasks with code hints, which drops to at best\n14% on tasks without. Adding more code hints to Javadoc can hence boost the\nrefactoring of code that uses deprecated APIs."
                },
                "authors": [
                    {
                        "name": "Cristina David"
                    },
                    {
                        "name": "Pascal Kesseli"
                    },
                    {
                        "name": "Daniel Kroening"
                    },
                    {
                        "name": "Hanliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hanliang Zhang"
                },
                "author": "Hanliang Zhang",
                "arxiv_doi": "10.1145/3696630.3728567",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696630.3728567",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.08041v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08041v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "33rd ACM International Conference on the Foundations of Software\n  Engineering (FSE Companion '25), June 23--28, 2025, Trondheim, Norway",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06635v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06635v4",
                "updated": "2025-04-21T09:48:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    48,
                    5,
                    0,
                    111,
                    0
                ],
                "published": "2024-09-10T16:46:18Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    46,
                    18,
                    1,
                    254,
                    0
                ],
                "title": "MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders"
                },
                "summary": "The rapid advancements in large language models (LLMs) have significantly\nenhanced natural language processing capabilities, facilitating the development\nof AudioLLMs that process and understand speech and audio inputs alongside\ntext. Existing AudioLLMs typically combine a pre-trained audio encoder with a\npre-trained LLM, which are subsequently finetuned on specific audio tasks.\nHowever, the pre-trained audio encoder has constrained capacity to capture\nfeatures for new tasks and datasets. To address this, we propose to incorporate\nmixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE\nsupplements a base encoder with a pool of relatively light weight encoders,\nselectively activated based on the audio input to enhance feature extraction\nwithout significantly increasing model size. Our empirical results demonstrate\nthat MoWE effectively improves multi-task performance, broadening the\napplicability of AudioLLMs to more diverse audio tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in large language models (LLMs) have significantly\nenhanced natural language processing capabilities, facilitating the development\nof AudioLLMs that process and understand speech and audio inputs alongside\ntext. Existing AudioLLMs typically combine a pre-trained audio encoder with a\npre-trained LLM, which are subsequently finetuned on specific audio tasks.\nHowever, the pre-trained audio encoder has constrained capacity to capture\nfeatures for new tasks and datasets. To address this, we propose to incorporate\nmixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE\nsupplements a base encoder with a pool of relatively light weight encoders,\nselectively activated based on the audio input to enhance feature extraction\nwithout significantly increasing model size. Our empirical results demonstrate\nthat MoWE effectively improves multi-task performance, broadening the\napplicability of AudioLLMs to more diverse audio tasks."
                },
                "authors": [
                    {
                        "name": "Wenyu Zhang"
                    },
                    {
                        "name": "Shuo Sun"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Xunlong Zou"
                    },
                    {
                        "name": "Zhuohan Liu"
                    },
                    {
                        "name": "Yingxu He"
                    },
                    {
                        "name": "Geyu Lin"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Ai Ti Aw"
                    }
                ],
                "author_detail": {
                    "name": "Ai Ti Aw"
                },
                "author": "Ai Ti Aw",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06635v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06635v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03312v2",
                "updated": "2025-04-21T09:34:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    34,
                    59,
                    0,
                    111,
                    0
                ],
                "published": "2024-11-05T18:54:21Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    18,
                    54,
                    21,
                    1,
                    310,
                    0
                ],
                "title": "Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters"
                },
                "summary": "Vision Language Models (VLMs) have demonstrated strong capabilities across\nvarious visual understanding and reasoning tasks, driven by incorporating image\nrepresentations into the token inputs of Large Language Models (LLMs). However,\ntheir real-world deployment is often constrained by high latency during\ninference due to the substantial compute required by the LLM to process the\nlarge number of input tokens, predominantly arising from the image. To reduce\ninference costs, one can either downsize the LLM or reduce the number of input\ntokens needed to represent the image, the latter of which has been the focus of\nmany recent efforts around token compression. However, it is unclear what the\noptimal trade-off is given a fixed inference budget. We first characterize this\noptimal trade-off between the number of visual tokens and LLM parameters by\nestablishing scaling laws that capture variations in performance with these two\nfactors. Our results reveal a surprising trend: for visual reasoning tasks, the\ninference-optimal behavior in VLMs is achieved by using the largest LLM that\nfits within the inference budget while minimizing visual token count - often to\na single token. While the token reduction literature has mainly focused on\nmaintaining base model performance by modestly reducing the token count (e.g.,\n$5-10\\times$), our results indicate that the compute-optimal inference regime\nrequires operating under even higher token compression ratios. Based on these\ninsights, we take the first steps toward designing token compression algorithms\ntailored for high-compression settings, utilizing prompt-based compression of\ntokens. Our work underscores the performance and efficiency benefits of\noperating in low visual token regimes and the importance of developing tailored\ntoken reduction algorithms for such conditions. Code is available at\nhttps://github.com/locuslab/llava-token-compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have demonstrated strong capabilities across\nvarious visual understanding and reasoning tasks, driven by incorporating image\nrepresentations into the token inputs of Large Language Models (LLMs). However,\ntheir real-world deployment is often constrained by high latency during\ninference due to the substantial compute required by the LLM to process the\nlarge number of input tokens, predominantly arising from the image. To reduce\ninference costs, one can either downsize the LLM or reduce the number of input\ntokens needed to represent the image, the latter of which has been the focus of\nmany recent efforts around token compression. However, it is unclear what the\noptimal trade-off is given a fixed inference budget. We first characterize this\noptimal trade-off between the number of visual tokens and LLM parameters by\nestablishing scaling laws that capture variations in performance with these two\nfactors. Our results reveal a surprising trend: for visual reasoning tasks, the\ninference-optimal behavior in VLMs is achieved by using the largest LLM that\nfits within the inference budget while minimizing visual token count - often to\na single token. While the token reduction literature has mainly focused on\nmaintaining base model performance by modestly reducing the token count (e.g.,\n$5-10\\times$), our results indicate that the compute-optimal inference regime\nrequires operating under even higher token compression ratios. Based on these\ninsights, we take the first steps toward designing token compression algorithms\ntailored for high-compression settings, utilizing prompt-based compression of\ntokens. Our work underscores the performance and efficiency benefits of\noperating in low visual token regimes and the importance of developing tailored\ntoken reduction algorithms for such conditions. Code is available at\nhttps://github.com/locuslab/llava-token-compression."
                },
                "authors": [
                    {
                        "name": "Kevin Y. Li"
                    },
                    {
                        "name": "Sachin Goyal"
                    },
                    {
                        "name": "Joao D. Semedo"
                    },
                    {
                        "name": "J. Zico Kolter"
                    }
                ],
                "author_detail": {
                    "name": "J. Zico Kolter"
                },
                "author": "J. Zico Kolter",
                "arxiv_comment": "Published at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14985v1",
                "updated": "2025-04-21T09:26:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    26,
                    5,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T09:26:05Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    26,
                    5,
                    0,
                    111,
                    0
                ],
                "title": "aiXamine: LLM Safety and Security Simplified",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "aiXamine: LLM Safety and Security Simplified"
                },
                "summary": "Evaluating Large Language Models (LLMs) for safety and security remains a\ncomplex task, often requiring users to navigate a fragmented landscape of ad\nhoc benchmarks, datasets, metrics, and reporting formats. To address this\nchallenge, we present aiXamine, a comprehensive black-box evaluation platform\nfor LLM safety and security. aiXamine integrates over 40 tests (i.e.,\nbenchmarks) organized into eight key services targeting specific dimensions of\nsafety and security: adversarial robustness, code security, fairness and bias,\nhallucination, model and data privacy, out-of-distribution (OOD) robustness,\nover-refusal, and safety alignment. The platform aggregates the evaluation\nresults into a single detailed report per model, providing a detailed breakdown\nof model performance, test examples, and rich visualizations. We used aiXamine\nto assess over 50 publicly available and proprietary LLMs, conducting over 2K\nexaminations. Our findings reveal notable vulnerabilities in leading models,\nincluding susceptibility to adversarial attacks in OpenAI's GPT-4o, biased\noutputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.\nAdditionally, we observe that open-source models can match or exceed\nproprietary models in specific services such as safety alignment, fairness and\nbias, and OOD robustness. Finally, we identify trade-offs between distillation\nstrategies, model size, training methods, and architectural choices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) for safety and security remains a\ncomplex task, often requiring users to navigate a fragmented landscape of ad\nhoc benchmarks, datasets, metrics, and reporting formats. To address this\nchallenge, we present aiXamine, a comprehensive black-box evaluation platform\nfor LLM safety and security. aiXamine integrates over 40 tests (i.e.,\nbenchmarks) organized into eight key services targeting specific dimensions of\nsafety and security: adversarial robustness, code security, fairness and bias,\nhallucination, model and data privacy, out-of-distribution (OOD) robustness,\nover-refusal, and safety alignment. The platform aggregates the evaluation\nresults into a single detailed report per model, providing a detailed breakdown\nof model performance, test examples, and rich visualizations. We used aiXamine\nto assess over 50 publicly available and proprietary LLMs, conducting over 2K\nexaminations. Our findings reveal notable vulnerabilities in leading models,\nincluding susceptibility to adversarial attacks in OpenAI's GPT-4o, biased\noutputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.\nAdditionally, we observe that open-source models can match or exceed\nproprietary models in specific services such as safety alignment, fairness and\nbias, and OOD robustness. Finally, we identify trade-offs between distillation\nstrategies, model size, training methods, and architectural choices."
                },
                "authors": [
                    {
                        "name": "Fatih Deniz"
                    },
                    {
                        "name": "Dorde Popovic"
                    },
                    {
                        "name": "Yazan Boshmaf"
                    },
                    {
                        "name": "Euisuh Jeong"
                    },
                    {
                        "name": "Minhaj Ahmad"
                    },
                    {
                        "name": "Sanjay Chawla"
                    },
                    {
                        "name": "Issa Khalil"
                    }
                ],
                "author_detail": {
                    "name": "Issa Khalil"
                },
                "author": "Issa Khalil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14969v1",
                "updated": "2025-04-21T08:56:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    56,
                    23,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T08:56:23Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    56,
                    23,
                    0,
                    111,
                    0
                ],
                "title": "Evaluating LLMs on Chinese Topic Constructions: A Research Proposal\n  Inspired by Tian et al. (2024)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs on Chinese Topic Constructions: A Research Proposal\n  Inspired by Tian et al. (2024)"
                },
                "summary": "This paper proposes a framework for evaluating large language models (LLMs)\non Chinese topic constructions, focusing on their sensitivity to island\nconstraints. Drawing inspiration from Tian et al. (2024), we outline an\nexperimental design for testing LLMs' grammatical knowledge of Mandarin syntax.\nWhile no experiments have been conducted yet, this proposal aims to provide a\nfoundation for future studies and invites feedback on the methodology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a framework for evaluating large language models (LLMs)\non Chinese topic constructions, focusing on their sensitivity to island\nconstraints. Drawing inspiration from Tian et al. (2024), we outline an\nexperimental design for testing LLMs' grammatical knowledge of Mandarin syntax.\nWhile no experiments have been conducted yet, this proposal aims to provide a\nfoundation for future studies and invites feedback on the methodology."
                },
                "authors": [
                    {
                        "name": "Xiaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong Yang"
                },
                "author": "Xiaodong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14966v1",
                "updated": "2025-04-21T08:48:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    48,
                    48,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T08:48:48Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    48,
                    48,
                    0,
                    111,
                    0
                ],
                "title": "SLO-Aware Scheduling for Large Language Model Inferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-Aware Scheduling for Large Language Model Inferences"
                },
                "summary": "Large language models (LLMs) have revolutionized applications such as code\ncompletion, chatbots, and online classification. To elevate user experiences,\nservice level objectives (SLOs) serve as crucial benchmarks for assessing\ninference services capabilities. In practice, an inference service processes\nmultiple types of tasks, each with its own distinct SLO. To ensure satisfactory\nuser experiences, each request's distinct SLOs should be considered in\nscheduling. However, existing designs lack this consideration, leading to\ninsufficient hardware utility and suboptimal performance.\n  This paper analyzes scenarios to process tasks with varying SLOs, and\nintroduces a simulated annealing-based scheduler to decide request priority\nsequence based on a request's SLO, input lengths, and possible output lengths.\nAs the first specialized scheduler for multi-SLO scenarios, this work improves\nSLO attainment by up to 5x and reduces average latency by 31.6% on\nPython-Code-23k-ShareGPT and ShareGPT_Vicuna_unfiltered datasets, compared to\ncurrent state-of-the-art framework vLLM and a new framework LMDeploy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized applications such as code\ncompletion, chatbots, and online classification. To elevate user experiences,\nservice level objectives (SLOs) serve as crucial benchmarks for assessing\ninference services capabilities. In practice, an inference service processes\nmultiple types of tasks, each with its own distinct SLO. To ensure satisfactory\nuser experiences, each request's distinct SLOs should be considered in\nscheduling. However, existing designs lack this consideration, leading to\ninsufficient hardware utility and suboptimal performance.\n  This paper analyzes scenarios to process tasks with varying SLOs, and\nintroduces a simulated annealing-based scheduler to decide request priority\nsequence based on a request's SLO, input lengths, and possible output lengths.\nAs the first specialized scheduler for multi-SLO scenarios, this work improves\nSLO attainment by up to 5x and reduces average latency by 31.6% on\nPython-Code-23k-ShareGPT and ShareGPT_Vicuna_unfiltered datasets, compared to\ncurrent state-of-the-art framework vLLM and a new framework LMDeploy."
                },
                "authors": [
                    {
                        "name": "Jinqi Huang"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Xuebing Yu"
                    },
                    {
                        "name": "Wenjie Huang"
                    },
                    {
                        "name": "Entong Li"
                    },
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Xin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xin Chen"
                },
                "author": "Xin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14964v1",
                "updated": "2025-04-21T08:45:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    45,
                    23,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T08:45:23Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    45,
                    23,
                    0,
                    111,
                    0
                ],
                "title": "Evaluating Code Generation of LLMs in Advanced Computer Science Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Code Generation of LLMs in Advanced Computer Science Problems"
                },
                "summary": "Large Language Models (LLMs), such as GitHub Copilot and ChatGPT have become\npopular among programming students. Students use LLMs to assist them in\nprogramming courses, including generating source code. Previous work has\nevaluated the ability of LLMs in solving introductory-course programming\nassignments. The results have shown that LLMs are highly effective in\ngenerating code for introductory Computer Science (CS) courses. However, there\nis a gap in research on evaluating LLMs' ability to generate code that solves\nadvanced programming assignments. In this work, we evaluate the ability of four\nLLM tools to solve programming assignments from advanced CS courses in three\npopular programming languages, Java, Python, and C. We manually select 12\nproblems, three problems from introductory courses as the baseline and nine\nprogramming assignments from second- and third-year CS courses. To evaluate the\nLLM-generated code, we generate a test suite of 1000 test cases per problem and\nanalyze the program output. Our evaluation shows that although LLMs are highly\neffective in generating source code for introductory programming courses,\nsolving advanced programming assignments is more challenging. Nonetheless, in\nmany cases, LLMs identify the base problem and provide partial solutions that\nmay be useful to CS students. Furthermore, our results may provide useful\nguidance for teachers of advanced programming courses on how to design\nprogramming assignments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GitHub Copilot and ChatGPT have become\npopular among programming students. Students use LLMs to assist them in\nprogramming courses, including generating source code. Previous work has\nevaluated the ability of LLMs in solving introductory-course programming\nassignments. The results have shown that LLMs are highly effective in\ngenerating code for introductory Computer Science (CS) courses. However, there\nis a gap in research on evaluating LLMs' ability to generate code that solves\nadvanced programming assignments. In this work, we evaluate the ability of four\nLLM tools to solve programming assignments from advanced CS courses in three\npopular programming languages, Java, Python, and C. We manually select 12\nproblems, three problems from introductory courses as the baseline and nine\nprogramming assignments from second- and third-year CS courses. To evaluate the\nLLM-generated code, we generate a test suite of 1000 test cases per problem and\nanalyze the program output. Our evaluation shows that although LLMs are highly\neffective in generating source code for introductory programming courses,\nsolving advanced programming assignments is more challenging. Nonetheless, in\nmany cases, LLMs identify the base problem and provide partial solutions that\nmay be useful to CS students. Furthermore, our results may provide useful\nguidance for teachers of advanced programming courses on how to design\nprogramming assignments."
                },
                "authors": [
                    {
                        "name": "Emir Catir"
                    },
                    {
                        "name": "Robin Claesson"
                    },
                    {
                        "name": "Rodothea Myrsini Tsoupidi"
                    }
                ],
                "author_detail": {
                    "name": "Rodothea Myrsini Tsoupidi"
                },
                "author": "Rodothea Myrsini Tsoupidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06193v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06193v3",
                "updated": "2025-04-21T08:41:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    41,
                    21,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-10T06:49:29Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    6,
                    49,
                    29,
                    0,
                    41,
                    0
                ],
                "title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge\n  in Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge\n  in Software Engineering"
                },
                "summary": "Recently, large language models (LLMs) have been deployed to tackle various\nsoftware engineering (SE) tasks like code generation, significantly advancing\nthe automation of SE tasks. However, assessing the quality of these\nLLM-generated code and text remains challenging. The commonly used Pass@k\nmetric necessitates extensive unit tests and configured environments, demands a\nhigh labor cost, and is not suitable for evaluating LLM-generated text.\nConventional metrics like BLEU, which measure only lexical rather than semantic\nsimilarity, have also come under scrutiny. In response, a new trend has emerged\nto employ LLMs for automated evaluation, known as LLM-as-a-judge. These\nLLM-as-a-judge methods are claimed to better mimic human assessment than\nconventional metrics without relying on high-quality reference answers.\nNevertheless, their exact human alignment in SE tasks remains unexplored.\n  In this paper, we empirically explore LLM-as-a-judge methods for evaluating\nSE tasks, focusing on their alignment with human judgments. We select seven\nLLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs\nspecifically fine-tuned for evaluation. After generating and manually scoring\nLLM responses on three recent SE datasets of code translation, code generation,\nand code summarization, we then prompt these methods to evaluate each response.\nFinally, we compare the scores generated by these methods with human\nevaluation. The results indicate that output-based methods reach the highest\nPearson correlation of 81.32 and 68.51 with human scores in code translation\nand generation, achieving near-human evaluation, noticeably outperforming\nChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such\noutput-based methods prompt LLMs to output judgments directly, and exhibit more\nbalanced score distributions that resemble human score patterns. Finally, we\nprovide...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been deployed to tackle various\nsoftware engineering (SE) tasks like code generation, significantly advancing\nthe automation of SE tasks. However, assessing the quality of these\nLLM-generated code and text remains challenging. The commonly used Pass@k\nmetric necessitates extensive unit tests and configured environments, demands a\nhigh labor cost, and is not suitable for evaluating LLM-generated text.\nConventional metrics like BLEU, which measure only lexical rather than semantic\nsimilarity, have also come under scrutiny. In response, a new trend has emerged\nto employ LLMs for automated evaluation, known as LLM-as-a-judge. These\nLLM-as-a-judge methods are claimed to better mimic human assessment than\nconventional metrics without relying on high-quality reference answers.\nNevertheless, their exact human alignment in SE tasks remains unexplored.\n  In this paper, we empirically explore LLM-as-a-judge methods for evaluating\nSE tasks, focusing on their alignment with human judgments. We select seven\nLLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs\nspecifically fine-tuned for evaluation. After generating and manually scoring\nLLM responses on three recent SE datasets of code translation, code generation,\nand code summarization, we then prompt these methods to evaluate each response.\nFinally, we compare the scores generated by these methods with human\nevaluation. The results indicate that output-based methods reach the highest\nPearson correlation of 81.32 and 68.51 with human scores in code translation\nand generation, achieving near-human evaluation, noticeably outperforming\nChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such\noutput-based methods prompt LLMs to output judgments directly, and exhibit more\nbalanced score distributions that resemble human score patterns. Finally, we\nprovide..."
                },
                "authors": [
                    {
                        "name": "Ruiqi Wang"
                    },
                    {
                        "name": "Jiyu Guo"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Guodong Fan"
                    },
                    {
                        "name": "Chun Yong Chong"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "arxiv_doi": "10.1145/3728963",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3728963",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.06193v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06193v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ISSTA 2025:\n  https://conf.researchr.org/details/issta-2025/issta-2025-papers/85/Can-LLMs-replace-Human-Evaluators-An-Empirical-Study-of-LLM-as-a-Judge-in-Software-E",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14955v1",
                "updated": "2025-04-21T08:27:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    27,
                    26,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T08:27:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    27,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Document Retrieval with G-Retriever",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Document Retrieval with G-Retriever"
                },
                "summary": "Textual data question answering has gained significant attention due to its\ngrowing applicability. Recently, a novel approach leveraging the\nRetrieval-Augmented Generation (RAG) method was introduced, utilizing the\nPrize-Collecting Steiner Tree (PCST) optimization for sub-graph construction.\nHowever, this method focused solely on node attributes, leading to incomplete\ncontextual understanding. In this paper, we propose an enhanced approach that\nreplaces the PCST method with an attention-based sub-graph construction\ntechnique, enabling more efficient and context-aware retrieval. Additionally,\nwe encode both node and edge attributes, leading to richer graph\nrepresentations. Our method also incorporates an improved projection layer and\nmulti-head attention pooling for better alignment with Large Language Models\n(LLMs). Experimental evaluations on the WebQSP dataset demonstrate that our\napproach is competitive and achieves marginally better results compared to the\noriginal method, underscoring its potential for more accurate question\nanswering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual data question answering has gained significant attention due to its\ngrowing applicability. Recently, a novel approach leveraging the\nRetrieval-Augmented Generation (RAG) method was introduced, utilizing the\nPrize-Collecting Steiner Tree (PCST) optimization for sub-graph construction.\nHowever, this method focused solely on node attributes, leading to incomplete\ncontextual understanding. In this paper, we propose an enhanced approach that\nreplaces the PCST method with an attention-based sub-graph construction\ntechnique, enabling more efficient and context-aware retrieval. Additionally,\nwe encode both node and edge attributes, leading to richer graph\nrepresentations. Our method also incorporates an improved projection layer and\nmulti-head attention pooling for better alignment with Large Language Models\n(LLMs). Experimental evaluations on the WebQSP dataset demonstrate that our\napproach is competitive and achieves marginally better results compared to the\noriginal method, underscoring its potential for more accurate question\nanswering."
                },
                "authors": [
                    {
                        "name": "Manthankumar Solanki"
                    }
                ],
                "author_detail": {
                    "name": "Manthankumar Solanki"
                },
                "author": "Manthankumar Solanki",
                "arxiv_comment": "Extended version of a paper presented at NeurIPS 2024\n  (arXiv:2402.07630)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06237v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06237v3",
                "updated": "2025-04-21T08:14:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    14,
                    52,
                    0,
                    111,
                    0
                ],
                "published": "2024-05-10T04:10:50Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    4,
                    10,
                    50,
                    4,
                    131,
                    0
                ],
                "title": "Risks of Practicing Large Language Models in Smart Grid: Threat Modeling\n  and Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risks of Practicing Large Language Models in Smart Grid: Threat Modeling\n  and Validation"
                },
                "summary": "Large language models (LLMs) represent significant breakthroughs in\nartificial intelligence and hold potential for applications within smart grids.\nHowever, as demonstrated in previous literature, AI technologies are\nsusceptible to various types of attacks. It is crucial to investigate and\nevaluate the risks associated with LLMs before deploying them in critical\ninfrastructure like smart grids. In this paper, we systematically evaluated the\nrisks of LLMs and identified two major types of attacks relevant to potential\nsmart grid LLM applications, presenting the corresponding threat models. We\nvalidated these attacks using popular LLMs and real smart grid data. Our\nvalidation demonstrates that attackers are capable of injecting bad data and\nretrieving domain knowledge from LLMs employed in different smart grid\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent significant breakthroughs in\nartificial intelligence and hold potential for applications within smart grids.\nHowever, as demonstrated in previous literature, AI technologies are\nsusceptible to various types of attacks. It is crucial to investigate and\nevaluate the risks associated with LLMs before deploying them in critical\ninfrastructure like smart grids. In this paper, we systematically evaluated the\nrisks of LLMs and identified two major types of attacks relevant to potential\nsmart grid LLM applications, presenting the corresponding threat models. We\nvalidated these attacks using popular LLMs and real smart grid data. Our\nvalidation demonstrates that attackers are capable of injecting bad data and\nretrieving domain knowledge from LLMs employed in different smart grid\napplications."
                },
                "authors": [
                    {
                        "name": "Jiangnan Li"
                    },
                    {
                        "name": "Yingyuan Yang"
                    },
                    {
                        "name": "Jinyuan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jinyuan Sun"
                },
                "author": "Jinyuan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06237v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06237v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14941v2",
                "updated": "2025-04-22T07:43:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    7,
                    43,
                    1,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T08:02:25Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    8,
                    2,
                    25,
                    0,
                    111,
                    0
                ],
                "title": "WindVE: Collaborative CPU-NPU Vector Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WindVE: Collaborative CPU-NPU Vector Embedding"
                },
                "summary": "Retrieval-Augmented Generation is a technology that enhances large language\nmodels by integrating information retrieval. In the industry, inference\nservices based on LLMs are highly sensitive to cost-performance ratio,\nprompting the need for improving hardware resource utilization in the inference\nservice. Specifically, vector embedding and retrieval processes take up to 20%\nof the total latency. Therefore, optimizing the utilization of computational\nresources in vector embeddings is crucial for enhancing the cost-performance\nratio of inference processes, which in turn boosts their product\ncompetitiveness.In this paper, we analyze the deployment costs of vector\nembedding technology in inference services, propose a theoretical formula, and\ndetermine through the mathematical expression that increasing the capacity to\nprocess concurrent queries is the key to reducing the deployment costs of\nvector embeddings. Therefore, in this paper, we focus on improving the\nproduct's capability to process concurrent queries. To optimize concurrency\nwithout sacrificing performance, we have designed a queue manager that adeptly\noffloads CPU peak queries. This manager utilizes a linear regression model to\nascertain the optimal queue depths, a critical parameter that significantly\ninfluences the efficacy of the system. We further develop a system named WindVE\nthat uses a CPU-NPU heterogeneous architecture to offload peak concurrent\nqueries, which leverages the performance differences between the two processors\nto effectively manage traffic surges. Through experiments, we compare WindVE to\nthe state-of-the-art vector embedding framework FlagEmbedding, and achieve a\nconcurrency level up to 22.3% higher than the scheme without offloading.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation is a technology that enhances large language\nmodels by integrating information retrieval. In the industry, inference\nservices based on LLMs are highly sensitive to cost-performance ratio,\nprompting the need for improving hardware resource utilization in the inference\nservice. Specifically, vector embedding and retrieval processes take up to 20%\nof the total latency. Therefore, optimizing the utilization of computational\nresources in vector embeddings is crucial for enhancing the cost-performance\nratio of inference processes, which in turn boosts their product\ncompetitiveness.In this paper, we analyze the deployment costs of vector\nembedding technology in inference services, propose a theoretical formula, and\ndetermine through the mathematical expression that increasing the capacity to\nprocess concurrent queries is the key to reducing the deployment costs of\nvector embeddings. Therefore, in this paper, we focus on improving the\nproduct's capability to process concurrent queries. To optimize concurrency\nwithout sacrificing performance, we have designed a queue manager that adeptly\noffloads CPU peak queries. This manager utilizes a linear regression model to\nascertain the optimal queue depths, a critical parameter that significantly\ninfluences the efficacy of the system. We further develop a system named WindVE\nthat uses a CPU-NPU heterogeneous architecture to offload peak concurrent\nqueries, which leverages the performance differences between the two processors\nto effectively manage traffic surges. Through experiments, we compare WindVE to\nthe state-of-the-art vector embedding framework FlagEmbedding, and achieve a\nconcurrency level up to 22.3% higher than the scheme without offloading."
                },
                "authors": [
                    {
                        "name": "Jinqi Huang"
                    },
                    {
                        "name": "Xuebing Yu"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Wenjie Huang"
                    },
                    {
                        "name": "Entong Li"
                    },
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Xin chen"
                    }
                ],
                "author_detail": {
                    "name": "Xin chen"
                },
                "author": "Xin chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14928v1",
                "updated": "2025-04-21T07:48:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    48,
                    20,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T07:48:20Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    48,
                    20,
                    0,
                    111,
                    0
                ],
                "title": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent\n  Dialogue Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent\n  Dialogue Framework"
                },
                "summary": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yao Shi"
                    },
                    {
                        "name": "Rongkeng Liang"
                    },
                    {
                        "name": "Yong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Xu"
                },
                "author": "Yong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14919v1",
                "updated": "2025-04-21T07:38:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    38,
                    25,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T07:38:25Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    38,
                    25,
                    0,
                    111,
                    0
                ],
                "title": "GenCLIP: Generalizing CLIP Prompts for Zero-shot Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenCLIP: Generalizing CLIP Prompts for Zero-shot Anomaly Detection"
                },
                "summary": "Zero-shot anomaly detection (ZSAD) aims to identify anomalies in unseen\ncategories by leveraging CLIP's zero-shot capabilities to match text prompts\nwith visual features. A key challenge in ZSAD is learning general prompts\nstably and utilizing them effectively, while maintaining both generalizability\nand category specificity. Although general prompts have been explored in prior\nworks, achieving their stable optimization and effective deployment remains a\nsignificant challenge. In this work, we propose GenCLIP, a novel framework that\nlearns and leverages general prompts more effectively through multi-layer\nprompting and dual-branch inference. Multi-layer prompting integrates\ncategory-specific visual cues from different CLIP layers, enriching general\nprompts with more comprehensive and robust feature representations. By\ncombining general prompts with multi-layer visual features, our method further\nenhances its generalization capability. To balance specificity and\ngeneralization, we introduce a dual-branch inference strategy, where a\nvision-enhanced branch captures fine-grained category-specific features, while\na query-only branch prioritizes generalization. The complementary outputs from\nboth branches improve the stability and reliability of anomaly detection across\nunseen categories. Additionally, we propose an adaptive text prompt filtering\nmechanism, which removes irrelevant or atypical class names not encountered\nduring CLIP's training, ensuring that only meaningful textual inputs contribute\nto the final vision-language alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot anomaly detection (ZSAD) aims to identify anomalies in unseen\ncategories by leveraging CLIP's zero-shot capabilities to match text prompts\nwith visual features. A key challenge in ZSAD is learning general prompts\nstably and utilizing them effectively, while maintaining both generalizability\nand category specificity. Although general prompts have been explored in prior\nworks, achieving their stable optimization and effective deployment remains a\nsignificant challenge. In this work, we propose GenCLIP, a novel framework that\nlearns and leverages general prompts more effectively through multi-layer\nprompting and dual-branch inference. Multi-layer prompting integrates\ncategory-specific visual cues from different CLIP layers, enriching general\nprompts with more comprehensive and robust feature representations. By\ncombining general prompts with multi-layer visual features, our method further\nenhances its generalization capability. To balance specificity and\ngeneralization, we introduce a dual-branch inference strategy, where a\nvision-enhanced branch captures fine-grained category-specific features, while\na query-only branch prioritizes generalization. The complementary outputs from\nboth branches improve the stability and reliability of anomaly detection across\nunseen categories. Additionally, we propose an adaptive text prompt filtering\nmechanism, which removes irrelevant or atypical class names not encountered\nduring CLIP's training, ensuring that only meaningful textual inputs contribute\nto the final vision-language alignment."
                },
                "authors": [
                    {
                        "name": "Donghyeong Kim"
                    },
                    {
                        "name": "Chaewon Park"
                    },
                    {
                        "name": "Suhwan Cho"
                    },
                    {
                        "name": "Hyeonjeong Lim"
                    },
                    {
                        "name": "Minseok Kang"
                    },
                    {
                        "name": "Jungho Lee"
                    },
                    {
                        "name": "Sangyoun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sangyoun Lee"
                },
                "author": "Sangyoun Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14917v1",
                "updated": "2025-04-21T07:35:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    35,
                    24,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T07:35:24Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    35,
                    24,
                    0,
                    111,
                    0
                ],
                "title": "POLYRAG: Integrating Polyviews into Retrieval-Augmented Generation for\n  Medical Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POLYRAG: Integrating Polyviews into Retrieval-Augmented Generation for\n  Medical Applications"
                },
                "summary": "Large language models (LLMs) have become a disruptive force in the industry,\nintroducing unprecedented capabilities in natural language processing, logical\nreasoning and so on. However, the challenges of knowledge updates and\nhallucination issues have limited the application of LLMs in medical scenarios,\nwhere retrieval-augmented generation (RAG) can offer significant assistance.\nNevertheless, existing retrieve-then-read approaches generally digest the\nretrieved documents, without considering the timeliness, authoritativeness and\ncommonality of retrieval. We argue that these approaches can be suboptimal,\nespecially in real-world applications where information from different sources\nmight conflict with each other and even information from the same source in\ndifferent time scale might be different, and totally relying on this would\ndeteriorate the performance of RAG approaches. We propose PolyRAG that\ncarefully incorporate judges from different perspectives and finally integrate\nthe polyviews for retrieval augmented generation in medical applications. Due\nto the scarcity of real-world benchmarks for evaluation, to bridge the gap we\npropose PolyEVAL, a benchmark consists of queries and documents collected from\nreal-world medical scenarios (including medical policy, hospital & doctor\ninquiry and healthcare) with multiple tagging (e.g., timeliness,\nauthoritativeness) on them. Extensive experiments and analysis on PolyEVAL have\ndemonstrated the superiority of PolyRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become a disruptive force in the industry,\nintroducing unprecedented capabilities in natural language processing, logical\nreasoning and so on. However, the challenges of knowledge updates and\nhallucination issues have limited the application of LLMs in medical scenarios,\nwhere retrieval-augmented generation (RAG) can offer significant assistance.\nNevertheless, existing retrieve-then-read approaches generally digest the\nretrieved documents, without considering the timeliness, authoritativeness and\ncommonality of retrieval. We argue that these approaches can be suboptimal,\nespecially in real-world applications where information from different sources\nmight conflict with each other and even information from the same source in\ndifferent time scale might be different, and totally relying on this would\ndeteriorate the performance of RAG approaches. We propose PolyRAG that\ncarefully incorporate judges from different perspectives and finally integrate\nthe polyviews for retrieval augmented generation in medical applications. Due\nto the scarcity of real-world benchmarks for evaluation, to bridge the gap we\npropose PolyEVAL, a benchmark consists of queries and documents collected from\nreal-world medical scenarios (including medical policy, hospital & doctor\ninquiry and healthcare) with multiple tagging (e.g., timeliness,\nauthoritativeness) on them. Extensive experiments and analysis on PolyEVAL have\ndemonstrated the superiority of PolyRAG."
                },
                "authors": [
                    {
                        "name": "Chunjing Gan"
                    },
                    {
                        "name": "Dan Yang"
                    },
                    {
                        "name": "Binbin Hu"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Yue Shen"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14915v1",
                "updated": "2025-04-21T07:33:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    33,
                    27,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T07:33:27Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    33,
                    27,
                    0,
                    111,
                    0
                ],
                "title": "StableQuant: Layer Adaptive Post-Training Quantization for Speech\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableQuant: Layer Adaptive Post-Training Quantization for Speech\n  Foundation Models"
                },
                "summary": "In this paper, we propose StableQuant, a novel adaptive post-training\nquantization (PTQ) algorithm for widely used speech foundation models (SFMs).\nWhile PTQ has been successfully employed for compressing large language models\n(LLMs) due to its ability to bypass additional fine-tuning, directly applying\nthese techniques to SFMs may not yield optimal results, as SFMs utilize\ndistinct network architecture for feature extraction. StableQuant demonstrates\noptimal quantization performance regardless of the network architecture type,\nas it adaptively determines the quantization range for each layer by analyzing\nboth the scale distributions and overall performance. We evaluate our algorithm\non two SFMs, HuBERT and wav2vec2.0, for an automatic speech recognition (ASR)\ntask, and achieve superior performance compared to traditional PTQ methods.\nStableQuant successfully reduces the sizes of SFM models to a quarter and\ndoubles the inference speed while limiting the word error rate (WER)\nperformance drop to less than 0.3% with 8-bit quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose StableQuant, a novel adaptive post-training\nquantization (PTQ) algorithm for widely used speech foundation models (SFMs).\nWhile PTQ has been successfully employed for compressing large language models\n(LLMs) due to its ability to bypass additional fine-tuning, directly applying\nthese techniques to SFMs may not yield optimal results, as SFMs utilize\ndistinct network architecture for feature extraction. StableQuant demonstrates\noptimal quantization performance regardless of the network architecture type,\nas it adaptively determines the quantization range for each layer by analyzing\nboth the scale distributions and overall performance. We evaluate our algorithm\non two SFMs, HuBERT and wav2vec2.0, for an automatic speech recognition (ASR)\ntask, and achieve superior performance compared to traditional PTQ methods.\nStableQuant successfully reduces the sizes of SFM models to a quarter and\ndoubles the inference speed while limiting the word error rate (WER)\nperformance drop to less than 0.3% with 8-bit quantization."
                },
                "authors": [
                    {
                        "name": "Yeona Hong"
                    },
                    {
                        "name": "Hyewon Han"
                    },
                    {
                        "name": "Woo-jin Chung"
                    },
                    {
                        "name": "Hong-Goo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Hong-Goo Kang"
                },
                "author": "Hong-Goo Kang",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12322v2",
                "updated": "2025-04-21T07:29:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    29,
                    28,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-11T06:13:43Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    6,
                    13,
                    43,
                    4,
                    101,
                    0
                ],
                "title": "A Strategic Coordination Framework of Small LLMs Matches Large LLMs in\n  Data Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Strategic Coordination Framework of Small LLMs Matches Large LLMs in\n  Data Synthesis"
                },
                "summary": "While data synthesis and distillation are promising strategies to enhance\nsmall language models, current approaches heavily rely on Large Language Models\n(LLMs), which suffer from high computational costs, environmental inefficiency,\nand potential biases inherited from monolithic architectures. In contrast,\nsmaller LLMs are more accessible and sustainable, but their individual\ncapabilities often fall short in generating high-quality, diverse, and reliable\ndata. Inspired by collaborative human processes (e.g., peer review), we propose\na multiple small LLMs involved framework, GRA, that aggregates specialized\nroles across small LLMs to iterative refinement and quality control typically\nachieved by a single large LLM. In this collaborative framework, multiple small\nLLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a\npeer-review-inspired data synthesis pipeline. The Generator proposes initial\ndata samples, the Reviewer critiques their quality and diversity, and the\nAdjudicator resolves conflicts to finalize the output. By decomposing the\nsynthesis process into specialized sub-tasks, collaborative small LLMs can\nachieve data-level parity with large LLM-based distillation. Through\nexperiments across multiple benchmarks, we demonstrate that GRA-produced data\nmatches or exceeds the quality of single large LLM outputs, e.g.,\nQwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large\nmodels for high-quality data synthesis, advocating instead for strategic\ncoordination of smaller agents. Our datasets, models, and code are publicly\navailable at https://github.com/GX-XinGao/GRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While data synthesis and distillation are promising strategies to enhance\nsmall language models, current approaches heavily rely on Large Language Models\n(LLMs), which suffer from high computational costs, environmental inefficiency,\nand potential biases inherited from monolithic architectures. In contrast,\nsmaller LLMs are more accessible and sustainable, but their individual\ncapabilities often fall short in generating high-quality, diverse, and reliable\ndata. Inspired by collaborative human processes (e.g., peer review), we propose\na multiple small LLMs involved framework, GRA, that aggregates specialized\nroles across small LLMs to iterative refinement and quality control typically\nachieved by a single large LLM. In this collaborative framework, multiple small\nLLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a\npeer-review-inspired data synthesis pipeline. The Generator proposes initial\ndata samples, the Reviewer critiques their quality and diversity, and the\nAdjudicator resolves conflicts to finalize the output. By decomposing the\nsynthesis process into specialized sub-tasks, collaborative small LLMs can\nachieve data-level parity with large LLM-based distillation. Through\nexperiments across multiple benchmarks, we demonstrate that GRA-produced data\nmatches or exceeds the quality of single large LLM outputs, e.g.,\nQwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large\nmodels for high-quality data synthesis, advocating instead for strategic\ncoordination of smaller agents. Our datasets, models, and code are publicly\navailable at https://github.com/GX-XinGao/GRA."
                },
                "authors": [
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Qizhi Pei"
                    },
                    {
                        "name": "Zinan Tang"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Honglin Lin"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01354v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01354v2",
                "updated": "2025-04-21T07:25:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    25,
                    13,
                    0,
                    111,
                    0
                ],
                "published": "2024-08-02T16:04:52Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    16,
                    4,
                    52,
                    4,
                    215,
                    0
                ],
                "title": "MCGMark: An Encodable and Robust Online Watermark for Tracing\n  LLM-Generated Malicious Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCGMark: An Encodable and Robust Online Watermark for Tracing\n  LLM-Generated Malicious Code"
                },
                "summary": "With the advent of large language models (LLMs), numerous software service\nproviders (SSPs) are dedicated to developing LLMs customized for code\ngeneration tasks, such as CodeLlama and Copilot. However, these LLMs can be\nleveraged by attackers to create malicious software, which may pose potential\nthreats to the software ecosystem. For example, they can automate the creation\nof advanced phishing malware. To address this issue, we first conduct an\nempirical study and design a prompt dataset, MCGTest, which involves\napproximately 400 person-hours of work and consists of 406 malicious code\ngeneration tasks. Utilizing this dataset, we propose MCGMark, the first robust,\ncode structure-aware, and encodable watermarking approach to trace\nLLM-generated code. We embed encodable information by controlling the token\nselection and ensuring the output quality based on probabilistic outliers.\nAdditionally, we enhance the robustness of the watermark by considering the\nstructural features of malicious code, preventing the embedding of the\nwatermark in easily modified positions, such as comments. We validate the\neffectiveness and robustness of MCGMark on the DeepSeek-Coder. MCGMark achieves\nan embedding success rate of 88.9% within a maximum output limit of 400 tokens.\nFurthermore, it also demonstrates strong robustness and has minimal impact on\nthe quality of the output code. Our approach assists SSPs in tracing and\nholding responsible parties accountable for malicious code generated by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of large language models (LLMs), numerous software service\nproviders (SSPs) are dedicated to developing LLMs customized for code\ngeneration tasks, such as CodeLlama and Copilot. However, these LLMs can be\nleveraged by attackers to create malicious software, which may pose potential\nthreats to the software ecosystem. For example, they can automate the creation\nof advanced phishing malware. To address this issue, we first conduct an\nempirical study and design a prompt dataset, MCGTest, which involves\napproximately 400 person-hours of work and consists of 406 malicious code\ngeneration tasks. Utilizing this dataset, we propose MCGMark, the first robust,\ncode structure-aware, and encodable watermarking approach to trace\nLLM-generated code. We embed encodable information by controlling the token\nselection and ensuring the output quality based on probabilistic outliers.\nAdditionally, we enhance the robustness of the watermark by considering the\nstructural features of malicious code, preventing the embedding of the\nwatermark in easily modified positions, such as comments. We validate the\neffectiveness and robustness of MCGMark on the DeepSeek-Coder. MCGMark achieves\nan embedding success rate of 88.9% within a maximum output limit of 400 tokens.\nFurthermore, it also demonstrates strong robustness and has minimal impact on\nthe quality of the output code. Our approach assists SSPs in tracing and\nholding responsible parties accountable for malicious code generated by LLMs."
                },
                "authors": [
                    {
                        "name": "Kaiwen Ning"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Qingyuan Zhong"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Jingwen Zhang"
                    },
                    {
                        "name": "Jianxing Yu"
                    },
                    {
                        "name": "Yuming Feng"
                    },
                    {
                        "name": "Weizhe Zhang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01354v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01354v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14905v1",
                "updated": "2025-04-21T07:20:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    20,
                    31,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T07:20:31Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    20,
                    31,
                    0,
                    111,
                    0
                ],
                "title": "CRAVE: A Conflicting Reasoning Approach for Explainable Claim\n  Verification Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRAVE: A Conflicting Reasoning Approach for Explainable Claim\n  Verification Using LLMs"
                },
                "summary": "The rapid spread of misinformation, driven by digital media and AI-generated\ncontent, has made automatic claim verification essential. Traditional methods,\nwhich depend on expert-annotated evidence, are labor-intensive and not\nscalable. Although recent automated systems have improved, they still struggle\nwith complex claims that require nuanced reasoning. To address this, we propose\nCRAVE, a Conflicting Reasoning Approach for explainable claim VErification,\nthat verify the complex claims based on the conflicting rationales reasoned by\nlarge language models (LLMs). Specifically, CRAVE introduces a three-module\nframework. Ambiguity Elimination enchanced Evidence Retrieval module performs\nambiguity elimination and entity-based search to gather relevant evidence\nrelated to claim verification from external sources like Wikipedia. Conflicting\nPerspective Reasoning and Preliminary Judgment module with LLMs adopts LLMs to\nreason rationales with conflicting stances about claim verification from\nretrieved evidence across four dimensions, i.e., direct evidence, semantic\nrelationships, linguistic patterns, and logical reasoning and make a\npreliminary judgment. Finally, Small Language Model (SLM) based Judge module is\nfine-tuned to make use of preliminary judgment from LLMs to assess the\nconfidence of the conflicting rationales and make a final authenticity\njudgment. This methodology allows CRAVE to capture subtle inconsistencies in\ncomplex claims, improving both the accuracy and transparency of claim\nverification. Extensive experiments on two public claim verification datasets\ndemonstrate that our CRAVE model achieves much better performance than\nstate-of-the-art methods and exhibits a superior capacity for finding relevant\nevidence and explaining the model predictions. The code is provided at\nhttps://github.com/8zym/CRAVE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid spread of misinformation, driven by digital media and AI-generated\ncontent, has made automatic claim verification essential. Traditional methods,\nwhich depend on expert-annotated evidence, are labor-intensive and not\nscalable. Although recent automated systems have improved, they still struggle\nwith complex claims that require nuanced reasoning. To address this, we propose\nCRAVE, a Conflicting Reasoning Approach for explainable claim VErification,\nthat verify the complex claims based on the conflicting rationales reasoned by\nlarge language models (LLMs). Specifically, CRAVE introduces a three-module\nframework. Ambiguity Elimination enchanced Evidence Retrieval module performs\nambiguity elimination and entity-based search to gather relevant evidence\nrelated to claim verification from external sources like Wikipedia. Conflicting\nPerspective Reasoning and Preliminary Judgment module with LLMs adopts LLMs to\nreason rationales with conflicting stances about claim verification from\nretrieved evidence across four dimensions, i.e., direct evidence, semantic\nrelationships, linguistic patterns, and logical reasoning and make a\npreliminary judgment. Finally, Small Language Model (SLM) based Judge module is\nfine-tuned to make use of preliminary judgment from LLMs to assess the\nconfidence of the conflicting rationales and make a final authenticity\njudgment. This methodology allows CRAVE to capture subtle inconsistencies in\ncomplex claims, improving both the accuracy and transparency of claim\nverification. Extensive experiments on two public claim verification datasets\ndemonstrate that our CRAVE model achieves much better performance than\nstate-of-the-art methods and exhibits a superior capacity for finding relevant\nevidence and explaining the model predictions. The code is provided at\nhttps://github.com/8zym/CRAVE."
                },
                "authors": [
                    {
                        "name": "Yingming Zheng"
                    },
                    {
                        "name": "Xiaoliang Liu"
                    },
                    {
                        "name": "Peng Wu"
                    },
                    {
                        "name": "Li Pan"
                    }
                ],
                "author_detail": {
                    "name": "Li Pan"
                },
                "author": "Li Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14904v1",
                "updated": "2025-04-21T07:20:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    20,
                    19,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T07:20:19Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    20,
                    19,
                    0,
                    111,
                    0
                ],
                "title": "VLM as Policy: Common-Law Content Moderation Framework for Short Video\n  Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLM as Policy: Common-Law Content Moderation Framework for Short Video\n  Platform"
                },
                "summary": "Exponentially growing short video platforms (SVPs) face significant\nchallenges in moderating content detrimental to users' mental health,\nparticularly for minors. The dissemination of such content on SVPs can lead to\ncatastrophic societal consequences. Although substantial efforts have been\ndedicated to moderating such content, existing methods suffer from critical\nlimitations: (1) Manual review is prone to human bias and incurs high\noperational costs. (2) Automated methods, though efficient, lack nuanced\ncontent understanding, resulting in lower accuracy. (3) Industrial moderation\nregulations struggle to adapt to rapidly evolving trends due to long update\ncycles. In this paper, we annotate the first SVP content moderation benchmark\nwith authentic user/reviewer feedback to fill the absence of benchmark in this\nfield. Then we evaluate various methods on the benchmark to verify the\nexistence of the aforementioned limitations. We further propose our common-law\ncontent moderation framework named KuaiMod to address these challenges. KuaiMod\nconsists of three components: training data construction, offline adaptation,\nand online deployment & refinement. Leveraging large vision language model\n(VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video\ntoxicity based on sparse user feedback and fosters dynamic moderation policy\nwith rapid update speed and high accuracy. Offline experiments and large-scale\nonline A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the\nbest moderation performance on our benchmark. The deployment of KuaiMod reduces\nthe user reporting rate by 20% and its application in video recommendation\nincreases both Daily Active User (DAU) and APP Usage Time (AUT) on several\nKuaishou scenarios. We have open-sourced our benchmark at\nhttps://kuaimod.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exponentially growing short video platforms (SVPs) face significant\nchallenges in moderating content detrimental to users' mental health,\nparticularly for minors. The dissemination of such content on SVPs can lead to\ncatastrophic societal consequences. Although substantial efforts have been\ndedicated to moderating such content, existing methods suffer from critical\nlimitations: (1) Manual review is prone to human bias and incurs high\noperational costs. (2) Automated methods, though efficient, lack nuanced\ncontent understanding, resulting in lower accuracy. (3) Industrial moderation\nregulations struggle to adapt to rapidly evolving trends due to long update\ncycles. In this paper, we annotate the first SVP content moderation benchmark\nwith authentic user/reviewer feedback to fill the absence of benchmark in this\nfield. Then we evaluate various methods on the benchmark to verify the\nexistence of the aforementioned limitations. We further propose our common-law\ncontent moderation framework named KuaiMod to address these challenges. KuaiMod\nconsists of three components: training data construction, offline adaptation,\nand online deployment & refinement. Leveraging large vision language model\n(VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video\ntoxicity based on sparse user feedback and fosters dynamic moderation policy\nwith rapid update speed and high accuracy. Offline experiments and large-scale\nonline A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the\nbest moderation performance on our benchmark. The deployment of KuaiMod reduces\nthe user reporting rate by 20% and its application in video recommendation\nincreases both Daily Active User (DAU) and APP Usage Time (AUT) on several\nKuaishou scenarios. We have open-sourced our benchmark at\nhttps://kuaimod.github.io."
                },
                "authors": [
                    {
                        "name": "Xingyu Lu"
                    },
                    {
                        "name": "Tianke Zhang"
                    },
                    {
                        "name": "Chang Meng"
                    },
                    {
                        "name": "Xiaobei Wang"
                    },
                    {
                        "name": "Jinpeng Wang"
                    },
                    {
                        "name": "YiFan Zhang"
                    },
                    {
                        "name": "Shisong Tang"
                    },
                    {
                        "name": "Changyi Liu"
                    },
                    {
                        "name": "Haojie Ding"
                    },
                    {
                        "name": "Kaiyu Jiang"
                    },
                    {
                        "name": "Kaiyu Tang"
                    },
                    {
                        "name": "Bin Wen"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Tingting Gao"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14903v1",
                "updated": "2025-04-21T07:18:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    18,
                    9,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T07:18:09Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    18,
                    9,
                    0,
                    111,
                    0
                ],
                "title": "ColBERT-serve: Efficient Multi-Stage Memory-Mapped Scoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ColBERT-serve: Efficient Multi-Stage Memory-Mapped Scoring"
                },
                "summary": "We study serving retrieval models, specifically late interaction models like\nColBERT, to many concurrent users at once and under a small budget, in which\nthe index may not fit in memory. We present ColBERT-serve, a novel serving\nsystem that applies a memory-mapping strategy to the ColBERT index, reducing\nRAM usage by 90% and permitting its deployment on cheap servers, and\nincorporates a multi-stage architecture with hybrid scoring, reducing ColBERT's\nquery latency and supporting many concurrent queries in parallel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study serving retrieval models, specifically late interaction models like\nColBERT, to many concurrent users at once and under a small budget, in which\nthe index may not fit in memory. We present ColBERT-serve, a novel serving\nsystem that applies a memory-mapping strategy to the ColBERT index, reducing\nRAM usage by 90% and permitting its deployment on cheap servers, and\nincorporates a multi-stage architecture with hybrid scoring, reducing ColBERT's\nquery latency and supporting many concurrent queries in parallel."
                },
                "authors": [
                    {
                        "name": "Kaili Huang"
                    },
                    {
                        "name": "Thejas Venkatesh"
                    },
                    {
                        "name": "Uma Dingankar"
                    },
                    {
                        "name": "Antonio Mallia"
                    },
                    {
                        "name": "Daniel Campos"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Christopher Potts"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Kwabena Boahen"
                    },
                    {
                        "name": "Omar Khattab"
                    },
                    {
                        "name": "Saarthak Sarup"
                    },
                    {
                        "name": "Keshav Santhanam"
                    }
                ],
                "author_detail": {
                    "name": "Keshav Santhanam"
                },
                "author": "Keshav Santhanam",
                "arxiv_comment": "Accepted by ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18908v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18908v3",
                "updated": "2025-04-21T07:17:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    17,
                    53,
                    0,
                    111,
                    0
                ],
                "published": "2024-10-24T16:59:28Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    59,
                    28,
                    3,
                    298,
                    0
                ],
                "title": "A Survey on Speech Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Speech Large Language Models"
                },
                "summary": "Large Language Models (LLMs) exhibit strong contextual understanding and\nremarkable multitask performance. As a result, researchers have been actively\nexploring the integration of LLMs into the domain of speech understanding, with\na primary focus on a broad range of speech-to-text tasks. These include\nautomatic speech recognition (ASR), speech-to-text translation (ST), speech\nemotion recognition (SER), and others. We refer to such models as Speech LLMs,\nwhich are typically built on a unified architecture that follows the pipeline\nof Audio Feature Extraction -> Multimodal Information Fusion -> LLM Inference.\nThis approach enables richer audio feature extraction while facilitating\nend-to-end fusion of audio and text modalities, thereby achieving deeper\nunderstanding and reasoning from audio data. This paper elucidates the\ndevelopment of Speech LLMs, offering an in-depth analysis of system\narchitectures. Through extensive research and a series of targeted experiments,\nthe paper assesses the advancements in Speech LLMs and their potential for\ncross-task integration within the speech understanding field. Furthermore, it\nhighlights key challenges identified through experimentation, such as the\ndormancy of LLMs under certain conditions. The paper further explores training\nstrategies for Speech LLMs, proposes potential solutions based on these\nfindings, and offers valuable insights and references for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit strong contextual understanding and\nremarkable multitask performance. As a result, researchers have been actively\nexploring the integration of LLMs into the domain of speech understanding, with\na primary focus on a broad range of speech-to-text tasks. These include\nautomatic speech recognition (ASR), speech-to-text translation (ST), speech\nemotion recognition (SER), and others. We refer to such models as Speech LLMs,\nwhich are typically built on a unified architecture that follows the pipeline\nof Audio Feature Extraction -> Multimodal Information Fusion -> LLM Inference.\nThis approach enables richer audio feature extraction while facilitating\nend-to-end fusion of audio and text modalities, thereby achieving deeper\nunderstanding and reasoning from audio data. This paper elucidates the\ndevelopment of Speech LLMs, offering an in-depth analysis of system\narchitectures. Through extensive research and a series of targeted experiments,\nthe paper assesses the advancements in Speech LLMs and their potential for\ncross-task integration within the speech understanding field. Furthermore, it\nhighlights key challenges identified through experimentation, such as the\ndormancy of LLMs under certain conditions. The paper further explores training\nstrategies for Speech LLMs, proposes potential solutions based on these\nfindings, and offers valuable insights and references for future research."
                },
                "authors": [
                    {
                        "name": "Jing Peng"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Yu Xi"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Xizhuo Zhang"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "This version has been updated to incorporate recent work in the field\n  and includes revised illustrations and textual descriptions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18908v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18908v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09602v2",
                "updated": "2025-04-21T07:04:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    4,
                    57,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-13T14:35:30Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    35,
                    30,
                    6,
                    103,
                    0
                ],
                "title": "Fine-tuning a Large Language Model for Automating Computational Fluid\n  Dynamics Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning a Large Language Model for Automating Computational Fluid\n  Dynamics Simulations"
                },
                "summary": "Configuring computational fluid dynamics (CFD) simulations typically demands\nextensive domain expertise, limiting broader access. Although large language\nmodels (LLMs) have advanced scientific computing, their use in automating CFD\nworkflows is underdeveloped. We introduce a novel approach centered on\ndomain-specific LLM adaptation. By fine-tuning Qwen2.5-7B-Instruct on NL2FOAM,\nour custom dataset of 28716 natural language-to-OpenFOAM configuration pairs\nwith chain-of-thought (CoT) annotations, we enable direct translation from\nnatural language descriptions to executable CFD setups. A multi-agent framework\norchestrates the process, autonomously verifying inputs, generating\nconfigurations, running simulations, and correcting errors. Evaluation on a\nbenchmark of 21 diverse flow cases demonstrates state-of-the-art performance,\nachieving 88.7% solution accuracy and 82.6% first-attempt success rate. This\nsignificantly outperforms larger general-purpose models like\nQwen2.5-72B-Instruct, DeepSeek-R1, and Llama3.3-70B-Instruct, while also\nrequiring fewer correction iterations and maintaining high computational\nefficiency. The results highlight the critical role of domain-specific\nadaptation in deploying LLM assistants for complex engineering workflows. Our\ncode and fine-tuned model have been deposited at\nhttps://github.com/YYgroup/AutoCFD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Configuring computational fluid dynamics (CFD) simulations typically demands\nextensive domain expertise, limiting broader access. Although large language\nmodels (LLMs) have advanced scientific computing, their use in automating CFD\nworkflows is underdeveloped. We introduce a novel approach centered on\ndomain-specific LLM adaptation. By fine-tuning Qwen2.5-7B-Instruct on NL2FOAM,\nour custom dataset of 28716 natural language-to-OpenFOAM configuration pairs\nwith chain-of-thought (CoT) annotations, we enable direct translation from\nnatural language descriptions to executable CFD setups. A multi-agent framework\norchestrates the process, autonomously verifying inputs, generating\nconfigurations, running simulations, and correcting errors. Evaluation on a\nbenchmark of 21 diverse flow cases demonstrates state-of-the-art performance,\nachieving 88.7% solution accuracy and 82.6% first-attempt success rate. This\nsignificantly outperforms larger general-purpose models like\nQwen2.5-72B-Instruct, DeepSeek-R1, and Llama3.3-70B-Instruct, while also\nrequiring fewer correction iterations and maintaining high computational\nefficiency. The results highlight the critical role of domain-specific\nadaptation in deploying LLM assistants for complex engineering workflows. Our\ncode and fine-tuned model have been deposited at\nhttps://github.com/YYgroup/AutoCFD."
                },
                "authors": [
                    {
                        "name": "Zhehao Dong"
                    },
                    {
                        "name": "Zhen Lu"
                    },
                    {
                        "name": "Yue Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Yang"
                },
                "author": "Yue Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14893v1",
                "updated": "2025-04-21T06:45:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    6,
                    45,
                    41,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T06:45:41Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    6,
                    45,
                    41,
                    0,
                    111,
                    0
                ],
                "title": "Hardware-based Heterogeneous Memory Management for Large Language Model\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-based Heterogeneous Memory Management for Large Language Model\n  Inference"
                },
                "summary": "A large language model (LLM) is one of the most important emerging machine\nlearning applications nowadays. However, due to its huge model size and runtime\nincrease of the memory footprint, LLM inferences suffer from the lack of memory\ncapacity in conventional systems consisting of multiple GPUs with a modest\namount of high bandwidth memory. Moreover, since LLM contains many\nbandwidthintensive kernels, only focusing on the memory capacity without\nconsidering the bandwidth incurs a serious performance degradation. To handle\nsuch conflicting memory capacity and bandwidth demands in a cost-effective way,\nthis study investigates the potential of heterogeneous memory systems,\nproposing H2M2. It uses an asymmetric memory architecture consisting of\ncapacity-centric and bandwidthcentric memory with computation units attached to\neach memory device. With the asymmetric memory, we first analyze the effect of\nkernel-memory mapping for the asymmetric memory. Second, we propose a dynamic\nruntime algorithm that finds a mapping solution considering the characteristics\nof LLM operations and the change of footprint during LLM inference. Third, we\nadvocate the need for memory abstraction for the efficient management of the\nasymmetric memory. H2M2 outperforms the conventional homogeneous memory system\nwith LPDDR by 1.46x, 1.55x, and 2.94x speedup in GPT3-175B, Chinchilla-70B, and\nLlama2-70B, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large language model (LLM) is one of the most important emerging machine\nlearning applications nowadays. However, due to its huge model size and runtime\nincrease of the memory footprint, LLM inferences suffer from the lack of memory\ncapacity in conventional systems consisting of multiple GPUs with a modest\namount of high bandwidth memory. Moreover, since LLM contains many\nbandwidthintensive kernels, only focusing on the memory capacity without\nconsidering the bandwidth incurs a serious performance degradation. To handle\nsuch conflicting memory capacity and bandwidth demands in a cost-effective way,\nthis study investigates the potential of heterogeneous memory systems,\nproposing H2M2. It uses an asymmetric memory architecture consisting of\ncapacity-centric and bandwidthcentric memory with computation units attached to\neach memory device. With the asymmetric memory, we first analyze the effect of\nkernel-memory mapping for the asymmetric memory. Second, we propose a dynamic\nruntime algorithm that finds a mapping solution considering the characteristics\nof LLM operations and the change of footprint during LLM inference. Third, we\nadvocate the need for memory abstraction for the efficient management of the\nasymmetric memory. H2M2 outperforms the conventional homogeneous memory system\nwith LPDDR by 1.46x, 1.55x, and 2.94x speedup in GPT3-175B, Chinchilla-70B, and\nLlama2-70B, respectively."
                },
                "authors": [
                    {
                        "name": "Soojin Hwang"
                    },
                    {
                        "name": "Jungwoo Kim"
                    },
                    {
                        "name": "Sanghyeon Lee"
                    },
                    {
                        "name": "Hongbeen Kim"
                    },
                    {
                        "name": "Jaehyuk Huh"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyuk Huh"
                },
                "author": "Jaehyuk Huh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14891v1",
                "updated": "2025-04-21T06:39:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    6,
                    39,
                    47,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T06:39:47Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    6,
                    39,
                    47,
                    0,
                    111,
                    0
                ],
                "title": "Retrieval Augmented Generation Evaluation in the Era of Large Language\n  Models: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation Evaluation in the Era of Large Language\n  Models: A Comprehensive Survey"
                },
                "summary": "Recent advancements in Retrieval-Augmented Generation (RAG) have\nrevolutionized natural language processing by integrating Large Language Models\n(LLMs) with external information retrieval, enabling accurate, up-to-date, and\nverifiable text generation across diverse applications. However, evaluating RAG\nsystems presents unique challenges due to their hybrid architecture that\ncombines retrieval and generation components, as well as their dependence on\ndynamic knowledge sources in the LLM era. In response, this paper provides a\ncomprehensive survey of RAG evaluation methods and frameworks, systematically\nreviewing traditional and emerging evaluation approaches, for system\nperformance, factual accuracy, safety, and computational efficiency in the LLM\nera. We also compile and categorize the RAG-specific datasets and evaluation\nframeworks, conducting a meta-analysis of evaluation practices in high-impact\nRAG research. To the best of our knowledge, this work represents the most\ncomprehensive survey for RAG evaluation, bridging traditional and LLM-driven\nmethods, and serves as a critical resource for advancing RAG development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Retrieval-Augmented Generation (RAG) have\nrevolutionized natural language processing by integrating Large Language Models\n(LLMs) with external information retrieval, enabling accurate, up-to-date, and\nverifiable text generation across diverse applications. However, evaluating RAG\nsystems presents unique challenges due to their hybrid architecture that\ncombines retrieval and generation components, as well as their dependence on\ndynamic knowledge sources in the LLM era. In response, this paper provides a\ncomprehensive survey of RAG evaluation methods and frameworks, systematically\nreviewing traditional and emerging evaluation approaches, for system\nperformance, factual accuracy, safety, and computational efficiency in the LLM\nera. We also compile and categorize the RAG-specific datasets and evaluation\nframeworks, conducting a meta-analysis of evaluation practices in high-impact\nRAG research. To the best of our knowledge, this work represents the most\ncomprehensive survey for RAG evaluation, bridging traditional and LLM-driven\nmethods, and serves as a critical resource for advancing RAG development."
                },
                "authors": [
                    {
                        "name": "Aoran Gan"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Wenyu Yan"
                    },
                    {
                        "name": "Zhenya Huang"
                    },
                    {
                        "name": "Shiwei Tong"
                    },
                    {
                        "name": "Guoping Hu"
                    }
                ],
                "author_detail": {
                    "name": "Guoping Hu"
                },
                "author": "Guoping Hu",
                "arxiv_comment": "18 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14871v1",
                "updated": "2025-04-21T05:48:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    48,
                    52,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T05:48:52Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    48,
                    52,
                    0,
                    111,
                    0
                ],
                "title": "Natural Fingerprints of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Fingerprints of Large Language Models"
                },
                "summary": "Large language models (LLMs) often exhibit biases -- systematic deviations\nfrom expected norms -- in their outputs. These range from overt issues, such as\nunfair responses, to subtler patterns that can reveal which model produced\nthem. We investigate the factors that give rise to identifiable characteristics\nin LLMs. Since LLMs model training data distribution, it is reasonable that\ndifferences in training data naturally lead to the characteristics. However,\nour findings reveal that even when LLMs are trained on the exact same data, it\nis still possible to distinguish the source model based on its generated text.\nWe refer to these unintended, distinctive characteristics as natural\nfingerprints. By systematically controlling training conditions, we show that\nthe natural fingerprints can emerge from subtle differences in the training\nprocess, such as parameter sizes, optimization settings, and even random seeds.\nWe believe that understanding natural fingerprints offers new insights into the\norigins of unintended bias and ways for improving control over LLM behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often exhibit biases -- systematic deviations\nfrom expected norms -- in their outputs. These range from overt issues, such as\nunfair responses, to subtler patterns that can reveal which model produced\nthem. We investigate the factors that give rise to identifiable characteristics\nin LLMs. Since LLMs model training data distribution, it is reasonable that\ndifferences in training data naturally lead to the characteristics. However,\nour findings reveal that even when LLMs are trained on the exact same data, it\nis still possible to distinguish the source model based on its generated text.\nWe refer to these unintended, distinctive characteristics as natural\nfingerprints. By systematically controlling training conditions, we show that\nthe natural fingerprints can emerge from subtle differences in the training\nprocess, such as parameter sizes, optimization settings, and even random seeds.\nWe believe that understanding natural fingerprints offers new insights into the\norigins of unintended bias and ways for improving control over LLM behavior."
                },
                "authors": [
                    {
                        "name": "Teppei Suzuki"
                    },
                    {
                        "name": "Ryokan Ri"
                    },
                    {
                        "name": "Sho Takase"
                    }
                ],
                "author_detail": {
                    "name": "Sho Takase"
                },
                "author": "Sho Takase",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14870v1",
                "updated": "2025-04-21T05:40:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    40,
                    5,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T05:40:05Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    40,
                    5,
                    0,
                    111,
                    0
                ],
                "title": "OTC: Optimal Tool Calls via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OTC: Optimal Tool Calls via Reinforcement Learning"
                },
                "summary": "Tool-integrated reasoning (TIR) augments large language models (LLMs) with\nthe ability to invoke external tools, such as search engines and code\ninterpreters, to solve tasks beyond the capabilities of language-only\nreasoning. While reinforcement learning (RL) has shown promise in improving TIR\nby optimizing final answer correctness, existing approaches often overlook the\nefficiency and cost associated with tool usage. This can lead to suboptimal\nbehavior, including excessive tool calls that increase computational and\nfinancial overhead, or insufficient tool use that compromises answer quality.\nIn this work, we propose Optimal Tool Call-controlled Policy Optimization\n(OTC-PO), a simple yet effective RL-based framework that encourages models to\nproduce accurate answers with minimal tool calls. Our method introduces a\ntool-integrated reward that jointly considers correctness and tool efficiency,\npromoting high tool productivity. We instantiate this framework within both\nProximal Policy Optimization (PPO) and Group Relative Preference Optimization\n(GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and\nQwen-Math across multiple QA benchmarks show that our approach reduces tool\ncalls by up to 73.1\\% and improves tool productivity by up to 229.4\\%, while\nmaintaining comparable answer accuracy. To the best of our knowledge, this is\nthe first RL-based framework that explicitly optimizes tool-use efficiency in\nTIR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-integrated reasoning (TIR) augments large language models (LLMs) with\nthe ability to invoke external tools, such as search engines and code\ninterpreters, to solve tasks beyond the capabilities of language-only\nreasoning. While reinforcement learning (RL) has shown promise in improving TIR\nby optimizing final answer correctness, existing approaches often overlook the\nefficiency and cost associated with tool usage. This can lead to suboptimal\nbehavior, including excessive tool calls that increase computational and\nfinancial overhead, or insufficient tool use that compromises answer quality.\nIn this work, we propose Optimal Tool Call-controlled Policy Optimization\n(OTC-PO), a simple yet effective RL-based framework that encourages models to\nproduce accurate answers with minimal tool calls. Our method introduces a\ntool-integrated reward that jointly considers correctness and tool efficiency,\npromoting high tool productivity. We instantiate this framework within both\nProximal Policy Optimization (PPO) and Group Relative Preference Optimization\n(GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and\nQwen-Math across multiple QA benchmarks show that our approach reduces tool\ncalls by up to 73.1\\% and improves tool productivity by up to 229.4\\%, while\nmaintaining comparable answer accuracy. To the best of our knowledge, this is\nthe first RL-based framework that explicitly optimizes tool-use efficiency in\nTIR."
                },
                "authors": [
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Wanjun Zhong"
                    },
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Shijue Huang"
                    },
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23512v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23512v2",
                "updated": "2025-04-21T05:40:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    40,
                    0,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-30T16:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    16,
                    48,
                    27,
                    6,
                    89,
                    0
                ],
                "title": "SCORE: Story Coherence and Retrieval Enhancement for AI Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCORE: Story Coherence and Retrieval Enhancement for AI Narratives"
                },
                "summary": "Large Language Models (LLMs) can generate creative and engaging narratives\nfrom user-specified input, but maintaining coherence and emotional depth\nthroughout these AI-generated stories remains a challenge. In this work, we\npropose SCORE, a framework for Story Coherence and Retrieval Enhancement,\ndesigned to detect and resolve narrative inconsistencies. By tracking key item\nstatuses and generating episode summaries, SCORE uses a Retrieval-Augmented\nGeneration (RAG) approach, incorporating TF-IDF and cosine similarity to\nidentify related episodes and enhance the overall story structure. Results from\ntesting multiple LLM-generated stories demonstrate that SCORE significantly\nimproves the consistency and stability of narrative coherence compared to\nbaseline GPT models, providing a more robust method for evaluating and refining\nAI-generated narratives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can generate creative and engaging narratives\nfrom user-specified input, but maintaining coherence and emotional depth\nthroughout these AI-generated stories remains a challenge. In this work, we\npropose SCORE, a framework for Story Coherence and Retrieval Enhancement,\ndesigned to detect and resolve narrative inconsistencies. By tracking key item\nstatuses and generating episode summaries, SCORE uses a Retrieval-Augmented\nGeneration (RAG) approach, incorporating TF-IDF and cosine similarity to\nidentify related episodes and enhance the overall story structure. Results from\ntesting multiple LLM-generated stories demonstrate that SCORE significantly\nimproves the consistency and stability of narrative coherence compared to\nbaseline GPT models, providing a more robust method for evaluating and refining\nAI-generated narratives."
                },
                "authors": [
                    {
                        "name": "Qiang Yi"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Shiyao Qian"
                    },
                    {
                        "name": "Xinhang Yuan"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Li Sun"
                    },
                    {
                        "name": "Keqin Li"
                    },
                    {
                        "name": "Kuan Lu"
                    },
                    {
                        "name": "Menghao Huo"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Tianyu Shi"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Shi"
                },
                "author": "Tianyu Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23512v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23512v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14815v2",
                "updated": "2025-04-21T05:29:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    29,
                    1,
                    0,
                    111,
                    0
                ],
                "published": "2024-10-18T18:35:19Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    18,
                    35,
                    19,
                    4,
                    292,
                    0
                ],
                "title": "Adapting Multilingual LLMs to Low-Resource Languages using Continued\n  Pre-training and Synthetic Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Multilingual LLMs to Low-Resource Languages using Continued\n  Pre-training and Synthetic Corpus"
                },
                "summary": "Multilingual LLMs support a variety of languages; however, their performance\nis suboptimal for low-resource languages. In this work, we emphasize the\nimportance of continued pre-training of multilingual LLMs and the use of\ntranslation-based synthetic pre-training corpora for improving LLMs in\nlow-resource languages. We conduct our study in the context of the low-resource\nIndic language Hindi. We introduce Nemotron-Mini-Hindi 4B, a bilingual SLM\nsupporting both Hindi and English, based on Nemotron-Mini 4B. The model is\ntrained using a mix of real and synthetic Hindi + English tokens, with\ncontinuous pre-training performed on 400B tokens. We demonstrate that both the\nbase and instruct models achieve state-of-the-art results on Hindi benchmarks\nwhile remaining competitive on English tasks. Additionally, we observe that the\ncontinued pre-training approach enhances the model's overall factual accuracy.\nWe perform an ablation study to highlight the impact of Hindi pre-training,\nshowing significant improvements in Hindi chat capabilities and factual\naccuracy, which cannot be achieved through Hindi alignment alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual LLMs support a variety of languages; however, their performance\nis suboptimal for low-resource languages. In this work, we emphasize the\nimportance of continued pre-training of multilingual LLMs and the use of\ntranslation-based synthetic pre-training corpora for improving LLMs in\nlow-resource languages. We conduct our study in the context of the low-resource\nIndic language Hindi. We introduce Nemotron-Mini-Hindi 4B, a bilingual SLM\nsupporting both Hindi and English, based on Nemotron-Mini 4B. The model is\ntrained using a mix of real and synthetic Hindi + English tokens, with\ncontinuous pre-training performed on 400B tokens. We demonstrate that both the\nbase and instruct models achieve state-of-the-art results on Hindi benchmarks\nwhile remaining competitive on English tasks. Additionally, we observe that the\ncontinued pre-training approach enhances the model's overall factual accuracy.\nWe perform an ablation study to highlight the impact of Hindi pre-training,\nshowing significant improvements in Hindi chat capabilities and factual\naccuracy, which cannot be achieved through Hindi alignment alone."
                },
                "authors": [
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Kanishk Singla"
                    },
                    {
                        "name": "Anusha Kamath"
                    },
                    {
                        "name": "Raunak Kalani"
                    },
                    {
                        "name": "Rakesh Paul"
                    },
                    {
                        "name": "Utkarsh Vaidya"
                    },
                    {
                        "name": "Sanjay Singh Chauhan"
                    },
                    {
                        "name": "Niranjan Wartikar"
                    },
                    {
                        "name": "Eileen Long"
                    }
                ],
                "author_detail": {
                    "name": "Eileen Long"
                },
                "author": "Eileen Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09407v2",
                "updated": "2025-04-21T05:22:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    22,
                    55,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-13T02:34:22Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    2,
                    34,
                    22,
                    6,
                    103,
                    0
                ],
                "title": "UXAgent: A System for Simulating Usability Testing of Web Design with\n  LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UXAgent: A System for Simulating Usability Testing of Web Design with\n  LLM Agents"
                },
                "summary": "Usability testing is a fundamental research method that user experience (UX)\nresearchers use to evaluate and iterate a web design, but\\textbf{ how to\nevaluate and iterate the usability testing study design } itself? Recent\nadvances in Large Language Model-simulated Agent (\\textbf{LLM Agent}) research\ninspired us to design \\textbf{UXAgent} to support UX researchers in evaluating\nand reiterating their usability testing study design before they conduct the\nreal human-subject study. Our system features a Persona Generator module, an\nLLM Agent module, and a Universal Browser Connector module to automatically\ngenerate thousands of simulated users to interactively test the target website.\nThe system also provides an Agent Interview Interface and a Video Replay\nInterface so that the UX researchers can easily review and analyze the\ngenerated qualitative and quantitative log data. Through a heuristic\nevaluation, five UX researcher participants praised the innovation of our\nsystem but also expressed concerns about the future of LLM Agent usage in UX\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Usability testing is a fundamental research method that user experience (UX)\nresearchers use to evaluate and iterate a web design, but\\textbf{ how to\nevaluate and iterate the usability testing study design } itself? Recent\nadvances in Large Language Model-simulated Agent (\\textbf{LLM Agent}) research\ninspired us to design \\textbf{UXAgent} to support UX researchers in evaluating\nand reiterating their usability testing study design before they conduct the\nreal human-subject study. Our system features a Persona Generator module, an\nLLM Agent module, and a Universal Browser Connector module to automatically\ngenerate thousands of simulated users to interactively test the target website.\nThe system also provides an Agent Interview Interface and a Video Replay\nInterface so that the UX researchers can easily review and analyze the\ngenerated qualitative and quantitative log data. Through a heuristic\nevaluation, five UX researcher participants praised the innovation of our\nsystem but also expressed concerns about the future of LLM Agent usage in UX\nstudies."
                },
                "authors": [
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Jessie Wang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Jiri Gesi"
                    },
                    {
                        "name": "Qi He"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20749v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20749v4",
                "updated": "2025-04-21T05:12:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    12,
                    56,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-26T17:33:27Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    33,
                    27,
                    2,
                    85,
                    0
                ],
                "title": "LLM Agents That Act Like Us: Accurate Human Behavior Simulation with\n  Real-World Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Agents That Act Like Us: Accurate Human Behavior Simulation with\n  Real-World Data"
                },
                "summary": "Recent research shows that LLMs can simulate ``believable'' human behaviors\nto power LLM agents via prompt-only methods. In this work, we focus on\nevaluating and improving LLM's objective ``accuracy'' rather than the\nsubjective ``believability'' in the web action generation task, leveraging a\nlarge-scale, real-world dataset collected from online shopping human actions.\nWe present the first comprehensive quantitative evaluation of state-of-the-art\nLLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web action\ngeneration. Our results show that fine-tuning LLMs on real-world behavioral\ndata substantially improves their ability to generate actions compared to\nprompt-only methods. Furthermore, incorporating synthesized reasoning traces\ninto model training leads to additional performance gains, demonstrating the\nvalue of explicit rationale in behavior modeling. This work establishes a new\nbenchmark for evaluating LLMs in behavior simulation and offers actionable\ninsights into how real-world action data and reasoning augmentation can enhance\nthe fidelity of LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research shows that LLMs can simulate ``believable'' human behaviors\nto power LLM agents via prompt-only methods. In this work, we focus on\nevaluating and improving LLM's objective ``accuracy'' rather than the\nsubjective ``believability'' in the web action generation task, leveraging a\nlarge-scale, real-world dataset collected from online shopping human actions.\nWe present the first comprehensive quantitative evaluation of state-of-the-art\nLLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web action\ngeneration. Our results show that fine-tuning LLMs on real-world behavioral\ndata substantially improves their ability to generate actions compared to\nprompt-only methods. Furthermore, incorporating synthesized reasoning traces\ninto model training leads to additional performance gains, demonstrating the\nvalue of explicit rationale in behavior modeling. This work establishes a new\nbenchmark for evaluating LLMs in behavior simulation and offers actionable\ninsights into how real-world action data and reasoning augmentation can enhance\nthe fidelity of LLM agents."
                },
                "authors": [
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Yan Han"
                    },
                    {
                        "name": "Bennet Bei"
                    },
                    {
                        "name": "Yaochen Xie"
                    },
                    {
                        "name": "Dakuo Wang"
                    },
                    {
                        "name": "Jessie Wang"
                    },
                    {
                        "name": "Qi He"
                    }
                ],
                "author_detail": {
                    "name": "Qi He"
                },
                "author": "Qi He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20749v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20749v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14856v1",
                "updated": "2025-04-21T04:50:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    4,
                    50,
                    16,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T04:50:16Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    4,
                    50,
                    16,
                    0,
                    111,
                    0
                ],
                "title": "Transparentize the Internal and External Knowledge Utilization in LLMs\n  with Trustworthy Citation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transparentize the Internal and External Knowledge Utilization in LLMs\n  with Trustworthy Citation"
                },
                "summary": "While hallucinations of large language models could been alleviated through\nretrieval-augmented generation and citation generation, how the model utilizes\ninternal knowledge is still opaque, and the trustworthiness of its generated\nanswers remains questionable. In this work, we introduce Context-Prior\nAugmented Citation Generation task, requiring models to generate citations\nconsidering both external and internal knowledge while providing trustworthy\nreferences, with 5 evaluation metrics focusing on 3 aspects: answer\nhelpfulness, citation faithfulness, and trustworthiness. We introduce RAEL, the\nparadigm for our task, and also design INTRALIGN, an integrated method\ncontaining customary data generation and an alignment algorithm. Our\nexperimental results show that our method achieves a better cross-scenario\nperformance with regard to other baselines. Our extended experiments further\nreveal that retrieval quality, question types, and model knowledge have\nconsiderable influence on the trustworthiness in citation generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While hallucinations of large language models could been alleviated through\nretrieval-augmented generation and citation generation, how the model utilizes\ninternal knowledge is still opaque, and the trustworthiness of its generated\nanswers remains questionable. In this work, we introduce Context-Prior\nAugmented Citation Generation task, requiring models to generate citations\nconsidering both external and internal knowledge while providing trustworthy\nreferences, with 5 evaluation metrics focusing on 3 aspects: answer\nhelpfulness, citation faithfulness, and trustworthiness. We introduce RAEL, the\nparadigm for our task, and also design INTRALIGN, an integrated method\ncontaining customary data generation and an alignment algorithm. Our\nexperimental results show that our method achieves a better cross-scenario\nperformance with regard to other baselines. Our extended experiments further\nreveal that retrieval quality, question types, and model knowledge have\nconsiderable influence on the trustworthiness in citation generation."
                },
                "authors": [
                    {
                        "name": "Jiajun Shen"
                    },
                    {
                        "name": "Tong Zhou"
                    },
                    {
                        "name": "Yubo Chen"
                    },
                    {
                        "name": "Delai Qiu"
                    },
                    {
                        "name": "Shengping Liu"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "19 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12206v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12206v2",
                "updated": "2025-04-21T04:44:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    4,
                    44,
                    14,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-15T17:11:41Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    17,
                    11,
                    41,
                    5,
                    74,
                    0
                ],
                "title": "TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification"
                },
                "summary": "Contrastive Language-Image Pretraining (CLIP) has shown impressive zero-shot\nperformance on image classification. However, state-of-the-art methods often\nrely on fine-tuning techniques like prompt learning and adapter-based tuning to\noptimize CLIP's performance. The necessity for fine-tuning significantly limits\nCLIP's adaptability to novel datasets and domains. This requirement mandates\nsubstantial time and computational resources for each new dataset. To overcome\nthis limitation, we introduce simple yet effective training-free approaches,\nSingle-stage LMM Augmented CLIP (SLAC) and Two-stage LMM Augmented CLIP (TLAC),\nthat leverages powerful Large Multimodal Models (LMMs), such as Gemini, for\nimage classification. The proposed methods leverages the capabilities of\npre-trained LMMs, allowing for seamless adaptation to diverse datasets and\ndomains without the need for additional training. Our approaches involve\nprompting the LMM to identify objects within an image. Subsequently, the CLIP\ntext encoder determines the image class by identifying the dataset class with\nthe highest semantic similarity to the LLM predicted object. Our models\nachieved superior accuracy on 9 of 11 base-to-novel datasets, including\nImageNet, SUN397, and Caltech101, while maintaining a strictly training-free\nparadigm. Our TLAC model achieved an overall accuracy of 83.44%, surpassing the\nprevious state-of-the-art few-shot methods by a margin of 6.75%. Compared to\nother training-free approaches, our TLAC method achieved 83.6% average accuracy\nacross 13 datasets, a 9.7% improvement over the previous methods. Our Code is\navailable at https://github.com/ans92/TLAC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Language-Image Pretraining (CLIP) has shown impressive zero-shot\nperformance on image classification. However, state-of-the-art methods often\nrely on fine-tuning techniques like prompt learning and adapter-based tuning to\noptimize CLIP's performance. The necessity for fine-tuning significantly limits\nCLIP's adaptability to novel datasets and domains. This requirement mandates\nsubstantial time and computational resources for each new dataset. To overcome\nthis limitation, we introduce simple yet effective training-free approaches,\nSingle-stage LMM Augmented CLIP (SLAC) and Two-stage LMM Augmented CLIP (TLAC),\nthat leverages powerful Large Multimodal Models (LMMs), such as Gemini, for\nimage classification. The proposed methods leverages the capabilities of\npre-trained LMMs, allowing for seamless adaptation to diverse datasets and\ndomains without the need for additional training. Our approaches involve\nprompting the LMM to identify objects within an image. Subsequently, the CLIP\ntext encoder determines the image class by identifying the dataset class with\nthe highest semantic similarity to the LLM predicted object. Our models\nachieved superior accuracy on 9 of 11 base-to-novel datasets, including\nImageNet, SUN397, and Caltech101, while maintaining a strictly training-free\nparadigm. Our TLAC model achieved an overall accuracy of 83.44%, surpassing the\nprevious state-of-the-art few-shot methods by a margin of 6.75%. Compared to\nother training-free approaches, our TLAC method achieved 83.6% average accuracy\nacross 13 datasets, a 9.7% improvement over the previous methods. Our Code is\navailable at https://github.com/ans92/TLAC"
                },
                "authors": [
                    {
                        "name": "Ans Munir"
                    },
                    {
                        "name": "Faisal Z. Qureshi"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    },
                    {
                        "name": "Mohsen Ali"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Ali"
                },
                "author": "Mohsen Ali",
                "arxiv_comment": "Added code link in the abstract",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12206v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12206v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19245v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19245v2",
                "updated": "2025-04-21T04:40:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    4,
                    40,
                    38,
                    0,
                    111,
                    0
                ],
                "published": "2024-10-25T01:52:15Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    1,
                    52,
                    15,
                    4,
                    299,
                    0
                ],
                "title": "MaCTG: Multi-Agent Collaborative Thought Graph for Automatic Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaCTG: Multi-Agent Collaborative Thought Graph for Automatic Programming"
                },
                "summary": "With the rapid advancement of Large Language Models (LLMs), LLM-based\napproaches have demonstrated strong problem-solving capabilities across various\ndomains. However, in automatic programming, a single LLM is typically limited\nto function-level code generation, while multi-agent systems composed of\nmultiple LLMs often suffer from inefficient task planning. This lack of\nstructured coordination can lead to cascading hallucinations, where accumulated\nerrors across agents result in suboptimal workflows and excessive computational\ncosts. To overcome these challenges, we introduce MaCTG (Multi-Agent\nCollaborative Thought Graph), a novel multi-agent framework that employs a\ndynamic graph structure to facilitate precise task allocation and controlled\ncollaboration among LLM agents. MaCTG autonomously assigns agent roles based on\nprogramming requirements, dynamically refines task distribution through\ncontext-aware adjustments, and systematically verifies and integrates\nproject-level code, effectively reducing hallucination errors and improving\noverall accuracy. MaCTG enhances cost-effectiveness by implementing a hybrid\nLLM deployment, where proprietary models handle complex reasoning, while\nopen-source models are used for routine coding and validation tasks. To\nevaluate MaCTG's effectiveness, we applied it to traditional image processing\nauto-programming tasks, achieving a state-of-the-art accuracy of 83.33%.\nAdditionally, by leveraging its hybrid LLM configuration, MaCTG significantly\nreduced operational costs by 89.09% compared to existing multi-agent\nframeworks, demonstrating its efficiency, scalability, and real-world\napplicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of Large Language Models (LLMs), LLM-based\napproaches have demonstrated strong problem-solving capabilities across various\ndomains. However, in automatic programming, a single LLM is typically limited\nto function-level code generation, while multi-agent systems composed of\nmultiple LLMs often suffer from inefficient task planning. This lack of\nstructured coordination can lead to cascading hallucinations, where accumulated\nerrors across agents result in suboptimal workflows and excessive computational\ncosts. To overcome these challenges, we introduce MaCTG (Multi-Agent\nCollaborative Thought Graph), a novel multi-agent framework that employs a\ndynamic graph structure to facilitate precise task allocation and controlled\ncollaboration among LLM agents. MaCTG autonomously assigns agent roles based on\nprogramming requirements, dynamically refines task distribution through\ncontext-aware adjustments, and systematically verifies and integrates\nproject-level code, effectively reducing hallucination errors and improving\noverall accuracy. MaCTG enhances cost-effectiveness by implementing a hybrid\nLLM deployment, where proprietary models handle complex reasoning, while\nopen-source models are used for routine coding and validation tasks. To\nevaluate MaCTG's effectiveness, we applied it to traditional image processing\nauto-programming tasks, achieving a state-of-the-art accuracy of 83.33%.\nAdditionally, by leveraging its hybrid LLM configuration, MaCTG significantly\nreduced operational costs by 89.09% compared to existing multi-agent\nframeworks, demonstrating its efficiency, scalability, and real-world\napplicability."
                },
                "authors": [
                    {
                        "name": "Zixiao Zhao"
                    },
                    {
                        "name": "Jing Sun"
                    },
                    {
                        "name": "Zhe Hou"
                    },
                    {
                        "name": "Zhiyuan Wei"
                    },
                    {
                        "name": "Cheng-Hao Cai"
                    },
                    {
                        "name": "Miao Qiao"
                    },
                    {
                        "name": "Jin Song Dong"
                    }
                ],
                "author_detail": {
                    "name": "Jin Song Dong"
                },
                "author": "Jin Song Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19245v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19245v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14852v1",
                "updated": "2025-04-21T04:24:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    4,
                    24,
                    49,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T04:24:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    4,
                    24,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "APIRAT: Integrating Multi-source API Knowledge for Enhanced Code\n  Translation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APIRAT: Integrating Multi-source API Knowledge for Enhanced Code\n  Translation with LLMs"
                },
                "summary": "Code translation is an essential task in software migration, multilingual\ndevelopment, and system refactoring. Recent advancements in large language\nmodels (LLMs) have demonstrated significant potential in this task. However,\nprior studies have highlighted that LLMs often struggle with domain-specific\ncode, particularly in resolving cross-lingual API mappings. To tackle this\nchallenge, we propose APIRAT, a novel code translation method that integrates\nmulti-source API knowledge. APIRAT employs three API knowledge augmentation\ntechniques, including API sequence retrieval, API sequence back-translation,\nand API mapping, to guide LLMs to translating code, ensuring both the correct\nstructure of API sequences and the accurate usage of individual APIs. Extensive\nexperiments on two public datasets, CodeNet and AVATAR, indicate that APIRAT\nsignificantly surpasses existing LLM-based methods, achieving improvements in\ncomputational accuracy ranging from 4% to 15.1%. Additionally, our evaluation\nacross different LLMs showcases the generalizability of APIRAT. An ablation\nstudy further confirms the individual contributions of each API knowledge\ncomponent, underscoring the effectiveness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code translation is an essential task in software migration, multilingual\ndevelopment, and system refactoring. Recent advancements in large language\nmodels (LLMs) have demonstrated significant potential in this task. However,\nprior studies have highlighted that LLMs often struggle with domain-specific\ncode, particularly in resolving cross-lingual API mappings. To tackle this\nchallenge, we propose APIRAT, a novel code translation method that integrates\nmulti-source API knowledge. APIRAT employs three API knowledge augmentation\ntechniques, including API sequence retrieval, API sequence back-translation,\nand API mapping, to guide LLMs to translating code, ensuring both the correct\nstructure of API sequences and the accurate usage of individual APIs. Extensive\nexperiments on two public datasets, CodeNet and AVATAR, indicate that APIRAT\nsignificantly surpasses existing LLM-based methods, achieving improvements in\ncomputational accuracy ranging from 4% to 15.1%. Additionally, our evaluation\nacross different LLMs showcases the generalizability of APIRAT. An ablation\nstudy further confirms the individual contributions of each API knowledge\ncomponent, underscoring the effectiveness of our approach."
                },
                "authors": [
                    {
                        "name": "Chaofan Wang"
                    },
                    {
                        "name": "Guanjie Qiu"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Beijun Shen"
                    }
                ],
                "author_detail": {
                    "name": "Beijun Shen"
                },
                "author": "Beijun Shen",
                "arxiv_comment": "accepted by COMPSAC2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11192v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11192v3",
                "updated": "2025-04-21T04:11:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    4,
                    11,
                    32,
                    0,
                    111,
                    0
                ],
                "published": "2024-06-17T03:57:35Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    3,
                    57,
                    35,
                    0,
                    169,
                    0
                ],
                "title": "Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets\n  and Languages for Open Named Entity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets\n  and Languages for Open Named Entity Recognition"
                },
                "summary": "Open Named Entity Recognition (NER), which involves identifying arbitrary\ntypes of entities from arbitrary domains, remains challenging for Large\nLanguage Models (LLMs). Recent studies suggest that fine-tuning LLMs on\nextensive NER data can boost their performance. However, training directly on\nexisting datasets neglects their inconsistent entity definitions and redundant\ndata, limiting LLMs to dataset-specific learning and hindering out-of-domain\nadaptation. To address this, we present B2NERD, a compact dataset designed to\nguide LLMs' generalization in Open NER under a universal entity taxonomy.\nB2NERD is refined from 54 existing English and Chinese datasets using a\ntwo-step process. First, we detect inconsistent entity definitions across\ndatasets and clarify them by distinguishable label names to construct a\nuniversal taxonomy of 400+ entity types. Second, we address redundancy using a\ndata pruning strategy that selects fewer samples with greater category and\nsemantic diversity. Comprehensive evaluation shows that B2NERD significantly\nenhances LLMs' Open NER capabilities. Our B2NER models, trained on B2NERD,\noutperform GPT-4 by 6.8-12.0 F1 points and surpass previous methods in 3\nout-of-domain benchmarks across 15 datasets and 6 languages. The data, models,\nand code are publicly available at https://github.com/UmeanNever/B2NER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Named Entity Recognition (NER), which involves identifying arbitrary\ntypes of entities from arbitrary domains, remains challenging for Large\nLanguage Models (LLMs). Recent studies suggest that fine-tuning LLMs on\nextensive NER data can boost their performance. However, training directly on\nexisting datasets neglects their inconsistent entity definitions and redundant\ndata, limiting LLMs to dataset-specific learning and hindering out-of-domain\nadaptation. To address this, we present B2NERD, a compact dataset designed to\nguide LLMs' generalization in Open NER under a universal entity taxonomy.\nB2NERD is refined from 54 existing English and Chinese datasets using a\ntwo-step process. First, we detect inconsistent entity definitions across\ndatasets and clarify them by distinguishable label names to construct a\nuniversal taxonomy of 400+ entity types. Second, we address redundancy using a\ndata pruning strategy that selects fewer samples with greater category and\nsemantic diversity. Comprehensive evaluation shows that B2NERD significantly\nenhances LLMs' Open NER capabilities. Our B2NER models, trained on B2NERD,\noutperform GPT-4 by 6.8-12.0 F1 points and surpass previous methods in 3\nout-of-domain benchmarks across 15 datasets and 6 languages. The data, models,\nand code are publicly available at https://github.com/UmeanNever/B2NER."
                },
                "authors": [
                    {
                        "name": "Yuming Yang"
                    },
                    {
                        "name": "Wantong Zhao"
                    },
                    {
                        "name": "Caishuang Huang"
                    },
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Huiyuan Zheng"
                    },
                    {
                        "name": "Yang Nan"
                    },
                    {
                        "name": "Yuran Wang"
                    },
                    {
                        "name": "Xueying Xu"
                    },
                    {
                        "name": "Kaixin Huang"
                    },
                    {
                        "name": "Yunke Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "Accepted at COLING 2025. Camera-ready version updated. Project page:\n  https://github.com/UmeanNever/B2NER",
                "arxiv_journal_ref": "Proceedings of the 31st International Conference on Computational\n  Linguistics (2025) 10902-10923",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11192v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11192v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03931v2",
                "updated": "2025-04-21T04:07:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    4,
                    7,
                    33,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-04T20:57:41Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    20,
                    57,
                    41,
                    4,
                    94,
                    0
                ],
                "title": "NAACL2025 Tutorial: Adaptation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NAACL2025 Tutorial: Adaptation of Large Language Models"
                },
                "summary": "This tutorial on adaptation of LLMs is designed to address the growing demand\nfor models that go beyond the static capabilities of generic LLMs by providing\nan overview of dynamic, domain-specific, and task-adaptive LLM adaptation\ntechniques. While general LLMs have demonstrated strong generalization across a\nvariety of tasks, they often struggle to perform well in specialized domains\nsuch as finance, healthcare, and code generation for underrepresented\nlanguages. Additionally, their static nature limits their ability to evolve\nwith the changing world, and they are often extremely large in size, making\nthem impractical and costly to deploy at scale. As a result, the adaptation of\nLLMs has drawn much attention since the birth of LLMs and is of core\nimportance, both for industry, which focuses on serving its targeted users, and\nacademia, which can greatly benefit from small but powerful LLMs. To address\nthis gap, this tutorial aims to provide an overview of the LLM adaptation\ntechniques. We start with an introduction to LLM adaptation, from both the data\nperspective and the model perspective. We then emphasize how the evaluation\nmetrics and benchmarks are different from other techniques. After establishing\nthe problems, we explore various adaptation techniques. We categorize\nadaptation techniques into two main families. The first is parametric knowledge\nadaptation, which focuses on updating the parametric knowledge within LLMs.\nAdditionally, we will discuss real-time adaptation techniques, including model\nediting, which allows LLMs to be updated dynamically in production\nenvironments. The second kind of adaptation is semi-parametric knowledge\nadaptation, where the goal is to update LLM parameters to better leverage\nexternal knowledge or tools through techniques like retrieval-augmented\ngeneration (RAG) and agent-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This tutorial on adaptation of LLMs is designed to address the growing demand\nfor models that go beyond the static capabilities of generic LLMs by providing\nan overview of dynamic, domain-specific, and task-adaptive LLM adaptation\ntechniques. While general LLMs have demonstrated strong generalization across a\nvariety of tasks, they often struggle to perform well in specialized domains\nsuch as finance, healthcare, and code generation for underrepresented\nlanguages. Additionally, their static nature limits their ability to evolve\nwith the changing world, and they are often extremely large in size, making\nthem impractical and costly to deploy at scale. As a result, the adaptation of\nLLMs has drawn much attention since the birth of LLMs and is of core\nimportance, both for industry, which focuses on serving its targeted users, and\nacademia, which can greatly benefit from small but powerful LLMs. To address\nthis gap, this tutorial aims to provide an overview of the LLM adaptation\ntechniques. We start with an introduction to LLM adaptation, from both the data\nperspective and the model perspective. We then emphasize how the evaluation\nmetrics and benchmarks are different from other techniques. After establishing\nthe problems, we explore various adaptation techniques. We categorize\nadaptation techniques into two main families. The first is parametric knowledge\nadaptation, which focuses on updating the parametric knowledge within LLMs.\nAdditionally, we will discuss real-time adaptation techniques, including model\nediting, which allows LLMs to be updated dynamically in production\nenvironments. The second kind of adaptation is semi-parametric knowledge\nadaptation, where the goal is to update LLM parameters to better leverage\nexternal knowledge or tools through techniques like retrieval-augmented\ngeneration (RAG) and agent-based systems."
                },
                "authors": [
                    {
                        "name": "Zixuan Ke"
                    },
                    {
                        "name": "Yifei Ming"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "NAACL2025 Tutorial",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14148v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14148v4",
                "updated": "2025-04-21T04:04:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    4,
                    4,
                    53,
                    0,
                    111,
                    0
                ],
                "published": "2024-10-18T03:34:32Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    34,
                    32,
                    4,
                    292,
                    0
                ],
                "title": "Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment"
                },
                "summary": "The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models."
                },
                "authors": [
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Yiyang Zhou"
                    },
                    {
                        "name": "Zhaorun Chen"
                    },
                    {
                        "name": "Gelei Deng"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "23 pages; Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14148v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14148v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14849v1",
                "updated": "2025-04-21T04:03:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    4,
                    3,
                    32,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T04:03:32Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    4,
                    3,
                    32,
                    0,
                    111,
                    0
                ],
                "title": "Language Models for Materials Discovery and Sustainability: Progress,\n  Challenges, and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models for Materials Discovery and Sustainability: Progress,\n  Challenges, and Opportunities"
                },
                "summary": "Significant advancements have been made in one of the most critical branches\nof artificial intelligence: natural language processing (NLP). These\nadvancements are exemplified by the remarkable success of OpenAI's GPT-3.5/4\nand the recent release of GPT-4.5, which have sparked a global surge of\ninterest akin to an NLP gold rush. In this article, we offer our perspective on\nthe development and application of NLP and large language models (LLMs) in\nmaterials science. We begin by presenting an overview of recent advancements in\nNLP within the broader scientific landscape, with a particular focus on their\nrelevance to materials science. Next, we examine how NLP can facilitate the\nunderstanding and design of novel materials and its potential integration with\nother methodologies. To highlight key challenges and opportunities, we delve\ninto three specific topics: (i) the limitations of LLMs and their implications\nfor materials science applications, (ii) the creation of a fully automated\nmaterials discovery pipeline, and (iii) the potential of GPT-like tools to\nsynthesize existing knowledge and aid in the design of sustainable materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant advancements have been made in one of the most critical branches\nof artificial intelligence: natural language processing (NLP). These\nadvancements are exemplified by the remarkable success of OpenAI's GPT-3.5/4\nand the recent release of GPT-4.5, which have sparked a global surge of\ninterest akin to an NLP gold rush. In this article, we offer our perspective on\nthe development and application of NLP and large language models (LLMs) in\nmaterials science. We begin by presenting an overview of recent advancements in\nNLP within the broader scientific landscape, with a particular focus on their\nrelevance to materials science. Next, we examine how NLP can facilitate the\nunderstanding and design of novel materials and its potential integration with\nother methodologies. To highlight key challenges and opportunities, we delve\ninto three specific topics: (i) the limitations of LLMs and their implications\nfor materials science applications, (ii) the creation of a fully automated\nmaterials discovery pipeline, and (iii) the potential of GPT-like tools to\nsynthesize existing knowledge and aid in the design of sustainable materials."
                },
                "authors": [
                    {
                        "name": "Zongrui Pei"
                    },
                    {
                        "name": "Junqi Yin"
                    },
                    {
                        "name": "Jiaxin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxin Zhang"
                },
                "author": "Jiaxin Zhang",
                "arxiv_comment": "89 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14845v1",
                "updated": "2025-04-21T03:56:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    56,
                    56,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T03:56:56Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    56,
                    56,
                    0,
                    111,
                    0
                ],
                "title": "Enhancing the Patent Matching Capability of Large Language Models via\n  the Memory Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the Patent Matching Capability of Large Language Models via\n  the Memory Graph"
                },
                "summary": "Intellectual Property (IP) management involves strategically protecting and\nutilizing intellectual assets to enhance organizational innovation,\ncompetitiveness, and value creation. Patent matching is a crucial task in\nintellectual property management, which facilitates the organization and\nutilization of patents. Existing models often rely on the emergent capabilities\nof Large Language Models (LLMs) and leverage them to identify related patents\ndirectly. However, these methods usually depend on matching keywords and\noverlook the hierarchical classification and categorical relationships of\npatents. In this paper, we propose MemGraph, a method that augments the patent\nmatching capabilities of LLMs by incorporating a memory graph derived from\ntheir parametric memory. Specifically, MemGraph prompts LLMs to traverse their\nmemory to identify relevant entities within patents, followed by attributing\nthese entities to corresponding ontologies. After traversing the memory graph,\nwe utilize extracted entities and ontologies to improve the capability of LLM\nin comprehending the semantics of patents. Experimental results on the\nPatentMatch dataset demonstrate the effectiveness of MemGraph, achieving a\n17.68% performance improvement over baseline LLMs. The further analysis\nhighlights the generalization ability of MemGraph across various LLMs, both\nin-domain and out-of-domain, and its capacity to enhance the internal reasoning\nprocesses of LLMs during patent matching. All data and codes are available at\nhttps://github.com/NEUIR/MemGraph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intellectual Property (IP) management involves strategically protecting and\nutilizing intellectual assets to enhance organizational innovation,\ncompetitiveness, and value creation. Patent matching is a crucial task in\nintellectual property management, which facilitates the organization and\nutilization of patents. Existing models often rely on the emergent capabilities\nof Large Language Models (LLMs) and leverage them to identify related patents\ndirectly. However, these methods usually depend on matching keywords and\noverlook the hierarchical classification and categorical relationships of\npatents. In this paper, we propose MemGraph, a method that augments the patent\nmatching capabilities of LLMs by incorporating a memory graph derived from\ntheir parametric memory. Specifically, MemGraph prompts LLMs to traverse their\nmemory to identify relevant entities within patents, followed by attributing\nthese entities to corresponding ontologies. After traversing the memory graph,\nwe utilize extracted entities and ontologies to improve the capability of LLM\nin comprehending the semantics of patents. Experimental results on the\nPatentMatch dataset demonstrate the effectiveness of MemGraph, achieving a\n17.68% performance improvement over baseline LLMs. The further analysis\nhighlights the generalization ability of MemGraph across various LLMs, both\nin-domain and out-of-domain, and its capacity to enhance the internal reasoning\nprocesses of LLMs during patent matching. All data and codes are available at\nhttps://github.com/NEUIR/MemGraph."
                },
                "authors": [
                    {
                        "name": "Qiushi Xiong"
                    },
                    {
                        "name": "Zhipeng Xu"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Mengjia Wang"
                    },
                    {
                        "name": "Zulong Chen"
                    },
                    {
                        "name": "Yue Sun"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Xiaohua Li"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14838v1",
                "updated": "2025-04-21T03:39:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    39,
                    33,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T03:39:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    39,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "Establishing Reliability Metrics for Reward Models in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Establishing Reliability Metrics for Reward Models in Large Language\n  Models"
                },
                "summary": "The reward model (RM) that represents human preferences plays a crucial role\nin optimizing the outputs of large language models (LLMs), e.g., through\nreinforcement learning from human feedback (RLHF) or rejection sampling.\nHowever, a long challenge for RM is its uncertain reliability, i.e., LLM\noutputs with higher rewards may not align with actual human preferences.\nCurrently, there is a lack of a convincing metric to quantify the reliability\nof RMs. To bridge this gap, we propose the \\textit{\\underline{R}eliable at\n\\underline{$\\eta$}} (RETA) metric, which directly measures the reliability of\nan RM by evaluating the average quality (scored by an oracle) of the top $\\eta$\nquantile responses assessed by an RM. On top of RETA, we present an integrated\nbenchmarking pipeline that allows anyone to evaluate their own RM without\nincurring additional Oracle labeling costs. Extensive experimental studies\ndemonstrate the superior stability of RETA metric, providing solid evaluations\nof the reliability of various publicly available and proprietary RMs. When\ndealing with an unreliable RM, we can use the RETA metric to identify the\noptimal quantile from which to select the responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reward model (RM) that represents human preferences plays a crucial role\nin optimizing the outputs of large language models (LLMs), e.g., through\nreinforcement learning from human feedback (RLHF) or rejection sampling.\nHowever, a long challenge for RM is its uncertain reliability, i.e., LLM\noutputs with higher rewards may not align with actual human preferences.\nCurrently, there is a lack of a convincing metric to quantify the reliability\nof RMs. To bridge this gap, we propose the \\textit{\\underline{R}eliable at\n\\underline{$\\eta$}} (RETA) metric, which directly measures the reliability of\nan RM by evaluating the average quality (scored by an oracle) of the top $\\eta$\nquantile responses assessed by an RM. On top of RETA, we present an integrated\nbenchmarking pipeline that allows anyone to evaluate their own RM without\nincurring additional Oracle labeling costs. Extensive experimental studies\ndemonstrate the superior stability of RETA metric, providing solid evaluations\nof the reliability of various publicly available and proprietary RMs. When\ndealing with an unreliable RM, we can use the RETA metric to identify the\noptimal quantile from which to select the responses."
                },
                "authors": [
                    {
                        "name": "Yizhou Chen"
                    },
                    {
                        "name": "Yawen Liu"
                    },
                    {
                        "name": "Xuesi Wang"
                    },
                    {
                        "name": "Qingtao Yu"
                    },
                    {
                        "name": "Guangda Huzhang"
                    },
                    {
                        "name": "Anxiang Zeng"
                    },
                    {
                        "name": "Han Yu"
                    },
                    {
                        "name": "Zhiming Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zhou"
                },
                "author": "Zhiming Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14835v1",
                "updated": "2025-04-21T03:32:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    32,
                    0,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T03:32:00Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    32,
                    0,
                    0,
                    111,
                    0
                ],
                "title": "Aligning Beam with Imbalanced Multi-modality: A Generative Federated\n  Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Beam with Imbalanced Multi-modality: A Generative Federated\n  Learning Approach"
                },
                "summary": "As vehicle intelligence advances, multi-modal sensing-aided communication\nemerges as a key enabler for reliable Vehicle-to-Everything (V2X) connectivity\nthrough precise environmental characterization. As centralized learning may\nsuffer from data privacy, model heterogeneity and communication overhead\nissues, federated learning (FL) has been introduced to support V2X. However,\nthe practical deployment of FL faces critical challenges: model performance\ndegradation from label imbalance across vehicles and training instability\ninduced by modality disparities in sensor-equipped agents. To overcome these\nlimitations, we propose a generative FL approach for beam selection (GFL4BS).\nOur solution features two core innovations: 1) An adaptive zero-shot\nmulti-modal generator coupled with spectral-regularized loss functions to\nenhance the expressiveness of synthetic data compensating for both label\nscarcity and missing modalities; 2) A hybrid training paradigm integrating\nfeature fusion with decentralized optimization to ensure training resilience\nwhile minimizing communication costs. Experimental evaluations demonstrate\nsignificant improvements over baselines achieving 16.2% higher accuracy than\nthe current state-of-the-art under severe label imbalance conditions while\nmaintaining over 70% successful rate even when two agents lack both LiDAR and\nRGB camera inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As vehicle intelligence advances, multi-modal sensing-aided communication\nemerges as a key enabler for reliable Vehicle-to-Everything (V2X) connectivity\nthrough precise environmental characterization. As centralized learning may\nsuffer from data privacy, model heterogeneity and communication overhead\nissues, federated learning (FL) has been introduced to support V2X. However,\nthe practical deployment of FL faces critical challenges: model performance\ndegradation from label imbalance across vehicles and training instability\ninduced by modality disparities in sensor-equipped agents. To overcome these\nlimitations, we propose a generative FL approach for beam selection (GFL4BS).\nOur solution features two core innovations: 1) An adaptive zero-shot\nmulti-modal generator coupled with spectral-regularized loss functions to\nenhance the expressiveness of synthetic data compensating for both label\nscarcity and missing modalities; 2) A hybrid training paradigm integrating\nfeature fusion with decentralized optimization to ensure training resilience\nwhile minimizing communication costs. Experimental evaluations demonstrate\nsignificant improvements over baselines achieving 16.2% higher accuracy than\nthe current state-of-the-art under severe label imbalance conditions while\nmaintaining over 70% successful rate even when two agents lack both LiDAR and\nRGB camera inputs."
                },
                "authors": [
                    {
                        "name": "Jiahui Liang"
                    },
                    {
                        "name": "Miaowen Wen"
                    },
                    {
                        "name": "Shuoyao Wang"
                    },
                    {
                        "name": "Yuxuan Liang"
                    },
                    {
                        "name": "Shijian Gao"
                    }
                ],
                "author_detail": {
                    "name": "Shijian Gao"
                },
                "author": "Shijian Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13592v2",
                "updated": "2025-04-21T03:29:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    29,
                    14,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-18T09:52:12Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    9,
                    52,
                    12,
                    4,
                    108,
                    0
                ],
                "title": "Improving Generalization in Intent Detection: GRPO with Reward-Based\n  Curriculum Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Generalization in Intent Detection: GRPO with Reward-Based\n  Curriculum Sampling"
                },
                "summary": "Intent detection, a critical component in task-oriented dialogue (TOD)\nsystems, faces significant challenges in adapting to the rapid influx of\nintegrable tools with complex interrelationships. Existing approaches, such as\nzero-shot reformulations and LLM-based dynamic recognition, struggle with\nperformance degradation when encountering unseen intents, leading to erroneous\ntask routing. To enhance the model's generalization performance on unseen\ntasks, we employ Reinforcement Learning (RL) combined with a Reward-based\nCurriculum Sampling (RCS) during Group Relative Policy Optimization (GRPO)\ntraining in intent detection tasks. Experiments demonstrate that RL-trained\nmodels substantially outperform supervised fine-tuning (SFT) baselines in\ngeneralization. Besides, the introduction of the RCS, significantly bolsters\nthe effectiveness of RL in intent detection by focusing the model on\nchallenging cases during training. Moreover, incorporating Chain-of-Thought\n(COT) processes in RL notably improves generalization in complex intent\ndetection tasks, underscoring the importance of thought in challenging\nscenarios. This work advances the generalization of intent detection tasks,\noffering practical insights for deploying adaptable dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent detection, a critical component in task-oriented dialogue (TOD)\nsystems, faces significant challenges in adapting to the rapid influx of\nintegrable tools with complex interrelationships. Existing approaches, such as\nzero-shot reformulations and LLM-based dynamic recognition, struggle with\nperformance degradation when encountering unseen intents, leading to erroneous\ntask routing. To enhance the model's generalization performance on unseen\ntasks, we employ Reinforcement Learning (RL) combined with a Reward-based\nCurriculum Sampling (RCS) during Group Relative Policy Optimization (GRPO)\ntraining in intent detection tasks. Experiments demonstrate that RL-trained\nmodels substantially outperform supervised fine-tuning (SFT) baselines in\ngeneralization. Besides, the introduction of the RCS, significantly bolsters\nthe effectiveness of RL in intent detection by focusing the model on\nchallenging cases during training. Moreover, incorporating Chain-of-Thought\n(COT) processes in RL notably improves generalization in complex intent\ndetection tasks, underscoring the importance of thought in challenging\nscenarios. This work advances the generalization of intent detection tasks,\noffering practical insights for deploying adaptable dialogue systems."
                },
                "authors": [
                    {
                        "name": "Zihao Feng"
                    },
                    {
                        "name": "Xiaoxue Wang"
                    },
                    {
                        "name": "Ziwei Bai"
                    },
                    {
                        "name": "Donghang Su"
                    },
                    {
                        "name": "Bowen Wu"
                    },
                    {
                        "name": "Qun Yu"
                    },
                    {
                        "name": "Baoxun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Baoxun Wang"
                },
                "author": "Baoxun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13775v2",
                "updated": "2025-04-21T03:12:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    12,
                    50,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-18T16:22:41Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    22,
                    41,
                    4,
                    108,
                    0
                ],
                "title": "BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of\n  Black-box Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of\n  Black-box Large Language Models"
                },
                "summary": "Previous insertion-based and paraphrase-based backdoors have achieved great\nsuccess in attack efficacy, but they ignore the text quality and semantic\nconsistency between poisoned and clean texts. Although recent studies introduce\nLLMs to generate poisoned texts and improve the stealthiness, semantic\nconsistency, and text quality, their hand-crafted prompts rely on expert\nexperiences, facing significant challenges in prompt adaptability and attack\nperformance after defenses. In this paper, we propose a novel backdoor attack\nbased on adaptive optimization mechanism of black-box large language models\n(BadApex), which leverages a black-box LLM to generate poisoned text through a\nrefined prompt. Specifically, an Adaptive Optimization Mechanism is designed to\nrefine an initial prompt iteratively using the generation and modification\nagents. The generation agent generates the poisoned text based on the initial\nprompt. Then the modification agent evaluates the quality of the poisoned text\nand refines a new prompt. After several iterations of the above process, the\nrefined prompt is used to generate poisoned texts through LLMs. We conduct\nextensive experiments on three dataset with six backdoor attacks and two\ndefenses. Extensive experimental results demonstrate that BadApex significantly\noutperforms state-of-the-art attacks. It improves prompt adaptability, semantic\nconsistency, and text quality. Furthermore, when two defense methods are\napplied, the average attack success rate (ASR) still up to 96.75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous insertion-based and paraphrase-based backdoors have achieved great\nsuccess in attack efficacy, but they ignore the text quality and semantic\nconsistency between poisoned and clean texts. Although recent studies introduce\nLLMs to generate poisoned texts and improve the stealthiness, semantic\nconsistency, and text quality, their hand-crafted prompts rely on expert\nexperiences, facing significant challenges in prompt adaptability and attack\nperformance after defenses. In this paper, we propose a novel backdoor attack\nbased on adaptive optimization mechanism of black-box large language models\n(BadApex), which leverages a black-box LLM to generate poisoned text through a\nrefined prompt. Specifically, an Adaptive Optimization Mechanism is designed to\nrefine an initial prompt iteratively using the generation and modification\nagents. The generation agent generates the poisoned text based on the initial\nprompt. Then the modification agent evaluates the quality of the poisoned text\nand refines a new prompt. After several iterations of the above process, the\nrefined prompt is used to generate poisoned texts through LLMs. We conduct\nextensive experiments on three dataset with six backdoor attacks and two\ndefenses. Extensive experimental results demonstrate that BadApex significantly\noutperforms state-of-the-art attacks. It improves prompt adaptability, semantic\nconsistency, and text quality. Furthermore, when two defense methods are\napplied, the average attack success rate (ASR) still up to 96.75%."
                },
                "authors": [
                    {
                        "name": "Zhengxian Wu"
                    },
                    {
                        "name": "Juan Wen"
                    },
                    {
                        "name": "Wanli Peng"
                    },
                    {
                        "name": "Ziwei Zhang"
                    },
                    {
                        "name": "Yinghan Zhou"
                    },
                    {
                        "name": "Yiming Xue"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Xue"
                },
                "author": "Yiming Xue",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14815v1",
                "updated": "2025-04-21T02:44:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    2,
                    44,
                    59,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T02:44:59Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    2,
                    44,
                    59,
                    0,
                    111,
                    0
                ],
                "title": "What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale"
                },
                "summary": "Diffusion models (DMs) have revolutionized text-to-image generation, enabling\nthe creation of highly realistic and customized images from text prompts. With\nthe rise of parameter-efficient fine-tuning (PEFT) techniques like LoRA, users\ncan now customize powerful pre-trained models using minimal computational\nresources. However, the widespread sharing of fine-tuned DMs on open platforms\nraises growing ethical and legal concerns, as these models may inadvertently or\ndeliberately generate sensitive or unauthorized content, such as copyrighted\nmaterial, private individuals, or harmful content. Despite the increasing\nregulatory attention on generative AI, there are currently no practical tools\nfor systematically auditing these models before deployment. In this paper, we\naddress the problem of concept auditing: determining whether a fine-tuned DM\nhas learned to generate a specific target concept. Existing approaches\ntypically rely on prompt-based input crafting and output-based image\nclassification but suffer from critical limitations, including prompt\nuncertainty, concept drift, and poor scalability. To overcome these challenges,\nwe introduce Prompt-Agnostic Image-Free Auditing (PAIA), a novel, model-centric\nconcept auditing framework. By treating the DM as the object of inspection,\nPAIA enables direct analysis of internal model behavior, bypassing the need for\noptimized prompts or generated images. We evaluate PAIA on 320 controlled model\nand 690 real-world community models sourced from a public DM sharing platform.\nPAIA achieves over 90% detection accuracy while reducing auditing time by\n18-40x compared to existing baselines. To our knowledge, PAIA is the first\nscalable and practical solution for pre-deployment concept auditing of\ndiffusion models, providing a practical foundation for safer and more\ntransparent diffusion model sharing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have revolutionized text-to-image generation, enabling\nthe creation of highly realistic and customized images from text prompts. With\nthe rise of parameter-efficient fine-tuning (PEFT) techniques like LoRA, users\ncan now customize powerful pre-trained models using minimal computational\nresources. However, the widespread sharing of fine-tuned DMs on open platforms\nraises growing ethical and legal concerns, as these models may inadvertently or\ndeliberately generate sensitive or unauthorized content, such as copyrighted\nmaterial, private individuals, or harmful content. Despite the increasing\nregulatory attention on generative AI, there are currently no practical tools\nfor systematically auditing these models before deployment. In this paper, we\naddress the problem of concept auditing: determining whether a fine-tuned DM\nhas learned to generate a specific target concept. Existing approaches\ntypically rely on prompt-based input crafting and output-based image\nclassification but suffer from critical limitations, including prompt\nuncertainty, concept drift, and poor scalability. To overcome these challenges,\nwe introduce Prompt-Agnostic Image-Free Auditing (PAIA), a novel, model-centric\nconcept auditing framework. By treating the DM as the object of inspection,\nPAIA enables direct analysis of internal model behavior, bypassing the need for\noptimized prompts or generated images. We evaluate PAIA on 320 controlled model\nand 690 real-world community models sourced from a public DM sharing platform.\nPAIA achieves over 90% detection accuracy while reducing auditing time by\n18-40x compared to existing baselines. To our knowledge, PAIA is the first\nscalable and practical solution for pre-deployment concept auditing of\ndiffusion models, providing a practical foundation for safer and more\ntransparent diffusion model sharing."
                },
                "authors": [
                    {
                        "name": "Xiaoyong Yuan"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Linke Guo"
                    },
                    {
                        "name": "Lan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lan Zhang"
                },
                "author": "Lan Zhang",
                "arxiv_comment": "17 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14810v1",
                "updated": "2025-04-21T02:25:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    2,
                    25,
                    3,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T02:25:03Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    2,
                    25,
                    3,
                    0,
                    111,
                    0
                ],
                "title": "DONOD: Robust and Generalizable Instruction Fine-Tuning for LLMs via\n  Model-Intrinsic Dataset Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DONOD: Robust and Generalizable Instruction Fine-Tuning for LLMs via\n  Model-Intrinsic Dataset Pruning"
                },
                "summary": "Ad-hoc instruction fine-tuning of large language models (LLMs) is widely\nadopted for domain-specific adaptation. While domain-specific supervised\nfine-tuning (SFT) is effective and efficient, it often weakens cross-domain\ngeneralization and struggles with noisy training data. To address these\nchallenges, we propose DONOD, a lightweight model-intrinsic data pruning\nmethod. Our approach evaluates data using two model-parameter-based metrics:\nDelta of Norm (DON), which captures the cumulative influence on model weights,\nand Norm of Delta (NOD), which quantifies weight instability. Moreover, by\nemploying the Technique for Order of Preference by Similarity to Ideal Solution\n(TOPSIS) algorithm, we effectively filter noisy, unlearnable, and\ngeneralization-harming samples without relying on auxiliary models during the\nSFT process. Experiments on mathematical tasks demonstrate that data selected\nby DONOD achieve superior fine-tuning efficiency and improved robustness\nagainst noisy data. By filtering out 70% of the full dataset, we improve\ntarget-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile,\nour selected data present superior cross-architecture generalization. Data\npruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger\nmodels (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD\ndemonstrates comparable or superior performance while remaining\ndataset-agnostic, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ad-hoc instruction fine-tuning of large language models (LLMs) is widely\nadopted for domain-specific adaptation. While domain-specific supervised\nfine-tuning (SFT) is effective and efficient, it often weakens cross-domain\ngeneralization and struggles with noisy training data. To address these\nchallenges, we propose DONOD, a lightweight model-intrinsic data pruning\nmethod. Our approach evaluates data using two model-parameter-based metrics:\nDelta of Norm (DON), which captures the cumulative influence on model weights,\nand Norm of Delta (NOD), which quantifies weight instability. Moreover, by\nemploying the Technique for Order of Preference by Similarity to Ideal Solution\n(TOPSIS) algorithm, we effectively filter noisy, unlearnable, and\ngeneralization-harming samples without relying on auxiliary models during the\nSFT process. Experiments on mathematical tasks demonstrate that data selected\nby DONOD achieve superior fine-tuning efficiency and improved robustness\nagainst noisy data. By filtering out 70% of the full dataset, we improve\ntarget-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile,\nour selected data present superior cross-architecture generalization. Data\npruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger\nmodels (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD\ndemonstrates comparable or superior performance while remaining\ndataset-agnostic, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Jucheng Hu"
                    },
                    {
                        "name": "Surong Yang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Lijun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Lijun Wu"
                },
                "author": "Lijun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07582v2",
                "updated": "2025-04-21T02:22:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    2,
                    22,
                    6,
                    0,
                    111,
                    0
                ],
                "published": "2024-10-10T03:31:16Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    31,
                    16,
                    3,
                    284,
                    0
                ],
                "title": "Detecting Training Data of Large Language Models via Expectation\n  Maximization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Training Data of Large Language Models via Expectation\n  Maximization"
                },
                "summary": "The advancement of large language models has grown parallel to the opacity of\ntheir training data. Membership inference attacks (MIAs) aim to determine\nwhether specific data was used to train a model. They offer valuable insights\ninto detecting data contamination and ensuring compliance with privacy and\ncopyright standards. However, MIA for LLMs is challenging due to the massive\nscale of training data and the inherent ambiguity of membership in texts.\nMoreover, creating realistic MIA evaluation benchmarks is difficult as training\nand test data distributions are often unknown. We introduce EM-MIA, a novel\nmembership inference method that iteratively refines membership scores and\nprefix scores via an expectation-maximization algorithm. Our approach leverages\nthe observation that these scores can improve each other: membership scores\nhelp identify effective prefixes for detecting training data, while prefix\nscores help determine membership. As a result, EM-MIA achieves state-of-the-art\nresults on WikiMIA. To enable comprehensive evaluation, we introduce OLMoMIA, a\nbenchmark built from OLMo resources, which allows controlling task difficulty\nthrough varying degrees of overlap between training and test data\ndistributions. Our experiments demonstrate EM-MIA is robust across different\nscenarios while also revealing fundamental limitations of current MIA\napproaches when member and non-member distributions are nearly identical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models has grown parallel to the opacity of\ntheir training data. Membership inference attacks (MIAs) aim to determine\nwhether specific data was used to train a model. They offer valuable insights\ninto detecting data contamination and ensuring compliance with privacy and\ncopyright standards. However, MIA for LLMs is challenging due to the massive\nscale of training data and the inherent ambiguity of membership in texts.\nMoreover, creating realistic MIA evaluation benchmarks is difficult as training\nand test data distributions are often unknown. We introduce EM-MIA, a novel\nmembership inference method that iteratively refines membership scores and\nprefix scores via an expectation-maximization algorithm. Our approach leverages\nthe observation that these scores can improve each other: membership scores\nhelp identify effective prefixes for detecting training data, while prefix\nscores help determine membership. As a result, EM-MIA achieves state-of-the-art\nresults on WikiMIA. To enable comprehensive evaluation, we introduce OLMoMIA, a\nbenchmark built from OLMo resources, which allows controlling task difficulty\nthrough varying degrees of overlap between training and test data\ndistributions. Our experiments demonstrate EM-MIA is robust across different\nscenarios while also revealing fundamental limitations of current MIA\napproaches when member and non-member distributions are nearly identical."
                },
                "authors": [
                    {
                        "name": "Gyuwan Kim"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Evangelia Spiliopoulou"
                    },
                    {
                        "name": "Jie Ma"
                    },
                    {
                        "name": "Miguel Ballesteros"
                    },
                    {
                        "name": "William Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "William Yang Wang"
                },
                "author": "William Yang Wang",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]