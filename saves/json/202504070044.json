[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.01380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v2",
                "updated": "2025-04-03T13:28:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    28,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity ."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02533v2",
                "updated": "2025-04-04T11:12:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    12,
                    18,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-03T12:36:01Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    36,
                    1,
                    3,
                    93,
                    0
                ],
                "title": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions"
                },
                "summary": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based ISA extensibility. Our implementation\nshows $30\\times$ to $84\\times$ performance improvement when operating on 8-bit\ndata over the same system with a traditional cache when executing a worst-case\n32-bit CNN workload, with only $41.3\\%$ area overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based ISA extensibility. Our implementation\nshows $30\\times$ to $84\\times$ performance improvement when operating on 8-bit\ndata over the same system with a traditional cache when executing a worst-case\n32-bit CNN workload, with only $41.3\\%$ area overhead."
                },
                "authors": [
                    {
                        "name": "Vincenzo Petrolo"
                    },
                    {
                        "name": "Flavia Guella"
                    },
                    {
                        "name": "Michele Caon"
                    },
                    {
                        "name": "Pasquale Davide Schiavone"
                    },
                    {
                        "name": "Guido Masera"
                    },
                    {
                        "name": "Maurizio Martina"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Martina"
                },
                "author": "Maurizio Martina",
                "arxiv_comment": "6 pages, 4 figures, accepted at the Design Automation Conference\n  (DAC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v1",
                "updated": "2025-04-03T09:58:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02268v1",
                "updated": "2025-04-03T04:27:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T04:27:02Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "title": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data"
                },
                "summary": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Justin Cechmanek"
                    },
                    {
                        "name": "Tyler Hutcherson"
                    },
                    {
                        "name": "Srijith Rajamohan"
                    },
                    {
                        "name": "Jen Agarwal"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    },
                    {
                        "name": "Manvinder Singh"
                    },
                    {
                        "name": "Benoit Dion"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Dion"
                },
                "arxiv_affiliation": "Redis",
                "author": "Benoit Dion",
                "arxiv_comment": "Initial study on embedding fine tuning for semantic cache. It also\n  explores synthetic data. Total pages are 12, including refrences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02220v1",
                "updated": "2025-04-03T02:24:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T02:24:21Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "title": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations"
                },
                "summary": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Helen Mayer"
                    },
                    {
                        "name": "James Richards"
                    }
                ],
                "author_detail": {
                    "name": "James Richards"
                },
                "author": "James Richards",
                "arxiv_comment": "International Conference on Computing Technologies and Artificial\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01281v2",
                "updated": "2025-04-03T01:23:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    1,
                    23,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T01:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    16,
                    10,
                    2,
                    92,
                    0
                ],
                "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding"
                },
                "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22875v2",
                "updated": "2025-04-02T18:51:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    18,
                    51,
                    53,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-28T21:02:32Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    2,
                    32,
                    4,
                    87,
                    0
                ],
                "title": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading"
                },
                "summary": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing."
                },
                "authors": [
                    {
                        "name": "Hang Liu"
                    },
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yinzhi Wang"
                },
                "author": "Yinzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v3",
                "updated": "2025-04-04T13:27:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    27,
                    49,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01582v1",
                "updated": "2025-04-02T10:38:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T10:38:25Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "title": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors"
                },
                "summary": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues."
                },
                "authors": [
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Xiaoxuan Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Zhihang Tan"
                    },
                    {
                        "name": "Wenbo Xu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Zhenyuan Wang"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v4",
                "updated": "2025-04-02T04:57:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    57,
                    15,
                    2,
                    92,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v5",
                "updated": "2025-04-02T01:58:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    58,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v1",
                "updated": "2025-04-02T01:49:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "11 pages, 6 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01157v1",
                "updated": "2025-04-01T19:48:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T19:48:17Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "title": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB"
                },
                "summary": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden."
                },
                "authors": [
                    {
                        "name": "Anas Dorbani"
                    },
                    {
                        "name": "Sunny Yasser"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Amine Mhedhbi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mhedhbi"
                },
                "author": "Amine Mhedhbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01104v1",
                "updated": "2025-04-01T18:21:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:21:43Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "title": "Fundamentals of Caching Layered Data objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamentals of Caching Layered Data objects"
                },
                "summary": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers."
                },
                "authors": [
                    {
                        "name": "Agrim Bari"
                    },
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "arxiv_comment": "An abridged version of this paper has been accepted at the 45th IEEE\n  International Conference on Distributed Computing Systems (ICDCS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01084v1",
                "updated": "2025-04-01T18:00:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:00:48Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "title": "Surfactants Screen Slide Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surfactants Screen Slide Electrification"
                },
                "summary": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop."
                },
                "authors": [
                    {
                        "name": "Xiaomei Li"
                    },
                    {
                        "name": "Zhongyuan Ni"
                    },
                    {
                        "name": "Xiaoteng Zhou"
                    },
                    {
                        "name": "Lisa S. Bauer"
                    },
                    {
                        "name": "Diego Diaz"
                    },
                    {
                        "name": "Gabriele Schäfer"
                    },
                    {
                        "name": "Hans-Jürgen Butt"
                    }
                ],
                "author_detail": {
                    "name": "Hans-Jürgen Butt"
                },
                "author": "Hans-Jürgen Butt",
                "arxiv_comment": "13 pages, 4 figures, 50 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00999v1",
                "updated": "2025-04-01T17:39:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:39:19Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization"
                },
                "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ."
                },
                "authors": [
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Luyuan Zhang"
                    },
                    {
                        "name": "Zedong Wang"
                    },
                    {
                        "name": "Juanxi Tian"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Qingsong Xie"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Haoqian Wang"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "CVPR2025 (in process for more analysis and extension)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v1",
                "updated": "2025-04-01T17:08:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v2",
                "updated": "2025-04-01T14:21:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    21,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00726v1",
                "updated": "2025-04-01T12:34:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T12:34:58Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "title": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning"
                },
                "summary": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL."
                },
                "authors": [
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Weibo He"
                    },
                    {
                        "name": "Wanglei Feng"
                    },
                    {
                        "name": "Zhenyu Wen"
                    },
                    {
                        "name": "Bin Qian"
                    },
                    {
                        "name": "Blesson Varghese"
                    }
                ],
                "author_detail": {
                    "name": "Blesson Varghese"
                },
                "author": "Blesson Varghese",
                "arxiv_comment": "Poster accepted at IEEE ICDCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00557v1",
                "updated": "2025-04-01T09:10:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T09:10:32Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features"
                },
                "summary": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity."
                },
                "authors": [
                    {
                        "name": "Jewon Lee"
                    },
                    {
                        "name": "Ki-Ung Song"
                    },
                    {
                        "name": "Seungmin Yang"
                    },
                    {
                        "name": "Donguk Lim"
                    },
                    {
                        "name": "Jaeyeon Kim"
                    },
                    {
                        "name": "Wooksu Shin"
                    },
                    {
                        "name": "Bo-Kyeong Kim"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "arxiv_comment": "accepted at CVPR 2025 Workshop on ELVM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00474v1",
                "updated": "2025-04-01T07:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T07:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "title": "High specific impulse electrospray propulsion with small capillary\n  emitters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High specific impulse electrospray propulsion with small capillary\n  emitters"
                },
                "summary": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$."
                },
                "authors": [
                    {
                        "name": "Manel Caballero-Pérez"
                    },
                    {
                        "name": "Marc Galobardes-Esteban"
                    },
                    {
                        "name": "Manuel Gamero-Castaño"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Gamero-Castaño"
                },
                "author": "Manuel Gamero-Castaño",
                "arxiv_comment": "29 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v1",
                "updated": "2025-03-31T17:37:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v1",
                "updated": "2025-03-31T12:32:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24000v1",
                "updated": "2025-03-31T12:23:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:23:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving"
                },
                "summary": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}."
                },
                "authors": [
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen",
                "arxiv_comment": "21 pages, 18 figures, published to MLSys2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23988v1",
                "updated": "2025-03-31T11:58:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:58:37Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "title": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments"
                },
                "summary": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups."
                },
                "authors": [
                    {
                        "name": "Elayne Lemos"
                    },
                    {
                        "name": "Rodrigo Oliveira"
                    },
                    {
                        "name": "Jairson Rodrigues"
                    },
                    {
                        "name": "Rosalvo F. Oliveira Neto"
                    }
                ],
                "author_detail": {
                    "name": "Rosalvo F. Oliveira Neto"
                },
                "author": "Rosalvo F. Oliveira Neto",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68U01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; I.2.0; B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v1",
                "updated": "2025-03-31T11:13:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18334v2",
                "updated": "2025-03-31T10:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    28,
                    4,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-24T04:32:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models"
                },
                "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability."
                },
                "authors": [
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Tianming Sha"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23897v1",
                "updated": "2025-03-31T09:46:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:46:56Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "title": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model"
                },
                "summary": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released."
                },
                "authors": [
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Pichao Wang"
                    },
                    {
                        "name": "Bihan Wen"
                    },
                    {
                        "name": "Jian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wang"
                },
                "author": "Jian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v4",
                "updated": "2025-03-31T03:28:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    3,
                    28,
                    44,
                    0,
                    90,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v2",
                "updated": "2025-03-31T02:19:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    2,
                    19,
                    29,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v2",
                "updated": "2025-03-30T11:14:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    14,
                    17,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages,fix figure mistake(inv/fwd skipping) in fig2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23397v1",
                "updated": "2025-03-30T11:09:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T11:09:06Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "title": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update"
                },
                "summary": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads."
                },
                "authors": [
                    {
                        "name": "Yuan Chen"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Wenhai Li"
                    },
                    {
                        "name": "Lingfeng Deng"
                    }
                ],
                "author_detail": {
                    "name": "Lingfeng Deng"
                },
                "author": "Lingfeng Deng",
                "arxiv_comment": "14 pages,17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23388v1",
                "updated": "2025-03-30T10:34:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T10:34:45Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation"
                },
                "summary": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC."
                },
                "authors": [
                    {
                        "name": "Fanding Huang"
                    },
                    {
                        "name": "Jingyan Jiang"
                    },
                    {
                        "name": "Qinting Jiang"
                    },
                    {
                        "name": "Hebei Li"
                    },
                    {
                        "name": "Faisal Nadeem Khan"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v2",
                "updated": "2025-03-30T09:46:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    46,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v2",
                "updated": "2025-03-30T09:19:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    19,
                    53,
                    6,
                    89,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v1",
                "updated": "2025-03-30T08:51:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12820v2",
                "updated": "2025-03-30T08:13:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    13,
                    50,
                    6,
                    89,
                    0
                ],
                "published": "2024-07-01T13:05:42Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    13,
                    5,
                    42,
                    0,
                    183,
                    0
                ],
                "title": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference"
                },
                "summary": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding."
                },
                "authors": [
                    {
                        "name": "Hailin Zhang"
                    },
                    {
                        "name": "Xiaodong Ji"
                    },
                    {
                        "name": "Yilin Chen"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23294v1",
                "updated": "2025-03-30T03:20:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T03:20:34Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "title": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference"
                },
                "summary": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets."
                },
                "authors": [
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Xiaoyang Qu"
                    },
                    {
                        "name": "Jiguang Wan"
                    },
                    {
                        "name": "Jianzong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianzong Wang"
                },
                "author": "Jianzong Wang",
                "arxiv_comment": "Accepted by the Design, Automation, and Test in Europe 2025 (DATE\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v2",
                "updated": "2025-03-30T02:45:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    2,
                    45,
                    0,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18278v2",
                "updated": "2025-03-29T23:00:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    23,
                    0,
                    27,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-24T01:47:26Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model"
                },
                "summary": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach."
                },
                "authors": [
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Lingyi Huang"
                    },
                    {
                        "name": "Yu Gong"
                    },
                    {
                        "name": "Chendi Li"
                    },
                    {
                        "name": "Jinghua Yan"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Ponnuswamy Sadayappan"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Bo Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Bo Yuan"
                },
                "author": "Bo Yuan",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v2",
                "updated": "2025-03-29T04:43:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    4,
                    43,
                    11,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v1",
                "updated": "2025-03-29T01:06:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22796v1",
                "updated": "2025-03-28T18:00:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T18:00:12Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "title": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers"
                },
                "summary": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity."
                },
                "authors": [
                    {
                        "name": "Hanling Zhang"
                    },
                    {
                        "name": "Rundong Su"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Mingzhu Shen Yibo Fan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v3",
                "updated": "2025-03-28T16:15:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    15,
                    19,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v3",
                "updated": "2025-03-28T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    11,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22329v1",
                "updated": "2025-03-28T11:08:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:08:34Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "title": "A Refined Analysis of Massive Activations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Refined Analysis of Massive Activations in LLMs"
                },
                "summary": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations."
                },
                "authors": [
                    {
                        "name": "Louis Owen"
                    },
                    {
                        "name": "Nilabhra Roy Chowdhury"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Fabian Güra"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Güra"
                },
                "author": "Fabian Güra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22196v1",
                "updated": "2025-03-28T07:26:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T07:26:37Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "title": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices"
                },
                "summary": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Renshou Wu"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxin Chen"
                },
                "author": "Xiaoxin Chen",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22017v1",
                "updated": "2025-03-27T22:16:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T22:16:57Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "title": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype"
                },
                "summary": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device."
                },
                "authors": [
                    {
                        "name": "Jianping Zeng"
                    },
                    {
                        "name": "Shuyi Pei"
                    },
                    {
                        "name": "Da Zhang"
                    },
                    {
                        "name": "Yuchen Zhou"
                    },
                    {
                        "name": "Amir Beygi"
                    },
                    {
                        "name": "Xuebin Yao"
                    },
                    {
                        "name": "Ramdas Kachare"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Zongwang Li"
                    },
                    {
                        "name": "Marie Nguyen"
                    },
                    {
                        "name": "Rekha Pitchumani"
                    },
                    {
                        "name": "Yang Soek Ki"
                    },
                    {
                        "name": "Changhee Jung"
                    }
                ],
                "author_detail": {
                    "name": "Changhee Jung"
                },
                "author": "Changhee Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v2",
                "updated": "2025-03-27T17:48:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    48,
                    14,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v1",
                "updated": "2025-03-27T17:37:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_comment": "34 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v4",
                "updated": "2025-03-27T15:21:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    21,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17922v2",
                "updated": "2025-03-27T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    11,
                    37,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-23T03:36:52Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    36,
                    52,
                    6,
                    82,
                    0
                ],
                "title": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference"
                },
                "summary": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Youhui Zuo"
                    },
                    {
                        "name": "Sibo Wei"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17038v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17038v3",
                "updated": "2025-03-27T12:14:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    14,
                    56,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-21T10:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation"
                },
                "summary": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference."
                },
                "authors": [
                    {
                        "name": "Ashutosh Pradhan"
                    },
                    {
                        "name": "Daniele Ottaviano"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Haozheng Huang"
                    },
                    {
                        "name": "Alexander Zuepke"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 31st IEEE\n  Real-Time and Embedded Technology and Applications Symposium (RTAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17038v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17038v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; C.4; D.4.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v3",
                "updated": "2025-03-27T11:46:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    46,
                    22,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT."
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v2",
                "updated": "2025-03-27T09:53:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    53,
                    15,
                    3,
                    86,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Gürkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_doi": "10.1109/TVLSI.2025.3527225",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2025.3527225",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.17606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems (\n  Volume: 33, Issue: 4, April 2025)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v2",
                "updated": "2025-03-27T07:02:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    2,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding"
                },
                "summary": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16302v2",
                "updated": "2025-03-26T15:08:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    8,
                    12,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-20T16:23:44Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Vecset Diffusion Model for Fast Shape Generation"
                },
                "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM."
                },
                "authors": [
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Yunfei Zhao"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Fuyun Wang"
                    },
                    {
                        "name": "Huiwen Shi"
                    },
                    {
                        "name": "Xianghui Yang"
                    },
                    {
                        "name": "Qingxiang Lin"
                    },
                    {
                        "name": "Jingwei Huang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v3",
                "updated": "2025-03-26T13:59:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    59,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs"
                },
                "summary": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yunzhe Li"
                    },
                    {
                        "name": "Zhifeng Jiang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_doi": "10.1145/3710848.3710863",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3710848.3710863",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 19 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20481v1",
                "updated": "2025-03-26T12:10:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:10:53Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "title": "Analyzing Modern NVIDIA GPU cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Modern NVIDIA GPU cores"
                },
                "summary": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area."
                },
                "authors": [
                    {
                        "name": "Rodrigo Huerta"
                    },
                    {
                        "name": "Mojtaba Abaie Shoushtary"
                    },
                    {
                        "name": "José-Lorenzo Cruz"
                    },
                    {
                        "name": "Antonio González"
                    }
                ],
                "author_detail": {
                    "name": "Antonio González"
                },
                "author": "Antonio González",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v2",
                "updated": "2025-03-26T11:08:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    8,
                    20,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20174v1",
                "updated": "2025-03-26T02:58:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T02:58:41Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "title": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration"
                },
                "summary": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Shihao Zhou"
                    },
                    {
                        "name": "Dayu Li"
                    },
                    {
                        "name": "Jinshan Pan"
                    },
                    {
                        "name": "Juncheng Zhou"
                    },
                    {
                        "name": "Jinglei Shi"
                    },
                    {
                        "name": "Jufeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jufeng Yang"
                },
                "author": "Jufeng Yang",
                "arxiv_comment": "11 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v2",
                "updated": "2025-03-26T01:58:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    1,
                    58,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v2",
                "updated": "2025-03-25T17:56:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    56,
                    1,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation"
                },
                "summary": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19950v1",
                "updated": "2025-03-25T16:24:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T16:24:45Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation"
                },
                "summary": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV."
                },
                "authors": [
                    {
                        "name": "Han Chen"
                    },
                    {
                        "name": "Zicong Jiang"
                    },
                    {
                        "name": "Zining Zhang"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Pingyi Luo"
                    },
                    {
                        "name": "Mian Lu"
                    },
                    {
                        "name": "Yuqiang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuqiang Chen"
                },
                "author": "Yuqiang Chen",
                "arxiv_comment": "Accepted by ICLR 2025 Workshop on Sparsity in LLMs (SLLM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19786v1",
                "updated": "2025-03-25T15:52:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:52:34Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "title": "Gemma 3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gemma 3 Technical Report"
                },
                "summary": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community."
                },
                "authors": [
                    {
                        "name": "Gemma Team"
                    },
                    {
                        "name": "Aishwarya Kamath"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Shreya Pathak"
                    },
                    {
                        "name": "Nino Vieillard"
                    },
                    {
                        "name": "Ramona Merhej"
                    },
                    {
                        "name": "Sarah Perrin"
                    },
                    {
                        "name": "Tatiana Matejovicova"
                    },
                    {
                        "name": "Alexandre Ramé"
                    },
                    {
                        "name": "Morgane Rivière"
                    },
                    {
                        "name": "Louis Rouillard"
                    },
                    {
                        "name": "Thomas Mesnard"
                    },
                    {
                        "name": "Geoffrey Cideron"
                    },
                    {
                        "name": "Jean-bastien Grill"
                    },
                    {
                        "name": "Sabela Ramos"
                    },
                    {
                        "name": "Edouard Yvinec"
                    },
                    {
                        "name": "Michelle Casbon"
                    },
                    {
                        "name": "Etienne Pot"
                    },
                    {
                        "name": "Ivo Penchev"
                    },
                    {
                        "name": "Gaël Liu"
                    },
                    {
                        "name": "Francesco Visin"
                    },
                    {
                        "name": "Kathleen Kenealy"
                    },
                    {
                        "name": "Lucas Beyer"
                    },
                    {
                        "name": "Xiaohai Zhai"
                    },
                    {
                        "name": "Anton Tsitsulin"
                    },
                    {
                        "name": "Robert Busa-Fekete"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Benjamin Coleman"
                    },
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Basil Mustafa"
                    },
                    {
                        "name": "Iain Barr"
                    },
                    {
                        "name": "Emilio Parisotto"
                    },
                    {
                        "name": "David Tian"
                    },
                    {
                        "name": "Matan Eyal"
                    },
                    {
                        "name": "Colin Cherry"
                    },
                    {
                        "name": "Jan-Thorsten Peter"
                    },
                    {
                        "name": "Danila Sinopalnikov"
                    },
                    {
                        "name": "Surya Bhupatiraju"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Dan Malkin"
                    },
                    {
                        "name": "Ravin Kumar"
                    },
                    {
                        "name": "David Vilar"
                    },
                    {
                        "name": "Idan Brusilovsky"
                    },
                    {
                        "name": "Jiaming Luo"
                    },
                    {
                        "name": "Andreas Steiner"
                    },
                    {
                        "name": "Abe Friesen"
                    },
                    {
                        "name": "Abhanshu Sharma"
                    },
                    {
                        "name": "Abheesht Sharma"
                    },
                    {
                        "name": "Adi Mayrav Gilady"
                    },
                    {
                        "name": "Adrian Goedeckemeyer"
                    },
                    {
                        "name": "Alaa Saade"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Alexander Kolesnikov"
                    },
                    {
                        "name": "Alexei Bendebury"
                    },
                    {
                        "name": "Alvin Abdagic"
                    },
                    {
                        "name": "Amit Vadi"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "André Susano Pinto"
                    },
                    {
                        "name": "Anil Das"
                    },
                    {
                        "name": "Ankur Bapna"
                    },
                    {
                        "name": "Antoine Miech"
                    },
                    {
                        "name": "Antoine Yang"
                    },
                    {
                        "name": "Antonia Paterson"
                    },
                    {
                        "name": "Ashish Shenoy"
                    },
                    {
                        "name": "Ayan Chakrabarti"
                    },
                    {
                        "name": "Bilal Piot"
                    },
                    {
                        "name": "Bo Wu"
                    },
                    {
                        "name": "Bobak Shahriari"
                    },
                    {
                        "name": "Bryce Petrini"
                    },
                    {
                        "name": "Charlie Chen"
                    },
                    {
                        "name": "Charline Le Lan"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "CJ Carey"
                    },
                    {
                        "name": "Cormac Brick"
                    },
                    {
                        "name": "Daniel Deutsch"
                    },
                    {
                        "name": "Danielle Eisenbud"
                    },
                    {
                        "name": "Dee Cattle"
                    },
                    {
                        "name": "Derek Cheng"
                    },
                    {
                        "name": "Dimitris Paparas"
                    },
                    {
                        "name": "Divyashree Shivakumar Sreepathihalli"
                    },
                    {
                        "name": "Doug Reid"
                    },
                    {
                        "name": "Dustin Tran"
                    },
                    {
                        "name": "Dustin Zelle"
                    },
                    {
                        "name": "Eric Noland"
                    },
                    {
                        "name": "Erwin Huizenga"
                    },
                    {
                        "name": "Eugene Kharitonov"
                    },
                    {
                        "name": "Frederick Liu"
                    },
                    {
                        "name": "Gagik Amirkhanyan"
                    },
                    {
                        "name": "Glenn Cameron"
                    },
                    {
                        "name": "Hadi Hashemi"
                    },
                    {
                        "name": "Hanna Klimczak-Plucińska"
                    },
                    {
                        "name": "Harman Singh"
                    },
                    {
                        "name": "Harsh Mehta"
                    },
                    {
                        "name": "Harshal Tushar Lehri"
                    },
                    {
                        "name": "Hussein Hazimeh"
                    },
                    {
                        "name": "Ian Ballantyne"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Ivan Nardini"
                    },
                    {
                        "name": "Jean Pouget-Abadie"
                    },
                    {
                        "name": "Jetha Chan"
                    },
                    {
                        "name": "Joe Stanton"
                    },
                    {
                        "name": "John Wieting"
                    },
                    {
                        "name": "Jonathan Lai"
                    },
                    {
                        "name": "Jordi Orbay"
                    },
                    {
                        "name": "Joseph Fernandez"
                    },
                    {
                        "name": "Josh Newlan"
                    },
                    {
                        "name": "Ju-yeong Ji"
                    },
                    {
                        "name": "Jyotinder Singh"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Kathy Yu"
                    },
                    {
                        "name": "Kevin Hui"
                    },
                    {
                        "name": "Kiran Vodrahalli"
                    },
                    {
                        "name": "Klaus Greff"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Marcella Valentine"
                    },
                    {
                        "name": "Marina Coelho"
                    },
                    {
                        "name": "Marvin Ritter"
                    },
                    {
                        "name": "Matt Hoffman"
                    },
                    {
                        "name": "Matthew Watson"
                    },
                    {
                        "name": "Mayank Chaturvedi"
                    },
                    {
                        "name": "Michael Moynihan"
                    },
                    {
                        "name": "Min Ma"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Natasha Noy"
                    },
                    {
                        "name": "Nathan Byrd"
                    },
                    {
                        "name": "Nick Roy"
                    },
                    {
                        "name": "Nikola Momchev"
                    },
                    {
                        "name": "Nilay Chauhan"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Oskar Bunyan"
                    },
                    {
                        "name": "Pankil Botarda"
                    },
                    {
                        "name": "Paul Caron"
                    },
                    {
                        "name": "Paul Kishan Rubenstein"
                    },
                    {
                        "name": "Phil Culliton"
                    },
                    {
                        "name": "Philipp Schmid"
                    },
                    {
                        "name": "Pier Giuseppe Sessa"
                    },
                    {
                        "name": "Pingmei Xu"
                    },
                    {
                        "name": "Piotr Stanczyk"
                    },
                    {
                        "name": "Pouya Tafti"
                    },
                    {
                        "name": "Rakesh Shivanna"
                    },
                    {
                        "name": "Renjie Wu"
                    },
                    {
                        "name": "Renke Pan"
                    },
                    {
                        "name": "Reza Rokni"
                    },
                    {
                        "name": "Rob Willoughby"
                    },
                    {
                        "name": "Rohith Vallu"
                    },
                    {
                        "name": "Ryan Mullins"
                    },
                    {
                        "name": "Sammy Jerome"
                    },
                    {
                        "name": "Sara Smoot"
                    },
                    {
                        "name": "Sertan Girgin"
                    },
                    {
                        "name": "Shariq Iqbal"
                    },
                    {
                        "name": "Shashir Reddy"
                    },
                    {
                        "name": "Shruti Sheth"
                    },
                    {
                        "name": "Siim Põder"
                    },
                    {
                        "name": "Sijal Bhatnagar"
                    },
                    {
                        "name": "Sindhu Raghuram Panyam"
                    },
                    {
                        "name": "Sivan Eiger"
                    },
                    {
                        "name": "Susan Zhang"
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Trevor Yacovone"
                    },
                    {
                        "name": "Tyler Liechty"
                    },
                    {
                        "name": "Uday Kalra"
                    },
                    {
                        "name": "Utku Evci"
                    },
                    {
                        "name": "Vedant Misra"
                    },
                    {
                        "name": "Vincent Roseberry"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Vlad Kolesnikov"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Yinlam Chow"
                    },
                    {
                        "name": "Yuvein Zhu"
                    },
                    {
                        "name": "Zichuan Wei"
                    },
                    {
                        "name": "Zoltan Egyed"
                    },
                    {
                        "name": "Victor Cotruta"
                    },
                    {
                        "name": "Minh Giang"
                    },
                    {
                        "name": "Phoebe Kirk"
                    },
                    {
                        "name": "Anand Rao"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Jessica Lo"
                    },
                    {
                        "name": "Erica Moreira"
                    },
                    {
                        "name": "Luiz Gustavo Martins"
                    },
                    {
                        "name": "Omar Sanseviero"
                    },
                    {
                        "name": "Lucas Gonzalez"
                    },
                    {
                        "name": "Zach Gleicher"
                    },
                    {
                        "name": "Tris Warkentin"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Evan Senter"
                    },
                    {
                        "name": "Eli Collins"
                    },
                    {
                        "name": "Joelle Barral"
                    },
                    {
                        "name": "Zoubin Ghahramani"
                    },
                    {
                        "name": "Raia Hadsell"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "D. Sculley"
                    },
                    {
                        "name": "Slav Petrov"
                    },
                    {
                        "name": "Noah Fiedel"
                    },
                    {
                        "name": "Noam Shazeer"
                    },
                    {
                        "name": "Oriol Vinyals"
                    },
                    {
                        "name": "Jeff Dean"
                    },
                    {
                        "name": "Demis Hassabis"
                    },
                    {
                        "name": "Koray Kavukcuoglu"
                    },
                    {
                        "name": "Clement Farabet"
                    },
                    {
                        "name": "Elena Buchatskaya"
                    },
                    {
                        "name": "Jean-Baptiste Alayrac"
                    },
                    {
                        "name": "Rohan Anil"
                    },
                    {
                        "name": "Dmitry"
                    },
                    {
                        "name": "Lepikhin"
                    },
                    {
                        "name": "Sebastian Borgeaud"
                    },
                    {
                        "name": "Olivier Bachem"
                    },
                    {
                        "name": "Armand Joulin"
                    },
                    {
                        "name": "Alek Andreev"
                    },
                    {
                        "name": "Cassidy Hardin"
                    },
                    {
                        "name": "Robert Dadashi"
                    },
                    {
                        "name": "Léonard Hussenot"
                    }
                ],
                "author_detail": {
                    "name": "Léonard Hussenot"
                },
                "author": "Léonard Hussenot",
                "arxiv_affiliation": "Dima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19390v1",
                "updated": "2025-03-25T06:45:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T06:45:13Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "title": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency"
                },
                "summary": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead."
                },
                "authors": [
                    {
                        "name": "Mengming Li"
                    },
                    {
                        "name": "Qijun Zhang"
                    },
                    {
                        "name": "Yongqing Ren"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie",
                "arxiv_comment": "In 31th IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14882v2",
                "updated": "2025-03-24T23:47:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    23,
                    47,
                    51,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-15T05:08:01Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "title": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Haiting Lin"
                    },
                    {
                        "name": "Mingjie Zhao"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Wentian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wentian Zhao"
                },
                "author": "Wentian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v2",
                "updated": "2025-03-24T21:27:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    27,
                    53,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "Devin A. Matthews"
                    },
                    {
                        "name": "Maggie Myers"
                    },
                    {
                        "name": "Robert van de Geijn"
                    },
                    {
                        "name": "RuQing G. Xu"
                    }
                ],
                "author_detail": {
                    "name": "RuQing G. Xu"
                },
                "author": "RuQing G. Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19145v1",
                "updated": "2025-03-24T21:00:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T21:00:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection"
                },
                "summary": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection."
                },
                "authors": [
                    {
                        "name": "Marco Garosi"
                    },
                    {
                        "name": "Alessandro Conti"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Elisa Ricci"
                    },
                    {
                        "name": "Massimiliano Mancini"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Mancini"
                },
                "author": "Massimiliano Mancini",
                "arxiv_comment": "CVPR 2025. Project website at https://comca-attributes.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13773v2",
                "updated": "2025-03-24T18:50:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    50,
                    9,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T23:38:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference"
                },
                "summary": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Tanaka"
                },
                "author": "Masahiro Tanaka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v2",
                "updated": "2025-03-24T18:16:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    16,
                    58,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18893v1",
                "updated": "2025-03-24T17:06:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:06:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "xKV: Cross-Layer SVD for KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xKV: Cross-Layer SVD for KV-Cache Compression"
                },
                "summary": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13064v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13064v2",
                "updated": "2025-03-24T16:47:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    47,
                    48,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T11:10:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads"
                },
                "summary": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications."
                },
                "authors": [
                    {
                        "name": "Pranav Suryadevara"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Suryadevara"
                },
                "author": "Pranav Suryadevara",
                "arxiv_comment": "5 pages, 5 figures. Individual Project",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13064v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13064v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.2; C.1.3; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18862v1",
                "updated": "2025-03-24T16:38:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:38:31Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "title": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation"
                },
                "summary": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation."
                },
                "authors": [
                    {
                        "name": "DeShin Hwa"
                    },
                    {
                        "name": "Tobias Holmes"
                    },
                    {
                        "name": "Klaus Drechsler"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Drechsler"
                },
                "author": "Klaus Drechsler",
                "arxiv_doi": "10.1007/978-3-658-47422-5_71",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-658-47422-5_71",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 3 figures, Preprint. Final version published in:\n  Bildverarbeitung f\\\"ur die Medizin 2025, Springer. DOI:\n  https://doi.org/10.1007/978-3-658-47422-5_71",
                "arxiv_journal_ref": "Bildverarbeitung f\\\"ur die Medizin 2025. BVM 2025. Informatik\n  aktuell. Springer Vieweg, Wiesbaden, pp 305-310",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v1",
                "updated": "2025-03-24T15:22:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache"
                },
                "summary": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v2",
                "updated": "2025-03-24T13:09:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    9,
                    3,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v1",
                "updated": "2025-03-24T11:56:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_comment": "15 pages, 14 figures, and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17333v2",
                "updated": "2025-03-24T11:00:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    0,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-21T17:33:03Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    33,
                    3,
                    4,
                    80,
                    0
                ],
                "title": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs"
                },
                "summary": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical."
                },
                "authors": [
                    {
                        "name": "Vasileios Titopoulos"
                    },
                    {
                        "name": "George Alexakis"
                    },
                    {
                        "name": "Kosmas Alexandridis"
                    },
                    {
                        "name": "Chrysostomos Nicopoulos"
                    },
                    {
                        "name": "Giorgos Dimitrakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Dimitrakopoulos"
                },
                "author": "Giorgos Dimitrakopoulos",
                "arxiv_comment": "22nd ACM International Conference on Computing Frontiers (CF' 25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16653v2",
                "updated": "2025-03-24T03:18:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    18,
                    49,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-20T19:10:37Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    19,
                    10,
                    37,
                    3,
                    79,
                    0
                ],
                "title": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation"
                },
                "summary": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse."
                },
                "authors": [
                    {
                        "name": "Hanxiao Wang"
                    },
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Weize Quan"
                    },
                    {
                        "name": "Dong-Ming Yan"
                    },
                    {
                        "name": "Peter Wonka"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wonka"
                },
                "author": "Peter Wonka",
                "arxiv_comment": "Project website: https://wanghanxiao123.github.io/iFa/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18292v1",
                "updated": "2025-03-24T02:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T02:28:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity"
                },
                "summary": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average)."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Kaichao You"
                    },
                    {
                        "name": "Zhuohan Li"
                    },
                    {
                        "name": "Mingsheng Long"
                    },
                    {
                        "name": "Jidong Zhai"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_comment": "16 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v5",
                "updated": "2025-03-24T02:17:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    17,
                    34,
                    0,
                    83,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench. Update metadata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18265v1",
                "updated": "2025-03-24T01:15:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T01:15:43Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "title": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence"
                },
                "summary": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems."
                },
                "authors": [
                    {
                        "name": "Akaash Vishal Hazarika"
                    },
                    {
                        "name": "Mahak Shah"
                    },
                    {
                        "name": "Swapnil Patil"
                    },
                    {
                        "name": "Pradyumna Shukla"
                    }
                ],
                "author_detail": {
                    "name": "Pradyumna Shukla"
                },
                "author": "Pradyumna Shukla",
                "arxiv_comment": "International Conference on AI and Financial Innovation AIFI-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v1",
                "updated": "2025-03-23T20:18:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18030v1",
                "updated": "2025-03-23T11:07:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T11:07:24Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "title": "Formal Verification of Parameterized Systems based on Induction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal Verification of Parameterized Systems based on Induction"
                },
                "summary": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors."
                },
                "authors": [
                    {
                        "name": "Jiaqi Xiu"
                    },
                    {
                        "name": "Yongjian Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongjian Li"
                },
                "author": "Yongjian Li",
                "arxiv_comment": "9 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10425v2",
                "updated": "2025-03-23T06:14:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    6,
                    14,
                    35,
                    6,
                    82,
                    0
                ],
                "published": "2023-12-16T11:40:49Z",
                "published_parsed": [
                    2023,
                    12,
                    16,
                    11,
                    40,
                    49,
                    5,
                    350,
                    0
                ],
                "title": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning"
                },
                "summary": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaorui Jiang"
                    },
                    {
                        "name": "Yu Gao"
                    },
                    {
                        "name": "Hengwei Xu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Yong Liao"
                    },
                    {
                        "name": "Pengyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Pengyuan Zhou"
                },
                "author": "Pengyuan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17913v1",
                "updated": "2025-03-23T03:20:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:20:25Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "title": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks"
                },
                "summary": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%."
                },
                "authors": [
                    {
                        "name": "Shuo Yuan"
                    },
                    {
                        "name": "Yaohua Sun"
                    },
                    {
                        "name": "Mugen Peng"
                    }
                ],
                "author_detail": {
                    "name": "Mugen Peng"
                },
                "author": "Mugen Peng",
                "arxiv_doi": "10.1109/TVT.2024.3463548",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVT.2024.3463548",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.17913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Vehicular Technology",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v1",
                "updated": "2025-03-23T03:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "16 pages, the report of open-source library VSAG\n  (https://github.com/antgroup/vsag)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17895v1",
                "updated": "2025-03-23T01:17:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T01:17:08Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "title": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO"
                },
                "summary": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Shane M. W. Witsell"
                    },
                    {
                        "name": "John F. Conley"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17603v1",
                "updated": "2025-03-22T01:17:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:17:56Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "title": "A Generative Caching System for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generative Caching System for Large Language Models"
                },
                "summary": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache."
                },
                "authors": [
                    {
                        "name": "Arun Iyengar"
                    },
                    {
                        "name": "Ashish Kundu"
                    },
                    {
                        "name": "Ramana Kompella"
                    },
                    {
                        "name": "Sai Nandan Mamidi"
                    }
                ],
                "author_detail": {
                    "name": "Sai Nandan Mamidi"
                },
                "author": "Sai Nandan Mamidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17602v1",
                "updated": "2025-03-22T01:16:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:16:24Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "title": "Multiport Support for Vortex OpenGPU Memory Hierarchy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiport Support for Vortex OpenGPU Memory Hierarchy"
                },
                "summary": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead."
                },
                "authors": [
                    {
                        "name": "Injae Shin"
                    },
                    {
                        "name": "Blaise Tine"
                    }
                ],
                "author_detail": {
                    "name": "Blaise Tine"
                },
                "author": "Blaise Tine",
                "arxiv_comment": "OSSMPIC2025, 1st workshop on Open Source Solutions for Massively\n  Parallel Integrated Circuits",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v2",
                "updated": "2025-03-21T21:10:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    21,
                    10,
                    2,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v2",
                "updated": "2025-03-21T19:26:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    19,
                    26,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang Katie Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "author": "Mingyi Hong",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v3",
                "updated": "2025-03-21T15:52:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    52,
                    39,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit"
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v3",
                "updated": "2025-03-21T15:47:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    47,
                    53,
                    4,
                    80,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "24 pages, 11 figures, ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v4",
                "updated": "2025-03-21T13:30:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    30,
                    33,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Accepted to ICLR 2025. Code is available at\n  https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v3",
                "updated": "2025-03-21T12:51:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    51,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v1",
                "updated": "2025-03-21T05:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Anshumann, Mohd Abbas Zaidi and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16131v2",
                "updated": "2025-03-21T01:59:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    1,
                    59,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T13:25:03Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    25,
                    3,
                    3,
                    79,
                    0
                ],
                "title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds."
                },
                "authors": [
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Han Yuan"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Edison Marrese Taylor"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v3",
                "updated": "2025-03-20T21:49:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    21,
                    49,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Róbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v1",
                "updated": "2025-03-20T15:52:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.02828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02828v1",
                "updated": "2025-04-03T17:59:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    59,
                    58,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:59:58Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    59,
                    58,
                    3,
                    93,
                    0
                ],
                "title": "Concept Lancet: Image Editing with Compositional Representation\n  Transplant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept Lancet: Image Editing with Compositional Representation\n  Transplant"
                },
                "summary": "Diffusion models are widely used for image editing tasks. Existing editing\nmethods often design a representation manipulation procedure by curating an\nedit direction in the text embedding or score space. However, such a procedure\nfaces a key challenge: overestimating the edit strength harms visual\nconsistency while underestimating it fails the editing task. Notably, each\nsource image may require a different editing strength, and it is costly to\nsearch for an appropriate strength via trial-and-error. To address this\nchallenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play\nframework for principled representation manipulation in diffusion-based image\nediting. At inference time, we decompose the source input in the latent (text\nembedding or diffusion score) space as a sparse linear combination of the\nrepresentations of the collected visual concepts. This allows us to accurately\nestimate the presence of concepts in each image, which informs the edit. Based\non the editing task (replace/add/remove), we perform a customized concept\ntransplant process to impose the corresponding editing direction. To\nsufficiently model the concept space, we curate a conceptual representation\ndataset, CoLan-150K, which contains diverse descriptions and scenarios of\nvisual terms and phrases for the latent dictionary. Experiments on multiple\ndiffusion-based image editing baselines show that methods equipped with CoLan\nachieve state-of-the-art performance in editing effectiveness and consistency\npreservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are widely used for image editing tasks. Existing editing\nmethods often design a representation manipulation procedure by curating an\nedit direction in the text embedding or score space. However, such a procedure\nfaces a key challenge: overestimating the edit strength harms visual\nconsistency while underestimating it fails the editing task. Notably, each\nsource image may require a different editing strength, and it is costly to\nsearch for an appropriate strength via trial-and-error. To address this\nchallenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play\nframework for principled representation manipulation in diffusion-based image\nediting. At inference time, we decompose the source input in the latent (text\nembedding or diffusion score) space as a sparse linear combination of the\nrepresentations of the collected visual concepts. This allows us to accurately\nestimate the presence of concepts in each image, which informs the edit. Based\non the editing task (replace/add/remove), we perform a customized concept\ntransplant process to impose the corresponding editing direction. To\nsufficiently model the concept space, we curate a conceptual representation\ndataset, CoLan-150K, which contains diverse descriptions and scenarios of\nvisual terms and phrases for the latent dictionary. Experiments on multiple\ndiffusion-based image editing baselines show that methods equipped with CoLan\nachieve state-of-the-art performance in editing effectiveness and consistency\npreservation."
                },
                "authors": [
                    {
                        "name": "Jinqi Luo"
                    },
                    {
                        "name": "Tianjiao Ding"
                    },
                    {
                        "name": "Kwan Ho Ryan Chan"
                    },
                    {
                        "name": "Hancheng Min"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    },
                    {
                        "name": "René Vidal"
                    }
                ],
                "author_detail": {
                    "name": "René Vidal"
                },
                "author": "René Vidal",
                "arxiv_comment": "Accepted in CVPR 2025. Project page at\n  https://peterljq.github.io/project/colan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02821v1",
                "updated": "2025-04-03T17:58:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    58,
                    35,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:58:35Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    58,
                    35,
                    3,
                    93,
                    0
                ],
                "title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language\n  Models"
                },
                "summary": "Sparse Autoencoders (SAEs) have recently been shown to enhance\ninterpretability and steerability in Large Language Models (LLMs). In this\nwork, we extend the application of SAEs to Vision-Language Models (VLMs), such\nas CLIP, and introduce a comprehensive framework for evaluating monosemanticity\nin vision representations. Our experimental results reveal that SAEs trained on\nVLMs significantly enhance the monosemanticity of individual neurons while also\nexhibiting hierarchical representations that align well with expert-defined\nstructures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that\napplying SAEs to intervene on a CLIP vision encoder, directly steer output from\nmultimodal LLMs (e.g., LLaVA) without any modifications to the underlying\nmodel. These findings emphasize the practicality and efficacy of SAEs as an\nunsupervised approach for enhancing both the interpretability and control of\nVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) have recently been shown to enhance\ninterpretability and steerability in Large Language Models (LLMs). In this\nwork, we extend the application of SAEs to Vision-Language Models (VLMs), such\nas CLIP, and introduce a comprehensive framework for evaluating monosemanticity\nin vision representations. Our experimental results reveal that SAEs trained on\nVLMs significantly enhance the monosemanticity of individual neurons while also\nexhibiting hierarchical representations that align well with expert-defined\nstructures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that\napplying SAEs to intervene on a CLIP vision encoder, directly steer output from\nmultimodal LLMs (e.g., LLaVA) without any modifications to the underlying\nmodel. These findings emphasize the practicality and efficacy of SAEs as an\nunsupervised approach for enhancing both the interpretability and control of\nVLMs."
                },
                "authors": [
                    {
                        "name": "Mateusz Pach"
                    },
                    {
                        "name": "Shyamgopal Karthik"
                    },
                    {
                        "name": "Quentin Bouniot"
                    },
                    {
                        "name": "Serge Belongie"
                    },
                    {
                        "name": "Zeynep Akata"
                    }
                ],
                "author_detail": {
                    "name": "Zeynep Akata"
                },
                "author": "Zeynep Akata",
                "arxiv_comment": "Preprint. The code is available at\n  https://github.com/ExplainableML/sae-for-vlm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02810v1",
                "updated": "2025-04-03T17:54:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    54,
                    18,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:54:18Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    54,
                    18,
                    3,
                    93,
                    0
                ],
                "title": "Generative Evaluation of Complex Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Evaluation of Complex Reasoning in Large Language Models"
                },
                "summary": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Haowei Lin"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Ruilin Yan"
                    },
                    {
                        "name": "Baizhou Huang"
                    },
                    {
                        "name": "Haotian Ye"
                    },
                    {
                        "name": "Jianhua Zhu"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Jianzhu Ma"
                    },
                    {
                        "name": "Yitao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Liang"
                },
                "author": "Yitao Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04188v2",
                "updated": "2025-04-03T17:53:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    53,
                    20,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-06T08:03:51Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    8,
                    3,
                    51,
                    3,
                    65,
                    0
                ],
                "title": "Measuring temporal effects of agent knowledge by date-controlled tool\n  use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring temporal effects of agent knowledge by date-controlled tool\n  use"
                },
                "summary": "Temporal progression is an integral part of knowledge accumulation and\nupdate. Web search is frequently adopted as grounding for agent knowledge, yet\nan improper configuration affects the quality of the agent's responses. Here,\nwe assess the agent behavior using distinct date-controlled tools (DCTs) as\nstress test to measure the knowledge variability of large language model (LLM)\nagents. We demonstrate the temporal effects of an LLM agent as a writing\nassistant, which uses web search to complete scientific publication abstracts.\nWe show that the temporality of search engine translates into tool-dependent\nagent performance but can be alleviated with base model choice and explicit\nreasoning instructions such as chain-of-thought prompting. Our results indicate\nthat agent design and evaluations should take a dynamical view and implement\nmeasures to account for the temporal influence of external resources to ensure\nreliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal progression is an integral part of knowledge accumulation and\nupdate. Web search is frequently adopted as grounding for agent knowledge, yet\nan improper configuration affects the quality of the agent's responses. Here,\nwe assess the agent behavior using distinct date-controlled tools (DCTs) as\nstress test to measure the knowledge variability of large language model (LLM)\nagents. We demonstrate the temporal effects of an LLM agent as a writing\nassistant, which uses web search to complete scientific publication abstracts.\nWe show that the temporality of search engine translates into tool-dependent\nagent performance but can be alleviated with base model choice and explicit\nreasoning instructions such as chain-of-thought prompting. Our results indicate\nthat agent design and evaluations should take a dynamical view and implement\nmeasures to account for the temporal influence of external resources to ensure\nreliability."
                },
                "authors": [
                    {
                        "name": "R. Patrick Xian"
                    },
                    {
                        "name": "Qiming Cui"
                    },
                    {
                        "name": "Stefan Bauer"
                    },
                    {
                        "name": "Reza Abbasi-Asl"
                    }
                ],
                "author_detail": {
                    "name": "Reza Abbasi-Asl"
                },
                "author": "Reza Abbasi-Asl",
                "arxiv_comment": "under review, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02807v1",
                "updated": "2025-04-03T17:52:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    52,
                    7,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:52:07Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    52,
                    7,
                    3,
                    93,
                    0
                ],
                "title": "MegaMath: Pushing the Limits of Open Math Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaMath: Pushing the Limits of Open Math Corpora"
                },
                "summary": "Mathematical reasoning is a cornerstone of human intelligence and a key\nbenchmark for advanced capabilities in large language models (LLMs). However,\nthe research community still lacks an open, large-scale, high-quality corpus\ntailored to the demands of math-centric LLM pre-training. We present MegaMath,\nan open dataset curated from diverse, math-focused sources through following\npractices: (1) Revisiting web data: We re-extracted mathematical documents from\nCommon Crawl with math-oriented HTML optimizations, fasttext-based filtering\nand deduplication, all for acquiring higher-quality data on the Internet. (2)\nRecalling Math-related code data: We identified high quality math-related code\nfrom large code training corpus, Stack-V2, further enhancing data diversity.\n(3) Exploring Synthetic data: We synthesized QA-style text, math-related code,\nand interleaved text-code blocks from web data or code data. By integrating\nthese strategies and validating their effectiveness through extensive\nablations, MegaMath delivers 371B tokens with the largest quantity and top\nquality among existing open math pre-training datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning is a cornerstone of human intelligence and a key\nbenchmark for advanced capabilities in large language models (LLMs). However,\nthe research community still lacks an open, large-scale, high-quality corpus\ntailored to the demands of math-centric LLM pre-training. We present MegaMath,\nan open dataset curated from diverse, math-focused sources through following\npractices: (1) Revisiting web data: We re-extracted mathematical documents from\nCommon Crawl with math-oriented HTML optimizations, fasttext-based filtering\nand deduplication, all for acquiring higher-quality data on the Internet. (2)\nRecalling Math-related code data: We identified high quality math-related code\nfrom large code training corpus, Stack-V2, further enhancing data diversity.\n(3) Exploring Synthetic data: We synthesized QA-style text, math-related code,\nand interleaved text-code blocks from web data or code data. By integrating\nthese strategies and validating their effectiveness through extensive\nablations, MegaMath delivers 371B tokens with the largest quantity and top\nquality among existing open math pre-training datasets."
                },
                "authors": [
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Zengzhi Wang"
                    },
                    {
                        "name": "Nikhil Ranjan"
                    },
                    {
                        "name": "Zhoujun Cheng"
                    },
                    {
                        "name": "Liping Tang"
                    },
                    {
                        "name": "Guowei He"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Eric P. Xing"
                    }
                ],
                "author_detail": {
                    "name": "Eric P. Xing"
                },
                "author": "Eric P. Xing",
                "arxiv_comment": "26 pages, 15 figures, 22 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02800v2",
                "updated": "2025-04-04T02:07:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    2,
                    7,
                    59,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-03T17:43:14Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    43,
                    14,
                    3,
                    93,
                    0
                ],
                "title": "A Survey of Large Language Models in Mental Health Disorder Detection on\n  Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Models in Mental Health Disorder Detection on\n  Social Media"
                },
                "summary": "The detection and intervention of mental health issues represent a critical\nglobal research focus, and social media data has been recognized as an\nimportant resource for mental health research. However, how to utilize Large\nLanguage Models (LLMs) for mental health problem detection on social media\nposes significant challenges. Hence, this paper aims to explore the potential\nof LLM applications in social media data analysis, focusing not only on the\nmost common psychological disorders such as depression and anxiety but also\nincorporating psychotic disorders and externalizing disorders, summarizing the\napplication methods of LLM from different dimensions, such as text data\nanalysis and detection of mental disorders, and revealing the major challenges\nand shortcomings of current research. In addition, the paper provides an\noverview of popular datasets, and evaluation metrics. The survey in this paper\nprovides a comprehensive frame of reference for researchers in the field of\nmental health, while demonstrating the great potential of LLMs in mental health\ndetection to facilitate the further application of LLMs in future mental health\ninterventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection and intervention of mental health issues represent a critical\nglobal research focus, and social media data has been recognized as an\nimportant resource for mental health research. However, how to utilize Large\nLanguage Models (LLMs) for mental health problem detection on social media\nposes significant challenges. Hence, this paper aims to explore the potential\nof LLM applications in social media data analysis, focusing not only on the\nmost common psychological disorders such as depression and anxiety but also\nincorporating psychotic disorders and externalizing disorders, summarizing the\napplication methods of LLM from different dimensions, such as text data\nanalysis and detection of mental disorders, and revealing the major challenges\nand shortcomings of current research. In addition, the paper provides an\noverview of popular datasets, and evaluation metrics. The survey in this paper\nprovides a comprehensive frame of reference for researchers in the field of\nmental health, while demonstrating the great potential of LLMs in mental health\ndetection to facilitate the further application of LLMs in future mental health\ninterventions."
                },
                "authors": [
                    {
                        "name": "Zhuohan Ge"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Shihao Qi"
                    },
                    {
                        "name": "Yuming Xu"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jason Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jason Zhang"
                },
                "author": "Jason Zhang",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.3; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12494v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12494v3",
                "updated": "2025-04-03T17:41:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    41,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2024-04-18T20:17:23Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    20,
                    17,
                    23,
                    3,
                    109,
                    0
                ],
                "title": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language\n  Models"
                },
                "summary": "Predictive models often need to work with incomplete information in\nreal-world tasks. Consequently, they must provide reliable probability or\nconfidence estimation, especially in large-scale decision-making and planning\ntasks. Current large language models (LLMs) are insufficient for accurate\nestimations, but they can generate relevant factors that may affect the\nprobabilities, produce coarse-grained probabilities when the information is\nmore complete, and help determine which factors are relevant to specific\ndownstream contexts. In this paper, we make use of these capabilities of LLMs\nto provide a significantly more accurate probabilistic estimation. We propose\nBIRD, a novel probabilistic inference framework that aligns a Bayesian network\nwith LLM abductions and then estimates more accurate probabilities in a\ndeduction step. We show BIRD provides reliable probability estimations that are\n30% better than those provided directly by LLM baselines. These estimates\nfurther contribute to better and more trustworthy decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive models often need to work with incomplete information in\nreal-world tasks. Consequently, they must provide reliable probability or\nconfidence estimation, especially in large-scale decision-making and planning\ntasks. Current large language models (LLMs) are insufficient for accurate\nestimations, but they can generate relevant factors that may affect the\nprobabilities, produce coarse-grained probabilities when the information is\nmore complete, and help determine which factors are relevant to specific\ndownstream contexts. In this paper, we make use of these capabilities of LLMs\nto provide a significantly more accurate probabilistic estimation. We propose\nBIRD, a novel probabilistic inference framework that aligns a Bayesian network\nwith LLM abductions and then estimates more accurate probabilities in a\ndeduction step. We show BIRD provides reliable probability estimations that are\n30% better than those provided directly by LLM baselines. These estimates\nfurther contribute to better and more trustworthy decision making."
                },
                "authors": [
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Ben Zhou"
                    },
                    {
                        "name": "Weidong Lin"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "arxiv_journal_ref": "ICLR 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12494v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12494v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18770v2",
                "updated": "2025-04-03T17:40:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    40,
                    58,
                    3,
                    93,
                    0
                ],
                "published": "2024-06-26T21:42:50Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    21,
                    42,
                    50,
                    2,
                    178,
                    0
                ],
                "title": "ADO-LLM: Analog Design Bayesian Optimization with In-Context Learning of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADO-LLM: Analog Design Bayesian Optimization with In-Context Learning of\n  Large Language Models"
                },
                "summary": "Analog circuit design requires substantial human expertise and involvement,\nwhich is a significant roadblock to design productivity. Bayesian Optimization\n(BO), a popular machine learning based optimization strategy, has been\nleveraged to automate analog design given its applicability across various\ncircuit topologies and technologies. Traditional BO methods employ black box\nGaussian Process surrogate models and optimized labeled data queries to find\noptimization solutions by trading off between exploration and exploitation.\nHowever, the search for the optimal design solution in BO can be expensive from\nboth a computational and data usage point of view, particularly for high\ndimensional optimization problems. This paper presents ADO-LLM, the first work\nintegrating large language models (LLMs) with Bayesian Optimization for analog\ndesign optimization. ADO-LLM leverages the LLM's ability to infuse domain\nknowledge to rapidly generate viable design points to remedy BO's inefficiency\nin finding high value design areas specifically under the limited design space\ncoverage of the BO's probabilistic surrogate model. In the meantime, sampling\nof design points evaluated in the iterative BO process provides quality\ndemonstrations for the LLM to generate high quality design points while\nleveraging infused broad design knowledge. Furthermore, the diversity brought\nby BO's exploration enriches the contextual understanding of the LLM and allows\nit to more broadly search in the design space and prevent repetitive and\nredundant suggestions. We evaluate the proposed framework on two different\ntypes of analog circuits and demonstrate notable improvements in design\nefficiency and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog circuit design requires substantial human expertise and involvement,\nwhich is a significant roadblock to design productivity. Bayesian Optimization\n(BO), a popular machine learning based optimization strategy, has been\nleveraged to automate analog design given its applicability across various\ncircuit topologies and technologies. Traditional BO methods employ black box\nGaussian Process surrogate models and optimized labeled data queries to find\noptimization solutions by trading off between exploration and exploitation.\nHowever, the search for the optimal design solution in BO can be expensive from\nboth a computational and data usage point of view, particularly for high\ndimensional optimization problems. This paper presents ADO-LLM, the first work\nintegrating large language models (LLMs) with Bayesian Optimization for analog\ndesign optimization. ADO-LLM leverages the LLM's ability to infuse domain\nknowledge to rapidly generate viable design points to remedy BO's inefficiency\nin finding high value design areas specifically under the limited design space\ncoverage of the BO's probabilistic surrogate model. In the meantime, sampling\nof design points evaluated in the iterative BO process provides quality\ndemonstrations for the LLM to generate high quality design points while\nleveraging infused broad design knowledge. Furthermore, the diversity brought\nby BO's exploration enriches the contextual understanding of the LLM and allows\nit to more broadly search in the design space and prevent repetitive and\nredundant suggestions. We evaluate the proposed framework on two different\ntypes of analog circuits and demonstrate notable improvements in design\nefficiency and effectiveness."
                },
                "authors": [
                    {
                        "name": "Yuxuan Yin"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Boxun Xu"
                    },
                    {
                        "name": "Peng Li"
                    }
                ],
                "author_detail": {
                    "name": "Peng Li"
                },
                "author": "Peng Li",
                "arxiv_doi": "10.1145/3676536.3676816",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676816",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.18770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 3 figures",
                "arxiv_journal_ref": "ICCAD: International Conference on Computer-Aided Design, 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02789v1",
                "updated": "2025-04-03T17:35:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    35,
                    54,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:35:54Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    35,
                    54,
                    3,
                    93,
                    0
                ],
                "title": "A Framework for Robust Cognitive Evaluation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Robust Cognitive Evaluation of LLMs"
                },
                "summary": "Emergent cognitive abilities in large language models (LLMs) have been widely\nobserved, but their nature and underlying mechanisms remain poorly understood.\nA growing body of research draws on cognitive science to investigate LLM\ncognition, but standard methodologies and experimen-tal pipelines have not yet\nbeen established. To address this gap we develop CognitivEval, a framework for\nsystematically evaluating the artificial cognitive capabilities of LLMs, with a\nparticular emphasis on robustness in response collection. The key features of\nCognitivEval include: (i) automatic prompt permutations, and (ii) testing that\ngathers both generations and model probability estimates. Our experiments\ndemonstrate that these features lead to more robust experimental outcomes.\nUsing CognitivEval, we replicate five classic experiments in cognitive science,\nillustrating the framework's generalizability across various experimental tasks\nand obtaining a cognitive profile of several state of the art LLMs.\nCognitivEval will be released publicly to foster broader collaboration within\nthe cognitive science community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent cognitive abilities in large language models (LLMs) have been widely\nobserved, but their nature and underlying mechanisms remain poorly understood.\nA growing body of research draws on cognitive science to investigate LLM\ncognition, but standard methodologies and experimen-tal pipelines have not yet\nbeen established. To address this gap we develop CognitivEval, a framework for\nsystematically evaluating the artificial cognitive capabilities of LLMs, with a\nparticular emphasis on robustness in response collection. The key features of\nCognitivEval include: (i) automatic prompt permutations, and (ii) testing that\ngathers both generations and model probability estimates. Our experiments\ndemonstrate that these features lead to more robust experimental outcomes.\nUsing CognitivEval, we replicate five classic experiments in cognitive science,\nillustrating the framework's generalizability across various experimental tasks\nand obtaining a cognitive profile of several state of the art LLMs.\nCognitivEval will be released publicly to foster broader collaboration within\nthe cognitive science community."
                },
                "authors": [
                    {
                        "name": "Karin de Langis"
                    },
                    {
                        "name": "Jong Inn Park"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Khanh Chi Le"
                    },
                    {
                        "name": "Andreas Schramm"
                    },
                    {
                        "name": "Michael C. Mensink"
                    },
                    {
                        "name": "Andrew Elfenbein"
                    },
                    {
                        "name": "Dongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyeop Kang"
                },
                "author": "Dongyeop Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16879v2",
                "updated": "2025-04-03T17:31:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    31,
                    57,
                    3,
                    93,
                    0
                ],
                "published": "2024-09-25T12:44:13Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    44,
                    13,
                    2,
                    269,
                    0
                ],
                "title": "GRACE: Generating Socially Appropriate Robot Actions Leveraging LLMs and\n  Human Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRACE: Generating Socially Appropriate Robot Actions Leveraging LLMs and\n  Human Explanations"
                },
                "summary": "When operating in human environments, robots need to handle complex tasks\nwhile both adhering to social norms and accommodating individual preferences.\nFor instance, based on common sense knowledge, a household robot can predict\nthat it should avoid vacuuming during a social gathering, but it may still be\nuncertain whether it should vacuum before or after having guests. In such\ncases, integrating common-sense knowledge with human preferences, often\nconveyed through human explanations, is fundamental yet a challenge for\nexisting systems. In this paper, we introduce GRACE, a novel approach\naddressing this while generating socially appropriate robot actions. GRACE\nleverages common sense knowledge from LLMs, and it integrates this knowledge\nwith human explanations through a generative network. The bidirectional\nstructure of GRACE enables robots to refine and enhance LLM predictions by\nutilizing human explanations and makes robots capable of generating such\nexplanations for human-specified actions. Our evaluations show that integrating\nhuman explanations boosts GRACE's performance, where it outperforms several\nbaselines and provides sensible explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When operating in human environments, robots need to handle complex tasks\nwhile both adhering to social norms and accommodating individual preferences.\nFor instance, based on common sense knowledge, a household robot can predict\nthat it should avoid vacuuming during a social gathering, but it may still be\nuncertain whether it should vacuum before or after having guests. In such\ncases, integrating common-sense knowledge with human preferences, often\nconveyed through human explanations, is fundamental yet a challenge for\nexisting systems. In this paper, we introduce GRACE, a novel approach\naddressing this while generating socially appropriate robot actions. GRACE\nleverages common sense knowledge from LLMs, and it integrates this knowledge\nwith human explanations through a generative network. The bidirectional\nstructure of GRACE enables robots to refine and enhance LLM predictions by\nutilizing human explanations and makes robots capable of generating such\nexplanations for human-specified actions. Our evaluations show that integrating\nhuman explanations boosts GRACE's performance, where it outperforms several\nbaselines and provides sensible explanations."
                },
                "authors": [
                    {
                        "name": "Fethiye Irmak Dogan"
                    },
                    {
                        "name": "Umut Ozyurt"
                    },
                    {
                        "name": "Gizem Cinar"
                    },
                    {
                        "name": "Hatice Gunes"
                    }
                ],
                "author_detail": {
                    "name": "Hatice Gunes"
                },
                "author": "Hatice Gunes",
                "arxiv_comment": "2025 IEEE International Conference on Robotics & Automation (ICRA),\n  Supplementary video: https://youtu.be/GTNCC1GkiQ4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02780v1",
                "updated": "2025-04-03T17:20:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    20,
                    36,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:20:36Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    20,
                    36,
                    3,
                    93,
                    0
                ],
                "title": "From Consumption to Collaboration: Measuring Interaction Patterns to\n  Augment Human Cognition in Open-Ended Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Consumption to Collaboration: Measuring Interaction Patterns to\n  Augment Human Cognition in Open-Ended Tasks"
                },
                "summary": "The rise of Generative AI, and Large Language Models (LLMs) in particular, is\nfundamentally changing cognitive processes in knowledge work, raising critical\nquestions about their impact on human reasoning and problem-solving\ncapabilities. As these AI systems become increasingly integrated into\nworkflows, they offer unprecedented opportunities for augmenting human thinking\nwhile simultaneously risking cognitive erosion through passive consumption of\ngenerated answers. This tension is particularly pronounced in open-ended tasks,\nwhere effective solutions require deep contextualization and integration of\ndomain knowledge. Unlike structured tasks with established metrics, measuring\nthe quality of human-LLM interaction in such open-ended tasks poses significant\nchallenges due to the absence of ground truth and the iterative nature of\nsolution development. To address this, we present a framework that analyzes\ninteraction patterns along two dimensions: cognitive activity mode (exploration\nvs. exploitation) and cognitive engagement mode (constructive vs. detrimental).\nThis framework provides systematic measurements to evaluate when LLMs are\neffective tools for thought rather than substitutes for human cognition,\nadvancing theoretical understanding and practical guidance for developing AI\nsystems that protect and augment human cognitive capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Generative AI, and Large Language Models (LLMs) in particular, is\nfundamentally changing cognitive processes in knowledge work, raising critical\nquestions about their impact on human reasoning and problem-solving\ncapabilities. As these AI systems become increasingly integrated into\nworkflows, they offer unprecedented opportunities for augmenting human thinking\nwhile simultaneously risking cognitive erosion through passive consumption of\ngenerated answers. This tension is particularly pronounced in open-ended tasks,\nwhere effective solutions require deep contextualization and integration of\ndomain knowledge. Unlike structured tasks with established metrics, measuring\nthe quality of human-LLM interaction in such open-ended tasks poses significant\nchallenges due to the absence of ground truth and the iterative nature of\nsolution development. To address this, we present a framework that analyzes\ninteraction patterns along two dimensions: cognitive activity mode (exploration\nvs. exploitation) and cognitive engagement mode (constructive vs. detrimental).\nThis framework provides systematic measurements to evaluate when LLMs are\neffective tools for thought rather than substitutes for human cognition,\nadvancing theoretical understanding and practical guidance for developing AI\nsystems that protect and augment human cognitive capabilities."
                },
                "authors": [
                    {
                        "name": "Joshua Holstein"
                    },
                    {
                        "name": "Moritz Diener"
                    },
                    {
                        "name": "Philipp Spitzer"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Spitzer"
                },
                "author": "Philipp Spitzer",
                "arxiv_comment": "Accepted at Tools for Thought Workshop (CHI'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02779v1",
                "updated": "2025-04-03T17:19:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    19,
                    52,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:19:52Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    19,
                    52,
                    3,
                    93,
                    0
                ],
                "title": "BT-ACTION: A Test-Driven Approach for Modular Understanding of User\n  Instruction Leveraging Behaviour Trees and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BT-ACTION: A Test-Driven Approach for Modular Understanding of User\n  Instruction Leveraging Behaviour Trees and LLMs"
                },
                "summary": "Natural language instructions are often abstract and complex, requiring\nrobots to execute multiple subtasks even for seemingly simple queries. For\nexample, when a user asks a robot to prepare avocado toast, the task involves\nseveral sequential steps. Moreover, such instructions can be ambiguous or\ninfeasible for the robot or may exceed the robot's existing knowledge. While\nLarge Language Models (LLMs) offer strong language reasoning capabilities to\nhandle these challenges, effectively integrating them into robotic systems\nremains a key challenge. To address this, we propose BT-ACTION, a test-driven\napproach that combines the modular structure of Behavior Trees (BT) with LLMs\nto generate coherent sequences of robot actions for following complex user\ninstructions, specifically in the context of preparing recipes in a\nkitchen-assistance setting. We evaluated BT-ACTION in a comprehensive user\nstudy with 45 participants, comparing its performance to direct LLM prompting.\nResults demonstrate that the modular design of BT-ACTION helped the robot make\nfewer mistakes and increased user trust, and participants showed a significant\npreference for the robot leveraging BT-ACTION. The code is publicly available\nat https://github.com/1Eggbert7/BT_LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language instructions are often abstract and complex, requiring\nrobots to execute multiple subtasks even for seemingly simple queries. For\nexample, when a user asks a robot to prepare avocado toast, the task involves\nseveral sequential steps. Moreover, such instructions can be ambiguous or\ninfeasible for the robot or may exceed the robot's existing knowledge. While\nLarge Language Models (LLMs) offer strong language reasoning capabilities to\nhandle these challenges, effectively integrating them into robotic systems\nremains a key challenge. To address this, we propose BT-ACTION, a test-driven\napproach that combines the modular structure of Behavior Trees (BT) with LLMs\nto generate coherent sequences of robot actions for following complex user\ninstructions, specifically in the context of preparing recipes in a\nkitchen-assistance setting. We evaluated BT-ACTION in a comprehensive user\nstudy with 45 participants, comparing its performance to direct LLM prompting.\nResults demonstrate that the modular design of BT-ACTION helped the robot make\nfewer mistakes and increased user trust, and participants showed a significant\npreference for the robot leveraging BT-ACTION. The code is publicly available\nat https://github.com/1Eggbert7/BT_LLM."
                },
                "authors": [
                    {
                        "name": "Alexander Leszczynski"
                    },
                    {
                        "name": "Sarah Gillet"
                    },
                    {
                        "name": "Iolanda Leite"
                    },
                    {
                        "name": "Fethiye Irmak Dogan"
                    }
                ],
                "author_detail": {
                    "name": "Fethiye Irmak Dogan"
                },
                "author": "Fethiye Irmak Dogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11202v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11202v3",
                "updated": "2025-04-03T17:14:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    14,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2025-01-20T00:22:44Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    0,
                    22,
                    44,
                    0,
                    20,
                    0
                ],
                "title": "Online Hybrid-Belief POMDP with Coupled Semantic-Geometric Models and\n  Semantic Safety Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Hybrid-Belief POMDP with Coupled Semantic-Geometric Models and\n  Semantic Safety Awareness"
                },
                "summary": "Robots operating in complex and unknown environments frequently require\ngeometric-semantic representations of the environment to safely perform their\ntasks. While inferring the environment, they must account for many possible\nscenarios when planning future actions. Since objects' class types are discrete\nand the robot's self-pose and the objects' poses are continuous, the\nenvironment can be represented by a hybrid discrete-continuous belief which is\nupdated according to models and incoming data. Prior probabilities and\nobservation models representing the environment can be learned from data using\ndeep learning algorithms. Such models often couple environmental semantic and\ngeometric properties. As a result, semantic variables are interconnected,\ncausing semantic state space dimensionality to increase exponentially. In this\npaper, we consider planning under uncertainty using partially observable Markov\ndecision processes (POMDPs) with hybrid semantic-geometric beliefs. The models\nand priors consider the coupling between semantic and geometric variables.\nWithin POMDP, we introduce the concept of semantically aware safety. Obtaining\nrepresentative samples of the theoretical hybrid belief, required for\nestimating the value function, is very challenging. As a key contribution, we\ndevelop a novel form of the hybrid belief and leverage it to sample\nrepresentative samples. We show that under certain conditions, the value\nfunction and probability of safety can be calculated efficiently with an\nexplicit expectation over all possible semantic mappings. Our simulations show\nthat our estimates of the objective function and probability of safety achieve\nsimilar levels of accuracy compared to estimators that run exhaustively on the\nentire semantic state-space using samples from the theoretical hybrid belief.\nNevertheless, the complexity of our estimators is polynomial rather than\nexponential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robots operating in complex and unknown environments frequently require\ngeometric-semantic representations of the environment to safely perform their\ntasks. While inferring the environment, they must account for many possible\nscenarios when planning future actions. Since objects' class types are discrete\nand the robot's self-pose and the objects' poses are continuous, the\nenvironment can be represented by a hybrid discrete-continuous belief which is\nupdated according to models and incoming data. Prior probabilities and\nobservation models representing the environment can be learned from data using\ndeep learning algorithms. Such models often couple environmental semantic and\ngeometric properties. As a result, semantic variables are interconnected,\ncausing semantic state space dimensionality to increase exponentially. In this\npaper, we consider planning under uncertainty using partially observable Markov\ndecision processes (POMDPs) with hybrid semantic-geometric beliefs. The models\nand priors consider the coupling between semantic and geometric variables.\nWithin POMDP, we introduce the concept of semantically aware safety. Obtaining\nrepresentative samples of the theoretical hybrid belief, required for\nestimating the value function, is very challenging. As a key contribution, we\ndevelop a novel form of the hybrid belief and leverage it to sample\nrepresentative samples. We show that under certain conditions, the value\nfunction and probability of safety can be calculated efficiently with an\nexplicit expectation over all possible semantic mappings. Our simulations show\nthat our estimates of the objective function and probability of safety achieve\nsimilar levels of accuracy compared to estimators that run exhaustively on the\nentire semantic state-space using samples from the theoretical hybrid belief.\nNevertheless, the complexity of our estimators is polynomial rather than\nexponential."
                },
                "authors": [
                    {
                        "name": "Tuvy Lemberg"
                    },
                    {
                        "name": "Vadim Indelman"
                    }
                ],
                "author_detail": {
                    "name": "Vadim Indelman"
                },
                "author": "Vadim Indelman",
                "arxiv_comment": "18 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11202v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11202v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "None",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02768v1",
                "updated": "2025-04-03T17:05:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    5,
                    50,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:05:50Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    5,
                    50,
                    3,
                    93,
                    0
                ],
                "title": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal\n  Pairs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal\n  Pairs"
                },
                "summary": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic\nminimal pairs, covering 101 languages, 6 linguistic phenomena and containing\nmore than 125,000 minimal pairs. Our minimal pairs are created using a fully\nautomated pipeline, leveraging the large-scale linguistic resources of\nUniversal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs\nat an unprecedented multilingual scale, and highlights the shortcomings of the\ncurrent state-of-the-art in modelling low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic\nminimal pairs, covering 101 languages, 6 linguistic phenomena and containing\nmore than 125,000 minimal pairs. Our minimal pairs are created using a fully\nautomated pipeline, leveraging the large-scale linguistic resources of\nUniversal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs\nat an unprecedented multilingual scale, and highlights the shortcomings of the\ncurrent state-of-the-art in modelling low-resource languages."
                },
                "authors": [
                    {
                        "name": "Jaap Jumelet"
                    },
                    {
                        "name": "Leonie Weissweiler"
                    },
                    {
                        "name": "Arianna Bisazza"
                    }
                ],
                "author_detail": {
                    "name": "Arianna Bisazza"
                },
                "author": "Arianna Bisazza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02767v1",
                "updated": "2025-04-03T17:04:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    4,
                    56,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:04:56Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    4,
                    56,
                    3,
                    93,
                    0
                ],
                "title": "How Deep Do Large Language Models Internalize Scientific Literature and\n  Citation Practices?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Deep Do Large Language Models Internalize Scientific Literature and\n  Citation Practices?"
                },
                "summary": "The spread of scientific knowledge depends on how researchers discover and\ncite previous work. The adoption of large language models (LLMs) in the\nscientific research process introduces a new layer to these citation practices.\nHowever, it remains unclear to what extent LLMs align with human citation\npractices, how they perform across domains, and may influence citation\ndynamics. Here, we show that LLMs systematically reinforce the Matthew effect\nin citations by consistently favoring highly cited papers when generating\nreferences. This pattern persists across scientific domains despite significant\nfield-specific variations in existence rates, which refer to the proportion of\ngenerated references that match existing records in external bibliometric\ndatabases. Analyzing 274,951 references generated by GPT-4o for 10,000 papers,\nwe find that LLM recommendations diverge from traditional citation patterns by\npreferring more recent references with shorter titles and fewer authors.\nEmphasizing their content-level relevance, the generated references are\nsemantically aligned with the content of each paper at levels comparable to the\nground truth references and display similar network effects while reducing\nauthor self-citations. These findings illustrate how LLMs may reshape citation\npractices and influence the trajectory of scientific discovery by reflecting\nand amplifying established trends. As LLMs become more integrated into the\nscientific research process, it is important to understand their role in\nshaping how scientific communities discover and build upon prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spread of scientific knowledge depends on how researchers discover and\ncite previous work. The adoption of large language models (LLMs) in the\nscientific research process introduces a new layer to these citation practices.\nHowever, it remains unclear to what extent LLMs align with human citation\npractices, how they perform across domains, and may influence citation\ndynamics. Here, we show that LLMs systematically reinforce the Matthew effect\nin citations by consistently favoring highly cited papers when generating\nreferences. This pattern persists across scientific domains despite significant\nfield-specific variations in existence rates, which refer to the proportion of\ngenerated references that match existing records in external bibliometric\ndatabases. Analyzing 274,951 references generated by GPT-4o for 10,000 papers,\nwe find that LLM recommendations diverge from traditional citation patterns by\npreferring more recent references with shorter titles and fewer authors.\nEmphasizing their content-level relevance, the generated references are\nsemantically aligned with the content of each paper at levels comparable to the\nground truth references and display similar network effects while reducing\nauthor self-citations. These findings illustrate how LLMs may reshape citation\npractices and influence the trajectory of scientific discovery by reflecting\nand amplifying established trends. As LLMs become more integrated into the\nscientific research process, it is important to understand their role in\nshaping how scientific communities discover and build upon prior work."
                },
                "authors": [
                    {
                        "name": "Andres Algaba"
                    },
                    {
                        "name": "Vincent Holst"
                    },
                    {
                        "name": "Floriano Tori"
                    },
                    {
                        "name": "Melika Mobini"
                    },
                    {
                        "name": "Brecht Verbeken"
                    },
                    {
                        "name": "Sylvia Wenmackers"
                    },
                    {
                        "name": "Vincent Ginis"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Ginis"
                },
                "author": "Vincent Ginis",
                "arxiv_comment": "32 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00164v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00164v2",
                "updated": "2025-04-03T16:54:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    54,
                    12,
                    3,
                    93,
                    0
                ],
                "published": "2024-12-30T22:15:57Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    22,
                    15,
                    57,
                    0,
                    365,
                    0
                ],
                "title": "Measuring Large Language Models Capacity to Annotate Journalistic\n  Sourcing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Large Language Models Capacity to Annotate Journalistic\n  Sourcing"
                },
                "summary": "Since the launch of ChatGPT in late 2022, the capacities of Large Language\nModels and their evaluation have been in constant discussion and evaluation\nboth in academic research and in the industry. Scenarios and benchmarks have\nbeen developed in several areas such as law, medicine and math (Bommasani et\nal., 2023) and there is continuous evaluation of model variants. One area that\nhas not received sufficient scenario development attention is journalism, and\nin particular journalistic sourcing and ethics. Journalism is a crucial\ntruth-determination function in democracy (Vincent, 2023), and sourcing is a\ncrucial pillar to all original journalistic output. Evaluating the capacities\nof LLMs to annotate stories for the different signals of sourcing and how\nreporters justify them is a crucial scenario that warrants a benchmark\napproach. It offers potential to build automated systems to contrast more\ntransparent and ethically rigorous forms of journalism with everyday fare. In\nthis paper we lay out a scenario to evaluate LLM performance on identifying and\nannotating sourcing in news stories on a five-category schema inspired from\njournalism studies (Gans, 2004). We offer the use case, our dataset and metrics\nand as the first step towards systematic benchmarking. Our accuracy findings\nindicate LLM-based approaches have more catching to do in identifying all the\nsourced statements in a story, and equally, in matching the type of sources. An\neven harder task is spotting source justifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the launch of ChatGPT in late 2022, the capacities of Large Language\nModels and their evaluation have been in constant discussion and evaluation\nboth in academic research and in the industry. Scenarios and benchmarks have\nbeen developed in several areas such as law, medicine and math (Bommasani et\nal., 2023) and there is continuous evaluation of model variants. One area that\nhas not received sufficient scenario development attention is journalism, and\nin particular journalistic sourcing and ethics. Journalism is a crucial\ntruth-determination function in democracy (Vincent, 2023), and sourcing is a\ncrucial pillar to all original journalistic output. Evaluating the capacities\nof LLMs to annotate stories for the different signals of sourcing and how\nreporters justify them is a crucial scenario that warrants a benchmark\napproach. It offers potential to build automated systems to contrast more\ntransparent and ethically rigorous forms of journalism with everyday fare. In\nthis paper we lay out a scenario to evaluate LLM performance on identifying and\nannotating sourcing in news stories on a five-category schema inspired from\njournalism studies (Gans, 2004). We offer the use case, our dataset and metrics\nand as the first step towards systematic benchmarking. Our accuracy findings\nindicate LLM-based approaches have more catching to do in identifying all the\nsourced statements in a story, and equally, in matching the type of sources. An\neven harder task is spotting source justifications."
                },
                "authors": [
                    {
                        "name": "Subramaniam Vincent"
                    },
                    {
                        "name": "Phoebe Wang"
                    },
                    {
                        "name": "Zhan Shi"
                    },
                    {
                        "name": "Sahas Koka"
                    },
                    {
                        "name": "Yi Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Fang"
                },
                "author": "Yi Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00164v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00164v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02758v1",
                "updated": "2025-04-03T16:53:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    53,
                    14,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T16:53:14Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    53,
                    14,
                    3,
                    93,
                    0
                ],
                "title": "ALMA uncovers optically thin and multi-component CO gas in the\n  outflowing circumnuclear disk of NGC1068",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALMA uncovers optically thin and multi-component CO gas in the\n  outflowing circumnuclear disk of NGC1068"
                },
                "summary": "Active galactic nuclei (AGNs) influence their host galaxies through winds and\njets that can drive molecular outflows, traceable with CO line emissions using\nthe Atacama Large Millimeter Array (ALMA). Recent studies leveraging ALMA data\nhave proposed a three-dimensional outflow geometry in the nearby Seyfert II\ngalaxy NGC 1068, a key target for AGN unification theories. Using ALMA\nobservations of CO(2-1), CO(3-2), and CO(6-5) transitions at roughly 0.1\narcseconds (approximately 7 parsecs) resolution, we analyzed the temperature,\ndensity, and kinematics of NGC 1068's circumnuclear disk (CND), focusing on the\nmolecular outflow. Through local thermodynamic equilibrium (LTE) analysis, we\nderived column densities and rotational temperatures, indicating optically thin\ngas and CO-to-H2 (XCO) conversion factors between 4.8 and 9.6 times smaller\nthan the Milky Way value. The inferred molecular mass outflow rate is mostly\nbelow 5.5 solar masses per year, with the dominant contribution northeast of\nthe AGN. After subtracting the rotation curve of the CND, we modeled averaged\nline profiles in each region using single and multi-component Gaussian fits.\nSeveral profiles within or near the AGN wind bicone required multiple\ncomponents with significant velocity shifts, suggesting multi-component complex\noutflow kinematics. We observe lateral variation in CO kinematics along the AGN\nwind bicone and a misalignment between the molecular and ionized outflow\ndirections. These results imply that the dynamic impact of the ionized AGN wind\non the molecular gas in the CND may be limited. However, the molecular outflow\nshows a complex, asymmetric structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active galactic nuclei (AGNs) influence their host galaxies through winds and\njets that can drive molecular outflows, traceable with CO line emissions using\nthe Atacama Large Millimeter Array (ALMA). Recent studies leveraging ALMA data\nhave proposed a three-dimensional outflow geometry in the nearby Seyfert II\ngalaxy NGC 1068, a key target for AGN unification theories. Using ALMA\nobservations of CO(2-1), CO(3-2), and CO(6-5) transitions at roughly 0.1\narcseconds (approximately 7 parsecs) resolution, we analyzed the temperature,\ndensity, and kinematics of NGC 1068's circumnuclear disk (CND), focusing on the\nmolecular outflow. Through local thermodynamic equilibrium (LTE) analysis, we\nderived column densities and rotational temperatures, indicating optically thin\ngas and CO-to-H2 (XCO) conversion factors between 4.8 and 9.6 times smaller\nthan the Milky Way value. The inferred molecular mass outflow rate is mostly\nbelow 5.5 solar masses per year, with the dominant contribution northeast of\nthe AGN. After subtracting the rotation curve of the CND, we modeled averaged\nline profiles in each region using single and multi-component Gaussian fits.\nSeveral profiles within or near the AGN wind bicone required multiple\ncomponents with significant velocity shifts, suggesting multi-component complex\noutflow kinematics. We observe lateral variation in CO kinematics along the AGN\nwind bicone and a misalignment between the molecular and ionized outflow\ndirections. These results imply that the dynamic impact of the ionized AGN wind\non the molecular gas in the CND may be limited. However, the molecular outflow\nshows a complex, asymmetric structure."
                },
                "authors": [
                    {
                        "name": "Yuze Zhang"
                    },
                    {
                        "name": "Serena Viti"
                    },
                    {
                        "name": "Santiago García-Burillo"
                    },
                    {
                        "name": "Ko-Yun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ko-Yun Huang"
                },
                "author": "Ko-Yun Huang",
                "arxiv_comment": "Accepted for publication in A&A (manuscript ID: aa53704-25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02757v1",
                "updated": "2025-04-03T16:49:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    49,
                    58,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T16:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    49,
                    58,
                    3,
                    93,
                    0
                ],
                "title": "Echoes of the hidden: Uncovering coordination beyond network structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Echoes of the hidden: Uncovering coordination beyond network structure"
                },
                "summary": "The study of connectivity and coordination has drawn increasing attention in\nrecent decades due to their central role in driving markets, shaping societal\ndynamics, and influencing biological systems. Traditionally, observable\nconnections, such as phone calls, financial transactions, or social media\nconnections, have been used to infer coordination and connectivity. However,\nincomplete, encrypted, or fragmented data, alongside the ubiquity of\ncommunication platforms and deliberate obfuscation, often leave many real-world\nconnections hidden. In this study, we demonstrate that coordinating individuals\nexhibit shared bursty activity patterns, enabling their detection even when\nobservable links between them are sparse or entirely absent. We further propose\na generative model based on the network of networks formalism to account for\nthe mechanisms driving this collaborative burstiness, attributing it to shock\npropagation across networks rather than isolated individual behavior. Model\nsimulations demonstrate that when observable connection density is below 70\\%,\nburstiness significantly improves coordination detection compared to\nstate-of-the-art temporal and structural methods. This work provides a new\nperspective on community and coordination dynamics, advancing both theoretical\nunderstanding and practical detection. By laying the foundation for identifying\nhidden connections beyond observable network structures, it enables detection\nacross different platforms, alongside enhancing system behavior understanding,\ninformed decision-making, and risk mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study of connectivity and coordination has drawn increasing attention in\nrecent decades due to their central role in driving markets, shaping societal\ndynamics, and influencing biological systems. Traditionally, observable\nconnections, such as phone calls, financial transactions, or social media\nconnections, have been used to infer coordination and connectivity. However,\nincomplete, encrypted, or fragmented data, alongside the ubiquity of\ncommunication platforms and deliberate obfuscation, often leave many real-world\nconnections hidden. In this study, we demonstrate that coordinating individuals\nexhibit shared bursty activity patterns, enabling their detection even when\nobservable links between them are sparse or entirely absent. We further propose\na generative model based on the network of networks formalism to account for\nthe mechanisms driving this collaborative burstiness, attributing it to shock\npropagation across networks rather than isolated individual behavior. Model\nsimulations demonstrate that when observable connection density is below 70\\%,\nburstiness significantly improves coordination detection compared to\nstate-of-the-art temporal and structural methods. This work provides a new\nperspective on community and coordination dynamics, advancing both theoretical\nunderstanding and practical detection. By laying the foundation for identifying\nhidden connections beyond observable network structures, it enables detection\nacross different platforms, alongside enhancing system behavior understanding,\ninformed decision-making, and risk mitigation."
                },
                "authors": [
                    {
                        "name": "Shahar Somin"
                    },
                    {
                        "name": "Tom Cohen"
                    },
                    {
                        "name": "Jeremy Kepner"
                    },
                    {
                        "name": "Alex Pentland"
                    }
                ],
                "author_detail": {
                    "name": "Alex Pentland"
                },
                "author": "Alex Pentland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02747v1",
                "updated": "2025-04-03T16:35:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    35,
                    17,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T16:35:17Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    35,
                    17,
                    3,
                    93,
                    0
                ],
                "title": "GEOPARD: Geometric Pretraining for Articulation Prediction in 3D Shapes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEOPARD: Geometric Pretraining for Articulation Prediction in 3D Shapes"
                },
                "summary": "We present GEOPARD, a transformer-based architecture for predicting\narticulation from a single static snapshot of a 3D shape. The key idea of our\nmethod is a pretraining strategy that allows our transformer to learn plausible\ncandidate articulations for 3D shapes based on a geometric-driven search\nwithout manual articulation annotation. The search automatically discovers\nphysically valid part motions that do not cause detachments or collisions with\nother shape parts. Our experiments indicate that this geometric pretraining\nstrategy, along with carefully designed choices in our transformer\narchitecture, yields state-of-the-art results in articulation inference in the\nPartNet-Mobility dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GEOPARD, a transformer-based architecture for predicting\narticulation from a single static snapshot of a 3D shape. The key idea of our\nmethod is a pretraining strategy that allows our transformer to learn plausible\ncandidate articulations for 3D shapes based on a geometric-driven search\nwithout manual articulation annotation. The search automatically discovers\nphysically valid part motions that do not cause detachments or collisions with\nother shape parts. Our experiments indicate that this geometric pretraining\nstrategy, along with carefully designed choices in our transformer\narchitecture, yields state-of-the-art results in articulation inference in the\nPartNet-Mobility dataset."
                },
                "authors": [
                    {
                        "name": "Pradyumn Goyal"
                    },
                    {
                        "name": "Dmitry Petrov"
                    },
                    {
                        "name": "Sheldon Andrews"
                    },
                    {
                        "name": "Yizhak Ben-Shabat"
                    },
                    {
                        "name": "Hsueh-Ti Derek Liu"
                    },
                    {
                        "name": "Evangelos Kalogerakis"
                    }
                ],
                "author_detail": {
                    "name": "Evangelos Kalogerakis"
                },
                "author": "Evangelos Kalogerakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02743v1",
                "updated": "2025-04-03T16:30:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    30,
                    40,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T16:30:40Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    30,
                    40,
                    3,
                    93,
                    0
                ],
                "title": "Sequential Binary Hypothesis Testing with Competing Agents under\n  Information Asymmetry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Binary Hypothesis Testing with Competing Agents under\n  Information Asymmetry"
                },
                "summary": "This paper concerns sequential hypothesis testing in competitive multi-agent\nsystems where agents exchange potentially manipulated information.\nSpecifically, a two-agent scenario is studied where each agent aims to\ncorrectly infer the true state of nature while optimizing decision speed and\naccuracy. At each iteration, agents collect private observations, update their\nbeliefs, and share (possibly corrupted) belief signals with their counterparts\nbefore deciding whether to stop and declare a state, or continue gathering more\ninformation. The analysis yields three main results: (1)~when agents share\ninformation strategically, the optimal signaling policy involves\nequal-probability randomization between truthful and inverted beliefs;\n(2)~agents maximize performance by relying solely on their own observations for\nbelief updating while using received information only to anticipate their\ncounterpart's stopping decision; and (3)~the agent reaching their confidence\nthreshold first cause the other agent to achieve a higher conditional\nprobability of error. Numerical simulations further demonstrate that agents\nwith higher KL divergence in their conditional distributions gain competitive\nadvantage. Furthermore, our results establish that information sharing --\ndespite strategic manipulation -- reduces overall system stopping time compared\nto non-interactive scenarios, which highlights the inherent value of\ncommunication even in this competitive setup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper concerns sequential hypothesis testing in competitive multi-agent\nsystems where agents exchange potentially manipulated information.\nSpecifically, a two-agent scenario is studied where each agent aims to\ncorrectly infer the true state of nature while optimizing decision speed and\naccuracy. At each iteration, agents collect private observations, update their\nbeliefs, and share (possibly corrupted) belief signals with their counterparts\nbefore deciding whether to stop and declare a state, or continue gathering more\ninformation. The analysis yields three main results: (1)~when agents share\ninformation strategically, the optimal signaling policy involves\nequal-probability randomization between truthful and inverted beliefs;\n(2)~agents maximize performance by relying solely on their own observations for\nbelief updating while using received information only to anticipate their\ncounterpart's stopping decision; and (3)~the agent reaching their confidence\nthreshold first cause the other agent to achieve a higher conditional\nprobability of error. Numerical simulations further demonstrate that agents\nwith higher KL divergence in their conditional distributions gain competitive\nadvantage. Furthermore, our results establish that information sharing --\ndespite strategic manipulation -- reduces overall system stopping time compared\nto non-interactive scenarios, which highlights the inherent value of\ncommunication even in this competitive setup."
                },
                "authors": [
                    {
                        "name": "Aneesh Raghavan"
                    },
                    {
                        "name": "M. Umar B. Niazi"
                    },
                    {
                        "name": "Karl H. Johansson"
                    }
                ],
                "author_detail": {
                    "name": "Karl H. Johansson"
                },
                "author": "Karl H. Johansson",
                "arxiv_comment": "8 pages, 4 figures, submitted to IEEE Conference on Decision and\n  Control 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02733v1",
                "updated": "2025-04-03T16:17:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    17,
                    56,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T16:17:56Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    17,
                    56,
                    3,
                    93,
                    0
                ],
                "title": "Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study"
                },
                "summary": "Large Language Models (LLMs) are highly vulnerable to input perturbations, as\neven a small prompt change may result in a substantially different output.\nExisting methods to enhance LLM robustness are primarily focused on perturbed\ndata samples, whereas improving resiliency to perturbations of task-level\ninstructions has remained relatively underexplored. In this work, we focus on\ncharacter- and word-level edits of task-specific instructions, which\nsubstantially degrade downstream performance. We experiment with a variety of\ntechniques to enhance the robustness of LLMs, including self-denoising and\nrepresentation alignment, testing different models (Llama 3 and Flan-T5),\ndatasets (CoLA, QNLI, SST-2) and instructions (both task-oriented and\nrole-oriented). We find that, on average, self-denoising -- whether performed\nby a frozen LLM or a fine-tuned model -- achieves substantially higher\nperformance gains than alternative strategies, including more complex baselines\nsuch as ensembling and supervised methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are highly vulnerable to input perturbations, as\neven a small prompt change may result in a substantially different output.\nExisting methods to enhance LLM robustness are primarily focused on perturbed\ndata samples, whereas improving resiliency to perturbations of task-level\ninstructions has remained relatively underexplored. In this work, we focus on\ncharacter- and word-level edits of task-specific instructions, which\nsubstantially degrade downstream performance. We experiment with a variety of\ntechniques to enhance the robustness of LLMs, including self-denoising and\nrepresentation alignment, testing different models (Llama 3 and Flan-T5),\ndatasets (CoLA, QNLI, SST-2) and instructions (both task-oriented and\nrole-oriented). We find that, on average, self-denoising -- whether performed\nby a frozen LLM or a fine-tuned model -- achieves substantially higher\nperformance gains than alternative strategies, including more complex baselines\nsuch as ensembling and supervised methods."
                },
                "authors": [
                    {
                        "name": "Aryan Agrawal"
                    },
                    {
                        "name": "Lisa Alazraki"
                    },
                    {
                        "name": "Shahin Honarvar"
                    },
                    {
                        "name": "Marek Rei"
                    }
                ],
                "author_detail": {
                    "name": "Marek Rei"
                },
                "author": "Marek Rei",
                "arxiv_comment": "Building Trust Workshop, ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02732v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02732v2",
                "updated": "2025-04-04T07:41:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    7,
                    41,
                    19,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-03T16:17:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    17,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Why do LLMs attend to the first token?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why do LLMs attend to the first token?"
                },
                "summary": "Large Language Models (LLMs) tend to attend heavily to the first token in the\nsequence -- creating a so-called attention sink. Many works have studied this\nphenomenon in detail, proposing various ways to either leverage or alleviate\nit. Attention sinks have been connected to quantisation difficulties, security\nissues, and streaming attention. Yet, while many works have provided conditions\nin which they occur or not, a critical question remains shallowly answered: Why\ndo LLMs learn such patterns and how are they being used? In this work, we argue\ntheoretically and empirically that this mechanism provides a method for LLMs to\navoid over-mixing, connecting this to existing lines of work that study\nmathematically how information propagates in Transformers. We conduct\nexperiments to validate our theoretical intuitions and show how choices such as\ncontext length, depth, and data packing influence the sink behaviour. We hope\nthat this study provides a new practical perspective on why attention sinks are\nuseful in LLMs, leading to a better understanding of the attention patterns\nthat form during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) tend to attend heavily to the first token in the\nsequence -- creating a so-called attention sink. Many works have studied this\nphenomenon in detail, proposing various ways to either leverage or alleviate\nit. Attention sinks have been connected to quantisation difficulties, security\nissues, and streaming attention. Yet, while many works have provided conditions\nin which they occur or not, a critical question remains shallowly answered: Why\ndo LLMs learn such patterns and how are they being used? In this work, we argue\ntheoretically and empirically that this mechanism provides a method for LLMs to\navoid over-mixing, connecting this to existing lines of work that study\nmathematically how information propagates in Transformers. We conduct\nexperiments to validate our theoretical intuitions and show how choices such as\ncontext length, depth, and data packing influence the sink behaviour. We hope\nthat this study provides a new practical perspective on why attention sinks are\nuseful in LLMs, leading to a better understanding of the attention patterns\nthat form during training."
                },
                "authors": [
                    {
                        "name": "Federico Barbero"
                    },
                    {
                        "name": "Álvaro Arroyo"
                    },
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Christos Perivolaropoulos"
                    },
                    {
                        "name": "Michael Bronstein"
                    },
                    {
                        "name": "Petar Veličković"
                    },
                    {
                        "name": "Razvan Pascanu"
                    }
                ],
                "author_detail": {
                    "name": "Razvan Pascanu"
                },
                "author": "Razvan Pascanu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02732v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02732v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09111v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09111v6",
                "updated": "2025-04-03T16:11:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    11,
                    23,
                    3,
                    93,
                    0
                ],
                "published": "2024-11-14T00:59:13Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    0,
                    59,
                    13,
                    3,
                    319,
                    0
                ],
                "title": "Reducing Reasoning Costs: The Path of Optimization for Chain of Thought\n  via Sparse Attention Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Reasoning Costs: The Path of Optimization for Chain of Thought\n  via Sparse Attention Mechanism"
                },
                "summary": "In order to address the chain of thought in the large language model\ninference cost surge, this research proposes to use a sparse attention\nmechanism that only focuses on a few relevant tokens. The researcher\nconstructed a new attention mechanism and used GiantRabbit trained with custom\nGPTs as an experimental tool. The experiment tested and compared the reasoning\ntime, correctness score and chain of thought length of this model and o1\nPreview in solving the linear algebra test questions of MIT OpenCourseWare. The\nresults show that GiantRabbit's reasoning time and chain of thought length are\nsignificantly lower than o1 Preview. It verifies the feasibility of sparse\nattention mechanism for optimizing chain of thought reasoning. Detailed\narchitectural details and experimental process have been uploaded to Github,\nthe link is:https://github.com/brucewang123456789/GeniusTrail.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to address the chain of thought in the large language model\ninference cost surge, this research proposes to use a sparse attention\nmechanism that only focuses on a few relevant tokens. The researcher\nconstructed a new attention mechanism and used GiantRabbit trained with custom\nGPTs as an experimental tool. The experiment tested and compared the reasoning\ntime, correctness score and chain of thought length of this model and o1\nPreview in solving the linear algebra test questions of MIT OpenCourseWare. The\nresults show that GiantRabbit's reasoning time and chain of thought length are\nsignificantly lower than o1 Preview. It verifies the feasibility of sparse\nattention mechanism for optimizing chain of thought reasoning. Detailed\narchitectural details and experimental process have been uploaded to Github,\nthe link is:https://github.com/brucewang123456789/GeniusTrail.git."
                },
                "authors": [
                    {
                        "name": "Libo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Libo Wang"
                },
                "author": "Libo Wang",
                "arxiv_comment": "The main text is 5 pages, totaling 9 pages; 4 figures, 1 table. It\n  have been submitted to NeurIPS 2024 Workshop MusIML and OpenReview",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09111v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09111v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02725v1",
                "updated": "2025-04-03T16:07:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    7,
                    38,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T16:07:38Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    7,
                    38,
                    3,
                    93,
                    0
                ],
                "title": "ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference\n  Optimization"
                },
                "summary": "Recent advancements in large language models (LLMs) have accelerated progress\ntoward artificial general intelligence, yet their potential to generate harmful\ncontent poses critical safety challenges. Existing alignment methods often\nstruggle to cover diverse safety scenarios and remain vulnerable to adversarial\nattacks. In this work, we propose Ex-Ante Reasoning Preference Optimization\n(ERPO), a novel safety alignment framework that equips LLMs with explicit\npreemptive reasoning through Chain-of-Thought and provides clear evidence for\nsafety judgments by embedding predefined safety rules. Specifically, our\napproach consists of three stages: first, equipping the model with Ex-Ante\nreasoning through supervised fine-tuning (SFT) using a constructed reasoning\nmodule; second, enhancing safety, usefulness, and efficiency via Direct\nPreference Optimization (DPO); and third, mitigating inference latency with a\nlength-controlled iterative preference optimization strategy. Experiments on\nmultiple open-source LLMs demonstrate that ERPO significantly enhances safety\nperformance while maintaining response efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have accelerated progress\ntoward artificial general intelligence, yet their potential to generate harmful\ncontent poses critical safety challenges. Existing alignment methods often\nstruggle to cover diverse safety scenarios and remain vulnerable to adversarial\nattacks. In this work, we propose Ex-Ante Reasoning Preference Optimization\n(ERPO), a novel safety alignment framework that equips LLMs with explicit\npreemptive reasoning through Chain-of-Thought and provides clear evidence for\nsafety judgments by embedding predefined safety rules. Specifically, our\napproach consists of three stages: first, equipping the model with Ex-Ante\nreasoning through supervised fine-tuning (SFT) using a constructed reasoning\nmodule; second, enhancing safety, usefulness, and efficiency via Direct\nPreference Optimization (DPO); and third, mitigating inference latency with a\nlength-controlled iterative preference optimization strategy. Experiments on\nmultiple open-source LLMs demonstrate that ERPO significantly enhances safety\nperformance while maintaining response efficiency."
                },
                "authors": [
                    {
                        "name": "Kehua Feng"
                    },
                    {
                        "name": "Keyan Ding"
                    },
                    {
                        "name": "Jing Yu"
                    },
                    {
                        "name": "Menghan Li"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Xinda Wang"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "18 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02712v1",
                "updated": "2025-04-03T15:52:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    52,
                    20,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T15:52:20Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    52,
                    20,
                    3,
                    93,
                    0
                ],
                "title": "TeleMoM: Consensus-Driven Telecom Intelligence via Mixture of Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeleMoM: Consensus-Driven Telecom Intelligence via Mixture of Models"
                },
                "summary": "Large language models (LLMs) face significant challenges in specialized\ndomains like telecommunication (Telecom) due to technical complexity,\nspecialized terminology, and rapidly evolving knowledge. Traditional methods,\nsuch as scaling model parameters or retraining on domain-specific corpora, are\ncomputationally expensive and yield diminishing returns, while existing\napproaches like retrieval-augmented generation, mixture of experts, and\nfine-tuning struggle with accuracy, efficiency, and coordination. To address\nthis issue, we propose Telecom mixture of models (TeleMoM), a consensus-driven\nensemble framework that integrates multiple LLMs for enhanced decision-making\nin Telecom. TeleMoM employs a two-stage process: proponent models generate\njustified responses, and an adjudicator finalizes decisions, supported by a\nquality-checking mechanism. This approach leverages strengths of diverse models\nto improve accuracy, reduce biases, and handle domain-specific complexities\neffectively. Evaluation results demonstrate that TeleMoM achieves a 9.7\\%\nincrease in answer accuracy, highlighting its effectiveness in Telecom\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face significant challenges in specialized\ndomains like telecommunication (Telecom) due to technical complexity,\nspecialized terminology, and rapidly evolving knowledge. Traditional methods,\nsuch as scaling model parameters or retraining on domain-specific corpora, are\ncomputationally expensive and yield diminishing returns, while existing\napproaches like retrieval-augmented generation, mixture of experts, and\nfine-tuning struggle with accuracy, efficiency, and coordination. To address\nthis issue, we propose Telecom mixture of models (TeleMoM), a consensus-driven\nensemble framework that integrates multiple LLMs for enhanced decision-making\nin Telecom. TeleMoM employs a two-stage process: proponent models generate\njustified responses, and an adjudicator finalizes decisions, supported by a\nquality-checking mechanism. This approach leverages strengths of diverse models\nto improve accuracy, reduce biases, and handle domain-specific complexities\neffectively. Evaluation results demonstrate that TeleMoM achieves a 9.7\\%\nincrease in answer accuracy, highlighting its effectiveness in Telecom\napplications."
                },
                "authors": [
                    {
                        "name": "Xinquan Wang"
                    },
                    {
                        "name": "Fenghao Zhu"
                    },
                    {
                        "name": "Chongwen Huang"
                    },
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Sami Muhaidat"
                    },
                    {
                        "name": "Chau Yuen"
                    },
                    {
                        "name": "Mérouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Mérouane Debbah"
                },
                "author": "Mérouane Debbah",
                "arxiv_comment": "6 pages; submitted to 2025 IEEE VTC Fall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02708v1",
                "updated": "2025-04-03T15:46:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    46,
                    46,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T15:46:46Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    46,
                    46,
                    3,
                    93,
                    0
                ],
                "title": "The Hidden Space of Safety: Understanding Preference-Tuned LLMs in\n  Multilingual context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Space of Safety: Understanding Preference-Tuned LLMs in\n  Multilingual context"
                },
                "summary": "Alignment tuning has enabled large language models to excel in reasoning,\ninstruction-following, and minimizing harmful generations. However, despite\ntheir widespread deployment, these models exhibit a monolingual bias, raising\nconcerns about the effectiveness of alignment across languages. Current\nalignment methods predominantly focus on English, leaving it unclear how\nalignment mechanism generalize to multilingual settings. To address this, we\nconduct a systematic analysis of distributional shifts in the embedding space\nof LLMs before and after alignment, uncovering its impact on model behavior\nacross diverse languages. We leverage the alignment-induced separation in\nsafety space as a quantitative tool to measure how alignment enforces safety\nconstraints. Our study evaluates seven LLMs using balanced toxicity datasets\nand parallel text-detoxification benchmarks, revealing substantial disparities\nin the latent representation space between high-resource and low-resource\nlanguages. These findings underscore the need for language-specific fine-tuning\nto ensure fair, reliable and robust multilingual alignment. Our insights\nprovide a foundation for developing truly safe multilingual LLMs, emphasizing\nthe urgency of addressing alignment gaps in underrepresented languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment tuning has enabled large language models to excel in reasoning,\ninstruction-following, and minimizing harmful generations. However, despite\ntheir widespread deployment, these models exhibit a monolingual bias, raising\nconcerns about the effectiveness of alignment across languages. Current\nalignment methods predominantly focus on English, leaving it unclear how\nalignment mechanism generalize to multilingual settings. To address this, we\nconduct a systematic analysis of distributional shifts in the embedding space\nof LLMs before and after alignment, uncovering its impact on model behavior\nacross diverse languages. We leverage the alignment-induced separation in\nsafety space as a quantitative tool to measure how alignment enforces safety\nconstraints. Our study evaluates seven LLMs using balanced toxicity datasets\nand parallel text-detoxification benchmarks, revealing substantial disparities\nin the latent representation space between high-resource and low-resource\nlanguages. These findings underscore the need for language-specific fine-tuning\nto ensure fair, reliable and robust multilingual alignment. Our insights\nprovide a foundation for developing truly safe multilingual LLMs, emphasizing\nthe urgency of addressing alignment gaps in underrepresented languages."
                },
                "authors": [
                    {
                        "name": "Nikhil Verma"
                    },
                    {
                        "name": "Manasa Bharadwaj"
                    }
                ],
                "author_detail": {
                    "name": "Manasa Bharadwaj"
                },
                "author": "Manasa Bharadwaj",
                "arxiv_comment": "14 pages, 11 Figures, 2 Tables, currently under review at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06786v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06786v3",
                "updated": "2025-04-04T07:48:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    7,
                    48,
                    19,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-09T18:59:46Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    59,
                    46,
                    0,
                    344,
                    0
                ],
                "title": "Retrieving Semantics from the Deep: an RAG Solution for Gesture\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieving Semantics from the Deep: an RAG Solution for Gesture\n  Synthesis"
                },
                "summary": "Non-verbal communication often comprises of semantically rich gestures that\nhelp convey the meaning of an utterance. Producing such semantic co-speech\ngestures has been a major challenge for the existing neural systems that can\ngenerate rhythmic beat gestures, but struggle to produce semantically\nmeaningful gestures. Therefore, we present RAG-Gesture, a diffusion-based\ngesture generation approach that leverages Retrieval Augmented Generation (RAG)\nto produce natural-looking and semantically rich gestures. Our neuro-explicit\ngesture generation approach is designed to produce semantic gestures grounded\nin interpretable linguistic knowledge. We achieve this by using explicit domain\nknowledge to retrieve exemplar motions from a database of co-speech gestures.\nOnce retrieved, we then inject these semantic exemplar gestures into our\ndiffusion-based gesture generation pipeline using DDIM inversion and retrieval\nguidance at the inference time without any need of training. Further, we\npropose a control paradigm for guidance, that allows the users to modulate the\namount of influence each retrieval insertion has over the generated sequence.\nOur comparative evaluations demonstrate the validity of our approach against\nrecent gesture generation approaches. The reader is urged to explore the\nresults on our project page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-verbal communication often comprises of semantically rich gestures that\nhelp convey the meaning of an utterance. Producing such semantic co-speech\ngestures has been a major challenge for the existing neural systems that can\ngenerate rhythmic beat gestures, but struggle to produce semantically\nmeaningful gestures. Therefore, we present RAG-Gesture, a diffusion-based\ngesture generation approach that leverages Retrieval Augmented Generation (RAG)\nto produce natural-looking and semantically rich gestures. Our neuro-explicit\ngesture generation approach is designed to produce semantic gestures grounded\nin interpretable linguistic knowledge. We achieve this by using explicit domain\nknowledge to retrieve exemplar motions from a database of co-speech gestures.\nOnce retrieved, we then inject these semantic exemplar gestures into our\ndiffusion-based gesture generation pipeline using DDIM inversion and retrieval\nguidance at the inference time without any need of training. Further, we\npropose a control paradigm for guidance, that allows the users to modulate the\namount of influence each retrieval insertion has over the generated sequence.\nOur comparative evaluations demonstrate the validity of our approach against\nrecent gesture generation approaches. The reader is urged to explore the\nresults on our project page."
                },
                "authors": [
                    {
                        "name": "M. Hamza Mughal"
                    },
                    {
                        "name": "Rishabh Dabral"
                    },
                    {
                        "name": "Merel C. J. Scholman"
                    },
                    {
                        "name": "Vera Demberg"
                    },
                    {
                        "name": "Christian Theobalt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Theobalt"
                },
                "author": "Christian Theobalt",
                "arxiv_comment": "CVPR 2025. Project page:\n  https://vcai.mpi-inf.mpg.de/projects/RAG-Gesture/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06786v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06786v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19472v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19472v2",
                "updated": "2025-04-03T15:36:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    36,
                    1,
                    3,
                    93,
                    0
                ],
                "published": "2025-02-26T19:00:01Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    19,
                    0,
                    1,
                    2,
                    57,
                    0
                ],
                "title": "Photometric vs dynamical stellar masses and their impact on scaling\n  relations in nearby disc galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photometric vs dynamical stellar masses and their impact on scaling\n  relations in nearby disc galaxies"
                },
                "summary": "The study of scaling relations of disc galaxies and their evolution across\ncosmic time requires accurate estimates of galaxy stellar masses $M_\\star$ over\nbroad redshift ranges. While photometric $M_\\star$ estimates ($M_{\\rm phot}$)\nbased on spectral energy distribution (SED) modelling methods are employed\nroutinely at high-$z$, it is unclear to what extent these are compatible with\ndynamical $M_\\star$ estimates ($M_{\\rm dyn}$), available for nearby galaxies.\nHere we compare newly determined, SED-model based $M_{\\rm phot}$ with\npreviously obtained $M_{\\rm dyn}$ inferred via rotation curve decomposition\ntechniques in a sample of $\\sim100$ nearby galaxies from the SPARC database. We\nfind that the two mass estimates show a systematic agreement at the $\\sim12\\%$\n($0.05$ dex) level and a $\\sim55\\%$ ($0.22$ dex) scatter across almost $5$ dex\nin $M_\\star$. Our $M_{\\rm phot}$ estimates correspond to mass-to-light ratios\nin the $3.6\\mu$m band that increase gradually with $3.6\\mu$m luminosity, as a\nconsequence of the earlier (later) assembly history of high-mass (low-mass)\ndisc galaxies. The choice of using either $M_{\\rm dyn}$ or $M_{\\rm phot}$ has\nonly a marginal impact on the slope and zero-point of the Tully-Fisher and Fall\nrelations: the observed orthogonal scatter in both relations is virtually the\nsame for the two methods, and indistinguishable from that derived using a\nconstant mass-to-light ratio in the $3.6\\mu$m band. $M_\\star$ estimates based\non the assumption that discs are marginally stable lead to the largest scatter\nin the scaling relations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study of scaling relations of disc galaxies and their evolution across\ncosmic time requires accurate estimates of galaxy stellar masses $M_\\star$ over\nbroad redshift ranges. While photometric $M_\\star$ estimates ($M_{\\rm phot}$)\nbased on spectral energy distribution (SED) modelling methods are employed\nroutinely at high-$z$, it is unclear to what extent these are compatible with\ndynamical $M_\\star$ estimates ($M_{\\rm dyn}$), available for nearby galaxies.\nHere we compare newly determined, SED-model based $M_{\\rm phot}$ with\npreviously obtained $M_{\\rm dyn}$ inferred via rotation curve decomposition\ntechniques in a sample of $\\sim100$ nearby galaxies from the SPARC database. We\nfind that the two mass estimates show a systematic agreement at the $\\sim12\\%$\n($0.05$ dex) level and a $\\sim55\\%$ ($0.22$ dex) scatter across almost $5$ dex\nin $M_\\star$. Our $M_{\\rm phot}$ estimates correspond to mass-to-light ratios\nin the $3.6\\mu$m band that increase gradually with $3.6\\mu$m luminosity, as a\nconsequence of the earlier (later) assembly history of high-mass (low-mass)\ndisc galaxies. The choice of using either $M_{\\rm dyn}$ or $M_{\\rm phot}$ has\nonly a marginal impact on the slope and zero-point of the Tully-Fisher and Fall\nrelations: the observed orthogonal scatter in both relations is virtually the\nsame for the two methods, and indistinguishable from that derived using a\nconstant mass-to-light ratio in the $3.6\\mu$m band. $M_\\star$ estimates based\non the assumption that discs are marginally stable lead to the largest scatter\nin the scaling relations."
                },
                "authors": [
                    {
                        "name": "A. Marasco"
                    },
                    {
                        "name": "S. M. Fall"
                    },
                    {
                        "name": "E. M. Di Teodoro"
                    },
                    {
                        "name": "P. E. Mancera Piña"
                    }
                ],
                "author_detail": {
                    "name": "P. E. Mancera Piña"
                },
                "author": "P. E. Mancera Piña",
                "arxiv_doi": "10.1051/0004-6361/202553925",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202553925",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.19472v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19472v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 9 figures, 3 Tables. Accepted for publication by A&A",
                "arxiv_journal_ref": "A&A 695, L23 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02697v1",
                "updated": "2025-04-03T15:33:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    33,
                    18,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T15:33:18Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    33,
                    18,
                    3,
                    93,
                    0
                ],
                "title": "Learning Phase Distortion with Selective State Space Models for Video\n  Turbulence Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Phase Distortion with Selective State Space Models for Video\n  Turbulence Mitigation"
                },
                "summary": "Atmospheric turbulence is a major source of image degradation in long-range\nimaging systems. Although numerous deep learning-based turbulence mitigation\n(TM) methods have been proposed, many are slow, memory-hungry, and do not\ngeneralize well. In the spatial domain, methods based on convolutional\noperators have a limited receptive field, so they cannot handle a large spatial\ndependency required by turbulence. In the temporal domain, methods relying on\nself-attention can, in theory, leverage the lucky effects of turbulence, but\ntheir quadratic complexity makes it difficult to scale to many frames.\nTraditional recurrent aggregation methods face parallelization challenges.\n  In this paper, we present a new TM method based on two concepts: (1) A\nturbulence mitigation network based on the Selective State Space Model\n(MambaTM). MambaTM provides a global receptive field in each layer across\nspatial and temporal dimensions while maintaining linear computational\ncomplexity. (2) Learned Latent Phase Distortion (LPD). LPD guides the state\nspace model. Unlike classical Zernike-based representations of phase\ndistortion, the new LPD map uniquely captures the actual effects of turbulence,\nsignificantly improving the model's capability to estimate degradation by\nreducing the ill-posedness. Our proposed method exceeds current\nstate-of-the-art networks on various synthetic and real-world TM benchmarks\nwith significantly faster inference speed. The code is available at\nhttp://github.com/xg416/MambaTM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atmospheric turbulence is a major source of image degradation in long-range\nimaging systems. Although numerous deep learning-based turbulence mitigation\n(TM) methods have been proposed, many are slow, memory-hungry, and do not\ngeneralize well. In the spatial domain, methods based on convolutional\noperators have a limited receptive field, so they cannot handle a large spatial\ndependency required by turbulence. In the temporal domain, methods relying on\nself-attention can, in theory, leverage the lucky effects of turbulence, but\ntheir quadratic complexity makes it difficult to scale to many frames.\nTraditional recurrent aggregation methods face parallelization challenges.\n  In this paper, we present a new TM method based on two concepts: (1) A\nturbulence mitigation network based on the Selective State Space Model\n(MambaTM). MambaTM provides a global receptive field in each layer across\nspatial and temporal dimensions while maintaining linear computational\ncomplexity. (2) Learned Latent Phase Distortion (LPD). LPD guides the state\nspace model. Unlike classical Zernike-based representations of phase\ndistortion, the new LPD map uniquely captures the actual effects of turbulence,\nsignificantly improving the model's capability to estimate degradation by\nreducing the ill-posedness. Our proposed method exceeds current\nstate-of-the-art networks on various synthetic and real-world TM benchmarks\nwith significantly faster inference speed. The code is available at\nhttp://github.com/xg416/MambaTM."
                },
                "authors": [
                    {
                        "name": "Xingguang Zhang"
                    },
                    {
                        "name": "Nicholas Chimitt"
                    },
                    {
                        "name": "Xijun Wang"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Stanley H. Chan"
                    }
                ],
                "author_detail": {
                    "name": "Stanley H. Chan"
                },
                "author": "Stanley H. Chan",
                "arxiv_comment": "CVPR 2025, project page: https://xg416.github.io/MambaTM/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02693v1",
                "updated": "2025-04-03T15:32:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    32,
                    25,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T15:32:25Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    32,
                    25,
                    3,
                    93,
                    0
                ],
                "title": "Joint Modeling of Spatial Dependencies Across Multiple Subjects in\n  Multiplexed Tissue Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Modeling of Spatial Dependencies Across Multiple Subjects in\n  Multiplexed Tissue Imaging"
                },
                "summary": "The tumor microenvironment (TME) is a spatially heterogeneous ecosystem where\ncellular interactions shape tumor progression and response to therapy.\nMultiplexed imaging technologies enable high-resolution spatial\ncharacterization of the TME, yet statistical methods for analyzing\nmulti-subject spatial tissue data remain limited. We propose a Bayesian\nhierarchical model for inferring spatial dependencies in multiplexed imaging\ndatasets across multiple subjects. Our model represents the TME as a\nmultivariate log-Gaussian Cox process, where spatial intensity functions of\ndifferent cell types are governed by a latent multivariate Gaussian process. By\npooling information across subjects, we estimate spatial correlation functions\nthat capture within-type and cross-type dependencies, enabling interpretable\ninference about disease-specific cellular organization. We validate our method\nusing simulations, demonstrating robustness to latent factor specification and\nspatial resolution. We apply our approach to two multiplexed imaging datasets:\npancreatic cancer and colorectal cancer, revealing distinct spatial\norganization patterns across disease subtypes and highlighting tumor-immune\ninteractions that differentiate immune-permissive and immune-exclusive\nmicroenvironments. These findings provide insight into mechanisms of immune\nevasion and may inform novel therapeutic strategies. Our approach offers a\nprincipled framework for modeling spatial dependencies in multi-subject data,\nwith broader applicability to spatially resolved omics and imaging studies. An\nR package, available online, implements our methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The tumor microenvironment (TME) is a spatially heterogeneous ecosystem where\ncellular interactions shape tumor progression and response to therapy.\nMultiplexed imaging technologies enable high-resolution spatial\ncharacterization of the TME, yet statistical methods for analyzing\nmulti-subject spatial tissue data remain limited. We propose a Bayesian\nhierarchical model for inferring spatial dependencies in multiplexed imaging\ndatasets across multiple subjects. Our model represents the TME as a\nmultivariate log-Gaussian Cox process, where spatial intensity functions of\ndifferent cell types are governed by a latent multivariate Gaussian process. By\npooling information across subjects, we estimate spatial correlation functions\nthat capture within-type and cross-type dependencies, enabling interpretable\ninference about disease-specific cellular organization. We validate our method\nusing simulations, demonstrating robustness to latent factor specification and\nspatial resolution. We apply our approach to two multiplexed imaging datasets:\npancreatic cancer and colorectal cancer, revealing distinct spatial\norganization patterns across disease subtypes and highlighting tumor-immune\ninteractions that differentiate immune-permissive and immune-exclusive\nmicroenvironments. These findings provide insight into mechanisms of immune\nevasion and may inform novel therapeutic strategies. Our approach offers a\nprincipled framework for modeling spatial dependencies in multi-subject data,\nwith broader applicability to spatially resolved omics and imaging studies. An\nR package, available online, implements our methods."
                },
                "authors": [
                    {
                        "name": "Joel Eliason"
                    },
                    {
                        "name": "Arvind Rao"
                    },
                    {
                        "name": "Timothy L Frankel"
                    },
                    {
                        "name": "Michele Peruzzi"
                    }
                ],
                "author_detail": {
                    "name": "Michele Peruzzi"
                },
                "author": "Michele Peruzzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02671v1",
                "updated": "2025-04-03T15:13:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    13,
                    36,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T15:13:36Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    13,
                    36,
                    3,
                    93,
                    0
                ],
                "title": "LLM for Complex Reasoning Task: An Exploratory Study in Fermi Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM for Complex Reasoning Task: An Exploratory Study in Fermi Problems"
                },
                "summary": "Fermi Problems (FPs) are mathematical reasoning tasks that require human-like\nlogic and numerical reasoning. Unlike other reasoning questions, FPs often\ninvolve real-world impracticalities or ambiguous concepts, making them\nchallenging even for humans to solve. Despite advancements in AI, particularly\nwith large language models (LLMs) in various reasoning tasks, FPs remain\nrelatively under-explored. This work conducted an exploratory study to examine\nthe capabilities and limitations of LLMs in solving FPs. We first evaluated the\noverall performance of three advanced LLMs using a publicly available FP\ndataset. We designed prompts according to the recently proposed TELeR taxonomy,\nincluding a zero-shot scenario. Results indicated that all three LLMs achieved\na fp_score (range between 0 - 1) below 0.5, underscoring the inherent\ndifficulty of these reasoning tasks. To further investigate, we categorized FPs\ninto standard and specific questions, hypothesizing that LLMs would perform\nbetter on standard questions, which are characterized by clarity and\nconciseness, than on specific ones. Comparative experiments confirmed this\nhypothesis, demonstrating that LLMs performed better on standard FPs in terms\nof both accuracy and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fermi Problems (FPs) are mathematical reasoning tasks that require human-like\nlogic and numerical reasoning. Unlike other reasoning questions, FPs often\ninvolve real-world impracticalities or ambiguous concepts, making them\nchallenging even for humans to solve. Despite advancements in AI, particularly\nwith large language models (LLMs) in various reasoning tasks, FPs remain\nrelatively under-explored. This work conducted an exploratory study to examine\nthe capabilities and limitations of LLMs in solving FPs. We first evaluated the\noverall performance of three advanced LLMs using a publicly available FP\ndataset. We designed prompts according to the recently proposed TELeR taxonomy,\nincluding a zero-shot scenario. Results indicated that all three LLMs achieved\na fp_score (range between 0 - 1) below 0.5, underscoring the inherent\ndifficulty of these reasoning tasks. To further investigate, we categorized FPs\ninto standard and specific questions, hypothesizing that LLMs would perform\nbetter on standard questions, which are characterized by clarity and\nconciseness, than on specific ones. Comparative experiments confirmed this\nhypothesis, demonstrating that LLMs performed better on standard FPs in terms\nof both accuracy and efficiency."
                },
                "authors": [
                    {
                        "name": "Zishuo Liu"
                    },
                    {
                        "name": "Carlos Rabat Villarreal"
                    },
                    {
                        "name": "Mostafa Rahgouy"
                    },
                    {
                        "name": "Amit Das"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Chang Ren"
                    },
                    {
                        "name": "Dongji Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dongji Feng"
                },
                "author": "Dongji Feng",
                "arxiv_comment": "7 pages,7 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02670v1",
                "updated": "2025-04-03T15:11:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    11,
                    55,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T15:11:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    11,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Affordable AI Assistants with Knowledge Graph of Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordable AI Assistants with Knowledge Graph of Thoughts"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose the Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively. For example, KGoT achieves a 29%\nimprovement in task success rates on the GAIA benchmark compared to Hugging\nFace Agents with GPT-4o mini, while reducing costs by over 36x compared to\nGPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and\n37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a\nscalable, affordable, and high-performing solution for AI assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose the Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively. For example, KGoT achieves a 29%\nimprovement in task success rates on the GAIA benchmark compared to Hugging\nFace Agents with GPT-4o mini, while reducing costs by over 36x compared to\nGPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and\n37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a\nscalable, affordable, and high-performing solution for AI assistants."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Lorenzo Paleari"
                    },
                    {
                        "name": "Jia Hao Andrea Jiang"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Diana Khimey"
                    },
                    {
                        "name": "Jón Gunnar Hannesson"
                    },
                    {
                        "name": "Grzegorz Kwaśniewski"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11433v2",
                "updated": "2025-04-03T15:05:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    5,
                    55,
                    3,
                    93,
                    0
                ],
                "published": "2024-09-15T14:21:28Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    21,
                    28,
                    6,
                    259,
                    0
                ],
                "title": "Ergodicity shapes inference in biological reactions driven by a latent\n  trajectory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ergodicity shapes inference in biological reactions driven by a latent\n  trajectory"
                },
                "summary": "Many natural phenomena are quantified by counts of observable events, from\nthe annihilation of quasiparticles in a lattice to predator-prey encounters on\na landscape to spikes in a neural network. These events are triggered at random\nintervals, when an underlying, often unobserved and therefore latent, dynamical\nsystem occupies a set of reactive states within its phase space. We show how\nthe ergodicity of this latent dynamical system, i.e. existence of a\nwell-behaved limiting stationary distribution, constrains the statistics of the\nreaction counts. This formulation makes explicit the conditions under which the\ncounting process approaches a limiting Poisson process, a subject of debate in\nthe application of counting processes to different fields. We show that the\noverdispersal relative to this limit encodes properties of the latent\ntrajectory through its hitting times. These results set bounds on how\ninformation about a latent process can be inferred from a local detector, which\nwe explore for two biophysical scenarios. First, in estimating an animal's\nactivity level by how often it crosses a detector, we show how the mean count\ncan fail to give any information on movement parameters, which are encoded in\nhigher order moments. Second, we show how the variance of the inter-reaction\ntime sets a fundamental limit on how precisely the size of a population of\ntrajectories can be inferred by a detector, vastly generalizing the\nBerg-Purcell limit for chemosensation. Overall, we develop a flexible\ntheoretical framework to quantify inter-event time distributions in\nreaction-diffusion systems that clarifies existing debates in the literature\nand explicitly shows which properties of latent processes can be inferred from\nobserved reactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many natural phenomena are quantified by counts of observable events, from\nthe annihilation of quasiparticles in a lattice to predator-prey encounters on\na landscape to spikes in a neural network. These events are triggered at random\nintervals, when an underlying, often unobserved and therefore latent, dynamical\nsystem occupies a set of reactive states within its phase space. We show how\nthe ergodicity of this latent dynamical system, i.e. existence of a\nwell-behaved limiting stationary distribution, constrains the statistics of the\nreaction counts. This formulation makes explicit the conditions under which the\ncounting process approaches a limiting Poisson process, a subject of debate in\nthe application of counting processes to different fields. We show that the\noverdispersal relative to this limit encodes properties of the latent\ntrajectory through its hitting times. These results set bounds on how\ninformation about a latent process can be inferred from a local detector, which\nwe explore for two biophysical scenarios. First, in estimating an animal's\nactivity level by how often it crosses a detector, we show how the mean count\ncan fail to give any information on movement parameters, which are encoded in\nhigher order moments. Second, we show how the variance of the inter-reaction\ntime sets a fundamental limit on how precisely the size of a population of\ntrajectories can be inferred by a detector, vastly generalizing the\nBerg-Purcell limit for chemosensation. Overall, we develop a flexible\ntheoretical framework to quantify inter-event time distributions in\nreaction-diffusion systems that clarifies existing debates in the literature\nand explicitly shows which properties of latent processes can be inferred from\nobserved reactions."
                },
                "authors": [
                    {
                        "name": "Benjamin Garcia de Figueiredo"
                    },
                    {
                        "name": "Justin M. Calabrese"
                    },
                    {
                        "name": "William F. Fagan"
                    },
                    {
                        "name": "Ricardo Martinez-Garcia"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Martinez-Garcia"
                },
                "author": "Ricardo Martinez-Garcia",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02658v1",
                "updated": "2025-04-03T14:54:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    54,
                    17,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:54:17Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    54,
                    17,
                    3,
                    93,
                    0
                ],
                "title": "MiLo: Efficient Quantized MoE Inference with Mixture of Low-Rank\n  Compensators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiLo: Efficient Quantized MoE Inference with Mixture of Low-Rank\n  Compensators"
                },
                "summary": "A critical approach for efficiently deploying Mixture-of-Experts (MoE) models\nwith massive parameters is quantization. However, state-of-the-art MoE models\nsuffer from non-negligible accuracy loss with extreme quantization, such as\nunder 4 bits. To address this, we introduce MiLo, a novel method that augments\nhighly quantized MoEs with a mixture of low-rank compensators. These\ncompensators consume only a small amount of additional memory but significantly\nrecover accuracy loss from extreme quantization. MiLo also identifies that\nMoEmodels exhibit distinctive characteristics across weights due to their\nhybrid dense-sparse architectures, and employs adaptive rank selection policies\nalong with iterative optimizations to close the accuracy gap. MiLo does not\nrely on calibration data, allowing it to generalize to different MoE models and\ndatasets without overfitting to a calibration set. To avoid the hardware\ninefficiencies of extreme quantization, such as 3-bit, MiLo develops Tensor\nCore-friendly 3-bit kernels, enabling measured latency speedups on 3-bit\nquantized MoE models. Our evaluation shows that MiLo outperforms existing\nmethods on SoTA MoE models across various tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical approach for efficiently deploying Mixture-of-Experts (MoE) models\nwith massive parameters is quantization. However, state-of-the-art MoE models\nsuffer from non-negligible accuracy loss with extreme quantization, such as\nunder 4 bits. To address this, we introduce MiLo, a novel method that augments\nhighly quantized MoEs with a mixture of low-rank compensators. These\ncompensators consume only a small amount of additional memory but significantly\nrecover accuracy loss from extreme quantization. MiLo also identifies that\nMoEmodels exhibit distinctive characteristics across weights due to their\nhybrid dense-sparse architectures, and employs adaptive rank selection policies\nalong with iterative optimizations to close the accuracy gap. MiLo does not\nrely on calibration data, allowing it to generalize to different MoE models and\ndatasets without overfitting to a calibration set. To avoid the hardware\ninefficiencies of extreme quantization, such as 3-bit, MiLo develops Tensor\nCore-friendly 3-bit kernels, enabling measured latency speedups on 3-bit\nquantized MoE models. Our evaluation shows that MiLo outperforms existing\nmethods on SoTA MoE models across various tasks."
                },
                "authors": [
                    {
                        "name": "Beichen Huang"
                    },
                    {
                        "name": "Yueming Yuan"
                    },
                    {
                        "name": "Zelei Shao"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14846v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14846v4",
                "updated": "2025-04-03T14:45:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    45,
                    58,
                    3,
                    93,
                    0
                ],
                "published": "2025-01-24T10:49:45Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    49,
                    45,
                    4,
                    24,
                    0
                ],
                "title": "Wormhole Memory: A Rubik's Cube for Cross-Dialogue Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wormhole Memory: A Rubik's Cube for Cross-Dialogue Retrieval"
                },
                "summary": "In view of the gap in the current large language model in sharing memory\nacross dialogues, this research proposes a wormhole memory module (WMM) to\nrealize memory as a Rubik's cube that can be arbitrarily retrieved between\ndifferent dialogues. Through simulation experiments, the researcher built an\nexperimental framework based on the Python environment and used setting memory\nbarriers to simulate the current situation where memories between LLMs\ndialogues are difficult to share. The CoQA development data set was imported\ninto the experiment, and the feasibility of its cross-dialogue memory retrieval\nfunction was verified for WMM's nonlinear indexing and dynamic retrieval, and a\ncomparative analysis was conducted with the capabilities of Titans and MemGPT\nmemory modules. Experimental results show that WMM demonstrated the ability to\nretrieve memory across dialogues and the stability of quantitative indicators\nin eight experiments. It contributes new technical approaches to the\noptimization of memory management of LLMs and provides experience for the\npractical application in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In view of the gap in the current large language model in sharing memory\nacross dialogues, this research proposes a wormhole memory module (WMM) to\nrealize memory as a Rubik's cube that can be arbitrarily retrieved between\ndifferent dialogues. Through simulation experiments, the researcher built an\nexperimental framework based on the Python environment and used setting memory\nbarriers to simulate the current situation where memories between LLMs\ndialogues are difficult to share. The CoQA development data set was imported\ninto the experiment, and the feasibility of its cross-dialogue memory retrieval\nfunction was verified for WMM's nonlinear indexing and dynamic retrieval, and a\ncomparative analysis was conducted with the capabilities of Titans and MemGPT\nmemory modules. Experimental results show that WMM demonstrated the ability to\nretrieve memory across dialogues and the stability of quantitative indicators\nin eight experiments. It contributes new technical approaches to the\noptimization of memory management of LLMs and provides experience for the\npractical application in the future."
                },
                "authors": [
                    {
                        "name": "Libo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Libo Wang"
                },
                "author": "Libo Wang",
                "arxiv_comment": "The experimental process and code have been uploaded to the Github\n  repository, the link is:\n  https://github.com/brucewang123456789/GeniusTrail/tree/main/Wormhole%20Memory%20Module",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14846v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14846v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02646v1",
                "updated": "2025-04-03T14:40:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    40,
                    40,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:40:40Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    40,
                    40,
                    3,
                    93,
                    0
                ],
                "title": "Prompt Optimization with Logged Bandit Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Optimization with Logged Bandit Data"
                },
                "summary": "We study how to use naturally available user feedback, such as clicks, to\noptimize large language model (LLM) pipelines for generating personalized\nsentences using prompts. Naive approaches, which estimate the policy gradient\nin the prompt space, suffer either from variance caused by the large action\nspace of prompts or bias caused by inaccurate reward predictions. To circumvent\nthese challenges, we propose a novel kernel-based off-policy gradient method,\nwhich estimates the policy gradient by leveraging similarity among generated\nsentences, substantially reducing variance while suppressing the bias.\nEmpirical results on our newly established suite of benchmarks demonstrate the\neffectiveness of the proposed approach in generating personalized descriptions\nfor movie recommendations, particularly when the number of candidate prompts is\nlarge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study how to use naturally available user feedback, such as clicks, to\noptimize large language model (LLM) pipelines for generating personalized\nsentences using prompts. Naive approaches, which estimate the policy gradient\nin the prompt space, suffer either from variance caused by the large action\nspace of prompts or bias caused by inaccurate reward predictions. To circumvent\nthese challenges, we propose a novel kernel-based off-policy gradient method,\nwhich estimates the policy gradient by leveraging similarity among generated\nsentences, substantially reducing variance while suppressing the bias.\nEmpirical results on our newly established suite of benchmarks demonstrate the\neffectiveness of the proposed approach in generating personalized descriptions\nfor movie recommendations, particularly when the number of candidate prompts is\nlarge."
                },
                "authors": [
                    {
                        "name": "Haruka Kiyohara"
                    },
                    {
                        "name": "Daniel Yiming Cao"
                    },
                    {
                        "name": "Yuta Saito"
                    },
                    {
                        "name": "Thorsten Joachims"
                    }
                ],
                "author_detail": {
                    "name": "Thorsten Joachims"
                },
                "author": "Thorsten Joachims",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03804v2",
                "updated": "2025-04-03T14:35:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    35,
                    1,
                    3,
                    93,
                    0
                ],
                "published": "2024-10-04T10:25:52Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    10,
                    25,
                    52,
                    4,
                    278,
                    0
                ],
                "title": "Mixture of Attentions For Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Attentions For Speculative Decoding"
                },
                "summary": "The growth in the number of parameters of Large Language Models (LLMs) has\nled to a significant surge in computational requirements, making them\nchallenging and costly to deploy. Speculative decoding (SD) leverages smaller\nmodels to efficiently propose future tokens, which are then verified by the LLM\nin parallel. Small models that utilise activations from the LLM currently\nachieve the fastest decoding speeds. However, we identify several limitations\nof SD models including the lack of on-policyness during training and partial\nobservability. To address these shortcomings, we propose a more grounded\narchitecture for small models by introducing a Mixture of Attentions for SD.\nOur novel architecture can be applied in two scenarios: a conventional single\ndevice deployment and a novel client-server deployment where the small model is\nhosted on a consumer device and the LLM on a server. In a single-device\nscenario, we demonstrate state-of-the-art speedups improving EAGLE-2 by 9.5%\nand its acceptance length by 25%. In a client-server setting, our experiments\ndemonstrate: 1) state-of-the-art latencies with minimal calls to the server for\ndifferent network conditions, and 2) in the event of a complete disconnection,\nour approach can maintain higher accuracy compared to other SD methods and\ndemonstrates advantages over API calls to LLMs, which would otherwise be unable\nto continue the generation process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth in the number of parameters of Large Language Models (LLMs) has\nled to a significant surge in computational requirements, making them\nchallenging and costly to deploy. Speculative decoding (SD) leverages smaller\nmodels to efficiently propose future tokens, which are then verified by the LLM\nin parallel. Small models that utilise activations from the LLM currently\nachieve the fastest decoding speeds. However, we identify several limitations\nof SD models including the lack of on-policyness during training and partial\nobservability. To address these shortcomings, we propose a more grounded\narchitecture for small models by introducing a Mixture of Attentions for SD.\nOur novel architecture can be applied in two scenarios: a conventional single\ndevice deployment and a novel client-server deployment where the small model is\nhosted on a consumer device and the LLM on a server. In a single-device\nscenario, we demonstrate state-of-the-art speedups improving EAGLE-2 by 9.5%\nand its acceptance length by 25%. In a client-server setting, our experiments\ndemonstrate: 1) state-of-the-art latencies with minimal calls to the server for\ndifferent network conditions, and 2) in the event of a complete disconnection,\nour approach can maintain higher accuracy compared to other SD methods and\ndemonstrates advantages over API calls to LLMs, which would otherwise be unable\nto continue the generation process."
                },
                "authors": [
                    {
                        "name": "Matthieu Zimmer"
                    },
                    {
                        "name": "Milan Gritta"
                    },
                    {
                        "name": "Gerasimos Lampouras"
                    },
                    {
                        "name": "Haitham Bou Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "Accepted at International Conference on Learning Representations\n  (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23037v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23037v2",
                "updated": "2025-04-03T14:32:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    32,
                    44,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-29T11:02:20Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    11,
                    2,
                    20,
                    5,
                    88,
                    0
                ],
                "title": "Agentic Large Language Models, a survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Large Language Models, a survey"
                },
                "summary": "There is great interest in agentic LLMs, large language models that act as\nagents. We review the growing body of work in this area and provide a research\nagenda. Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We\norganize the literature according to these three categories. The research in\nthe first category focuses on reasoning, reflection, and retrieval, aiming to\nimprove decision making; the second category focuses on action models, robots,\nand tools, aiming for agents that act as useful assistants; the third category\nfocuses on multi-agent systems, aiming for collaborative task solving and\nsimulating interaction to study emergent social behavior. We find that works\nmutually benefit from results in other categories: retrieval enables tool use,\nreflection improves multi-agent collaboration, and reasoning benefits all\ncategories. We discuss applications of agentic LLMs and provide an agenda for\nfurther research. Important applications are in medical diagnosis, logistics\nand financial market analysis. Meanwhile, self-reflective agents playing roles\nand interacting with one another augment the process of scientific research\nitself. Further, agentic LLMs may provide a solution for the problem of LLMs\nrunning out of training data: inference-time behavior generates new training\nstates, such that LLMs can keep learning without needing ever larger datasets.\nWe note that there is risk associated with LLM assistants taking action in the\nreal world, while agentic LLMs are also likely to benefit society.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is great interest in agentic LLMs, large language models that act as\nagents. We review the growing body of work in this area and provide a research\nagenda. Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We\norganize the literature according to these three categories. The research in\nthe first category focuses on reasoning, reflection, and retrieval, aiming to\nimprove decision making; the second category focuses on action models, robots,\nand tools, aiming for agents that act as useful assistants; the third category\nfocuses on multi-agent systems, aiming for collaborative task solving and\nsimulating interaction to study emergent social behavior. We find that works\nmutually benefit from results in other categories: retrieval enables tool use,\nreflection improves multi-agent collaboration, and reasoning benefits all\ncategories. We discuss applications of agentic LLMs and provide an agenda for\nfurther research. Important applications are in medical diagnosis, logistics\nand financial market analysis. Meanwhile, self-reflective agents playing roles\nand interacting with one another augment the process of scientific research\nitself. Further, agentic LLMs may provide a solution for the problem of LLMs\nrunning out of training data: inference-time behavior generates new training\nstates, such that LLMs can keep learning without needing ever larger datasets.\nWe note that there is risk associated with LLM assistants taking action in the\nreal world, while agentic LLMs are also likely to benefit society."
                },
                "authors": [
                    {
                        "name": "Aske Plaat"
                    },
                    {
                        "name": "Max van Duijn"
                    },
                    {
                        "name": "Niki van Stein"
                    },
                    {
                        "name": "Mike Preuss"
                    },
                    {
                        "name": "Peter van der Putten"
                    },
                    {
                        "name": "Kees Joost Batenburg"
                    }
                ],
                "author_detail": {
                    "name": "Kees Joost Batenburg"
                },
                "author": "Kees Joost Batenburg",
                "arxiv_comment": "Website: https://askeplaat.github.io/agentic-llm-survey-site/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23037v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23037v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02630v1",
                "updated": "2025-04-03T14:28:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    28,
                    13,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:28:13Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    28,
                    13,
                    3,
                    93,
                    0
                ],
                "title": "Grammar-based Ordinary Differential Equation Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grammar-based Ordinary Differential Equation Discovery"
                },
                "summary": "The understanding and modeling of complex physical phenomena through\ndynamical systems has historically driven scientific progress, as it provides\nthe tools for predicting the behavior of different systems under diverse\nconditions through time. The discovery of dynamical systems has been\nindispensable in engineering, as it allows for the analysis and prediction of\ncomplex behaviors for computational modeling, diagnostics, prognostics, and\ncontrol of engineered systems. Joining recent efforts that harness the power of\nsymbolic regression in this domain, we propose a novel framework for the\nend-to-end discovery of ordinary differential equations (ODEs), termed\nGrammar-based ODE Discovery Engine (GODE). The proposed methodology combines\nformal grammars with dimensionality reduction and stochastic search for\nefficiently navigating high-dimensional combinatorial spaces. Grammars allow us\nto seed domain knowledge and structure for both constraining, as well as,\nexploring the space of candidate expressions. GODE proves to be more sample-\nand parameter-efficient than state-of-the-art transformer-based models and to\ndiscover more accurate and parsimonious ODE expressions than both genetic\nprogramming- and other grammar-based methods for more complex inference tasks,\nsuch as the discovery of structural dynamics. Thus, we introduce a tool that\ncould play a catalytic role in dynamics discovery tasks, including modeling,\nsystem identification, and monitoring tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The understanding and modeling of complex physical phenomena through\ndynamical systems has historically driven scientific progress, as it provides\nthe tools for predicting the behavior of different systems under diverse\nconditions through time. The discovery of dynamical systems has been\nindispensable in engineering, as it allows for the analysis and prediction of\ncomplex behaviors for computational modeling, diagnostics, prognostics, and\ncontrol of engineered systems. Joining recent efforts that harness the power of\nsymbolic regression in this domain, we propose a novel framework for the\nend-to-end discovery of ordinary differential equations (ODEs), termed\nGrammar-based ODE Discovery Engine (GODE). The proposed methodology combines\nformal grammars with dimensionality reduction and stochastic search for\nefficiently navigating high-dimensional combinatorial spaces. Grammars allow us\nto seed domain knowledge and structure for both constraining, as well as,\nexploring the space of candidate expressions. GODE proves to be more sample-\nand parameter-efficient than state-of-the-art transformer-based models and to\ndiscover more accurate and parsimonious ODE expressions than both genetic\nprogramming- and other grammar-based methods for more complex inference tasks,\nsuch as the discovery of structural dynamics. Thus, we introduce a tool that\ncould play a catalytic role in dynamics discovery tasks, including modeling,\nsystem identification, and monitoring tasks."
                },
                "authors": [
                    {
                        "name": "Karin L. Yu"
                    },
                    {
                        "name": "Eleni Chatzi"
                    },
                    {
                        "name": "Georgios Kissas"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Kissas"
                },
                "author": "Georgios Kissas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02627v1",
                "updated": "2025-04-03T14:25:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    25,
                    19,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:25:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    25,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Incorporating the ChEES Criterion into Sequential Monte Carlo Samplers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating the ChEES Criterion into Sequential Monte Carlo Samplers"
                },
                "summary": "Markov chain Monte Carlo (MCMC) methods are a powerful but computationally\nexpensive way of performing non-parametric Bayesian inference. MCMC proposals\nwhich utilise gradients, such as Hamiltonian Monte Carlo (HMC), can better\nexplore the parameter space of interest if the additional hyper-parameters are\nchosen well. The No-U-Turn Sampler (NUTS) is a variant of HMC which is\nextremely effective at selecting these hyper-parameters but is slow to run and\nis not suited to GPU architectures. An alternative to NUTS, Change in the\nEstimator of the Expected Square HMC (ChEES-HMC) was shown not only to run\nfaster than NUTS on GPU but also sample from posteriors more efficiently.\nSequential Monte Carlo (SMC) samplers are another sampling method which instead\noutput weighted samples from the posterior. They are very amenable to\nparallelisation and therefore being run on GPUs while having additional\nflexibility in their choice of proposal over MCMC. We incorporate (ChEEs-HMC)\nas a proposal into SMC samplers and demonstrate competitive but faster\nperformance than NUTS on a number of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov chain Monte Carlo (MCMC) methods are a powerful but computationally\nexpensive way of performing non-parametric Bayesian inference. MCMC proposals\nwhich utilise gradients, such as Hamiltonian Monte Carlo (HMC), can better\nexplore the parameter space of interest if the additional hyper-parameters are\nchosen well. The No-U-Turn Sampler (NUTS) is a variant of HMC which is\nextremely effective at selecting these hyper-parameters but is slow to run and\nis not suited to GPU architectures. An alternative to NUTS, Change in the\nEstimator of the Expected Square HMC (ChEES-HMC) was shown not only to run\nfaster than NUTS on GPU but also sample from posteriors more efficiently.\nSequential Monte Carlo (SMC) samplers are another sampling method which instead\noutput weighted samples from the posterior. They are very amenable to\nparallelisation and therefore being run on GPUs while having additional\nflexibility in their choice of proposal over MCMC. We incorporate (ChEEs-HMC)\nas a proposal into SMC samplers and demonstrate competitive but faster\nperformance than NUTS on a number of tasks."
                },
                "authors": [
                    {
                        "name": "Andrew Millard"
                    },
                    {
                        "name": "Joshua Murphy"
                    },
                    {
                        "name": "Daniel Frisch"
                    },
                    {
                        "name": "Simon Maskell"
                    }
                ],
                "author_detail": {
                    "name": "Simon Maskell"
                },
                "author": "Simon Maskell",
                "arxiv_comment": "16 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02624v1",
                "updated": "2025-04-03T14:21:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    21,
                    35,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:21:35Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    21,
                    35,
                    3,
                    93,
                    0
                ],
                "title": "EmbodiedSense: Understanding Embodied Activities with Earphones",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmbodiedSense: Understanding Embodied Activities with Earphones"
                },
                "summary": "In this paper, we propose EmbodiedSense, a sensing system based on commercial\nearphones, which enables fine-grained activity logs using existing sensors. The\nactivity logs record both user activities and the scenario in which the\nactivities took place, benefiting detailed behavior understanding. By\nunderstanding both the user and the environment, EmbodiedSense addresses three\nmain challenges: the limited recognition capability caused by\ninformation-hungry configurations (i.e., limited sensors available), the\nineffective fusion to extract ambient information such as contextual scenarios,\nand the interference from ambient noise. Specifically, EmbodiedSense consists\nof a context-aware scenario recognition module and spatial-aware activity\ndetection, which is further integrated with other attributes by expert\nknowledge. We implement our system on commercial earphones equipped with\nbinaural microphones and an Inertial Measurement Unit (IMU). By distinguishing\nusage scenarios and identifying the source of sounds, EmbodiedSense enables\nfine-grained activity logs in a zero-shot manner (evaluated with up to 41\ncategories) and outperforms strong baselines like ImageBind-LLM by 38%\nF1-score. Extensive evaluations demonstrate that EmbodiedSense is a promising\nsolution for long-term and short-term activity logs and provides significant\nbenefits in monitoring the wearer's daily life.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose EmbodiedSense, a sensing system based on commercial\nearphones, which enables fine-grained activity logs using existing sensors. The\nactivity logs record both user activities and the scenario in which the\nactivities took place, benefiting detailed behavior understanding. By\nunderstanding both the user and the environment, EmbodiedSense addresses three\nmain challenges: the limited recognition capability caused by\ninformation-hungry configurations (i.e., limited sensors available), the\nineffective fusion to extract ambient information such as contextual scenarios,\nand the interference from ambient noise. Specifically, EmbodiedSense consists\nof a context-aware scenario recognition module and spatial-aware activity\ndetection, which is further integrated with other attributes by expert\nknowledge. We implement our system on commercial earphones equipped with\nbinaural microphones and an Inertial Measurement Unit (IMU). By distinguishing\nusage scenarios and identifying the source of sounds, EmbodiedSense enables\nfine-grained activity logs in a zero-shot manner (evaluated with up to 41\ncategories) and outperforms strong baselines like ImageBind-LLM by 38%\nF1-score. Extensive evaluations demonstrate that EmbodiedSense is a promising\nsolution for long-term and short-term activity logs and provides significant\nbenefits in monitoring the wearer's daily life."
                },
                "authors": [
                    {
                        "name": "Lixing He"
                    },
                    {
                        "name": "Bufang Yang"
                    },
                    {
                        "name": "Di Duan"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    }
                ],
                "author_detail": {
                    "name": "Guoliang Xing"
                },
                "author": "Guoliang Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02623v1",
                "updated": "2025-04-03T14:21:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    21,
                    33,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:21:33Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    21,
                    33,
                    3,
                    93,
                    0
                ],
                "title": "Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents\n  through Related and Dynamic Missions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents\n  through Related and Dynamic Missions"
                },
                "summary": "Large language models (LLMs) demonstrate strong potential as agents for tool\ninvocation due to their advanced comprehension and planning capabilities. Users\nincreasingly rely on LLM-based agents to solve complex missions through\niterative interactions. However, existing benchmarks predominantly access\nagents in single-mission scenarios, failing to capture real-world complexity.\nTo bridge this gap, we propose the Multi-Mission Tool Bench. In the benchmark,\neach test case comprises multiple interrelated missions. This design requires\nagents to dynamically adapt to evolving demands. Moreover, the proposed\nbenchmark explores all possible mission-switching patterns within a fixed\nmission number. Specifically, we propose a multi-agent data generation\nframework to construct the benchmark. We also propose a novel method to\nevaluate the accuracy and efficiency of agent decisions with dynamic decision\ntrees. Experiments on diverse open-source and closed-source LLMs reveal\ncritical factors influencing agent robustness and provide actionable insights\nto the tool invocation society.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate strong potential as agents for tool\ninvocation due to their advanced comprehension and planning capabilities. Users\nincreasingly rely on LLM-based agents to solve complex missions through\niterative interactions. However, existing benchmarks predominantly access\nagents in single-mission scenarios, failing to capture real-world complexity.\nTo bridge this gap, we propose the Multi-Mission Tool Bench. In the benchmark,\neach test case comprises multiple interrelated missions. This design requires\nagents to dynamically adapt to evolving demands. Moreover, the proposed\nbenchmark explores all possible mission-switching patterns within a fixed\nmission number. Specifically, we propose a multi-agent data generation\nframework to construct the benchmark. We also propose a novel method to\nevaluate the accuracy and efficiency of agent decisions with dynamic decision\ntrees. Experiments on diverse open-source and closed-source LLMs reveal\ncritical factors influencing agent robustness and provide actionable insights\nto the tool invocation society."
                },
                "authors": [
                    {
                        "name": "PeiJie Yu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Haorui Wang"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Feng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhang"
                },
                "author": "Feng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02622v1",
                "updated": "2025-04-03T14:21:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    21,
                    9,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:21:09Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    21,
                    9,
                    3,
                    93,
                    0
                ],
                "title": "Exploring undercurrents of learning tensions in an LLM-enhanced\n  landscape: A student-centered qualitative perspective on LLM vs Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring undercurrents of learning tensions in an LLM-enhanced\n  landscape: A student-centered qualitative perspective on LLM vs Search"
                },
                "summary": "Large language models (LLMs) are transforming how students learn by providing\nreadily available tools that can quickly augment or complete various learning\nactivities with non-trivial performance. Similar paradigm shifts have occurred\nin the past with the introduction of search engines and Wikipedia, which\nreplaced or supplemented traditional information sources such as libraries and\nbooks. This study investigates the potential for LLMs to represent the next\nshift in learning, focusing on their role in information discovery and\nsynthesis compared to existing technologies, such as search engines. Using a\nwithin-subjects, counterbalanced design, participants learned new topics using\na search engine (Google) and an LLM (ChatGPT). Post-task follow-up interviews\nexplored students' reflections, preferences, pain points, and overall\nperceptions. We present analysis of their responses that show nuanced insights\ninto when, why, and how students prefer LLMs over search engines, offering\nimplications for educators, policymakers, and technology developers navigating\nthe evolving educational landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are transforming how students learn by providing\nreadily available tools that can quickly augment or complete various learning\nactivities with non-trivial performance. Similar paradigm shifts have occurred\nin the past with the introduction of search engines and Wikipedia, which\nreplaced or supplemented traditional information sources such as libraries and\nbooks. This study investigates the potential for LLMs to represent the next\nshift in learning, focusing on their role in information discovery and\nsynthesis compared to existing technologies, such as search engines. Using a\nwithin-subjects, counterbalanced design, participants learned new topics using\na search engine (Google) and an LLM (ChatGPT). Post-task follow-up interviews\nexplored students' reflections, preferences, pain points, and overall\nperceptions. We present analysis of their responses that show nuanced insights\ninto when, why, and how students prefer LLMs over search engines, offering\nimplications for educators, policymakers, and technology developers navigating\nthe evolving educational landscape."
                },
                "authors": [
                    {
                        "name": "Rahul R. Divekar"
                    },
                    {
                        "name": "Sophia Guerra"
                    },
                    {
                        "name": "Lisette Gonzalez"
                    },
                    {
                        "name": "Natasha Boos"
                    },
                    {
                        "name": "Helen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Helen Zhou"
                },
                "author": "Helen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02620v1",
                "updated": "2025-04-03T14:20:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    20,
                    6,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:20:06Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    20,
                    6,
                    3,
                    93,
                    0
                ],
                "title": "Efficient Model Editing with Task-Localized Sparse Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Model Editing with Task-Localized Sparse Fine-tuning"
                },
                "summary": "Task arithmetic has emerged as a promising approach for editing models by\nrepresenting task-specific knowledge as composable task vectors. However,\nexisting methods rely on network linearization to derive task vectors, leading\nto computational bottlenecks during training and inference. Moreover,\nlinearization alone does not ensure weight disentanglement, the key property\nthat enables conflict-free composition of task vectors. To address this, we\npropose TaLoS which allows to build sparse task vectors with minimal\ninterference without requiring explicit linearization and sharing information\nacross tasks. We find that pre-trained models contain a subset of parameters\nwith consistently low gradient sensitivity across tasks, and that sparsely\nupdating only these parameters allows for promoting weight disentanglement\nduring fine-tuning. Our experiments prove that TaLoS improves training and\ninference efficiency while outperforming current methods in task addition and\nnegation. By enabling modular parameter editing, our approach fosters practical\ndeployment of adaptable foundation models in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task arithmetic has emerged as a promising approach for editing models by\nrepresenting task-specific knowledge as composable task vectors. However,\nexisting methods rely on network linearization to derive task vectors, leading\nto computational bottlenecks during training and inference. Moreover,\nlinearization alone does not ensure weight disentanglement, the key property\nthat enables conflict-free composition of task vectors. To address this, we\npropose TaLoS which allows to build sparse task vectors with minimal\ninterference without requiring explicit linearization and sharing information\nacross tasks. We find that pre-trained models contain a subset of parameters\nwith consistently low gradient sensitivity across tasks, and that sparsely\nupdating only these parameters allows for promoting weight disentanglement\nduring fine-tuning. Our experiments prove that TaLoS improves training and\ninference efficiency while outperforming current methods in task addition and\nnegation. By enabling modular parameter editing, our approach fosters practical\ndeployment of adaptable foundation models in real-world applications."
                },
                "authors": [
                    {
                        "name": "Leonardo Iurada"
                    },
                    {
                        "name": "Marco Ciccone"
                    },
                    {
                        "name": "Tatiana Tommasi"
                    }
                ],
                "author_detail": {
                    "name": "Tatiana Tommasi"
                },
                "author": "Tatiana Tommasi",
                "arxiv_comment": "Accepted ICLR 2025 - https://github.com/iurada/talos-task-arithmetic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02612v1",
                "updated": "2025-04-03T14:12:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    12,
                    55,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:12:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    12,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Fine-Tuning Visual Autoregressive Models for Subject-Driven Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning Visual Autoregressive Models for Subject-Driven Generation"
                },
                "summary": "Recent advances in text-to-image generative models have enabled numerous\npractical applications, including subject-driven generation, which fine-tunes\npretrained models to capture subject semantics from only a few examples. While\ndiffusion-based models produce high-quality images, their extensive denoising\nsteps result in significant computational overhead, limiting real-world\napplicability. Visual autoregressive~(VAR) models, which predict next-scale\ntokens rather than spatially adjacent ones, offer significantly faster\ninference suitable for practical deployment. In this paper, we propose the\nfirst VAR-based approach for subject-driven generation. However, na\\\"{\\i}ve\nfine-tuning VAR leads to computational overhead, language drift, and reduced\ndiversity. To address these challenges, we introduce selective layer tuning to\nreduce complexity and prior distillation to mitigate language drift.\nAdditionally, we found that the early stages have a greater influence on the\ngeneration of subject than the latter stages, which merely synthesize local\ndetails. Based on this finding, we propose scale-wise weighted tuning, which\nprioritizes coarser resolutions for promoting the model to focus on the\nsubject-relevant information instead of local details. Extensive experiments\nvalidate that our method significantly outperforms diffusion-based baselines\nacross various metrics and demonstrates its practical usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-to-image generative models have enabled numerous\npractical applications, including subject-driven generation, which fine-tunes\npretrained models to capture subject semantics from only a few examples. While\ndiffusion-based models produce high-quality images, their extensive denoising\nsteps result in significant computational overhead, limiting real-world\napplicability. Visual autoregressive~(VAR) models, which predict next-scale\ntokens rather than spatially adjacent ones, offer significantly faster\ninference suitable for practical deployment. In this paper, we propose the\nfirst VAR-based approach for subject-driven generation. However, na\\\"{\\i}ve\nfine-tuning VAR leads to computational overhead, language drift, and reduced\ndiversity. To address these challenges, we introduce selective layer tuning to\nreduce complexity and prior distillation to mitigate language drift.\nAdditionally, we found that the early stages have a greater influence on the\ngeneration of subject than the latter stages, which merely synthesize local\ndetails. Based on this finding, we propose scale-wise weighted tuning, which\nprioritizes coarser resolutions for promoting the model to focus on the\nsubject-relevant information instead of local details. Extensive experiments\nvalidate that our method significantly outperforms diffusion-based baselines\nacross various metrics and demonstrates its practical usage."
                },
                "authors": [
                    {
                        "name": "Jiwoo Chung"
                    },
                    {
                        "name": "Sangeek Hyun"
                    },
                    {
                        "name": "Hyunjun Kim"
                    },
                    {
                        "name": "Eunseo Koh"
                    },
                    {
                        "name": "MinKyu Lee"
                    },
                    {
                        "name": "Jae-Pil Heo"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Pil Heo"
                },
                "author": "Jae-Pil Heo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02609v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02609v1",
                "updated": "2025-04-03T14:10:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    10,
                    50,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:10:50Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    10,
                    50,
                    3,
                    93,
                    0
                ],
                "title": "One-loop correction to primordial tensor modes during radiation era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-loop correction to primordial tensor modes during radiation era"
                },
                "summary": "The ability to infer properties of primordial inflation relies on the\nconservation of the superhorizon perturbations between their exit during\ninflation, and their re-entry during radiation era. Any considerable departure\nfrom this property would require reinterpreting the data. This is why it is\nimportant to understand how superhorizon perturbations interact with the\nthermal plasma driving the radiation dominated Universe. We model the plasma by\nfree photons in a thermal state and compute the one-loop correction to the\npower spectrum of primordial tensor perturbations. This correction grows in\ntime and is not suppressed by any small parameter. While one-loop result is not\nreliable because it invalidates perturbation theory, it signals potentially\ninteresting effects that should be investigated further.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to infer properties of primordial inflation relies on the\nconservation of the superhorizon perturbations between their exit during\ninflation, and their re-entry during radiation era. Any considerable departure\nfrom this property would require reinterpreting the data. This is why it is\nimportant to understand how superhorizon perturbations interact with the\nthermal plasma driving the radiation dominated Universe. We model the plasma by\nfree photons in a thermal state and compute the one-loop correction to the\npower spectrum of primordial tensor perturbations. This correction grows in\ntime and is not suppressed by any small parameter. While one-loop result is not\nreliable because it invalidates perturbation theory, it signals potentially\ninteresting effects that should be investigated further."
                },
                "authors": [
                    {
                        "name": "Markus B. Fröb"
                    },
                    {
                        "name": "Dražen Glavan"
                    },
                    {
                        "name": "Paolo Meda"
                    },
                    {
                        "name": "Ignacy Sawicki"
                    }
                ],
                "author_detail": {
                    "name": "Ignacy Sawicki"
                },
                "author": "Ignacy Sawicki",
                "arxiv_comment": "20 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02609v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01956v2",
                "updated": "2025-04-03T14:07:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    7,
                    13,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T17:59:21Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    59,
                    21,
                    2,
                    92,
                    0
                ],
                "title": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in\n  One Step",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in\n  One Step"
                },
                "summary": "Recovering 3D scenes from sparse views is a challenging task due to its\ninherent ill-posed problem. Conventional methods have developed specialized\nsolutions (e.g., geometry regularization or feed-forward deterministic model)\nto mitigate the issue. However, they still suffer from performance degradation\nby minimal overlap across input views with insufficient visual information.\nFortunately, recent video generative models show promise in addressing this\nchallenge as they are capable of generating video clips with plausible 3D\nstructures. Powered by large pretrained video diffusion models, some pioneering\nresearch start to explore the potential of video generative prior and create 3D\nscenes from sparse views. Despite impressive improvements, they are limited by\nslow inference time and the lack of 3D constraint, leading to inefficiencies\nand reconstruction artifacts that do not align with real-world geometry\nstructure. In this paper, we propose VideoScene to distill the video diffusion\nmodel to generate 3D scenes in one step, aiming to build an efficient and\neffective tool to bridge the gap from video to 3D. Specifically, we design a\n3D-aware leap flow distillation strategy to leap over time-consuming redundant\ninformation and train a dynamic denoising policy network to adaptively\ndetermine the optimal leap timestep during inference. Extensive experiments\ndemonstrate that our VideoScene achieves faster and superior 3D scene\ngeneration results than previous video diffusion models, highlighting its\npotential as an efficient tool for future video to 3D applications. Project\nPage: https://hanyang-21.github.io/VideoScene",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovering 3D scenes from sparse views is a challenging task due to its\ninherent ill-posed problem. Conventional methods have developed specialized\nsolutions (e.g., geometry regularization or feed-forward deterministic model)\nto mitigate the issue. However, they still suffer from performance degradation\nby minimal overlap across input views with insufficient visual information.\nFortunately, recent video generative models show promise in addressing this\nchallenge as they are capable of generating video clips with plausible 3D\nstructures. Powered by large pretrained video diffusion models, some pioneering\nresearch start to explore the potential of video generative prior and create 3D\nscenes from sparse views. Despite impressive improvements, they are limited by\nslow inference time and the lack of 3D constraint, leading to inefficiencies\nand reconstruction artifacts that do not align with real-world geometry\nstructure. In this paper, we propose VideoScene to distill the video diffusion\nmodel to generate 3D scenes in one step, aiming to build an efficient and\neffective tool to bridge the gap from video to 3D. Specifically, we design a\n3D-aware leap flow distillation strategy to leap over time-consuming redundant\ninformation and train a dynamic denoising policy network to adaptively\ndetermine the optimal leap timestep during inference. Extensive experiments\ndemonstrate that our VideoScene achieves faster and superior 3D scene\ngeneration results than previous video diffusion models, highlighting its\npotential as an efficient tool for future video to 3D applications. Project\nPage: https://hanyang-21.github.io/VideoScene"
                },
                "authors": [
                    {
                        "name": "Hanyang Wang"
                    },
                    {
                        "name": "Fangfu Liu"
                    },
                    {
                        "name": "Jiawei Chi"
                    },
                    {
                        "name": "Yueqi Duan"
                    }
                ],
                "author_detail": {
                    "name": "Yueqi Duan"
                },
                "author": "Yueqi Duan",
                "arxiv_comment": "Accepted by CVPR 2025; Project Page:\n  https://hanyang-21.github.io/VideoScene",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02605v1",
                "updated": "2025-04-03T14:06:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    6,
                    17,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:06:17Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    6,
                    17,
                    3,
                    93,
                    0
                ],
                "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving"
                },
                "summary": "The task of issue resolving is to modify a codebase to generate a patch that\naddresses a given issue. However, existing benchmarks, such as SWE-bench, focus\nalmost exclusively on Python, making them insufficient for evaluating Large\nLanguage Models (LLMs) across diverse software ecosystems. To address this, we\nintroduce a multilingual issue-resolving benchmark, called Multi-SWE-bench,\ncovering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a\ntotal of 1,632 high-quality instances, which were carefully annotated from\n2,456 candidates by 68 expert annotators, ensuring that the benchmark can\nprovide an accurate and reliable evaluation. Based on Multi-SWE-bench, we\nevaluate a series of state-of-the-art models using three representative methods\n(Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with\nkey empirical insights. In addition, we launch a Multi-SWE-RL open-source\ncommunity, aimed at building large-scale reinforcement learning (RL) training\ndatasets for issue-resolving tasks. As an initial contribution, we release a\nset of 4,723 well-structured instances spanning seven programming languages,\nlaying a solid foundation for RL research in this domain. More importantly, we\nopen-source our entire data production pipeline, along with detailed tutorials,\nencouraging the open-source community to continuously contribute and expand the\ndataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL\ncommunity as catalysts for advancing RL toward its full potential, bringing us\none step closer to the dawn of AGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of issue resolving is to modify a codebase to generate a patch that\naddresses a given issue. However, existing benchmarks, such as SWE-bench, focus\nalmost exclusively on Python, making them insufficient for evaluating Large\nLanguage Models (LLMs) across diverse software ecosystems. To address this, we\nintroduce a multilingual issue-resolving benchmark, called Multi-SWE-bench,\ncovering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a\ntotal of 1,632 high-quality instances, which were carefully annotated from\n2,456 candidates by 68 expert annotators, ensuring that the benchmark can\nprovide an accurate and reliable evaluation. Based on Multi-SWE-bench, we\nevaluate a series of state-of-the-art models using three representative methods\n(Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with\nkey empirical insights. In addition, we launch a Multi-SWE-RL open-source\ncommunity, aimed at building large-scale reinforcement learning (RL) training\ndatasets for issue-resolving tasks. As an initial contribution, we release a\nset of 4,723 well-structured instances spanning seven programming languages,\nlaying a solid foundation for RL research in this domain. More importantly, we\nopen-source our entire data production pipeline, along with detailed tutorials,\nencouraging the open-source community to continuously contribute and expand the\ndataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL\ncommunity as catalysts for advancing RL toward its full potential, bringing us\none step closer to the dawn of AGI."
                },
                "authors": [
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Zhirong Huang"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Hanwu Chen"
                    },
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Shulin Xin"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Xiaojian Zhong"
                    },
                    {
                        "name": "Aoyan Li"
                    },
                    {
                        "name": "Siyao Liu"
                    },
                    {
                        "name": "Yongsheng Xiao"
                    },
                    {
                        "name": "Liangqiang Chen"
                    },
                    {
                        "name": "Yuyu Zhang"
                    },
                    {
                        "name": "Jing Su"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Rui Long"
                    },
                    {
                        "name": "Kai Shen"
                    },
                    {
                        "name": "Liang Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Xiang"
                },
                "author": "Liang Xiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15316v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15316v3",
                "updated": "2025-04-04T08:29:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    8,
                    29,
                    19,
                    4,
                    94,
                    0
                ],
                "published": "2024-10-20T07:03:49Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    7,
                    3,
                    49,
                    6,
                    294,
                    0
                ],
                "title": "Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nbut their application to speech-based tasks remains challenging due to the\ncomplexities of integrating audio and text modalities. This paper introduces\nIchigo, a mixed-modal model that seamlessly processes interleaved sequences of\nspeech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes\nspeech into discrete tokens and employs a uniform transformer-based\narchitecture for both speech and text modalities. This method enables joint\nreasoning and generation across modalities without the need for separate\nadapters. We present a comprehensive training methodology, including\npre-training on multilingual speech recognition datasets and fine-tuning on a\ncurated instruction dataset. Ichigo demonstrates state-of-the-art performance\non speech question-answering benchmarks, outperforming existing open-source\nspeech language models and achieving comparable results to cascaded systems.\nNotably, Ichigo exhibits a latency of just 111 ms to first token generation,\nsignificantly lower than current models. Our approach not only advances the\nfield of multimodal AI but also provides a framework for smaller research teams\nto contribute effectively to open-source speech-language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing,\nbut their application to speech-based tasks remains challenging due to the\ncomplexities of integrating audio and text modalities. This paper introduces\nIchigo, a mixed-modal model that seamlessly processes interleaved sequences of\nspeech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes\nspeech into discrete tokens and employs a uniform transformer-based\narchitecture for both speech and text modalities. This method enables joint\nreasoning and generation across modalities without the need for separate\nadapters. We present a comprehensive training methodology, including\npre-training on multilingual speech recognition datasets and fine-tuning on a\ncurated instruction dataset. Ichigo demonstrates state-of-the-art performance\non speech question-answering benchmarks, outperforming existing open-source\nspeech language models and achieving comparable results to cascaded systems.\nNotably, Ichigo exhibits a latency of just 111 ms to first token generation,\nsignificantly lower than current models. Our approach not only advances the\nfield of multimodal AI but also provides a framework for smaller research teams\nto contribute effectively to open-source speech-language models."
                },
                "authors": [
                    {
                        "name": "Alan Dao"
                    },
                    {
                        "name": "Dinh Bach Vu"
                    },
                    {
                        "name": "Huy Hoang Ha"
                    }
                ],
                "author_detail": {
                    "name": "Huy Hoang Ha"
                },
                "arxiv_affiliation": "Gia Tuan Dao",
                "author": "Huy Hoang Ha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15316v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15316v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02590v1",
                "updated": "2025-04-03T13:54:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    54,
                    53,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T13:54:53Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    54,
                    53,
                    3,
                    93,
                    0
                ],
                "title": "LexPam: Legal Procedure Awareness-Guided Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LexPam: Legal Procedure Awareness-Guided Mathematical Reasoning"
                },
                "summary": "The legal mathematical reasoning ability of LLMs is crucial when applying\nthem to real-world scenarios, as it directly affects the credibility of the\nLLM. While existing legal LLMs can perform general judicial question answering,\ntheir legal mathematical reasoning capabilities have not been trained.\nOpen-domain reasoning models, though able to generate detailed calculation\nsteps, do not follow the reasoning logic required for legal scenarios.\nAdditionally, there is currently a lack of legal mathematical reasoning\ndatasets to help validate and enhance LLMs' reasoning abilities in legal\ncontexts. To address these issues, we propose the first Chinese legal\nMathematical Reasoning Dataset, LexNum, which includes three common legal\nmathematical reasoning scenarios: economic compensation, work injury\ncompensation, and traffic accident compensation. Based on LexNum, we tested the\nperformance of existing legal LLMs and reasoning LLMs, and introduced LexPam, a\nreinforcement learning algorithm guided by legal procedural awareness to train\nLLMs, enhancing their mathematical reasoning abilities in legal scenarios.\nExperiments on tasks in the three legal scenarios show that the performance of\nexisting legal LLMs and reasoning models in legal mathematical reasoning tasks\nis unsatisfactory. LexPam can enhance the LLM's ability in these tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The legal mathematical reasoning ability of LLMs is crucial when applying\nthem to real-world scenarios, as it directly affects the credibility of the\nLLM. While existing legal LLMs can perform general judicial question answering,\ntheir legal mathematical reasoning capabilities have not been trained.\nOpen-domain reasoning models, though able to generate detailed calculation\nsteps, do not follow the reasoning logic required for legal scenarios.\nAdditionally, there is currently a lack of legal mathematical reasoning\ndatasets to help validate and enhance LLMs' reasoning abilities in legal\ncontexts. To address these issues, we propose the first Chinese legal\nMathematical Reasoning Dataset, LexNum, which includes three common legal\nmathematical reasoning scenarios: economic compensation, work injury\ncompensation, and traffic accident compensation. Based on LexNum, we tested the\nperformance of existing legal LLMs and reasoning LLMs, and introduced LexPam, a\nreinforcement learning algorithm guided by legal procedural awareness to train\nLLMs, enhancing their mathematical reasoning abilities in legal scenarios.\nExperiments on tasks in the three legal scenarios show that the performance of\nexisting legal LLMs and reasoning models in legal mathematical reasoning tasks\nis unsatisfactory. LexPam can enhance the LLM's ability in these tasks."
                },
                "authors": [
                    {
                        "name": "Kepu Zhang"
                    },
                    {
                        "name": "Guofu Xie"
                    },
                    {
                        "name": "Weijie Yu"
                    },
                    {
                        "name": "Mingyue Xu"
                    },
                    {
                        "name": "Xu Tang"
                    },
                    {
                        "name": "Yaxin Li"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02577v1",
                "updated": "2025-04-03T13:40:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    40,
                    55,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T13:40:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    40,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Reasoning Inconsistencies and How to Mitigate Them in Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Inconsistencies and How to Mitigate Them in Deep Learning"
                },
                "summary": "The recent advancements in Deep Learning models and techniques have led to\nsignificant strides in performance across diverse tasks and modalities.\nHowever, while the overall capabilities of models show promising growth, our\nunderstanding of their internal reasoning processes remains limited,\nparticularly concerning systematic inconsistencies or errors patterns of\nlogical or inferential flaws. These inconsistencies may manifest as\ncontradictory outputs, failure to generalize across similar tasks, or erroneous\nconclusions in specific contexts. Even detecting and measuring such reasoning\ndiscrepancies is challenging, as they may arise from opaque internal\nprocedures, biases and imbalances in training data, or the inherent complexity\nof the task. Without effective methods to detect, measure, and mitigate these\nerrors, there is a risk of deploying models that are biased, exploitable, or\nlogically unreliable. This thesis aims to address these issues by producing\nnovel methods for deep learning models that reason over knowledge graphs,\nnatural language, and images. The thesis contributes two techniques for\ndetecting and quantifying predictive inconsistencies originating from opaque\ninternal procedures in natural language and image processing models. To\nmitigate inconsistencies from biases in training data, this thesis presents a\ndata efficient sampling method to improve fairness and performance and a\nsynthetic dataset generation approach in low resource scenarios. Finally, the\nthesis offers two techniques to optimize the models for complex reasoning\ntasks. These methods enhance model performance while allowing for more faithful\nand interpretable exploration and exploitation during inference. Critically,\nthis thesis provides a comprehensive framework to improve the robustness,\nfairness, and interpretability of deep learning models across diverse tasks and\nmodalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in Deep Learning models and techniques have led to\nsignificant strides in performance across diverse tasks and modalities.\nHowever, while the overall capabilities of models show promising growth, our\nunderstanding of their internal reasoning processes remains limited,\nparticularly concerning systematic inconsistencies or errors patterns of\nlogical or inferential flaws. These inconsistencies may manifest as\ncontradictory outputs, failure to generalize across similar tasks, or erroneous\nconclusions in specific contexts. Even detecting and measuring such reasoning\ndiscrepancies is challenging, as they may arise from opaque internal\nprocedures, biases and imbalances in training data, or the inherent complexity\nof the task. Without effective methods to detect, measure, and mitigate these\nerrors, there is a risk of deploying models that are biased, exploitable, or\nlogically unreliable. This thesis aims to address these issues by producing\nnovel methods for deep learning models that reason over knowledge graphs,\nnatural language, and images. The thesis contributes two techniques for\ndetecting and quantifying predictive inconsistencies originating from opaque\ninternal procedures in natural language and image processing models. To\nmitigate inconsistencies from biases in training data, this thesis presents a\ndata efficient sampling method to improve fairness and performance and a\nsynthetic dataset generation approach in low resource scenarios. Finally, the\nthesis offers two techniques to optimize the models for complex reasoning\ntasks. These methods enhance model performance while allowing for more faithful\nand interpretable exploration and exploitation during inference. Critically,\nthis thesis provides a comprehensive framework to improve the robustness,\nfairness, and interpretability of deep learning models across diverse tasks and\nmodalities."
                },
                "authors": [
                    {
                        "name": "Erik Arakelyan"
                    }
                ],
                "author_detail": {
                    "name": "Erik Arakelyan"
                },
                "author": "Erik Arakelyan",
                "arxiv_comment": "PhD thesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00441v2",
                "updated": "2025-04-03T13:34:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    34,
                    57,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-01T05:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    5,
                    46,
                    54,
                    1,
                    91,
                    0
                ],
                "title": "No Free Lunch with Guardrails",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Free Lunch with Guardrails"
                },
                "summary": "As large language models (LLMs) and generative AI become widely adopted,\nguardrails have emerged as a key tool to ensure their safe use. However, adding\nguardrails isn't without tradeoffs; stronger security measures can reduce\nusability, while more flexible systems may leave gaps for adversarial attacks.\nIn this work, we explore whether current guardrails effectively prevent misuse\nwhile maintaining practical utility. We introduce a framework to evaluate these\ntradeoffs, measuring how different guardrails balance risk, security, and\nusability, and build an efficient guardrail.\n  Our findings confirm that there is no free lunch with guardrails;\nstrengthening security often comes at the cost of usability. To address this,\nwe propose a blueprint for designing better guardrails that minimize risk while\nmaintaining usability. We evaluate various industry guardrails, including Azure\nContent Safety, Bedrock Guardrails, OpenAI's Moderation API, Guardrails AI,\nNemo Guardrails, and Enkrypt AI guardrails. Additionally, we assess how LLMs\nlike GPT-4o, Gemini 2.0-Flash, Claude 3.5-Sonnet, and Mistral Large-Latest\nrespond under different system prompts, including simple prompts, detailed\nprompts, and detailed prompts with chain-of-thought (CoT) reasoning. Our study\nprovides a clear comparison of how different guardrails perform, highlighting\nthe challenges in balancing security and usability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) and generative AI become widely adopted,\nguardrails have emerged as a key tool to ensure their safe use. However, adding\nguardrails isn't without tradeoffs; stronger security measures can reduce\nusability, while more flexible systems may leave gaps for adversarial attacks.\nIn this work, we explore whether current guardrails effectively prevent misuse\nwhile maintaining practical utility. We introduce a framework to evaluate these\ntradeoffs, measuring how different guardrails balance risk, security, and\nusability, and build an efficient guardrail.\n  Our findings confirm that there is no free lunch with guardrails;\nstrengthening security often comes at the cost of usability. To address this,\nwe propose a blueprint for designing better guardrails that minimize risk while\nmaintaining usability. We evaluate various industry guardrails, including Azure\nContent Safety, Bedrock Guardrails, OpenAI's Moderation API, Guardrails AI,\nNemo Guardrails, and Enkrypt AI guardrails. Additionally, we assess how LLMs\nlike GPT-4o, Gemini 2.0-Flash, Claude 3.5-Sonnet, and Mistral Large-Latest\nrespond under different system prompts, including simple prompts, detailed\nprompts, and detailed prompts with chain-of-thought (CoT) reasoning. Our study\nprovides a clear comparison of how different guardrails perform, highlighting\nthe challenges in balancing security and usability."
                },
                "authors": [
                    {
                        "name": "Divyanshu Kumar"
                    },
                    {
                        "name": "Nitin Aravind Birur"
                    },
                    {
                        "name": "Tanay Baswa"
                    },
                    {
                        "name": "Sahil Agarwal"
                    },
                    {
                        "name": "Prashanth Harshangi"
                    }
                ],
                "author_detail": {
                    "name": "Prashanth Harshangi"
                },
                "author": "Prashanth Harshangi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01919v2",
                "updated": "2025-04-03T13:30:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    30,
                    35,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T17:26:40Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    26,
                    40,
                    2,
                    92,
                    0
                ],
                "title": "Bridging the Linguistic Divide: A Survey on Leveraging Large Language\n  Models for Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Linguistic Divide: A Survey on Leveraging Large Language\n  Models for Machine Translation"
                },
                "summary": "The advent of Large Language Models (LLMs) has significantly reshaped the\nlandscape of machine translation (MT), particularly for low-resource languages\nand domains that lack sufficient parallel corpora, linguistic tools, and\ncomputational infrastructure. This survey presents a comprehensive overview of\nrecent progress in leveraging LLMs for MT. We analyze techniques such as\nfew-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning\nthat enable effective adaptation to under-resourced settings. The paper also\nexplores synthetic data generation strategies using LLMs, including\nback-translation and lexical augmentation. Additionally, we compare LLM-based\ntranslation with traditional encoder-decoder models across diverse language\npairs, highlighting the strengths and limitations of each. We discuss\npersistent challenges such as hallucinations, evaluation inconsistencies, and\ninherited biases while also evaluating emerging LLM-driven metrics for\ntranslation quality. This survey offers practical insights and outlines future\ndirections for building robust, inclusive, and scalable MT systems in the era\nof large-scale generative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has significantly reshaped the\nlandscape of machine translation (MT), particularly for low-resource languages\nand domains that lack sufficient parallel corpora, linguistic tools, and\ncomputational infrastructure. This survey presents a comprehensive overview of\nrecent progress in leveraging LLMs for MT. We analyze techniques such as\nfew-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning\nthat enable effective adaptation to under-resourced settings. The paper also\nexplores synthetic data generation strategies using LLMs, including\nback-translation and lexical augmentation. Additionally, we compare LLM-based\ntranslation with traditional encoder-decoder models across diverse language\npairs, highlighting the strengths and limitations of each. We discuss\npersistent challenges such as hallucinations, evaluation inconsistencies, and\ninherited biases while also evaluating emerging LLM-driven metrics for\ntranslation quality. This survey offers practical insights and outlines future\ndirections for building robust, inclusive, and scalable MT systems in the era\nof large-scale generative models."
                },
                "authors": [
                    {
                        "name": "Baban Gain"
                    },
                    {
                        "name": "Dibyanayan Bandyopadhyay"
                    },
                    {
                        "name": "Asif Ekbal"
                    }
                ],
                "author_detail": {
                    "name": "Asif Ekbal"
                },
                "author": "Asif Ekbal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v2",
                "updated": "2025-04-03T13:28:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    28,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity ."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08794v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08794v2",
                "updated": "2025-04-03T13:22:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    22,
                    17,
                    3,
                    93,
                    0
                ],
                "published": "2024-08-16T15:07:54Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    7,
                    54,
                    4,
                    229,
                    0
                ],
                "title": "Xpikeformer: Hybrid Analog-Digital Hardware Acceleration for Spiking\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Xpikeformer: Hybrid Analog-Digital Hardware Acceleration for Spiking\n  Transformers"
                },
                "summary": "The integration of neuromorphic computing and transformers through spiking\nneural networks (SNNs) offers a promising path to energy-efficient sequence\nmodeling, with the potential to overcome the energy-intensive nature of the\nartificial neural network (ANN)-based transformers. However, the algorithmic\nefficiency of SNN-based transformers cannot be fully exploited on GPUs due to\narchitectural incompatibility. This paper introduces Xpikeformer, a hybrid\nanalog-digital hardware architecture designed to accelerate SNN-based\ntransformer models. The architecture integrates analog in-memory computing\n(AIMC) for feedforward and fully connected layers, and a stochastic spiking\nattention (SSA) engine for efficient attention mechanisms. We detail the\ndesign, implementation, and evaluation of Xpikeformer, demonstrating\nsignificant improvements in energy consumption and computational efficiency.\nThrough image classification tasks and wireless communication symbol detection\ntasks, we show that Xpikeformer can achieve inference accuracy comparable to\nthe GPU implementation of ANN-based transformers. Evaluations reveal that\nXpikeformer achieves $13\\times$ reduction in energy consumption at\napproximately the same throughput as the state-of-the-art (SOTA) digital\naccelerator for ANN-based transformers. Additionally, Xpikeformer achieves up\nto $1.9\\times$ energy reduction compared to the optimal digital ASIC projection\nof SOTA SNN-based transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of neuromorphic computing and transformers through spiking\nneural networks (SNNs) offers a promising path to energy-efficient sequence\nmodeling, with the potential to overcome the energy-intensive nature of the\nartificial neural network (ANN)-based transformers. However, the algorithmic\nefficiency of SNN-based transformers cannot be fully exploited on GPUs due to\narchitectural incompatibility. This paper introduces Xpikeformer, a hybrid\nanalog-digital hardware architecture designed to accelerate SNN-based\ntransformer models. The architecture integrates analog in-memory computing\n(AIMC) for feedforward and fully connected layers, and a stochastic spiking\nattention (SSA) engine for efficient attention mechanisms. We detail the\ndesign, implementation, and evaluation of Xpikeformer, demonstrating\nsignificant improvements in energy consumption and computational efficiency.\nThrough image classification tasks and wireless communication symbol detection\ntasks, we show that Xpikeformer can achieve inference accuracy comparable to\nthe GPU implementation of ANN-based transformers. Evaluations reveal that\nXpikeformer achieves $13\\times$ reduction in energy consumption at\napproximately the same throughput as the state-of-the-art (SOTA) digital\naccelerator for ANN-based transformers. Additionally, Xpikeformer achieves up\nto $1.9\\times$ energy reduction compared to the optimal digital ASIC projection\nof SOTA SNN-based transformers."
                },
                "authors": [
                    {
                        "name": "Zihang Song"
                    },
                    {
                        "name": "Prabodh Katti"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    },
                    {
                        "name": "Bipin Rajendran"
                    }
                ],
                "author_detail": {
                    "name": "Bipin Rajendran"
                },
                "author": "Bipin Rajendran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08794v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08794v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02559v1",
                "updated": "2025-04-03T13:15:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    15,
                    18,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T13:15:18Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    15,
                    18,
                    3,
                    93,
                    0
                ],
                "title": "Leveraging LLM For Synchronizing Information Across Multilingual Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLM For Synchronizing Information Across Multilingual Tables"
                },
                "summary": "The vast amount of online information today poses challenges for non-English\nspeakers, as much of it is concentrated in high-resource languages such as\nEnglish and French. Wikipedia reflects this imbalance, with content in\nlow-resource languages frequently outdated or incomplete. Recent research has\nsought to improve cross-language synchronization of Wikipedia tables using\nrule-based methods. These approaches can be effective, but they struggle with\ncomplexity and generalization. This paper explores large language models (LLMs)\nfor multilingual information synchronization, using zero-shot prompting as a\nscalable solution. We introduce the Information Updation dataset, simulating\nthe real-world process of updating outdated Wikipedia tables, and evaluate LLM\nperformance. Our findings reveal that single-prompt approaches often produce\nsuboptimal results, prompting us to introduce a task decomposition strategy\nthat enhances coherence and accuracy. Our proposed method outperforms existing\nbaselines, particularly in Information Updation (1.79%) and Information\nAddition (20.58%), highlighting the model strength in dynamically updating and\nenriching data across architectures",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vast amount of online information today poses challenges for non-English\nspeakers, as much of it is concentrated in high-resource languages such as\nEnglish and French. Wikipedia reflects this imbalance, with content in\nlow-resource languages frequently outdated or incomplete. Recent research has\nsought to improve cross-language synchronization of Wikipedia tables using\nrule-based methods. These approaches can be effective, but they struggle with\ncomplexity and generalization. This paper explores large language models (LLMs)\nfor multilingual information synchronization, using zero-shot prompting as a\nscalable solution. We introduce the Information Updation dataset, simulating\nthe real-world process of updating outdated Wikipedia tables, and evaluate LLM\nperformance. Our findings reveal that single-prompt approaches often produce\nsuboptimal results, prompting us to introduce a task decomposition strategy\nthat enhances coherence and accuracy. Our proposed method outperforms existing\nbaselines, particularly in Information Updation (1.79%) and Information\nAddition (20.58%), highlighting the model strength in dynamically updating and\nenriching data across architectures"
                },
                "authors": [
                    {
                        "name": "Siddharth Khincha"
                    },
                    {
                        "name": "Tushar Kataria"
                    },
                    {
                        "name": "Ankita Anand"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Vivek Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Gupta"
                },
                "author": "Vivek Gupta",
                "arxiv_comment": "17 Pages, 11 Tables, 2 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02553v1",
                "updated": "2025-04-03T13:07:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    7,
                    4,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T13:07:04Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    7,
                    4,
                    3,
                    93,
                    0
                ],
                "title": "Exploring Individual Factors in the Adoption of LLMs for Specific\n  Software Engineering Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Individual Factors in the Adoption of LLMs for Specific\n  Software Engineering Tasks"
                },
                "summary": "The advent of Large Language Models (LLMs) is transforming software\ndevelopment, significantly enhancing software engineering processes. Research\nhas explored their role within development teams, focusing on specific tasks\nsuch as artifact generation, decision-making support, and information\nretrieval. Despite the growing body of work on LLMs in software engineering,\nmost studies have centered on broad adoption trends, neglecting the nuanced\nrelationship between individual cognitive and behavioral factors and their\nimpact on task-specific adoption. While factors such as perceived effort and\nperformance expectancy have been explored at a general level, their influence\non distinct software engineering tasks remains underexamined. This gap hinders\nthe development of tailored LLM-based systems (e.g., Generative AI Agents) that\nalign with engineers' specific needs and limits the ability of team leaders to\ndevise effective strategies for fostering LLM adoption in targeted workflows.\nThis study bridges this gap by surveying N=188 software engineers to test the\nrelationship between individual attributes related to technology adoption and\nLLM adoption across five key tasks, using structural equation modeling (SEM).\nThe Unified Theory of Acceptance and Use of Technology (UTAUT2) was applied to\ncharacterize individual adoption behaviors. The findings reveal that\ntask-specific adoption is influenced by distinct factors, some of which\nnegatively impact adoption when considered in isolation, underscoring the\ncomplexity of LLM integration in software engineering. To support effective\nadoption, this article provides actionable recommendations, such as seamlessly\nintegrating LLMs into existing development environments and encouraging\npeer-driven knowledge sharing to enhance information retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) is transforming software\ndevelopment, significantly enhancing software engineering processes. Research\nhas explored their role within development teams, focusing on specific tasks\nsuch as artifact generation, decision-making support, and information\nretrieval. Despite the growing body of work on LLMs in software engineering,\nmost studies have centered on broad adoption trends, neglecting the nuanced\nrelationship between individual cognitive and behavioral factors and their\nimpact on task-specific adoption. While factors such as perceived effort and\nperformance expectancy have been explored at a general level, their influence\non distinct software engineering tasks remains underexamined. This gap hinders\nthe development of tailored LLM-based systems (e.g., Generative AI Agents) that\nalign with engineers' specific needs and limits the ability of team leaders to\ndevise effective strategies for fostering LLM adoption in targeted workflows.\nThis study bridges this gap by surveying N=188 software engineers to test the\nrelationship between individual attributes related to technology adoption and\nLLM adoption across five key tasks, using structural equation modeling (SEM).\nThe Unified Theory of Acceptance and Use of Technology (UTAUT2) was applied to\ncharacterize individual adoption behaviors. The findings reveal that\ntask-specific adoption is influenced by distinct factors, some of which\nnegatively impact adoption when considered in isolation, underscoring the\ncomplexity of LLM integration in software engineering. To support effective\nadoption, this article provides actionable recommendations, such as seamlessly\nintegrating LLMs into existing development environments and encouraging\npeer-driven knowledge sharing to enhance information retrieval."
                },
                "authors": [
                    {
                        "name": "Stefano Lambiase"
                    },
                    {
                        "name": "Gemma Catolino"
                    },
                    {
                        "name": "Fabio Palomba"
                    },
                    {
                        "name": "Filomena Ferrucci"
                    },
                    {
                        "name": "Daniel Russo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Russo"
                },
                "author": "Daniel Russo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02534v1",
                "updated": "2025-04-03T12:37:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    37,
                    4,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T12:37:04Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    37,
                    4,
                    3,
                    93,
                    0
                ],
                "title": "Delineate Anything: Resolution-Agnostic Field Boundary Delineation on\n  Satellite Imagery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delineate Anything: Resolution-Agnostic Field Boundary Delineation on\n  Satellite Imagery"
                },
                "summary": "The accurate delineation of agricultural field boundaries from satellite\nimagery is vital for land management and crop monitoring. However, current\nmethods face challenges due to limited dataset sizes, resolution discrepancies,\nand diverse environmental conditions. We address this by reformulating the task\nas instance segmentation and introducing the Field Boundary Instance\nSegmentation - 22M dataset (FBIS-22M), a large-scale, multi-resolution dataset\ncomprising 672,909 high-resolution satellite image patches (ranging from 0.25 m\nto 10 m) and 22,926,427 instance masks of individual fields, significantly\nnarrowing the gap between agricultural datasets and those in other computer\nvision domains. We further propose Delineate Anything, an instance segmentation\nmodel trained on our new FBIS-22M dataset. Our proposed model sets a new\nstate-of-the-art, achieving a substantial improvement of 88.5% in mAP@0.5 and\n103% in mAP@0.5:0.95 over existing methods, while also demonstrating\nsignificantly faster inference and strong zero-shot generalization across\ndiverse image resolutions and unseen geographic regions. Code, pre-trained\nmodels, and the FBIS-22M dataset are available at\nhttps://lavreniuk.github.io/Delineate-Anything.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The accurate delineation of agricultural field boundaries from satellite\nimagery is vital for land management and crop monitoring. However, current\nmethods face challenges due to limited dataset sizes, resolution discrepancies,\nand diverse environmental conditions. We address this by reformulating the task\nas instance segmentation and introducing the Field Boundary Instance\nSegmentation - 22M dataset (FBIS-22M), a large-scale, multi-resolution dataset\ncomprising 672,909 high-resolution satellite image patches (ranging from 0.25 m\nto 10 m) and 22,926,427 instance masks of individual fields, significantly\nnarrowing the gap between agricultural datasets and those in other computer\nvision domains. We further propose Delineate Anything, an instance segmentation\nmodel trained on our new FBIS-22M dataset. Our proposed model sets a new\nstate-of-the-art, achieving a substantial improvement of 88.5% in mAP@0.5 and\n103% in mAP@0.5:0.95 over existing methods, while also demonstrating\nsignificantly faster inference and strong zero-shot generalization across\ndiverse image resolutions and unseen geographic regions. Code, pre-trained\nmodels, and the FBIS-22M dataset are available at\nhttps://lavreniuk.github.io/Delineate-Anything."
                },
                "authors": [
                    {
                        "name": "Mykola Lavreniuk"
                    },
                    {
                        "name": "Nataliia Kussul"
                    },
                    {
                        "name": "Andrii Shelestov"
                    },
                    {
                        "name": "Bohdan Yailymov"
                    },
                    {
                        "name": "Yevhenii Salii"
                    },
                    {
                        "name": "Volodymyr Kuzin"
                    },
                    {
                        "name": "Zoltan Szantoi"
                    }
                ],
                "author_detail": {
                    "name": "Zoltan Szantoi"
                },
                "author": "Zoltan Szantoi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00030v2",
                "updated": "2025-04-03T12:31:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    31,
                    40,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-28T23:41:55Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    23,
                    41,
                    55,
                    4,
                    87,
                    0
                ],
                "title": "Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative\n  Decoding"
                },
                "summary": "Speculative decoding accelerates large language model (LLM) inference by\nusing a smaller draft model to propose tokens, which are then verified by a\nlarger target model. However, selecting an optimal speculation length is\ncritical for maximizing speedup while minimizing wasted computation. We\nintroduce \\textit{GammaTune} and \\textit{GammaTune+}, training-free adaptive\nalgorithms that dynamically adjust speculation length based on token acceptance\nrates using a heuristic-based switching mechanism. Evaluated on SpecBench\nacross multiple tasks and model pairs, our method outperforms other\nheuristic-based approaches and fixed-length speculative decoding, achieving an\naverage speedup of 15\\% ($\\pm$5\\%) with \\textit{GammaTune} and 16\\% ($\\pm$3\\%)\nwith \\textit{GammaTune+}, while reducing performance variance. This makes\n\\textit{GammaTune} a robust and efficient solution for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding accelerates large language model (LLM) inference by\nusing a smaller draft model to propose tokens, which are then verified by a\nlarger target model. However, selecting an optimal speculation length is\ncritical for maximizing speedup while minimizing wasted computation. We\nintroduce \\textit{GammaTune} and \\textit{GammaTune+}, training-free adaptive\nalgorithms that dynamically adjust speculation length based on token acceptance\nrates using a heuristic-based switching mechanism. Evaluated on SpecBench\nacross multiple tasks and model pairs, our method outperforms other\nheuristic-based approaches and fixed-length speculative decoding, achieving an\naverage speedup of 15\\% ($\\pm$5\\%) with \\textit{GammaTune} and 16\\% ($\\pm$3\\%)\nwith \\textit{GammaTune+}, while reducing performance variance. This makes\n\\textit{GammaTune} a robust and efficient solution for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Aayush Gautam"
                    },
                    {
                        "name": "Susav Shrestha"
                    },
                    {
                        "name": "Narasimha Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Narasimha Reddy"
                },
                "author": "Narasimha Reddy",
                "arxiv_comment": "6 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02521v1",
                "updated": "2025-04-03T12:18:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    18,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T12:18:51Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    18,
                    51,
                    3,
                    93,
                    0
                ],
                "title": "UNDO: Understanding Distillation as Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNDO: Understanding Distillation as Optimization"
                },
                "summary": "Knowledge distillation has emerged as an effective strategy for compressing\nlarge language models' (LLMs) knowledge into smaller, more efficient student\nmodels. However, standard one-shot distillation methods often produce\nsuboptimal results due to a mismatch between teacher-generated rationales and\nthe student's specific learning requirements. In this paper, we introduce the\nUNDO: UNderstanding Distillation as Optimization framework, designed to bridge\nthis gap by iteratively identifying the student's errors and prompting the\nteacher to refine its explanations accordingly. Each iteration directly targets\nthe student's learning deficiencies, motivating the teacher to provide tailored\nand enhanced rationales that specifically address these weaknesses. Empirical\nevaluations on various challenging mathematical and commonsense reasoning tasks\ndemonstrate that our iterative distillation method, UNDO, significantly\noutperforms standard one-step distillation methods, achieving performance gains\nof up to 20%. Additionally, we show that teacher-generated data refined through\nour iterative process remains effective even when applied to different student\nmodels, underscoring the broad applicability of our approach. Our work\nfundamentally reframes knowledge distillation as an iterative teacher-student\ninteraction, effectively leveraging dynamic refinement by the teacher for\nbetter knowledge distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation has emerged as an effective strategy for compressing\nlarge language models' (LLMs) knowledge into smaller, more efficient student\nmodels. However, standard one-shot distillation methods often produce\nsuboptimal results due to a mismatch between teacher-generated rationales and\nthe student's specific learning requirements. In this paper, we introduce the\nUNDO: UNderstanding Distillation as Optimization framework, designed to bridge\nthis gap by iteratively identifying the student's errors and prompting the\nteacher to refine its explanations accordingly. Each iteration directly targets\nthe student's learning deficiencies, motivating the teacher to provide tailored\nand enhanced rationales that specifically address these weaknesses. Empirical\nevaluations on various challenging mathematical and commonsense reasoning tasks\ndemonstrate that our iterative distillation method, UNDO, significantly\noutperforms standard one-step distillation methods, achieving performance gains\nof up to 20%. Additionally, we show that teacher-generated data refined through\nour iterative process remains effective even when applied to different student\nmodels, underscoring the broad applicability of our approach. Our work\nfundamentally reframes knowledge distillation as an iterative teacher-student\ninteraction, effectively leveraging dynamic refinement by the teacher for\nbetter knowledge distillation."
                },
                "authors": [
                    {
                        "name": "Kushal Jain"
                    },
                    {
                        "name": "Piyushi Goyal"
                    },
                    {
                        "name": "Kumar Shridhar"
                    }
                ],
                "author_detail": {
                    "name": "Kumar Shridhar"
                },
                "author": "Kumar Shridhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02509v1",
                "updated": "2025-04-03T11:50:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    50,
                    29,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T11:50:29Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    50,
                    29,
                    3,
                    93,
                    0
                ],
                "title": "A Memory-Augmented LLM-Driven Method for Autonomous Merging of 3D\n  Printing Work Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Memory-Augmented LLM-Driven Method for Autonomous Merging of 3D\n  Printing Work Orders"
                },
                "summary": "With the rapid development of 3D printing, the demand for personalized and\ncustomized production on the manufacturing line is steadily increasing.\nEfficient merging of printing workpieces can significantly enhance the\nprocessing efficiency of the production line. Addressing the challenge, a Large\nLanguage Model (LLM)-driven method is established in this paper for the\nautonomous merging of 3D printing work orders, integrated with a\nmemory-augmented learning strategy. In industrial scenarios, both device and\norder features are modeled into LLM-readable natural language prompt templates,\nand develop an order-device matching tool along with a merging interference\nchecking module. By incorporating a self-memory learning strategy, an\nintelligent agent for autonomous order merging is constructed, resulting in\nimproved accuracy and precision in order allocation. The proposed method\neffectively leverages the strengths of LLMs in industrial applications while\nreducing hallucination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of 3D printing, the demand for personalized and\ncustomized production on the manufacturing line is steadily increasing.\nEfficient merging of printing workpieces can significantly enhance the\nprocessing efficiency of the production line. Addressing the challenge, a Large\nLanguage Model (LLM)-driven method is established in this paper for the\nautonomous merging of 3D printing work orders, integrated with a\nmemory-augmented learning strategy. In industrial scenarios, both device and\norder features are modeled into LLM-readable natural language prompt templates,\nand develop an order-device matching tool along with a merging interference\nchecking module. By incorporating a self-memory learning strategy, an\nintelligent agent for autonomous order merging is constructed, resulting in\nimproved accuracy and precision in order allocation. The proposed method\neffectively leverages the strengths of LLMs in industrial applications while\nreducing hallucination."
                },
                "authors": [
                    {
                        "name": "Yuhao Liu"
                    },
                    {
                        "name": "Maolin Yang"
                    },
                    {
                        "name": "Pingyu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Pingyu Jiang"
                },
                "author": "Pingyu Jiang",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02507v1",
                "updated": "2025-04-03T11:41:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    41,
                    55,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T11:41:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    41,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "ZClip: Adaptive Spike Mitigation for LLM Pre-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZClip: Adaptive Spike Mitigation for LLM Pre-Training"
                },
                "summary": "Training large language models (LLMs) presents numerous challenges, including\ngradient instability and loss spikes. These phenomena can lead to catastrophic\ndivergence, requiring costly checkpoint restoration and data batch skipping.\nTraditional gradient clipping techniques, such as constant or norm-based\nmethods, fail to address these issues effectively due to their reliance on\nfixed thresholds or heuristics, leading to inefficient learning and requiring\nfrequent manual intervention. In this work, we propose ZClip, an adaptive\ngradient clipping algorithm that dynamically adjusts the clipping threshold\nbased on statistical properties of gradient norms over time. Unlike prior\nreactive strategies, ZClip proactively adapts to training dynamics without\nmaking any prior assumptions on the scale and the temporal evolution of\ngradient norms. At its core, it leverages z-score-based anomaly detection to\nidentify and mitigate large gradient spikes, preventing malignant loss spikes\nwhile not interfering with convergence otherwise. Our code is available at:\nhttps://github.com/bluorion-com/ZClip.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) presents numerous challenges, including\ngradient instability and loss spikes. These phenomena can lead to catastrophic\ndivergence, requiring costly checkpoint restoration and data batch skipping.\nTraditional gradient clipping techniques, such as constant or norm-based\nmethods, fail to address these issues effectively due to their reliance on\nfixed thresholds or heuristics, leading to inefficient learning and requiring\nfrequent manual intervention. In this work, we propose ZClip, an adaptive\ngradient clipping algorithm that dynamically adjusts the clipping threshold\nbased on statistical properties of gradient norms over time. Unlike prior\nreactive strategies, ZClip proactively adapts to training dynamics without\nmaking any prior assumptions on the scale and the temporal evolution of\ngradient norms. At its core, it leverages z-score-based anomaly detection to\nidentify and mitigate large gradient spikes, preventing malignant loss spikes\nwhile not interfering with convergence otherwise. Our code is available at:\nhttps://github.com/bluorion-com/ZClip."
                },
                "authors": [
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Louis Owen"
                    },
                    {
                        "name": "Nilabhra Roy Chowdhury"
                    },
                    {
                        "name": "Fabian Güra"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Güra"
                },
                "author": "Fabian Güra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01667v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01667v2",
                "updated": "2025-04-03T11:39:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    39,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T12:16:14Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    16,
                    14,
                    2,
                    92,
                    0
                ],
                "title": "Testing Low-Resource Language Support in LLMs Using Language Proficiency\n  Exams: the Case of Luxembourgish",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing Low-Resource Language Support in LLMs Using Language Proficiency\n  Exams: the Case of Luxembourgish"
                },
                "summary": "Large Language Models (LLMs) have become an increasingly important tool in\nresearch and society at large. While LLMs are regularly used all over the world\nby experts and lay-people alike, they are predominantly developed with\nEnglish-speaking users in mind, performing well in English and other\nwide-spread languages while less-resourced languages such as Luxembourgish are\nseen as a lower priority. This lack of attention is also reflected in the\nsparsity of available evaluation tools and datasets. In this study, we\ninvestigate the viability of language proficiency exams as such evaluation\ntools for the Luxembourgish language. We find that large models such as\nChatGPT, Claude and DeepSeek-R1 typically achieve high scores, while smaller\nmodels show weak performances. We also find that the performances in such\nlanguage exams can be used to predict performances in other NLP tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become an increasingly important tool in\nresearch and society at large. While LLMs are regularly used all over the world\nby experts and lay-people alike, they are predominantly developed with\nEnglish-speaking users in mind, performing well in English and other\nwide-spread languages while less-resourced languages such as Luxembourgish are\nseen as a lower priority. This lack of attention is also reflected in the\nsparsity of available evaluation tools and datasets. In this study, we\ninvestigate the viability of language proficiency exams as such evaluation\ntools for the Luxembourgish language. We find that large models such as\nChatGPT, Claude and DeepSeek-R1 typically achieve high scores, while smaller\nmodels show weak performances. We also find that the performances in such\nlanguage exams can be used to predict performances in other NLP tasks."
                },
                "authors": [
                    {
                        "name": "Cedric Lothritz"
                    },
                    {
                        "name": "Jordi Cabot"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Cabot"
                },
                "author": "Jordi Cabot",
                "arxiv_comment": "18 pages, 2 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01667v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01667v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02495v1",
                "updated": "2025-04-03T11:19:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    19,
                    49,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T11:19:49Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    19,
                    49,
                    3,
                    93,
                    0
                ],
                "title": "Inference-Time Scaling for Generalist Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Scaling for Generalist Reward Modeling"
                },
                "summary": "Reinforcement learning (RL) has been widely adopted in post-training for\nlarge language models (LLMs) at scale. Recently, the incentivization of\nreasoning capabilities in LLMs from RL indicates that $\\textit{proper learning\nmethods could enable effective inference-time scalability}$. A key challenge of\nRL is to obtain accurate reward signals for LLMs in various domains beyond\nverifiable questions or artificial rules. In this work, we investigate how to\nimprove reward modeling (RM) with more inference compute for general queries,\ni.e. the $\\textbf{inference-time scalability of generalist RM}$, and further,\nhow to improve the effectiveness of performance-compute scaling with proper\nlearning methods. For the RM approach, we adopt pointwise generative reward\nmodeling (GRM) to enable flexibility for different input types and potential\nfor inference-time scaling. For the learning method, we propose Self-Principled\nCritique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs\nthrough online RL, to generate principles adaptively and critiques accurately,\nresulting in $\\textbf{DeepSeek-GRM}$ models. Furthermore, for effective\ninference-time scaling, we use parallel sampling to expand compute usage, and\nintroduce a meta RM to guide voting process for better scaling performance.\nEmpirically, we show that SPCT significantly improves the quality and\nscalability of GRMs, outperforming existing methods and models in various RM\nbenchmarks without severe biases, and could achieve better performance compared\nto training-time scaling. DeepSeek-GRM still meets challenges in some tasks,\nwhich we believe can be addressed by future efforts in generalist reward\nsystems. The models will be released and open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has been widely adopted in post-training for\nlarge language models (LLMs) at scale. Recently, the incentivization of\nreasoning capabilities in LLMs from RL indicates that $\\textit{proper learning\nmethods could enable effective inference-time scalability}$. A key challenge of\nRL is to obtain accurate reward signals for LLMs in various domains beyond\nverifiable questions or artificial rules. In this work, we investigate how to\nimprove reward modeling (RM) with more inference compute for general queries,\ni.e. the $\\textbf{inference-time scalability of generalist RM}$, and further,\nhow to improve the effectiveness of performance-compute scaling with proper\nlearning methods. For the RM approach, we adopt pointwise generative reward\nmodeling (GRM) to enable flexibility for different input types and potential\nfor inference-time scaling. For the learning method, we propose Self-Principled\nCritique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs\nthrough online RL, to generate principles adaptively and critiques accurately,\nresulting in $\\textbf{DeepSeek-GRM}$ models. Furthermore, for effective\ninference-time scaling, we use parallel sampling to expand compute usage, and\nintroduce a meta RM to guide voting process for better scaling performance.\nEmpirically, we show that SPCT significantly improves the quality and\nscalability of GRMs, outperforming existing methods and models in various RM\nbenchmarks without severe biases, and could achieve better performance compared\nto training-time scaling. DeepSeek-GRM still meets challenges in some tasks,\nwhich we believe can be addressed by future efforts in generalist reward\nsystems. The models will be released and open-sourced."
                },
                "authors": [
                    {
                        "name": "Zijun Liu"
                    },
                    {
                        "name": "Peiyi Wang"
                    },
                    {
                        "name": "Runxin Xu"
                    },
                    {
                        "name": "Shirong Ma"
                    },
                    {
                        "name": "Chong Ruan"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wu"
                },
                "author": "Yu Wu",
                "arxiv_comment": "Preprint, under review. 42 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02483v1",
                "updated": "2025-04-03T11:07:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    7,
                    25,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T11:07:25Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    7,
                    25,
                    3,
                    93,
                    0
                ],
                "title": "Robust direction-dependent gain-calibration of beam-modelling errors far\n  from the target field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust direction-dependent gain-calibration of beam-modelling errors far\n  from the target field"
                },
                "summary": "Many astronomical questions require deep, wide-field observations at low\nradio frequencies. Phased arrays like LOFAR and SKA-low are designed for this,\nbut have inherently unstable element gains, leading to time, frequency and\ndirection-dependent gain errors. Precise direction-dependent calibration of\nobservations is therefore key to reaching the highest possible dynamic range.\nMany tools for direction-dependent calibration utilise sky and beam models to\ninfer gains. However, these calibration tools struggle with precision\ncalibration for relatively bright (e.g. A-team) sources far from the beam\ncentre. Therefore, the point-spread-function of these sources can potentially\nobscure a faint signal of interest. We show that, and why, the assumption of a\nsmooth gain solution per station fails for realistic radio interferometers, and\nhow this affects gain-calibration results. Subsequently, we introduce an\nimprovement for smooth spectral gain constraints for direction-dependent\ngain-calibration algorithms, in which the level of regularisation is weighted\nby the expected station response to the sky model. We test this method using\ndirection-dependent calibration method DDECal and physically-motivated beam\nmodelling errors for LOFAR-HBA stations. The new method outperforms the\nstandard method for various calibration settings near nulls in the beam, and\nmatches the standard inverse-variance-weighted method's performance for the\nremainder of the data. The proposed method is especially effective for short\nbaselines, both in visibility and image space. Improved direction-dependent\ngain-calibration is critical for future high-precision SKA-low observations,\nwhere higher sensitivity, increased antenna beam complexity, and mutual\ncoupling call for better off-axis source subtraction, which may not be achieved\nthrough improved beam models alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many astronomical questions require deep, wide-field observations at low\nradio frequencies. Phased arrays like LOFAR and SKA-low are designed for this,\nbut have inherently unstable element gains, leading to time, frequency and\ndirection-dependent gain errors. Precise direction-dependent calibration of\nobservations is therefore key to reaching the highest possible dynamic range.\nMany tools for direction-dependent calibration utilise sky and beam models to\ninfer gains. However, these calibration tools struggle with precision\ncalibration for relatively bright (e.g. A-team) sources far from the beam\ncentre. Therefore, the point-spread-function of these sources can potentially\nobscure a faint signal of interest. We show that, and why, the assumption of a\nsmooth gain solution per station fails for realistic radio interferometers, and\nhow this affects gain-calibration results. Subsequently, we introduce an\nimprovement for smooth spectral gain constraints for direction-dependent\ngain-calibration algorithms, in which the level of regularisation is weighted\nby the expected station response to the sky model. We test this method using\ndirection-dependent calibration method DDECal and physically-motivated beam\nmodelling errors for LOFAR-HBA stations. The new method outperforms the\nstandard method for various calibration settings near nulls in the beam, and\nmatches the standard inverse-variance-weighted method's performance for the\nremainder of the data. The proposed method is especially effective for short\nbaselines, both in visibility and image space. Improved direction-dependent\ngain-calibration is critical for future high-precision SKA-low observations,\nwhere higher sensitivity, increased antenna beam complexity, and mutual\ncoupling call for better off-axis source subtraction, which may not be achieved\nthrough improved beam models alone."
                },
                "authors": [
                    {
                        "name": "S. A. Brackenhoff"
                    },
                    {
                        "name": "A. R. Offringa"
                    },
                    {
                        "name": "M. Mevius"
                    },
                    {
                        "name": "L. V. E. Koopmans"
                    },
                    {
                        "name": "J. K. Chege"
                    },
                    {
                        "name": "E. Ceccotti"
                    },
                    {
                        "name": "C. Höfer"
                    },
                    {
                        "name": "L. Gao"
                    },
                    {
                        "name": "S. Ghosh"
                    },
                    {
                        "name": "F. G. Mertens"
                    },
                    {
                        "name": "S. Munshi"
                    }
                ],
                "author_detail": {
                    "name": "S. Munshi"
                },
                "author": "S. Munshi",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02480v1",
                "updated": "2025-04-03T10:57:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    57,
                    26,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T10:57:26Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    57,
                    26,
                    3,
                    93,
                    0
                ],
                "title": "Graph Attention-Driven Bayesian Deep Unrolling for Dual-Peak\n  Single-Photon Lidar Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Attention-Driven Bayesian Deep Unrolling for Dual-Peak\n  Single-Photon Lidar Imaging"
                },
                "summary": "Single-photon Lidar imaging offers a significant advantage in 3D imaging due\nto its high resolution and long-range capabilities, however it is challenging\nto apply in noisy environments with multiple targets per pixel. To tackle these\nchallenges, several methods have been proposed. Statistical methods demonstrate\ninterpretability on the inferred parameters, but they are often limited in\ntheir ability to handle complex scenes. Deep learning-based methods have shown\nsuperior performance in terms of accuracy and robustness, but they lack\ninterpretability or they are limited to a single-peak per pixel. In this paper,\nwe propose a deep unrolling algorithm for dual-peak single-photon Lidar\nimaging. We introduce a hierarchical Bayesian model for multiple targets and\npropose a neural network that unrolls the underlying statistical method. To\nsupport multiple targets, we adopt a dual depth maps representation and exploit\ngeometric deep learning to extract features from the point cloud. The proposed\nmethod takes advantages of statistical methods and learning-based methods in\nterms of accuracy and quantifying uncertainty. The experimental results on\nsynthetic and real data demonstrate the competitive performance when compared\nto existing methods, while also providing uncertainty information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-photon Lidar imaging offers a significant advantage in 3D imaging due\nto its high resolution and long-range capabilities, however it is challenging\nto apply in noisy environments with multiple targets per pixel. To tackle these\nchallenges, several methods have been proposed. Statistical methods demonstrate\ninterpretability on the inferred parameters, but they are often limited in\ntheir ability to handle complex scenes. Deep learning-based methods have shown\nsuperior performance in terms of accuracy and robustness, but they lack\ninterpretability or they are limited to a single-peak per pixel. In this paper,\nwe propose a deep unrolling algorithm for dual-peak single-photon Lidar\nimaging. We introduce a hierarchical Bayesian model for multiple targets and\npropose a neural network that unrolls the underlying statistical method. To\nsupport multiple targets, we adopt a dual depth maps representation and exploit\ngeometric deep learning to extract features from the point cloud. The proposed\nmethod takes advantages of statistical methods and learning-based methods in\nterms of accuracy and quantifying uncertainty. The experimental results on\nsynthetic and real data demonstrate the competitive performance when compared\nto existing methods, while also providing uncertainty information."
                },
                "authors": [
                    {
                        "name": "Kyungmin Choi"
                    },
                    {
                        "name": "JaKeoung Koo"
                    },
                    {
                        "name": "Stephen McLaughlin"
                    },
                    {
                        "name": "Abderrahim Halimi"
                    }
                ],
                "author_detail": {
                    "name": "Abderrahim Halimi"
                },
                "author": "Abderrahim Halimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02477v1",
                "updated": "2025-04-03T10:53:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    53,
                    7,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T10:53:07Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    53,
                    7,
                    3,
                    93,
                    0
                ],
                "title": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision"
                },
                "summary": "Robot vision has greatly benefited from advancements in multimodal fusion\ntechniques and vision-language models (VLMs). We systematically review the\napplications of multimodal fusion in key robotic vision tasks, including\nsemantic scene understanding, simultaneous localization and mapping (SLAM), 3D\nobject detection, navigation and localization, and robot manipulation. We\ncompare VLMs based on large language models (LLMs) with traditional multimodal\nfusion methods, analyzing their advantages, limitations, and synergies.\nAdditionally, we conduct an in-depth analysis of commonly used datasets,\nevaluating their applicability and challenges in real-world robotic scenarios.\nFurthermore, we identify critical research challenges such as cross-modal\nalignment, efficient fusion strategies, real-time deployment, and domain\nadaptation, and propose future research directions, including self-supervised\nlearning for robust multimodal representations, transformer-based fusion\narchitectures, and scalable multimodal frameworks. Through a comprehensive\nreview, comparative analysis, and forward-looking discussion, we provide a\nvaluable reference for advancing multimodal perception and interaction in\nrobotic vision. A comprehensive list of studies in this survey is available at\nhttps://github.com/Xiaofeng-Han-Res/MF-RV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot vision has greatly benefited from advancements in multimodal fusion\ntechniques and vision-language models (VLMs). We systematically review the\napplications of multimodal fusion in key robotic vision tasks, including\nsemantic scene understanding, simultaneous localization and mapping (SLAM), 3D\nobject detection, navigation and localization, and robot manipulation. We\ncompare VLMs based on large language models (LLMs) with traditional multimodal\nfusion methods, analyzing their advantages, limitations, and synergies.\nAdditionally, we conduct an in-depth analysis of commonly used datasets,\nevaluating their applicability and challenges in real-world robotic scenarios.\nFurthermore, we identify critical research challenges such as cross-modal\nalignment, efficient fusion strategies, real-time deployment, and domain\nadaptation, and propose future research directions, including self-supervised\nlearning for robust multimodal representations, transformer-based fusion\narchitectures, and scalable multimodal frameworks. Through a comprehensive\nreview, comparative analysis, and forward-looking discussion, we provide a\nvaluable reference for advancing multimodal perception and interaction in\nrobotic vision. A comprehensive list of studies in this survey is available at\nhttps://github.com/Xiaofeng-Han-Res/MF-RV."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Han"
                    },
                    {
                        "name": "Shunpeng Chen"
                    },
                    {
                        "name": "Zenghuang Fu"
                    },
                    {
                        "name": "Zhe Feng"
                    },
                    {
                        "name": "Lue Fan"
                    },
                    {
                        "name": "Dong An"
                    },
                    {
                        "name": "Changwei Wang"
                    },
                    {
                        "name": "Li Guo"
                    },
                    {
                        "name": "Weiliang Meng"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    },
                    {
                        "name": "Rongtao Xu"
                    },
                    {
                        "name": "Shibiao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Shibiao Xu"
                },
                "author": "Shibiao Xu",
                "arxiv_comment": "27 pages, 11 figures, survey paper submitted to Information Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02470v1",
                "updated": "2025-04-03T10:44:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    44,
                    32,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T10:44:32Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    44,
                    32,
                    3,
                    93,
                    0
                ],
                "title": "Impact of Global Warming on Extreme Rainfall in Taiwan",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Global Warming on Extreme Rainfall in Taiwan"
                },
                "summary": "The relationship between global warming and extreme rainfalls in Taiwan was\nexamined in this study. Taiwan rainfall data from TCCIP, a project led by MOST,\nwere analyzed. North Hemisphere reference temperature data from NCEI led by\nNOAA. The yearly maximum of daily rainfall was focused on and the PGEV model,\nas proposed by Olafsdottir et al. \\citep{olafsdottir2021extreme}, was used to\nfit the extreme values and make inferences. The PGEV model integrates the\nGeneral Extreme Value (GEV) and Peak over Threshold (PoT) approaches, which are\ncommonly used to analyze extreme data. Relative intensity and return value were\nused to show the connection between temperature and extreme rainfall.\n  Results indicated that the intensity of extreme rainfall in Taiwan increases\nas the temperature rises. However, the effects of global warming on the\nfrequency and intensity of extreme rainfalls varied by region. In the north and\nsouth regions, the frequency of extreme rainfalls changed, while in the center\nand east regions, the intensity of extreme rainfalls changed. Furthermore,\naccording to the return value analysis, extreme rainfalls are likely to occur\nmore frequently in the future.\n  To account for differences between locations, Gaussian Process was used to\nsmooth the results obtained using the PGEV model. In addition, simulations\nusing the Gaussian copula and Gaussian Process were conducted to determine the\nquantile confidence intervals for each PGEV model. The simulations showed that\nall tests comparing with models with and without covariates are significant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The relationship between global warming and extreme rainfalls in Taiwan was\nexamined in this study. Taiwan rainfall data from TCCIP, a project led by MOST,\nwere analyzed. North Hemisphere reference temperature data from NCEI led by\nNOAA. The yearly maximum of daily rainfall was focused on and the PGEV model,\nas proposed by Olafsdottir et al. \\citep{olafsdottir2021extreme}, was used to\nfit the extreme values and make inferences. The PGEV model integrates the\nGeneral Extreme Value (GEV) and Peak over Threshold (PoT) approaches, which are\ncommonly used to analyze extreme data. Relative intensity and return value were\nused to show the connection between temperature and extreme rainfall.\n  Results indicated that the intensity of extreme rainfall in Taiwan increases\nas the temperature rises. However, the effects of global warming on the\nfrequency and intensity of extreme rainfalls varied by region. In the north and\nsouth regions, the frequency of extreme rainfalls changed, while in the center\nand east regions, the intensity of extreme rainfalls changed. Furthermore,\naccording to the return value analysis, extreme rainfalls are likely to occur\nmore frequently in the future.\n  To account for differences between locations, Gaussian Process was used to\nsmooth the results obtained using the PGEV model. In addition, simulations\nusing the Gaussian copula and Gaussian Process were conducted to determine the\nquantile confidence intervals for each PGEV model. The simulations showed that\nall tests comparing with models with and without covariates are significant."
                },
                "authors": [
                    {
                        "name": "Cheng-Ching Lin"
                    }
                ],
                "author_detail": {
                    "name": "Cheng-Ching Lin"
                },
                "author": "Cheng-Ching Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17598v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17598v3",
                "updated": "2025-04-03T10:36:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    36,
                    31,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-22T00:59:22Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    0,
                    59,
                    22,
                    5,
                    81,
                    0
                ],
                "title": "Coarse-Grained Games: A Framework for Bounded Perception in Game Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Games: A Framework for Bounded Perception in Game Theory"
                },
                "summary": "In everyday life, we frequently make coarse-grained judgments. When we say\nthat Olivia and Noah excel in mathematics, we disregard the specific\ndifferences in their mathematical abilities. Similarly, when we claim that a\nparticular automobile manufacturer produces high-quality cars, we overlook the\nminor variations among individual vehicles. These coarse-grained assessments\nare distinct from erroneous or deceptive judgments, such as those resulting\nfrom student cheating or false advertising by corporations. Despite the\nprevalence of such judgments, little attention has been given to their\nunderlying mathematical structure. In this paper, we introduce the concept of\ncoarse-graining into game theory, analyzing games where players may perceive\ndifferent payoffs as identical while preserving the underlying order structure.\nWe call it a Coarse-Grained Game (CGG). This framework allows us to examine the\nrational inference processes that arise when players equate distinct\nmicro-level payoffs at a macro level, and to explore how Nash equilibria are\npreserved or altered as a result. Our key findings suggest that CGGs possess\nseveral desirable properties that make them suitable for modeling phenomena in\nthe social sciences. This paper demonstrates two such applications: first, in\ncases of overly minor product updates, consumers may encounter an equilibrium\nselection problem, resulting in market behavior that is not driven by objective\nquality differences; second, the lemon market can be analyzed not only through\nobjective information asymmetry but also through asymmetries in perceptual\nresolution or recognition ability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In everyday life, we frequently make coarse-grained judgments. When we say\nthat Olivia and Noah excel in mathematics, we disregard the specific\ndifferences in their mathematical abilities. Similarly, when we claim that a\nparticular automobile manufacturer produces high-quality cars, we overlook the\nminor variations among individual vehicles. These coarse-grained assessments\nare distinct from erroneous or deceptive judgments, such as those resulting\nfrom student cheating or false advertising by corporations. Despite the\nprevalence of such judgments, little attention has been given to their\nunderlying mathematical structure. In this paper, we introduce the concept of\ncoarse-graining into game theory, analyzing games where players may perceive\ndifferent payoffs as identical while preserving the underlying order structure.\nWe call it a Coarse-Grained Game (CGG). This framework allows us to examine the\nrational inference processes that arise when players equate distinct\nmicro-level payoffs at a macro level, and to explore how Nash equilibria are\npreserved or altered as a result. Our key findings suggest that CGGs possess\nseveral desirable properties that make them suitable for modeling phenomena in\nthe social sciences. This paper demonstrates two such applications: first, in\ncases of overly minor product updates, consumers may encounter an equilibrium\nselection problem, resulting in market behavior that is not driven by objective\nquality differences; second, the lemon market can be analyzed not only through\nobjective information asymmetry but also through asymmetries in perceptual\nresolution or recognition ability."
                },
                "authors": [
                    {
                        "name": "Takashi Izumo"
                    }
                ],
                "author_detail": {
                    "name": "Takashi Izumo"
                },
                "author": "Takashi Izumo",
                "arxiv_comment": "49 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17598v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17598v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.TH",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01543v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01543v2",
                "updated": "2025-04-03T10:25:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    25,
                    40,
                    3,
                    93,
                    0
                ],
                "published": "2024-12-02T14:32:19Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    14,
                    32,
                    19,
                    0,
                    337,
                    0
                ],
                "title": "6DOPE-GS: Online 6D Object Pose Estimation using Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6DOPE-GS: Online 6D Object Pose Estimation using Gaussian Splatting"
                },
                "summary": "Efficient and accurate object pose estimation is an essential component for\nmodern vision systems in many applications such as Augmented Reality,\nautonomous driving, and robotics. While research in model-based 6D object pose\nestimation has delivered promising results, model-free methods are hindered by\nthe high computational load in rendering and inferring consistent poses of\narbitrary objects in a live RGB-D video stream. To address this issue, we\npresent 6DOPE-GS, a novel method for online 6D object pose estimation \\&\ntracking with a single RGB-D camera by effectively leveraging advances in\nGaussian Splatting. Thanks to the fast differentiable rendering capabilities of\nGaussian Splatting, 6DOPE-GS can simultaneously optimize for 6D object poses\nand 3D object reconstruction. To achieve the necessary efficiency and accuracy\nfor live tracking, our method uses incremental 2D Gaussian Splatting with an\nintelligent dynamic keyframe selection procedure to achieve high spatial object\ncoverage and prevent erroneous pose updates. We also propose an opacity\nstatistic-based pruning mechanism for adaptive Gaussian density control, to\nensure training stability and efficiency. We evaluate our method on the HO3D\nand YCBInEOAT datasets and show that 6DOPE-GS matches the performance of\nstate-of-the-art baselines for model-free simultaneous 6D pose tracking and\nreconstruction while providing a 5$\\times$ speedup. We also demonstrate the\nmethod's suitability for live, dynamic object tracking and reconstruction in a\nreal-world setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and accurate object pose estimation is an essential component for\nmodern vision systems in many applications such as Augmented Reality,\nautonomous driving, and robotics. While research in model-based 6D object pose\nestimation has delivered promising results, model-free methods are hindered by\nthe high computational load in rendering and inferring consistent poses of\narbitrary objects in a live RGB-D video stream. To address this issue, we\npresent 6DOPE-GS, a novel method for online 6D object pose estimation \\&\ntracking with a single RGB-D camera by effectively leveraging advances in\nGaussian Splatting. Thanks to the fast differentiable rendering capabilities of\nGaussian Splatting, 6DOPE-GS can simultaneously optimize for 6D object poses\nand 3D object reconstruction. To achieve the necessary efficiency and accuracy\nfor live tracking, our method uses incremental 2D Gaussian Splatting with an\nintelligent dynamic keyframe selection procedure to achieve high spatial object\ncoverage and prevent erroneous pose updates. We also propose an opacity\nstatistic-based pruning mechanism for adaptive Gaussian density control, to\nensure training stability and efficiency. We evaluate our method on the HO3D\nand YCBInEOAT datasets and show that 6DOPE-GS matches the performance of\nstate-of-the-art baselines for model-free simultaneous 6D pose tracking and\nreconstruction while providing a 5$\\times$ speedup. We also demonstrate the\nmethod's suitability for live, dynamic object tracking and reconstruction in a\nreal-world setting."
                },
                "authors": [
                    {
                        "name": "Yufeng Jin"
                    },
                    {
                        "name": "Vignesh Prasad"
                    },
                    {
                        "name": "Snehal Jauhri"
                    },
                    {
                        "name": "Mathias Franzius"
                    },
                    {
                        "name": "Georgia Chalvatzaki"
                    }
                ],
                "author_detail": {
                    "name": "Georgia Chalvatzaki"
                },
                "author": "Georgia Chalvatzaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01543v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02459v1",
                "updated": "2025-04-03T10:24:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    24,
                    0,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T10:24:00Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    24,
                    0,
                    3,
                    93,
                    0
                ],
                "title": "A Physics-Informed Meta-Learning Framework for the Continuous Solution\n  of Parametric PDEs on Arbitrary Geometries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Physics-Informed Meta-Learning Framework for the Continuous Solution\n  of Parametric PDEs on Arbitrary Geometries"
                },
                "summary": "In this work, we introduce implicit Finite Operator Learning (iFOL) for the\ncontinuous and parametric solution of partial differential equations (PDEs) on\narbitrary geometries. We propose a physics-informed encoder-decoder network to\nestablish the mapping between continuous parameter and solution spaces. The\ndecoder constructs the parametric solution field by leveraging an implicit\nneural field network conditioned on a latent or feature code. Instance-specific\ncodes are derived through a PDE encoding process based on the second-order\nmeta-learning technique. In training and inference, a physics-informed loss\nfunction is minimized during the PDE encoding and decoding. iFOL expresses the\nloss function in an energy or weighted residual form and evaluates it using\ndiscrete residuals derived from standard numerical PDE methods. This approach\nresults in the backpropagation of discrete residuals during both training and\ninference.\n  iFOL features several key properties: (1) its unique loss formulation\neliminates the need for the conventional encode-process-decode pipeline\npreviously used in operator learning with conditional neural fields for PDEs;\n(2) it not only provides accurate parametric and continuous fields but also\ndelivers solution-to-parameter gradients without requiring additional loss\nterms or sensitivity analysis; (3) it can effectively capture sharp\ndiscontinuities in the solution; and (4) it removes constraints on the geometry\nand mesh, making it applicable to arbitrary geometries and spatial sampling\n(zero-shot super-resolution capability). We critically assess these features\nand analyze the network's ability to generalize to unseen samples across both\nstationary and transient PDEs. The overall performance of the proposed method\nis promising, demonstrating its applicability to a range of challenging\nproblems in computational mechanics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce implicit Finite Operator Learning (iFOL) for the\ncontinuous and parametric solution of partial differential equations (PDEs) on\narbitrary geometries. We propose a physics-informed encoder-decoder network to\nestablish the mapping between continuous parameter and solution spaces. The\ndecoder constructs the parametric solution field by leveraging an implicit\nneural field network conditioned on a latent or feature code. Instance-specific\ncodes are derived through a PDE encoding process based on the second-order\nmeta-learning technique. In training and inference, a physics-informed loss\nfunction is minimized during the PDE encoding and decoding. iFOL expresses the\nloss function in an energy or weighted residual form and evaluates it using\ndiscrete residuals derived from standard numerical PDE methods. This approach\nresults in the backpropagation of discrete residuals during both training and\ninference.\n  iFOL features several key properties: (1) its unique loss formulation\neliminates the need for the conventional encode-process-decode pipeline\npreviously used in operator learning with conditional neural fields for PDEs;\n(2) it not only provides accurate parametric and continuous fields but also\ndelivers solution-to-parameter gradients without requiring additional loss\nterms or sensitivity analysis; (3) it can effectively capture sharp\ndiscontinuities in the solution; and (4) it removes constraints on the geometry\nand mesh, making it applicable to arbitrary geometries and spatial sampling\n(zero-shot super-resolution capability). We critically assess these features\nand analyze the network's ability to generalize to unseen samples across both\nstationary and transient PDEs. The overall performance of the proposed method\nis promising, demonstrating its applicability to a range of challenging\nproblems in computational mechanics."
                },
                "authors": [
                    {
                        "name": "Reza Najian Asl"
                    },
                    {
                        "name": "Yusuke Yamazaki"
                    },
                    {
                        "name": "Kianoosh Taghikhani"
                    },
                    {
                        "name": "Mayu Muramatsu"
                    },
                    {
                        "name": "Markus Apel"
                    },
                    {
                        "name": "Shahed Rezaei"
                    }
                ],
                "author_detail": {
                    "name": "Shahed Rezaei"
                },
                "author": "Shahed Rezaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20313v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20313v3",
                "updated": "2025-04-03T10:23:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    23,
                    19,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-26T08:25:12Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    8,
                    25,
                    12,
                    2,
                    85,
                    0
                ],
                "title": "TileLink: Generating Efficient Compute-Communication Overlapping Kernels\n  using Tile-Centric Primitives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TileLink: Generating Efficient Compute-Communication Overlapping Kernels\n  using Tile-Centric Primitives"
                },
                "summary": "Large deep learning models have achieved state-of-the-art performance in a\nwide range of tasks. These models often necessitate distributed systems for\nefficient training and inference. The fundamental building blocks for\ndistributed model execution are intra-layer parallel operators. The most\neffective approach to enhancing the performance of intra-layer parallel\noperators involves overlapping computation with communication. The overlapping\ncan be achieved through either operator decomposition or kernel fusion. While\ndecomposing operators is straightforward to implement, it often results in\nsuboptimal performance. On the other hand, fusing communication kernels with\ncompute kernels demands significant expertise and is error-prone.\n  In this paper, we propose TileLink to enable efficient compilation and\ngeneration of overlapped compute-communication kernels. TileLink is composed of\nfrontend and backend. In the frontend, TileLink decouples the design space of\ncommunication and computation, linking these two parts via tile-centric\nprimitives. In the backend, TileLink translates these primitives into low-level\ncommunication instructions, integrating the communication and computation\ncomponents to achieve overlapped execution. In experiments, TileLink achieves\nfrom $1.17\\times$ to $20.76\\times$ speedup to non-overlapping baseline and\nachieves performance comparable to state-of-the-art overlapping libraries on\nGPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large deep learning models have achieved state-of-the-art performance in a\nwide range of tasks. These models often necessitate distributed systems for\nefficient training and inference. The fundamental building blocks for\ndistributed model execution are intra-layer parallel operators. The most\neffective approach to enhancing the performance of intra-layer parallel\noperators involves overlapping computation with communication. The overlapping\ncan be achieved through either operator decomposition or kernel fusion. While\ndecomposing operators is straightforward to implement, it often results in\nsuboptimal performance. On the other hand, fusing communication kernels with\ncompute kernels demands significant expertise and is error-prone.\n  In this paper, we propose TileLink to enable efficient compilation and\ngeneration of overlapped compute-communication kernels. TileLink is composed of\nfrontend and backend. In the frontend, TileLink decouples the design space of\ncommunication and computation, linking these two parts via tile-centric\nprimitives. In the backend, TileLink translates these primitives into low-level\ncommunication instructions, integrating the communication and computation\ncomponents to achieve overlapped execution. In experiments, TileLink achieves\nfrom $1.17\\times$ to $20.76\\times$ speedup to non-overlapping baseline and\nachieves performance comparable to state-of-the-art overlapping libraries on\nGPUs."
                },
                "authors": [
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Jin Fang"
                    },
                    {
                        "name": "Xuegui Zheng"
                    },
                    {
                        "name": "Qi Hou"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Ziheng Jiang"
                    },
                    {
                        "name": "Dongyang Wang"
                    },
                    {
                        "name": "Jianxi Ye"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20313v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20313v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02458v1",
                "updated": "2025-04-03T10:22:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    22,
                    30,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T10:22:30Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    22,
                    30,
                    3,
                    93,
                    0
                ],
                "title": "Retrieval-Augmented Purifier for Robust LLM-Empowered Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Purifier for Robust LLM-Empowered Recommendation"
                },
                "summary": "Recently, Large Language Model (LLM)-empowered recommender systems have\nrevolutionized personalized recommendation frameworks and attracted extensive\nattention. Despite the remarkable success, existing LLM-empowered RecSys have\nbeen demonstrated to be highly vulnerable to minor perturbations. To mitigate\nthe negative impact of such vulnerabilities, one potential solution is to\nemploy collaborative signals based on item-item co-occurrence to purify the\nmalicious collaborative knowledge from the user's historical interactions\ninserted by attackers. On the other hand, due to the capabilities to expand\ninsufficient internal knowledge of LLMs, Retrieval-Augmented Generation (RAG)\ntechniques provide unprecedented opportunities to enhance the robustness of\nLLM-empowered recommender systems by introducing external collaborative\nknowledge. Therefore, in this paper, we propose a novel framework (RETURN) by\nretrieving external collaborative signals to purify the poisoned user profiles\nand enhance the robustness of LLM-empowered RecSys in a plug-and-play manner.\nSpecifically, retrieval-augmented perturbation positioning is proposed to\nidentify potential perturbations within the users' historical sequences by\nretrieving external knowledge from collaborative item graphs. After that, we\nfurther retrieve the collaborative knowledge to cleanse the perturbations by\nusing either deletion or replacement strategies and introduce a robust ensemble\nrecommendation strategy to generate final robust predictions. Extensive\nexperiments on three real-world datasets demonstrate the effectiveness of the\nproposed RETURN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Model (LLM)-empowered recommender systems have\nrevolutionized personalized recommendation frameworks and attracted extensive\nattention. Despite the remarkable success, existing LLM-empowered RecSys have\nbeen demonstrated to be highly vulnerable to minor perturbations. To mitigate\nthe negative impact of such vulnerabilities, one potential solution is to\nemploy collaborative signals based on item-item co-occurrence to purify the\nmalicious collaborative knowledge from the user's historical interactions\ninserted by attackers. On the other hand, due to the capabilities to expand\ninsufficient internal knowledge of LLMs, Retrieval-Augmented Generation (RAG)\ntechniques provide unprecedented opportunities to enhance the robustness of\nLLM-empowered recommender systems by introducing external collaborative\nknowledge. Therefore, in this paper, we propose a novel framework (RETURN) by\nretrieving external collaborative signals to purify the poisoned user profiles\nand enhance the robustness of LLM-empowered RecSys in a plug-and-play manner.\nSpecifically, retrieval-augmented perturbation positioning is proposed to\nidentify potential perturbations within the users' historical sequences by\nretrieving external knowledge from collaborative item graphs. After that, we\nfurther retrieve the collaborative knowledge to cleanse the perturbations by\nusing either deletion or replacement strategies and introduce a robust ensemble\nrecommendation strategy to generate final robust predictions. Extensive\nexperiments on three real-world datasets demonstrate the effectiveness of the\nproposed RETURN."
                },
                "authors": [
                    {
                        "name": "Liangbo Ning"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02456v2",
                "updated": "2025-04-04T02:16:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    2,
                    16,
                    54,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-03T10:20:48Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    20,
                    48,
                    3,
                    93,
                    0
                ],
                "title": "The Amenability Framework: Rethinking Causal Ordering Without Estimating\n  Causal Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Amenability Framework: Rethinking Causal Ordering Without Estimating\n  Causal Effects"
                },
                "summary": "Who should we prioritize for intervention when we cannot estimate\nintervention effects? In many applied domains (e.g., advertising, customer\nretention, and behavioral nudging) prioritization is guided by predictive\nmodels that estimate outcome probabilities rather than causal effects. This\npaper investigates when these predictions (scores) can effectively rank\nindividuals by their intervention effects, particularly when direct effect\nestimation is infeasible or unreliable. We propose a conceptual framework based\non amenability: an individual's latent proclivity to be influenced by an\nintervention. We then formalize conditions under which predictive scores serve\nas effective proxies for amenability. These conditions justify using non-causal\nscores for intervention prioritization, even when the scores do not directly\nestimate effects. We further show that, under plausible assumptions, predictive\nmodels can outperform causal effect estimators in ranking individuals by\nintervention effects. Empirical evidence from an advertising context supports\nour theoretical findings, demonstrating that predictive modeling can offer a\nmore robust approach to targeting than effect estimation. Our framework\nsuggests a shift in focus, from estimating effects to inferring who is\namenable, as a practical and theoretically grounded strategy for prioritizing\ninterventions in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who should we prioritize for intervention when we cannot estimate\nintervention effects? In many applied domains (e.g., advertising, customer\nretention, and behavioral nudging) prioritization is guided by predictive\nmodels that estimate outcome probabilities rather than causal effects. This\npaper investigates when these predictions (scores) can effectively rank\nindividuals by their intervention effects, particularly when direct effect\nestimation is infeasible or unreliable. We propose a conceptual framework based\non amenability: an individual's latent proclivity to be influenced by an\nintervention. We then formalize conditions under which predictive scores serve\nas effective proxies for amenability. These conditions justify using non-causal\nscores for intervention prioritization, even when the scores do not directly\nestimate effects. We further show that, under plausible assumptions, predictive\nmodels can outperform causal effect estimators in ranking individuals by\nintervention effects. Empirical evidence from an advertising context supports\nour theoretical findings, demonstrating that predictive modeling can offer a\nmore robust approach to targeting than effect estimation. Our framework\nsuggests a shift in focus, from estimating effects to inferring who is\namenable, as a practical and theoretically grounded strategy for prioritizing\ninterventions in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Carlos Fernández-Loría"
                    },
                    {
                        "name": "Jorge Loría"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Loría"
                },
                "author": "Jorge Loría",
                "arxiv_comment": "This was meant to serve as a replacement of arXiv:2206.12532, not a\n  new submission. I have already submitted a replacement for the original, so I\n  would like to withdraw this version to prevent duplication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05445v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05445v2",
                "updated": "2025-04-03T10:16:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    16,
                    53,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-07T14:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    16,
                    48,
                    4,
                    66,
                    0
                ],
                "title": "ToxicSQL: Migrating SQL Injection Threats into Text-to-SQL Models via\n  Backdoor Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToxicSQL: Migrating SQL Injection Threats into Text-to-SQL Models via\n  Backdoor Attack"
                },
                "summary": "Large language models (LLMs) have shown state-of-the-art results in\ntranslating natural language questions into SQL queries (Text-to-SQL), a\nlong-standing challenge within the database community. However, security\nconcerns remain largely unexplored, particularly the threat of backdoor\nattacks, which can introduce malicious behaviors into models through\nfine-tuning with poisoned datasets. In this work, we systematically investigate\nthe vulnerabilities of LLM-based Text-to-SQL models and present ToxicSQL, a\nnovel backdoor attack framework. Our approach leverages stealthy {semantic and\ncharacter-level triggers} to make backdoors difficult to detect and remove,\nensuring that malicious behaviors remain covert while maintaining high model\naccuracy on benign inputs. Furthermore, we propose leveraging SQL injection\npayloads as backdoor targets, enabling the generation of malicious yet\nexecutable SQL queries, which pose severe security and privacy risks in\nlanguage model-based SQL development. We demonstrate that injecting only 0.44%\nof poisoned data can result in an attack success rate of 79.41%, posing a\nsignificant risk to database security. Additionally, we propose detection and\nmitigation strategies to enhance model reliability. Our findings highlight the\nurgent need for security-aware Text-to-SQL development, emphasizing the\nimportance of robust defenses against backdoor threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown state-of-the-art results in\ntranslating natural language questions into SQL queries (Text-to-SQL), a\nlong-standing challenge within the database community. However, security\nconcerns remain largely unexplored, particularly the threat of backdoor\nattacks, which can introduce malicious behaviors into models through\nfine-tuning with poisoned datasets. In this work, we systematically investigate\nthe vulnerabilities of LLM-based Text-to-SQL models and present ToxicSQL, a\nnovel backdoor attack framework. Our approach leverages stealthy {semantic and\ncharacter-level triggers} to make backdoors difficult to detect and remove,\nensuring that malicious behaviors remain covert while maintaining high model\naccuracy on benign inputs. Furthermore, we propose leveraging SQL injection\npayloads as backdoor targets, enabling the generation of malicious yet\nexecutable SQL queries, which pose severe security and privacy risks in\nlanguage model-based SQL development. We demonstrate that injecting only 0.44%\nof poisoned data can result in an attack success rate of 79.41%, posing a\nsignificant risk to database security. Additionally, we propose detection and\nmitigation strategies to enhance model reliability. Our findings highlight the\nurgent need for security-aware Text-to-SQL development, emphasizing the\nimportance of robust defenses against backdoor threats."
                },
                "authors": [
                    {
                        "name": "Meiyu Lin"
                    },
                    {
                        "name": "Haichuan Zhang"
                    },
                    {
                        "name": "Jiale Lao"
                    },
                    {
                        "name": "Renyuan Li"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    },
                    {
                        "name": "Carl Yang"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Mingjie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Mingjie Tang"
                },
                "author": "Mingjie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05445v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05445v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02444v1",
                "updated": "2025-04-03T10:00:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    0,
                    1,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T10:00:01Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    0,
                    1,
                    3,
                    93,
                    0
                ],
                "title": "Isospectral oscillators as a resource for quantum information processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Isospectral oscillators as a resource for quantum information processing"
                },
                "summary": "We address quantum systems isospectral to the harmonic oscillator, as those\nfound within the framework of supersymmetric quantum mechanics, as potential\nresources for continuous variable quantum information. These deformed\noscillator potentials share the equally spaced energy levels of the shifted\nharmonic oscillator but differ significantly in that they are non-harmonic.\nConsequently, their ground states and thermal equilibrium states are no longer\nGaussian and exhibit non-classical properties. We quantify their\nnon-Gaussianity and evaluate their non-classicality using various measures,\nincluding quadrature squeezing, photon number squeezing, Wigner function\nnegativity, and quadrature coherence scale. Additionally, we employ quantum\nestimation theory to identify optimal measurement strategies and establish\nultimate precision bounds for inferring the deformation parameter. Our findings\nprove that quantum systems isospectral to the harmonic oscillator may represent\npromising platforms for quantum information with continuous variables. In turn,\nnon-Gaussian and non-classical stationary states may be obtained and these\nfeatures persist at non-zero temperature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address quantum systems isospectral to the harmonic oscillator, as those\nfound within the framework of supersymmetric quantum mechanics, as potential\nresources for continuous variable quantum information. These deformed\noscillator potentials share the equally spaced energy levels of the shifted\nharmonic oscillator but differ significantly in that they are non-harmonic.\nConsequently, their ground states and thermal equilibrium states are no longer\nGaussian and exhibit non-classical properties. We quantify their\nnon-Gaussianity and evaluate their non-classicality using various measures,\nincluding quadrature squeezing, photon number squeezing, Wigner function\nnegativity, and quadrature coherence scale. Additionally, we employ quantum\nestimation theory to identify optimal measurement strategies and establish\nultimate precision bounds for inferring the deformation parameter. Our findings\nprove that quantum systems isospectral to the harmonic oscillator may represent\npromising platforms for quantum information with continuous variables. In turn,\nnon-Gaussian and non-classical stationary states may be obtained and these\nfeatures persist at non-zero temperature."
                },
                "authors": [
                    {
                        "name": "Abdelatif Chabane"
                    },
                    {
                        "name": "Sidali Mohammdi"
                    },
                    {
                        "name": "Abdelhakim Gharbi"
                    },
                    {
                        "name": "Matteo G. A. Paris"
                    }
                ],
                "author_detail": {
                    "name": "Matteo G. A. Paris"
                },
                "author": "Matteo G. A. Paris",
                "arxiv_doi": "10.1142/S0219749925500042",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1142/S0219749925500042",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.02444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 7 figures",
                "arxiv_journal_ref": "Int. J. Quantum Inf. (2025) 2550004",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v1",
                "updated": "2025-04-03T09:58:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15289v2",
                "updated": "2025-04-03T09:56:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    56,
                    4,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-19T15:09:39Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    9,
                    39,
                    2,
                    78,
                    0
                ],
                "title": "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence\n  Tracing and Relationship Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence\n  Tracing and Relationship Classification"
                },
                "summary": "LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains such as healthcare, law, and news, it is\ncrucial to understand where and how the content is created. To address this, we\nintroduce the Text pROVEnance (TROVE) challenge, designed to trace each\nsentence of a target text back to specific source sentences within potentially\nlengthy or multi-document inputs. Beyond identifying sources, TROVE annotates\nthe fine-grained relationships (quotation, compression, inference, and others),\nproviding a deep understanding of how each target sentence is formed. To\nbenchmark TROVE, we construct our dataset by leveraging three public datasets\ncovering 11 diverse scenarios (e.g., QA and summarization) in English and\nChinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+),\nemphasizing the multi-document and long-document settings essential for\nprovenance. To ensure high-quality data, we employ a three-stage annotation\nprocess: sentence retrieval, GPT provenance, and human provenance. We evaluate\n11 LLMs under direct prompting and retrieval-augmented paradigms, revealing\nthat retrieval is essential for robust performance, larger models perform\nbetter in complex relationship classification, and closed-source models often\nlead, yet open-source models show significant promise, particularly with\nretrieval augmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains such as healthcare, law, and news, it is\ncrucial to understand where and how the content is created. To address this, we\nintroduce the Text pROVEnance (TROVE) challenge, designed to trace each\nsentence of a target text back to specific source sentences within potentially\nlengthy or multi-document inputs. Beyond identifying sources, TROVE annotates\nthe fine-grained relationships (quotation, compression, inference, and others),\nproviding a deep understanding of how each target sentence is formed. To\nbenchmark TROVE, we construct our dataset by leveraging three public datasets\ncovering 11 diverse scenarios (e.g., QA and summarization) in English and\nChinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+),\nemphasizing the multi-document and long-document settings essential for\nprovenance. To ensure high-quality data, we employ a three-stage annotation\nprocess: sentence retrieval, GPT provenance, and human provenance. We evaluate\n11 LLMs under direct prompting and retrieval-augmented paradigms, revealing\nthat retrieval is essential for robust performance, larger models perform\nbetter in complex relationship classification, and closed-source models often\nlead, yet open-source models show significant promise, particularly with\nretrieval augmentation."
                },
                "authors": [
                    {
                        "name": "Junnan Zhu"
                    },
                    {
                        "name": "Min Xiao"
                    },
                    {
                        "name": "Yining Wang"
                    },
                    {
                        "name": "Feifei Zhai"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Chengqing Zong"
                    }
                ],
                "author_detail": {
                    "name": "Chengqing Zong"
                },
                "author": "Chengqing Zong",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11167v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11167v3",
                "updated": "2025-04-03T09:54:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    54,
                    20,
                    3,
                    93,
                    0
                ],
                "published": "2025-02-16T15:38:19Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    15,
                    38,
                    19,
                    6,
                    47,
                    0
                ],
                "title": "SURGE: On the Potential of Large Language Models as General-Purpose\n  Surrogate Code Executors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SURGE: On the Potential of Large Language Models as General-Purpose\n  Surrogate Code Executors"
                },
                "summary": "Neural surrogate models have emerged as powerful and efficient tools in data\nmining. Meanwhile, large language models (LLMs) have demonstrated remarkable\ncapabilities in code-related tasks. We investigate a novel application: using\nLLMs as surrogate models for code execution prediction. Given LLMs' unique\nability to understand and process diverse programs, they present a promising\ndirection for building general-purpose surrogate models. To systematically\ninvestigate this capability, we introduce SURGE, a comprehensive benchmark with\n$1160$ problems covering $8$ key aspects: multi-language programming tasks,\ncompetition-level programming problems, repository-level code analysis,\nhigh-cost scientific computing, time-complexity-intensive algorithms, buggy\ncode analysis, programs dependent on specific compilers or execution\nenvironments, and formal mathematical proof verification. Through extensive\nempirical analysis of $21$ open-source and proprietary LLMs, we examine scaling\nlaws, data efficiency, and predictive accuracy. Our findings reveal important\ninsights about the feasibility of LLMs as efficient surrogates for\ncomputational processes, with implications for automated software testing,\nprogram analysis, and computational resource optimization in data mining\napplications. Code and dataset are released at\nhttps://github.com/Imbernoulli/SURGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural surrogate models have emerged as powerful and efficient tools in data\nmining. Meanwhile, large language models (LLMs) have demonstrated remarkable\ncapabilities in code-related tasks. We investigate a novel application: using\nLLMs as surrogate models for code execution prediction. Given LLMs' unique\nability to understand and process diverse programs, they present a promising\ndirection for building general-purpose surrogate models. To systematically\ninvestigate this capability, we introduce SURGE, a comprehensive benchmark with\n$1160$ problems covering $8$ key aspects: multi-language programming tasks,\ncompetition-level programming problems, repository-level code analysis,\nhigh-cost scientific computing, time-complexity-intensive algorithms, buggy\ncode analysis, programs dependent on specific compilers or execution\nenvironments, and formal mathematical proof verification. Through extensive\nempirical analysis of $21$ open-source and proprietary LLMs, we examine scaling\nlaws, data efficiency, and predictive accuracy. Our findings reveal important\ninsights about the feasibility of LLMs as efficient surrogates for\ncomputational processes, with implications for automated software testing,\nprogram analysis, and computational resource optimization in data mining\napplications. Code and dataset are released at\nhttps://github.com/Imbernoulli/SURGE."
                },
                "authors": [
                    {
                        "name": "Bohan Lyu"
                    },
                    {
                        "name": "Siqiao Huang"
                    },
                    {
                        "name": "Zichen Liang"
                    },
                    {
                        "name": "Qi-An Sun"
                    },
                    {
                        "name": "Jiaming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaming Zhang"
                },
                "author": "Jiaming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11167v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11167v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02436v1",
                "updated": "2025-04-03T09:50:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    50,
                    50,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:50:50Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    50,
                    50,
                    3,
                    93,
                    0
                ],
                "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyReels-A2: Compose Anything in Video Diffusion Transformers"
                },
                "summary": "This paper presents SkyReels-A2, a controllable video generation framework\ncapable of assembling arbitrary visual elements (e.g., characters, objects,\nbackgrounds) into synthesized videos based on textual prompts while maintaining\nstrict consistency with reference images for each element. We term this task\nelements-to-video (E2V), whose primary challenges lie in preserving the\nfidelity of each reference element, ensuring coherent composition of the scene,\nand achieving natural outputs. To address these, we first design a\ncomprehensive data pipeline to construct prompt-reference-video triplets for\nmodel training. Next, we propose a novel image-text joint embedding model to\ninject multi-element representations into the generative process, balancing\nelement-specific consistency with global coherence and text alignment. We also\noptimize the inference pipeline for both speed and output stability. Moreover,\nwe introduce a carefully curated benchmark for systematic evaluation, i.e, A2\nBench. Experiments demonstrate that our framework can generate diverse,\nhigh-quality videos with precise element control. SkyReels-A2 is the first\nopen-source commercial grade model for the generation of E2V, performing\nfavorably against advanced closed-source commercial models. We anticipate\nSkyReels-A2 will advance creative applications such as drama and virtual\ne-commerce, pushing the boundaries of controllable video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents SkyReels-A2, a controllable video generation framework\ncapable of assembling arbitrary visual elements (e.g., characters, objects,\nbackgrounds) into synthesized videos based on textual prompts while maintaining\nstrict consistency with reference images for each element. We term this task\nelements-to-video (E2V), whose primary challenges lie in preserving the\nfidelity of each reference element, ensuring coherent composition of the scene,\nand achieving natural outputs. To address these, we first design a\ncomprehensive data pipeline to construct prompt-reference-video triplets for\nmodel training. Next, we propose a novel image-text joint embedding model to\ninject multi-element representations into the generative process, balancing\nelement-specific consistency with global coherence and text alignment. We also\noptimize the inference pipeline for both speed and output stability. Moreover,\nwe introduce a carefully curated benchmark for systematic evaluation, i.e, A2\nBench. Experiments demonstrate that our framework can generate diverse,\nhigh-quality videos with precise element control. SkyReels-A2 is the first\nopen-source commercial grade model for the generation of E2V, performing\nfavorably against advanced closed-source commercial models. We anticipate\nSkyReels-A2 will advance creative applications such as drama and virtual\ne-commerce, pushing the boundaries of controllable video generation."
                },
                "authors": [
                    {
                        "name": "Zhengcong Fei"
                    },
                    {
                        "name": "Debang Li"
                    },
                    {
                        "name": "Di Qiu"
                    },
                    {
                        "name": "Jiahua Wang"
                    },
                    {
                        "name": "Yikun Dou"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Jingtao Xu"
                    },
                    {
                        "name": "Mingyuan Fan"
                    },
                    {
                        "name": "Guibin Chen"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yahui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yahui Zhou"
                },
                "author": "Yahui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02433v1",
                "updated": "2025-04-03T09:48:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    48,
                    13,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:48:13Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    48,
                    13,
                    3,
                    93,
                    0
                ],
                "title": "OmniTalker: Real-Time Text-Driven Talking Head Generation with\n  In-Context Audio-Visual Style Replication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniTalker: Real-Time Text-Driven Talking Head Generation with\n  In-Context Audio-Visual Style Replication"
                },
                "summary": "Recent years have witnessed remarkable advances in talking head generation,\nowing to its potential to revolutionize the human-AI interaction from text\ninterfaces into realistic video chats. However, research on text-driven talking\nheads remains underexplored, with existing methods predominantly adopting a\ncascaded pipeline that combines TTS systems with audio-driven talking head\nmodels. This conventional pipeline not only introduces system complexity and\nlatency overhead but also fundamentally suffers from asynchronous audiovisual\noutput and stylistic discrepancies between generated speech and visual\nexpressions. To address these limitations, we introduce OmniTalker, an\nend-to-end unified framework that simultaneously generates synchronized speech\nand talking head videos from text and reference video in real-time zero-shot\nscenarios, while preserving both speech style and facial styles. The framework\nemploys a dual-branch diffusion transformer architecture: the audio branch\nsynthesizes mel-spectrograms from text, while the visual branch predicts\nfine-grained head poses and facial dynamics. To bridge modalities, we introduce\na novel audio-visual fusion module that integrates cross-modal information to\nensure temporal synchronization and stylistic coherence between audio and\nvisual outputs. Furthermore, our in-context reference learning module\neffectively captures both speech and facial style characteristics from a single\nreference video without introducing an extra style extracting module. To the\nbest of our knowledge, OmniTalker presents the first unified framework that\njointly models speech style and facial style in a zero-shot setting, achieving\nreal-time inference speed of 25 FPS. Extensive experiments demonstrate that our\nmethod surpasses existing approaches in generation quality, particularly\nexcelling in style preservation and audio-video synchronization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed remarkable advances in talking head generation,\nowing to its potential to revolutionize the human-AI interaction from text\ninterfaces into realistic video chats. However, research on text-driven talking\nheads remains underexplored, with existing methods predominantly adopting a\ncascaded pipeline that combines TTS systems with audio-driven talking head\nmodels. This conventional pipeline not only introduces system complexity and\nlatency overhead but also fundamentally suffers from asynchronous audiovisual\noutput and stylistic discrepancies between generated speech and visual\nexpressions. To address these limitations, we introduce OmniTalker, an\nend-to-end unified framework that simultaneously generates synchronized speech\nand talking head videos from text and reference video in real-time zero-shot\nscenarios, while preserving both speech style and facial styles. The framework\nemploys a dual-branch diffusion transformer architecture: the audio branch\nsynthesizes mel-spectrograms from text, while the visual branch predicts\nfine-grained head poses and facial dynamics. To bridge modalities, we introduce\na novel audio-visual fusion module that integrates cross-modal information to\nensure temporal synchronization and stylistic coherence between audio and\nvisual outputs. Furthermore, our in-context reference learning module\neffectively captures both speech and facial style characteristics from a single\nreference video without introducing an extra style extracting module. To the\nbest of our knowledge, OmniTalker presents the first unified framework that\njointly models speech style and facial style in a zero-shot setting, achieving\nreal-time inference speed of 25 FPS. Extensive experiments demonstrate that our\nmethod surpasses existing approaches in generation quality, particularly\nexcelling in style preservation and audio-video synchronization."
                },
                "authors": [
                    {
                        "name": "Zhongjian Wang"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Jinwei Qi"
                    },
                    {
                        "name": "Guangyuan Wang Sheng Xu"
                    },
                    {
                        "name": "Bang Zhang"
                    },
                    {
                        "name": "Liefeng Bo"
                    }
                ],
                "author_detail": {
                    "name": "Liefeng Bo"
                },
                "author": "Liefeng Bo",
                "arxiv_comment": "Project Page https://humanaigc.github.io/omnitalker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02429v1",
                "updated": "2025-04-03T09:35:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    35,
                    7,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:35:07Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    35,
                    7,
                    3,
                    93,
                    0
                ],
                "title": "A Multi-Level Sentiment Analysis Framework for Financial Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Level Sentiment Analysis Framework for Financial Texts"
                },
                "summary": "Existing financial sentiment analysis methods often fail to capture the\nmulti-faceted nature of risk in bond markets due to their single-level approach\nand neglect of temporal dynamics. We propose Multi-Level Sentiment Analysis\nbased on pre-trained language models (PLMs) and large language models (LLMs), a\nnovel framework that systematically integrates firm-specific micro-level\nsentiment, industry-specific meso-level sentiment, and duration-aware smoothing\nto model the latency and persistence of textual impact. Applying our framework\nto the comprehensive Chinese bond market corpus constructed by us (2013-2023,\n1.39M texts), we extracted a daily composite sentiment index. Empirical results\nshow statistically measurable improvements in credit spread forecasting when\nincorporating sentiment (3.25% MAE and 10.96% MAPE reduction), with sentiment\nshifts closely correlating with major social risk events and firm-specific\ncrises. This framework provides a more nuanced understanding of sentiment\nacross different market levels while accounting for the temporal evolution of\nsentiment effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing financial sentiment analysis methods often fail to capture the\nmulti-faceted nature of risk in bond markets due to their single-level approach\nand neglect of temporal dynamics. We propose Multi-Level Sentiment Analysis\nbased on pre-trained language models (PLMs) and large language models (LLMs), a\nnovel framework that systematically integrates firm-specific micro-level\nsentiment, industry-specific meso-level sentiment, and duration-aware smoothing\nto model the latency and persistence of textual impact. Applying our framework\nto the comprehensive Chinese bond market corpus constructed by us (2013-2023,\n1.39M texts), we extracted a daily composite sentiment index. Empirical results\nshow statistically measurable improvements in credit spread forecasting when\nincorporating sentiment (3.25% MAE and 10.96% MAPE reduction), with sentiment\nshifts closely correlating with major social risk events and firm-specific\ncrises. This framework provides a more nuanced understanding of sentiment\nacross different market levels while accounting for the temporal evolution of\nsentiment effects."
                },
                "authors": [
                    {
                        "name": "Yiwei Liu"
                    },
                    {
                        "name": "Junbo Wang"
                    },
                    {
                        "name": "Lei Long"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Ruiting Ma"
                    },
                    {
                        "name": "Yuankai Wu"
                    },
                    {
                        "name": "Xuebin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xuebin Chen"
                },
                "author": "Xuebin Chen",
                "arxiv_comment": "14pages, 11 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02426v1",
                "updated": "2025-04-03T09:31:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    31,
                    7,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:31:07Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    31,
                    7,
                    3,
                    93,
                    0
                ],
                "title": "Narrative Studio: Visual narrative exploration using LLMs and Monte\n  Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Narrative Studio: Visual narrative exploration using LLMs and Monte\n  Carlo Tree Search"
                },
                "summary": "Interactive storytelling benefits from planning and exploring multiple 'what\nif' scenarios. Modern LLMs are useful tools for ideation and exploration, but\ncurrent chat-based user interfaces restrict users to a single linear flow. To\naddress this limitation, we propose Narrative Studio -- a novel in-browser\nnarrative exploration environment featuring a tree-like interface that allows\nbranching exploration from user-defined points in a story. Each branch is\nextended via iterative LLM inference guided by system and user-defined prompts.\nAdditionally, we employ Monte Carlo Tree Search (MCTS) to automatically expand\npromising narrative paths based on user-specified criteria, enabling more\ndiverse and robust story development. We also allow users to enhance narrative\ncoherence by grounding the generated text in an entity graph that represents\nthe actors and environment of the story.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive storytelling benefits from planning and exploring multiple 'what\nif' scenarios. Modern LLMs are useful tools for ideation and exploration, but\ncurrent chat-based user interfaces restrict users to a single linear flow. To\naddress this limitation, we propose Narrative Studio -- a novel in-browser\nnarrative exploration environment featuring a tree-like interface that allows\nbranching exploration from user-defined points in a story. Each branch is\nextended via iterative LLM inference guided by system and user-defined prompts.\nAdditionally, we employ Monte Carlo Tree Search (MCTS) to automatically expand\npromising narrative paths based on user-specified criteria, enabling more\ndiverse and robust story development. We also allow users to enhance narrative\ncoherence by grounding the generated text in an entity graph that represents\nthe actors and environment of the story."
                },
                "authors": [
                    {
                        "name": "Parsa Ghaffari"
                    },
                    {
                        "name": "Chris Hokamp"
                    }
                ],
                "author_detail": {
                    "name": "Chris Hokamp"
                },
                "author": "Chris Hokamp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02417v1",
                "updated": "2025-04-03T09:14:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    14,
                    41,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:14:41Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    14,
                    41,
                    3,
                    93,
                    0
                ],
                "title": "Leveraging Static Relationships for Intra-Type and Inter-Type Message\n  Passing in Video Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Static Relationships for Intra-Type and Inter-Type Message\n  Passing in Video Question Answering"
                },
                "summary": "Video Question Answering (VideoQA) is an important research direction in the\nfield of artificial intelligence, enabling machines to understand video content\nand perform reasoning and answering based on natural language questions.\nAlthough methods based on static relationship reasoning have made certain\nprogress, there are still deficiencies in the accuracy of static relationship\nrecognition and representation, and they have not fully utilized the static\nrelationship information in videos for in-depth reasoning and analysis.\nTherefore, this paper proposes a reasoning method for intra-type and inter-type\nmessage passing based on static relationships. This method constructs a dual\ngraph for intra-type message passing reasoning and builds a heterogeneous graph\nbased on static relationships for inter-type message passing reasoning. The\nintra-type message passing reasoning model captures the neighborhood\ninformation of targets and relationships related to the question in the dual\ngraph, updating the dual graph to obtain intra-type clues for answering the\nquestion. The inter-type message passing reasoning model captures the\nneighborhood information of targets and relationships from different categories\nrelated to the question in the heterogeneous graph, updating the heterogeneous\ngraph to obtain inter-type clues for answering the question. Finally, the\nanswers are inferred by combining the intra-type and inter-type clues based on\nstatic relationships. Experimental results on the ANetQA and Next-QA datasets\ndemonstrate the effectiveness of this method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Question Answering (VideoQA) is an important research direction in the\nfield of artificial intelligence, enabling machines to understand video content\nand perform reasoning and answering based on natural language questions.\nAlthough methods based on static relationship reasoning have made certain\nprogress, there are still deficiencies in the accuracy of static relationship\nrecognition and representation, and they have not fully utilized the static\nrelationship information in videos for in-depth reasoning and analysis.\nTherefore, this paper proposes a reasoning method for intra-type and inter-type\nmessage passing based on static relationships. This method constructs a dual\ngraph for intra-type message passing reasoning and builds a heterogeneous graph\nbased on static relationships for inter-type message passing reasoning. The\nintra-type message passing reasoning model captures the neighborhood\ninformation of targets and relationships related to the question in the dual\ngraph, updating the dual graph to obtain intra-type clues for answering the\nquestion. The inter-type message passing reasoning model captures the\nneighborhood information of targets and relationships from different categories\nrelated to the question in the heterogeneous graph, updating the heterogeneous\ngraph to obtain inter-type clues for answering the question. Finally, the\nanswers are inferred by combining the intra-type and inter-type clues based on\nstatic relationships. Experimental results on the ANetQA and Next-QA datasets\ndemonstrate the effectiveness of this method."
                },
                "authors": [
                    {
                        "name": "Lili Liang"
                    },
                    {
                        "name": "Guanglu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guanglu Sun"
                },
                "author": "Guanglu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22036v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22036v2",
                "updated": "2025-04-03T09:08:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    8,
                    48,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-27T23:06:30Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    23,
                    6,
                    30,
                    3,
                    86,
                    0
                ],
                "title": "Cognitive Prompts Using Guilford's Structure of Intellect Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Prompts Using Guilford's Structure of Intellect Model"
                },
                "summary": "Large language models (LLMs) demonstrate strong language generation\ncapabilities but often struggle with structured reasoning, leading to\ninconsistent or suboptimal problem-solving. To mitigate this limitation,\nGuilford's Structure of Intellect (SOI) model - a foundational framework from\nintelligence theory - is leveraged as the basis for cognitive prompt\nengineering. The SOI model categorizes cognitive operations such as pattern\nrecognition, memory retrieval, and evaluation, offering a systematic approach\nto enhancing LLM reasoning and decision-making. This position paper presents a\nnovel cognitive prompting approach for enforcing SOI-inspired reasoning for\nimproving clarity, coherence, and adaptability in model responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate strong language generation\ncapabilities but often struggle with structured reasoning, leading to\ninconsistent or suboptimal problem-solving. To mitigate this limitation,\nGuilford's Structure of Intellect (SOI) model - a foundational framework from\nintelligence theory - is leveraged as the basis for cognitive prompt\nengineering. The SOI model categorizes cognitive operations such as pattern\nrecognition, memory retrieval, and evaluation, offering a systematic approach\nto enhancing LLM reasoning and decision-making. This position paper presents a\nnovel cognitive prompting approach for enforcing SOI-inspired reasoning for\nimproving clarity, coherence, and adaptability in model responses."
                },
                "authors": [
                    {
                        "name": "Oliver Kramer"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Kramer"
                },
                "author": "Oliver Kramer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22036v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22036v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14614v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14614v3",
                "updated": "2025-04-03T09:07:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    7,
                    7,
                    3,
                    93,
                    0
                ],
                "published": "2025-02-20T14:52:36Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    52,
                    36,
                    3,
                    51,
                    0
                ],
                "title": "FIND: Fine-grained Information Density Guided Adaptive\n  Retrieval-Augmented Generation for Disease Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIND: Fine-grained Information Density Guided Adaptive\n  Retrieval-Augmented Generation for Disease Diagnosis"
                },
                "summary": "Retrieval-Augmented Large Language Models (LLMs), which integrate external\nknowledge into LLMs, have shown remarkable performance in various medical\ndomains, including clinical diagnosis. However, existing RAG methods struggle\nto effectively assess task difficulty to make retrieval decisions, thereby\nfailing to meet the clinical requirements for balancing efficiency and\naccuracy. So in this paper, we propose FIND (\\textbf{F}ine-grained\n\\textbf{In}formation \\textbf{D}ensity Guided Adaptive RAG), a novel framework\nthat improves the reliability of RAG in disease diagnosis scenarios. FIND\nincorporates a fine-grained adaptive control module to determine whether\nretrieval is necessary based on the information density of the input. By\noptimizing the retrieval process and implementing a knowledge filtering module,\nFIND ensures that the retrieval is better suited to clinical scenarios.\nExperiments on three Chinese electronic medical record datasets demonstrate\nthat FIND significantly outperforms various baseline methods, highlighting its\neffectiveness in clinical diagnosis tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Large Language Models (LLMs), which integrate external\nknowledge into LLMs, have shown remarkable performance in various medical\ndomains, including clinical diagnosis. However, existing RAG methods struggle\nto effectively assess task difficulty to make retrieval decisions, thereby\nfailing to meet the clinical requirements for balancing efficiency and\naccuracy. So in this paper, we propose FIND (\\textbf{F}ine-grained\n\\textbf{In}formation \\textbf{D}ensity Guided Adaptive RAG), a novel framework\nthat improves the reliability of RAG in disease diagnosis scenarios. FIND\nincorporates a fine-grained adaptive control module to determine whether\nretrieval is necessary based on the information density of the input. By\noptimizing the retrieval process and implementing a knowledge filtering module,\nFIND ensures that the retrieval is better suited to clinical scenarios.\nExperiments on three Chinese electronic medical record datasets demonstrate\nthat FIND significantly outperforms various baseline methods, highlighting its\neffectiveness in clinical diagnosis tasks."
                },
                "authors": [
                    {
                        "name": "Mingyi Jia"
                    },
                    {
                        "name": "Junwen Duan"
                    },
                    {
                        "name": "Yan Song"
                    },
                    {
                        "name": "Jianxin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Wang"
                },
                "author": "Jianxin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14614v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14614v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15127v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15127v3",
                "updated": "2025-04-03T09:05:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    5,
                    45,
                    3,
                    93,
                    0
                ],
                "published": "2024-09-23T15:33:38Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    33,
                    38,
                    0,
                    267,
                    0
                ],
                "title": "Pareto-Optimized Open-Source LLMs for Healthcare via Context Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pareto-Optimized Open-Source LLMs for Healthcare via Context Retrieval"
                },
                "summary": "This study leverages optimized context retrieval to enhance open-source Large\nLanguage Models (LLMs) for cost-effective, high performance healthcare AI. We\ndemonstrate that this approach achieves state-of-the-art accuracy on medical\nquestion answering at a fraction of the cost of proprietary models,\nsignificantly improving the cost-accuracy Pareto frontier on the MedQA\nbenchmark. Key contributions include: (1) OpenMedQA, a novel benchmark\nrevealing a performance gap in open-ended medical QA compared to\nmultiple-choice formats; (2) a practical, reproducible pipeline for context\nretrieval optimization; and (3) open-source resources (Prompt Engine,\nCoT/ToT/Thinking databases) to empower healthcare AI development. By advancing\nretrieval techniques and QA evaluation, we enable more affordable and reliable\nLLM solutions for healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study leverages optimized context retrieval to enhance open-source Large\nLanguage Models (LLMs) for cost-effective, high performance healthcare AI. We\ndemonstrate that this approach achieves state-of-the-art accuracy on medical\nquestion answering at a fraction of the cost of proprietary models,\nsignificantly improving the cost-accuracy Pareto frontier on the MedQA\nbenchmark. Key contributions include: (1) OpenMedQA, a novel benchmark\nrevealing a performance gap in open-ended medical QA compared to\nmultiple-choice formats; (2) a practical, reproducible pipeline for context\nretrieval optimization; and (3) open-source resources (Prompt Engine,\nCoT/ToT/Thinking databases) to empower healthcare AI development. By advancing\nretrieval techniques and QA evaluation, we enable more affordable and reliable\nLLM solutions for healthcare."
                },
                "authors": [
                    {
                        "name": "Jordi Bayarri-Planas"
                    },
                    {
                        "name": "Ashwin Kumar Gururajan"
                    },
                    {
                        "name": "Dario Garcia-Gasulla"
                    }
                ],
                "author_detail": {
                    "name": "Dario Garcia-Gasulla"
                },
                "author": "Dario Garcia-Gasulla",
                "arxiv_comment": "14 pages, 3 figures, 5 tables, Accepted for publication at the 21st\n  International Conference on Artificial Intelligence Applications and\n  Innovations (AIAI 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15127v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15127v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02411v1",
                "updated": "2025-04-03T09:03:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    3,
                    40,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:03:40Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    3,
                    40,
                    3,
                    93,
                    0
                ],
                "title": "Adapting Large Language Models for Multi-Domain\n  Retrieval-Augmented-Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Large Language Models for Multi-Domain\n  Retrieval-Augmented-Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enhances LLM factuality, but\nmulti-domain applications face challenges like lack of diverse benchmarks and\npoor out-of-domain generalization. The first contribution of this work is to\nintroduce a diverse benchmark comprising a variety of question-answering tasks\nfrom 8 sources and covering 13 domains. Our second contribution consists in\nsystematically testing out-of-domain generalization for typical RAG tuning\nstrategies. While our findings reveal that standard fine-tuning fails to\ngeneralize effectively, we show that sequence-level distillation with\nteacher-generated labels improves out-of-domain performance by providing more\ncoherent supervision. Our findings highlight key strategies for improving\nmulti-domain RAG robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enhances LLM factuality, but\nmulti-domain applications face challenges like lack of diverse benchmarks and\npoor out-of-domain generalization. The first contribution of this work is to\nintroduce a diverse benchmark comprising a variety of question-answering tasks\nfrom 8 sources and covering 13 domains. Our second contribution consists in\nsystematically testing out-of-domain generalization for typical RAG tuning\nstrategies. While our findings reveal that standard fine-tuning fails to\ngeneralize effectively, we show that sequence-level distillation with\nteacher-generated labels improves out-of-domain performance by providing more\ncoherent supervision. Our findings highlight key strategies for improving\nmulti-domain RAG robustness."
                },
                "authors": [
                    {
                        "name": "Alexandre Misrahi"
                    },
                    {
                        "name": "Nadezhda Chirkova"
                    },
                    {
                        "name": "Maxime Louis"
                    },
                    {
                        "name": "Vassilina Nikoulina"
                    }
                ],
                "author_detail": {
                    "name": "Vassilina Nikoulina"
                },
                "author": "Vassilina Nikoulina",
                "arxiv_comment": "25 pages, 8 figures, 21 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02404v1",
                "updated": "2025-04-03T08:54:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    54,
                    23,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T08:54:23Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    54,
                    23,
                    3,
                    93,
                    0
                ],
                "title": "AnesBench: Multi-Dimensional Evaluation of LLM Reasoning in\n  Anesthesiology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnesBench: Multi-Dimensional Evaluation of LLM Reasoning in\n  Anesthesiology"
                },
                "summary": "The application of large language models (LLMs) in the medical field has\ngained significant attention, yet their reasoning capabilities in more\nspecialized domains like anesthesiology remain underexplored. In this paper, we\nsystematically evaluate the reasoning capabilities of LLMs in anesthesiology\nand analyze key factors influencing their performance. To this end, we\nintroduce AnesBench, a cross-lingual benchmark designed to assess\nanesthesiology-related reasoning across three levels: factual retrieval (System\n1), hybrid reasoning (System 1.x), and complex decision-making (System 2).\nThrough extensive experiments, we first explore how model characteristics,\nincluding model scale, Chain of Thought (CoT) length, and language\ntransferability, affect reasoning performance. Then, we further evaluate the\neffectiveness of different training strategies, leveraging our curated\nanesthesiology-related dataset, including continuous pre-training (CPT) and\nsupervised fine-tuning (SFT). Additionally, we also investigate how the\ntest-time reasoning techniques, such as Best-of-N sampling and beam search,\ninfluence reasoning performance, and assess the impact of reasoning-enhanced\nmodel distillation, specifically DeepSeek-R1. We will publicly release\nAnesBench, along with our CPT and SFT training datasets and evaluation code at\nhttps://github.com/MiliLab/AnesBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of large language models (LLMs) in the medical field has\ngained significant attention, yet their reasoning capabilities in more\nspecialized domains like anesthesiology remain underexplored. In this paper, we\nsystematically evaluate the reasoning capabilities of LLMs in anesthesiology\nand analyze key factors influencing their performance. To this end, we\nintroduce AnesBench, a cross-lingual benchmark designed to assess\nanesthesiology-related reasoning across three levels: factual retrieval (System\n1), hybrid reasoning (System 1.x), and complex decision-making (System 2).\nThrough extensive experiments, we first explore how model characteristics,\nincluding model scale, Chain of Thought (CoT) length, and language\ntransferability, affect reasoning performance. Then, we further evaluate the\neffectiveness of different training strategies, leveraging our curated\nanesthesiology-related dataset, including continuous pre-training (CPT) and\nsupervised fine-tuning (SFT). Additionally, we also investigate how the\ntest-time reasoning techniques, such as Best-of-N sampling and beam search,\ninfluence reasoning performance, and assess the impact of reasoning-enhanced\nmodel distillation, specifically DeepSeek-R1. We will publicly release\nAnesBench, along with our CPT and SFT training datasets and evaluation code at\nhttps://github.com/MiliLab/AnesBench."
                },
                "authors": [
                    {
                        "name": "Xiang Feng"
                    },
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Yong Luo"
                    },
                    {
                        "name": "Pingbo Xu"
                    },
                    {
                        "name": "Baosheng Yu"
                    },
                    {
                        "name": "Hua Jin"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang",
                "arxiv_comment": "23 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01707v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01707v2",
                "updated": "2025-04-03T08:53:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    53,
                    6,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T13:15:44Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    15,
                    44,
                    2,
                    92,
                    0
                ],
                "title": "InfiniteICL: Breaking the Limit of Context Window Size via Long\n  Short-term Memory Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteICL: Breaking the Limit of Context Window Size via Long\n  Short-term Memory Transformation"
                },
                "summary": "In-context learning (ICL) is critical for large language models (LLMs), but\nits effectiveness is constrained by finite context windows, particularly in\nultra-long contexts. To overcome this, we introduce InfiniteICL, a framework\nthat parallels context and parameters in LLMs with short- and long-term memory\nin human cognitive systems, focusing on transforming temporary context\nknowledge into permanent parameter updates. This approach significantly reduces\nmemory usage, maintains robust performance across varying input lengths, and\ntheoretically enables infinite context integration through the principles of\ncontext knowledge elicitation, selection, and consolidation. Evaluations\ndemonstrate that our method reduces context length by 90% while achieving 103%\naverage performance of full-context prompting across fact recall, grounded\nreasoning, and skill acquisition tasks. When conducting sequential multi-turn\ntransformations on complex, real-world contexts (with length up to 2M tokens),\nour approach surpasses full-context prompting while using only 0.4% of the\noriginal contexts. These findings highlight InfiniteICL's potential to enhance\nthe scalability and efficiency of LLMs by breaking the limitations of\nconventional context window sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) is critical for large language models (LLMs), but\nits effectiveness is constrained by finite context windows, particularly in\nultra-long contexts. To overcome this, we introduce InfiniteICL, a framework\nthat parallels context and parameters in LLMs with short- and long-term memory\nin human cognitive systems, focusing on transforming temporary context\nknowledge into permanent parameter updates. This approach significantly reduces\nmemory usage, maintains robust performance across varying input lengths, and\ntheoretically enables infinite context integration through the principles of\ncontext knowledge elicitation, selection, and consolidation. Evaluations\ndemonstrate that our method reduces context length by 90% while achieving 103%\naverage performance of full-context prompting across fact recall, grounded\nreasoning, and skill acquisition tasks. When conducting sequential multi-turn\ntransformations on complex, real-world contexts (with length up to 2M tokens),\nour approach surpasses full-context prompting while using only 0.4% of the\noriginal contexts. These findings highlight InfiniteICL's potential to enhance\nthe scalability and efficiency of LLMs by breaking the limitations of\nconventional context window sizes."
                },
                "authors": [
                    {
                        "name": "Bowen Cao"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01707v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01707v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02403v1",
                "updated": "2025-04-03T08:52:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    52,
                    42,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T08:52:42Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    52,
                    42,
                    3,
                    93,
                    0
                ],
                "title": "DaKultur: Evaluating the Cultural Awareness of Language Models for\n  Danish with Native Speakers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DaKultur: Evaluating the Cultural Awareness of Language Models for\n  Danish with Native Speakers"
                },
                "summary": "Large Language Models (LLMs) have seen widespread societal adoption. However,\nwhile they are able to interact with users in languages beyond English, they\nhave been shown to lack cultural awareness, providing anglocentric or\ninappropriate responses for underrepresented language communities. To\ninvestigate this gap and disentangle linguistic versus cultural proficiency, we\nconduct the first cultural evaluation study for the mid-resource language of\nDanish, in which native speakers prompt different models to solve tasks\nrequiring cultural awareness. Our analysis of the resulting 1,038 interactions\nfrom 63 demographically diverse participants highlights open challenges to\ncultural adaptation: Particularly, how currently employed automatically\ntranslated data are insufficient to train or measure cultural adaptation, and\nhow training on native-speaker data can more than double response acceptance\nrates. We release our study data as DaKultur - the first native Danish cultural\nawareness dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have seen widespread societal adoption. However,\nwhile they are able to interact with users in languages beyond English, they\nhave been shown to lack cultural awareness, providing anglocentric or\ninappropriate responses for underrepresented language communities. To\ninvestigate this gap and disentangle linguistic versus cultural proficiency, we\nconduct the first cultural evaluation study for the mid-resource language of\nDanish, in which native speakers prompt different models to solve tasks\nrequiring cultural awareness. Our analysis of the resulting 1,038 interactions\nfrom 63 demographically diverse participants highlights open challenges to\ncultural adaptation: Particularly, how currently employed automatically\ntranslated data are insufficient to train or measure cultural adaptation, and\nhow training on native-speaker data can more than double response acceptance\nrates. We release our study data as DaKultur - the first native Danish cultural\nawareness dataset."
                },
                "authors": [
                    {
                        "name": "Max Müller-Eberstein"
                    },
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Elisa Bassignana"
                    },
                    {
                        "name": "Peter Brunsgaard Trolle"
                    },
                    {
                        "name": "Rob van der Goot"
                    }
                ],
                "author_detail": {
                    "name": "Rob van der Goot"
                },
                "author": "Rob van der Goot",
                "arxiv_comment": "Accepted at C3NLP at NAACL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02395v1",
                "updated": "2025-04-03T08:41:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    41,
                    26,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T08:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    41,
                    26,
                    3,
                    93,
                    0
                ],
                "title": "The quasi-semantic competence of LLMs: a case study on the part-whole\n  relation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quasi-semantic competence of LLMs: a case study on the part-whole\n  relation"
                },
                "summary": "Understanding the extent and depth of the semantic competence of \\emph{Large\nLanguage Models} (LLMs) is at the center of the current scientific agenda in\nArtificial Intelligence (AI) and Computational Linguistics (CL). We contribute\nto this endeavor by investigating their knowledge of the \\emph{part-whole}\nrelation, a.k.a. \\emph{meronymy}, which plays a crucial role in lexical\norganization, but it is significantly understudied. We used data from\nConceptNet relations \\citep{speer2016conceptnet} and human-generated semantic\nfeature norms \\citep{McRae:2005} to explore the abilities of LLMs to deal with\n\\textit{part-whole} relations. We employed several methods based on three\nlevels of analysis: i.) \\textbf{behavioral} testing via prompting, where we\ndirectly queried the models on their knowledge of meronymy, ii.) sentence\n\\textbf{probability} scoring, where we tested models' abilities to discriminate\ncorrect (real) and incorrect (asymmetric counterfactual) \\textit{part-whole}\nrelations, and iii.) \\textbf{concept representation} analysis in vector space,\nwhere we proved the linear organization of the \\textit{part-whole} concept in\nthe embedding and unembedding spaces. These analyses present a complex picture\nthat reveals that the LLMs' knowledge of this relation is only partial. They\nhave just a ``\\emph{quasi}-semantic'' competence and still fall short of\ncapturing deep inferential properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the extent and depth of the semantic competence of \\emph{Large\nLanguage Models} (LLMs) is at the center of the current scientific agenda in\nArtificial Intelligence (AI) and Computational Linguistics (CL). We contribute\nto this endeavor by investigating their knowledge of the \\emph{part-whole}\nrelation, a.k.a. \\emph{meronymy}, which plays a crucial role in lexical\norganization, but it is significantly understudied. We used data from\nConceptNet relations \\citep{speer2016conceptnet} and human-generated semantic\nfeature norms \\citep{McRae:2005} to explore the abilities of LLMs to deal with\n\\textit{part-whole} relations. We employed several methods based on three\nlevels of analysis: i.) \\textbf{behavioral} testing via prompting, where we\ndirectly queried the models on their knowledge of meronymy, ii.) sentence\n\\textbf{probability} scoring, where we tested models' abilities to discriminate\ncorrect (real) and incorrect (asymmetric counterfactual) \\textit{part-whole}\nrelations, and iii.) \\textbf{concept representation} analysis in vector space,\nwhere we proved the linear organization of the \\textit{part-whole} concept in\nthe embedding and unembedding spaces. These analyses present a complex picture\nthat reveals that the LLMs' knowledge of this relation is only partial. They\nhave just a ``\\emph{quasi}-semantic'' competence and still fall short of\ncapturing deep inferential properties."
                },
                "authors": [
                    {
                        "name": "Mattia Proietti"
                    },
                    {
                        "name": "Alessandro Lenci"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Lenci"
                },
                "author": "Alessandro Lenci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02166v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02166v3",
                "updated": "2025-04-03T08:36:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    36,
                    20,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-04T01:04:14Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    1,
                    4,
                    14,
                    1,
                    63,
                    0
                ],
                "title": "The impact of local noise recorded at the ET candidate sites on the\n  signal to noise ratio of CBC gravitational wave signals for the ET triangle\n  configuration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of local noise recorded at the ET candidate sites on the\n  signal to noise ratio of CBC gravitational wave signals for the ET triangle\n  configuration"
                },
                "summary": "We present an evaluation of how site dependent noise can affect the signal to\nnoise ratio (SNR) of compact binary coalescence (CBC) signals in the future 3rd\ngeneration gravitational wave (GW) detector Einstein Telescope (ET). The design\nof ET is currently pushing the scientific community to study its scientific\npotential with respect to known, and possibly unexpected, GW signals using its\ndesign sensitivity curves. However, local ambient noise may have an impact on\nthe ET sensitivity at low frequency and therefore affect the SNR of CBC signals\nat low frequency. Therefore, we study the impact of ambient noise on the ET\nsensitivity curve at the two sites candidate to host ET - Sardinia, in Italy,\nand the Euregio Meuse-Rhine (EMR) at the Netherlands-Belgium border - and infer\nthe impact on the ET sensitivity curve and how the SNR of CBC signals at low\nfrequencies is affected. We find that Sardinia shows results which are on par,\nif not better, than the design case. On the other hand, ambient noise for the\ncurrent EMR sensitivity curve in Terziet causes a higher degradation of the SNR\nperformances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an evaluation of how site dependent noise can affect the signal to\nnoise ratio (SNR) of compact binary coalescence (CBC) signals in the future 3rd\ngeneration gravitational wave (GW) detector Einstein Telescope (ET). The design\nof ET is currently pushing the scientific community to study its scientific\npotential with respect to known, and possibly unexpected, GW signals using its\ndesign sensitivity curves. However, local ambient noise may have an impact on\nthe ET sensitivity at low frequency and therefore affect the SNR of CBC signals\nat low frequency. Therefore, we study the impact of ambient noise on the ET\nsensitivity curve at the two sites candidate to host ET - Sardinia, in Italy,\nand the Euregio Meuse-Rhine (EMR) at the Netherlands-Belgium border - and infer\nthe impact on the ET sensitivity curve and how the SNR of CBC signals at low\nfrequencies is affected. We find that Sardinia shows results which are on par,\nif not better, than the design case. On the other hand, ambient noise for the\ncurrent EMR sensitivity curve in Terziet causes a higher degradation of the SNR\nperformances."
                },
                "authors": [
                    {
                        "name": "Matteo Di Giovanni"
                    },
                    {
                        "name": "Davide Rozza"
                    },
                    {
                        "name": "Rosario De Rosa"
                    },
                    {
                        "name": "Enrico Calloni"
                    },
                    {
                        "name": "Domenico D'Urso"
                    },
                    {
                        "name": "Luca Naticchioni"
                    },
                    {
                        "name": "Annalisa Allocca"
                    },
                    {
                        "name": "Giovanni Luca Cardello"
                    },
                    {
                        "name": "Alessandro Cardini"
                    },
                    {
                        "name": "Andrea Contu"
                    },
                    {
                        "name": "Giovanni Diaferia"
                    },
                    {
                        "name": "Luciano Errico"
                    },
                    {
                        "name": "Carlo Giunchi"
                    },
                    {
                        "name": "Jan Harms"
                    },
                    {
                        "name": "Irene Molinari"
                    },
                    {
                        "name": "Marco Olivieri"
                    },
                    {
                        "name": "Piero Rapagnani"
                    },
                    {
                        "name": "Fulvio Ricci"
                    },
                    {
                        "name": "Valeria Sipala"
                    },
                    {
                        "name": "Lucia Trozzo"
                    }
                ],
                "author_detail": {
                    "name": "Lucia Trozzo"
                },
                "author": "Lucia Trozzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02166v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02166v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17233v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17233v3",
                "updated": "2025-04-03T08:27:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    27,
                    46,
                    3,
                    93,
                    0
                ],
                "published": "2024-10-22T17:53:34Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    17,
                    53,
                    34,
                    1,
                    296,
                    0
                ],
                "title": "ICPL: Few-shot In-context Preference Learning via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICPL: Few-shot In-context Preference Learning via LLMs"
                },
                "summary": "Preference-based reinforcement learning is an effective way to handle tasks\nwhere rewards are hard to specify but can be exceedingly inefficient as\npreference learning is often tabula rasa. We demonstrate that Large Language\nModels (LLMs) have native preference-learning capabilities that allow them to\nachieve sample-efficient preference learning, addressing this challenge. We\npropose In-Context Preference Learning (ICPL), which uses in-context learning\ncapabilities of LLMs to reduce human query inefficiency. ICPL uses the task\ndescription and basic environment code to create sets of reward functions which\nare iteratively refined by placing human feedback over videos of the resultant\npolicies into the context of an LLM and then requesting better rewards. We\nfirst demonstrate ICPL's effectiveness through a synthetic preference study,\nproviding quantitative evidence that it significantly outperforms baseline\npreference-based methods with much higher performance and orders of magnitude\ngreater efficiency. We observe that these improvements are not solely coming\nfrom LLM grounding in the task but that the quality of the rewards improves\nover time, indicating preference learning capabilities. Additionally, we\nperform a series of real human preference-learning trials and observe that ICPL\nextends beyond synthetic settings and can work effectively with\nhumans-in-the-loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference-based reinforcement learning is an effective way to handle tasks\nwhere rewards are hard to specify but can be exceedingly inefficient as\npreference learning is often tabula rasa. We demonstrate that Large Language\nModels (LLMs) have native preference-learning capabilities that allow them to\nachieve sample-efficient preference learning, addressing this challenge. We\npropose In-Context Preference Learning (ICPL), which uses in-context learning\ncapabilities of LLMs to reduce human query inefficiency. ICPL uses the task\ndescription and basic environment code to create sets of reward functions which\nare iteratively refined by placing human feedback over videos of the resultant\npolicies into the context of an LLM and then requesting better rewards. We\nfirst demonstrate ICPL's effectiveness through a synthetic preference study,\nproviding quantitative evidence that it significantly outperforms baseline\npreference-based methods with much higher performance and orders of magnitude\ngreater efficiency. We observe that these improvements are not solely coming\nfrom LLM grounding in the task but that the quality of the rewards improves\nover time, indicating preference learning capabilities. Additionally, we\nperform a series of real human preference-learning trials and observe that ICPL\nextends beyond synthetic settings and can work effectively with\nhumans-in-the-loop."
                },
                "authors": [
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Qixin Tan"
                    },
                    {
                        "name": "Hong Lu"
                    },
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Xinting Yang"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Eugene Vinitsky"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Vinitsky"
                },
                "author": "Eugene Vinitsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17233v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17233v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11377v2",
                "updated": "2025-04-03T08:22:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    22,
                    27,
                    3,
                    93,
                    0
                ],
                "published": "2024-10-15T08:16:43Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    16,
                    43,
                    1,
                    289,
                    0
                ],
                "title": "A Framework for Adapting Human-Robot Interaction to Diverse User Groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Adapting Human-Robot Interaction to Diverse User Groups"
                },
                "summary": "To facilitate natural and intuitive interactions with diverse user groups in\nreal-world settings, social robots must be capable of addressing the varying\nrequirements and expectations of these groups while adapting their behavior\nbased on user feedback. While previous research often focuses on specific\ndemographics, we present a novel framework for adaptive Human-Robot Interaction\n(HRI) that tailors interactions to different user groups and enables individual\nusers to modulate interactions through both minor and major interruptions. Our\nprimary contributions include the development of an adaptive, ROS-based HRI\nframework with an open-source code base. This framework supports natural\ninteractions through advanced speech recognition and voice activity detection,\nand leverages a large language model (LLM) as a dialogue bridge. We validate\nthe efficiency of our framework through module tests and system trials,\ndemonstrating its high accuracy in age recognition and its robustness to\nrepeated user inputs and plan changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To facilitate natural and intuitive interactions with diverse user groups in\nreal-world settings, social robots must be capable of addressing the varying\nrequirements and expectations of these groups while adapting their behavior\nbased on user feedback. While previous research often focuses on specific\ndemographics, we present a novel framework for adaptive Human-Robot Interaction\n(HRI) that tailors interactions to different user groups and enables individual\nusers to modulate interactions through both minor and major interruptions. Our\nprimary contributions include the development of an adaptive, ROS-based HRI\nframework with an open-source code base. This framework supports natural\ninteractions through advanced speech recognition and voice activity detection,\nand leverages a large language model (LLM) as a dialogue bridge. We validate\nthe efficiency of our framework through module tests and system trials,\ndemonstrating its high accuracy in age recognition and its robustness to\nrepeated user inputs and plan changes."
                },
                "authors": [
                    {
                        "name": "Theresa Pekarek Rosin"
                    },
                    {
                        "name": "Vanessa Hassouna"
                    },
                    {
                        "name": "Xiaowen Sun"
                    },
                    {
                        "name": "Luca Krohm"
                    },
                    {
                        "name": "Henri-Leon Kordt"
                    },
                    {
                        "name": "Michael Beetz"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_doi": "10.1007/978-981-96-3525-2_3",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-981-96-3525-2_3",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.11377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the Proceedings of the 16th International Conference on\n  Social Robotics (ICSR) 2024",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03017v2",
                "updated": "2025-04-03T07:58:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    7,
                    58,
                    27,
                    3,
                    93,
                    0
                ],
                "published": "2024-12-04T04:07:49Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    7,
                    49,
                    2,
                    339,
                    0
                ],
                "title": "Pixel-level and Semantic-level Adjustable Super-resolution: A Dual-LoRA\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pixel-level and Semantic-level Adjustable Super-resolution: A Dual-LoRA\n  Approach"
                },
                "summary": "Diffusion prior-based methods have shown impressive results in real-world\nimage super-resolution (SR). However, most existing methods entangle\npixel-level and semantic-level SR objectives in the training process,\nstruggling to balance pixel-wise fidelity and perceptual quality. Meanwhile,\nusers have varying preferences on SR results, thus it is demanded to develop an\nadjustable SR model that can be tailored to different fidelity-perception\npreferences during inference without re-training. We present Pixel-level and\nSemantic-level Adjustable SR (PiSA-SR), which learns two LoRA modules upon the\npre-trained stable-diffusion (SD) model to achieve improved and adjustable SR\nresults. We first formulate the SD-based SR problem as learning the residual\nbetween the low-quality input and the high-quality output, then show that the\nlearning objective can be decoupled into two distinct LoRA weight spaces: one\nis characterized by the $\\ell_2$-loss for pixel-level regression, and another\nis characterized by the LPIPS and classifier score distillation losses to\nextract semantic information from pre-trained classification and SD models. In\nits default setting, PiSA-SR can be performed in a single diffusion step,\nachieving leading real-world SR results in both quality and efficiency. By\nintroducing two adjustable guidance scales on the two LoRA modules to control\nthe strengths of pixel-wise fidelity and semantic-level details during\ninference, PiSASR can offer flexible SR results according to user preference\nwithout re-training. Codes and models can be found at\nhttps://github.com/csslc/PiSA-SR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion prior-based methods have shown impressive results in real-world\nimage super-resolution (SR). However, most existing methods entangle\npixel-level and semantic-level SR objectives in the training process,\nstruggling to balance pixel-wise fidelity and perceptual quality. Meanwhile,\nusers have varying preferences on SR results, thus it is demanded to develop an\nadjustable SR model that can be tailored to different fidelity-perception\npreferences during inference without re-training. We present Pixel-level and\nSemantic-level Adjustable SR (PiSA-SR), which learns two LoRA modules upon the\npre-trained stable-diffusion (SD) model to achieve improved and adjustable SR\nresults. We first formulate the SD-based SR problem as learning the residual\nbetween the low-quality input and the high-quality output, then show that the\nlearning objective can be decoupled into two distinct LoRA weight spaces: one\nis characterized by the $\\ell_2$-loss for pixel-level regression, and another\nis characterized by the LPIPS and classifier score distillation losses to\nextract semantic information from pre-trained classification and SD models. In\nits default setting, PiSA-SR can be performed in a single diffusion step,\nachieving leading real-world SR results in both quality and efficiency. By\nintroducing two adjustable guidance scales on the two LoRA modules to control\nthe strengths of pixel-wise fidelity and semantic-level details during\ninference, PiSASR can offer flexible SR results according to user preference\nwithout re-training. Codes and models can be found at\nhttps://github.com/csslc/PiSA-SR."
                },
                "authors": [
                    {
                        "name": "Lingchen Sun"
                    },
                    {
                        "name": "Rongyuan Wu"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Shuaizheng Liu"
                    },
                    {
                        "name": "Qiaosi Yi"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05549v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05549v2",
                "updated": "2025-04-03T07:55:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    7,
                    55,
                    9,
                    3,
                    93,
                    0
                ],
                "published": "2024-11-08T13:12:17Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    12,
                    17,
                    4,
                    313,
                    0
                ],
                "title": "STEAK: Streaming Network for Continual Learning of Object Relocations\n  under Household Context Drifts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STEAK: Streaming Network for Continual Learning of Object Relocations\n  under Household Context Drifts"
                },
                "summary": "In real-world settings, robots are expected to assist humans across diverse\ntasks and still continuously adapt to dynamic changes over time. For example,\nin domestic environments, robots can proactively help users by fetching needed\nobjects based on learned routines, which they infer by observing how objects\nmove over time. However, data from these interactions are inherently\nnon-independent and non-identically distributed (non-i.i.d.), e.g., a robot\nassisting multiple users may encounter varying data distributions as\nindividuals follow distinct habits. This creates a challenge: integrating new\nknowledge without catastrophic forgetting. To address this, we propose STREAK\n(Spatio Temporal RElocation with Adaptive Knowledge retention), a continual\nlearning framework for real-world robotic learning. It leverages a streaming\ngraph neural network with regularization and rehearsal techniques to mitigate\ncontext drifts while retaining past knowledge. Our method is time- and\nmemory-efficient, enabling long-term learning without retraining on all past\ndata, which becomes infeasible as data grows in real-world interactions. We\nevaluate STREAK on the task of incrementally predicting human routines over 50+\ndays across different households. Results show that it effectively prevents\ncatastrophic forgetting while maintaining generalization, making it a scalable\nsolution for long-term human-robot interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world settings, robots are expected to assist humans across diverse\ntasks and still continuously adapt to dynamic changes over time. For example,\nin domestic environments, robots can proactively help users by fetching needed\nobjects based on learned routines, which they infer by observing how objects\nmove over time. However, data from these interactions are inherently\nnon-independent and non-identically distributed (non-i.i.d.), e.g., a robot\nassisting multiple users may encounter varying data distributions as\nindividuals follow distinct habits. This creates a challenge: integrating new\nknowledge without catastrophic forgetting. To address this, we propose STREAK\n(Spatio Temporal RElocation with Adaptive Knowledge retention), a continual\nlearning framework for real-world robotic learning. It leverages a streaming\ngraph neural network with regularization and rehearsal techniques to mitigate\ncontext drifts while retaining past knowledge. Our method is time- and\nmemory-efficient, enabling long-term learning without retraining on all past\ndata, which becomes infeasible as data grows in real-world interactions. We\nevaluate STREAK on the task of incrementally predicting human routines over 50+\ndays across different households. Results show that it effectively prevents\ncatastrophic forgetting while maintaining generalization, making it a scalable\nsolution for long-term human-robot interactions."
                },
                "authors": [
                    {
                        "name": "Ermanno Bartoli"
                    },
                    {
                        "name": "Fethiye Irmak Dogan"
                    },
                    {
                        "name": "Iolanda Leite"
                    }
                ],
                "author_detail": {
                    "name": "Iolanda Leite"
                },
                "author": "Iolanda Leite",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05549v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05549v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02357v1",
                "updated": "2025-04-03T07:45:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    7,
                    45,
                    9,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T07:45:09Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    7,
                    45,
                    9,
                    3,
                    93,
                    0
                ],
                "title": "ReuseDroid: A VLM-empowered Android UI Test Migrator Boosted by Active\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReuseDroid: A VLM-empowered Android UI Test Migrator Boosted by Active\n  Feedback"
                },
                "summary": "GUI testing is an essential quality assurance process in mobile app\ndevelopment. However, the creation and maintenance of GUI tests for mobile apps\nare resource-intensive and costly. Recognizing that many apps share similar\nfunctionalities, researchers have proposed various techniques to migrate GUI\ntests from one app to another with similar features. For example, some\ntechniques employ mapping-based approaches to align the GUI elements traversed\nby the tests of a source app to those present in the target app. Other test\nmigration techniques have also been proposed to leverage large language models\n(LLMs) by adapting the GUI tasks in source tests. However, these techniques are\nineffective in dealing with different operational logic between the source and\ntarget apps. The semantics of GUI elements may not be correctly inferred due to\nthe missing analysis of these flows. In this work, we propose REUSEDROID, a\nnovel multiagent framework for GUI test migration empowered by Large\nVision-Language Models (VLMs). REUSEDROID is powered by multiple VLM-based\nagents, each tackling a stage of the test migration process by leveraging the\nrelevant visual and textual information embedded in GUI pages. An insight of\nREUSEDROID is to migrate tests based only on the core logic shared across\nsimilar apps, while their entire operational logic could differ. We evaluate\nREUSEDROID on LinPro, a new test migration dataset that consists of 578\nmigration tasks for 39 popular apps across 4 categories. The experimental\nresult shows that REUSEDROID can successfully migrate 90.3% of the migration\ntasks, outperforming the best mapping-based and LLM-based baselines by 318.1%\nand 109.1%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI testing is an essential quality assurance process in mobile app\ndevelopment. However, the creation and maintenance of GUI tests for mobile apps\nare resource-intensive and costly. Recognizing that many apps share similar\nfunctionalities, researchers have proposed various techniques to migrate GUI\ntests from one app to another with similar features. For example, some\ntechniques employ mapping-based approaches to align the GUI elements traversed\nby the tests of a source app to those present in the target app. Other test\nmigration techniques have also been proposed to leverage large language models\n(LLMs) by adapting the GUI tasks in source tests. However, these techniques are\nineffective in dealing with different operational logic between the source and\ntarget apps. The semantics of GUI elements may not be correctly inferred due to\nthe missing analysis of these flows. In this work, we propose REUSEDROID, a\nnovel multiagent framework for GUI test migration empowered by Large\nVision-Language Models (VLMs). REUSEDROID is powered by multiple VLM-based\nagents, each tackling a stage of the test migration process by leveraging the\nrelevant visual and textual information embedded in GUI pages. An insight of\nREUSEDROID is to migrate tests based only on the core logic shared across\nsimilar apps, while their entire operational logic could differ. We evaluate\nREUSEDROID on LinPro, a new test migration dataset that consists of 578\nmigration tasks for 39 popular apps across 4 categories. The experimental\nresult shows that REUSEDROID can successfully migrate 90.3% of the migration\ntasks, outperforming the best mapping-based and LLM-based baselines by 318.1%\nand 109.1%, respectively."
                },
                "authors": [
                    {
                        "name": "Xiaolei Li"
                    },
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Yepang Liu"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    },
                    {
                        "name": "Hailong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hailong Wang"
                },
                "author": "Hailong Wang",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02351v1",
                "updated": "2025-04-03T07:38:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    7,
                    38,
                    9,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T07:38:09Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    7,
                    38,
                    9,
                    3,
                    93,
                    0
                ],
                "title": "Agglomerating Large Vision Encoders via Distillation for VFSS\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agglomerating Large Vision Encoders via Distillation for VFSS\n  Segmentation"
                },
                "summary": "The deployment of foundation models for medical imaging has demonstrated\nconsiderable success. However, their training overheads associated with\ndownstream tasks remain substantial due to the size of the image encoders\nemployed, and the inference complexity is also significantly high. Although\nlightweight variants have been obtained for these foundation models, their\nperformance is constrained by their limited model capacity and suboptimal\ntraining strategies. In order to achieve an improved tradeoff between\ncomplexity and performance, we propose a new framework to improve the\nperformance of low complexity models via knowledge distillation from multiple\nlarge medical foundation models (e.g., MedSAM, RAD-DINO, MedCLIP), each\nspecializing in different vision tasks, with the goal to effectively bridge the\nperformance gap for medical image segmentation tasks. The agglomerated model\ndemonstrates superior generalization across 12 segmentation tasks, whereas\nspecialized models require explicit training for each task. Our approach\nachieved an average performance gain of 2\\% in Dice coefficient compared to\nsimple distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of foundation models for medical imaging has demonstrated\nconsiderable success. However, their training overheads associated with\ndownstream tasks remain substantial due to the size of the image encoders\nemployed, and the inference complexity is also significantly high. Although\nlightweight variants have been obtained for these foundation models, their\nperformance is constrained by their limited model capacity and suboptimal\ntraining strategies. In order to achieve an improved tradeoff between\ncomplexity and performance, we propose a new framework to improve the\nperformance of low complexity models via knowledge distillation from multiple\nlarge medical foundation models (e.g., MedSAM, RAD-DINO, MedCLIP), each\nspecializing in different vision tasks, with the goal to effectively bridge the\nperformance gap for medical image segmentation tasks. The agglomerated model\ndemonstrates superior generalization across 12 segmentation tasks, whereas\nspecialized models require explicit training for each task. Our approach\nachieved an average performance gain of 2\\% in Dice coefficient compared to\nsimple distillation."
                },
                "authors": [
                    {
                        "name": "Chengxi Zeng"
                    },
                    {
                        "name": "Yuxuan Jiang"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Alberto Gambaruto"
                    },
                    {
                        "name": "Tilo Burghardt"
                    }
                ],
                "author_detail": {
                    "name": "Tilo Burghardt"
                },
                "author": "Tilo Burghardt",
                "arxiv_journal_ref": "IEEE / CVF Computer Vision and Pattern Recognition Conference\n  (CVPR) 2025, 2nd Efficient Large Vision Models Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02349v1",
                "updated": "2025-04-03T07:33:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    7,
                    33,
                    2,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T07:33:02Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    7,
                    33,
                    2,
                    3,
                    93,
                    0
                ],
                "title": "Large (Vision) Language Models are Unsupervised In-Context Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large (Vision) Language Models are Unsupervised In-Context Learners"
                },
                "summary": "Recent advances in large language and vision-language models have enabled\nzero-shot inference, allowing models to solve new tasks without task-specific\ntraining. Various adaptation techniques such as prompt engineering, In-Context\nLearning (ICL), and supervised fine-tuning can further enhance the model's\nperformance on a downstream task, but they require substantial manual effort to\nconstruct effective prompts or labeled examples. In this work, we introduce a\njoint inference framework for fully unsupervised adaptation, eliminating the\nneed for manual prompt engineering and labeled examples. Unlike zero-shot\ninference, which makes independent predictions, the joint inference makes\npredictions simultaneously for all inputs in a given task. Since direct joint\ninference involves computationally expensive optimization, we develop efficient\napproximation techniques, leading to two unsupervised adaptation methods:\nunsupervised fine-tuning and unsupervised ICL. We demonstrate the effectiveness\nof our methods across diverse tasks and models, including language-only\nLlama-3.1 on natural language processing tasks, reasoning-oriented Qwen2.5-Math\non grade school math problems, vision-language OpenFlamingo on vision tasks,\nand the API-only access GPT-4o model on massive multi-discipline tasks. Our\nexperiments demonstrate substantial improvements over the standard zero-shot\napproach, including 39% absolute improvement on the challenging GSM8K math\nreasoning dataset. Remarkably, despite being fully unsupervised, our framework\noften performs on par with supervised approaches that rely on ground truth\nlabels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language and vision-language models have enabled\nzero-shot inference, allowing models to solve new tasks without task-specific\ntraining. Various adaptation techniques such as prompt engineering, In-Context\nLearning (ICL), and supervised fine-tuning can further enhance the model's\nperformance on a downstream task, but they require substantial manual effort to\nconstruct effective prompts or labeled examples. In this work, we introduce a\njoint inference framework for fully unsupervised adaptation, eliminating the\nneed for manual prompt engineering and labeled examples. Unlike zero-shot\ninference, which makes independent predictions, the joint inference makes\npredictions simultaneously for all inputs in a given task. Since direct joint\ninference involves computationally expensive optimization, we develop efficient\napproximation techniques, leading to two unsupervised adaptation methods:\nunsupervised fine-tuning and unsupervised ICL. We demonstrate the effectiveness\nof our methods across diverse tasks and models, including language-only\nLlama-3.1 on natural language processing tasks, reasoning-oriented Qwen2.5-Math\non grade school math problems, vision-language OpenFlamingo on vision tasks,\nand the API-only access GPT-4o model on massive multi-discipline tasks. Our\nexperiments demonstrate substantial improvements over the standard zero-shot\napproach, including 39% absolute improvement on the challenging GSM8K math\nreasoning dataset. Remarkably, despite being fully unsupervised, our framework\noften performs on par with supervised approaches that rely on ground truth\nlabels."
                },
                "authors": [
                    {
                        "name": "Artyom Gadetsky"
                    },
                    {
                        "name": "Andrei Atanov"
                    },
                    {
                        "name": "Yulun Jiang"
                    },
                    {
                        "name": "Zhitong Gao"
                    },
                    {
                        "name": "Ghazal Hosseini Mighan"
                    },
                    {
                        "name": "Amir Zamir"
                    },
                    {
                        "name": "Maria Brbic"
                    }
                ],
                "author_detail": {
                    "name": "Maria Brbic"
                },
                "author": "Maria Brbic",
                "arxiv_comment": "ICLR 2025 camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.02821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02821v1",
                "updated": "2025-04-03T17:58:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    58,
                    35,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:58:35Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    58,
                    35,
                    3,
                    93,
                    0
                ],
                "title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language\n  Models"
                },
                "summary": "Sparse Autoencoders (SAEs) have recently been shown to enhance\ninterpretability and steerability in Large Language Models (LLMs). In this\nwork, we extend the application of SAEs to Vision-Language Models (VLMs), such\nas CLIP, and introduce a comprehensive framework for evaluating monosemanticity\nin vision representations. Our experimental results reveal that SAEs trained on\nVLMs significantly enhance the monosemanticity of individual neurons while also\nexhibiting hierarchical representations that align well with expert-defined\nstructures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that\napplying SAEs to intervene on a CLIP vision encoder, directly steer output from\nmultimodal LLMs (e.g., LLaVA) without any modifications to the underlying\nmodel. These findings emphasize the practicality and efficacy of SAEs as an\nunsupervised approach for enhancing both the interpretability and control of\nVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) have recently been shown to enhance\ninterpretability and steerability in Large Language Models (LLMs). In this\nwork, we extend the application of SAEs to Vision-Language Models (VLMs), such\nas CLIP, and introduce a comprehensive framework for evaluating monosemanticity\nin vision representations. Our experimental results reveal that SAEs trained on\nVLMs significantly enhance the monosemanticity of individual neurons while also\nexhibiting hierarchical representations that align well with expert-defined\nstructures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that\napplying SAEs to intervene on a CLIP vision encoder, directly steer output from\nmultimodal LLMs (e.g., LLaVA) without any modifications to the underlying\nmodel. These findings emphasize the practicality and efficacy of SAEs as an\nunsupervised approach for enhancing both the interpretability and control of\nVLMs."
                },
                "authors": [
                    {
                        "name": "Mateusz Pach"
                    },
                    {
                        "name": "Shyamgopal Karthik"
                    },
                    {
                        "name": "Quentin Bouniot"
                    },
                    {
                        "name": "Serge Belongie"
                    },
                    {
                        "name": "Zeynep Akata"
                    }
                ],
                "author_detail": {
                    "name": "Zeynep Akata"
                },
                "author": "Zeynep Akata",
                "arxiv_comment": "Preprint. The code is available at\n  https://github.com/ExplainableML/sae-for-vlm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02810v1",
                "updated": "2025-04-03T17:54:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    54,
                    18,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:54:18Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    54,
                    18,
                    3,
                    93,
                    0
                ],
                "title": "Generative Evaluation of Complex Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Evaluation of Complex Reasoning in Large Language Models"
                },
                "summary": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Haowei Lin"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Ruilin Yan"
                    },
                    {
                        "name": "Baizhou Huang"
                    },
                    {
                        "name": "Haotian Ye"
                    },
                    {
                        "name": "Jianhua Zhu"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Jianzhu Ma"
                    },
                    {
                        "name": "Yitao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Liang"
                },
                "author": "Yitao Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04188v2",
                "updated": "2025-04-03T17:53:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    53,
                    20,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-06T08:03:51Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    8,
                    3,
                    51,
                    3,
                    65,
                    0
                ],
                "title": "Measuring temporal effects of agent knowledge by date-controlled tool\n  use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring temporal effects of agent knowledge by date-controlled tool\n  use"
                },
                "summary": "Temporal progression is an integral part of knowledge accumulation and\nupdate. Web search is frequently adopted as grounding for agent knowledge, yet\nan improper configuration affects the quality of the agent's responses. Here,\nwe assess the agent behavior using distinct date-controlled tools (DCTs) as\nstress test to measure the knowledge variability of large language model (LLM)\nagents. We demonstrate the temporal effects of an LLM agent as a writing\nassistant, which uses web search to complete scientific publication abstracts.\nWe show that the temporality of search engine translates into tool-dependent\nagent performance but can be alleviated with base model choice and explicit\nreasoning instructions such as chain-of-thought prompting. Our results indicate\nthat agent design and evaluations should take a dynamical view and implement\nmeasures to account for the temporal influence of external resources to ensure\nreliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal progression is an integral part of knowledge accumulation and\nupdate. Web search is frequently adopted as grounding for agent knowledge, yet\nan improper configuration affects the quality of the agent's responses. Here,\nwe assess the agent behavior using distinct date-controlled tools (DCTs) as\nstress test to measure the knowledge variability of large language model (LLM)\nagents. We demonstrate the temporal effects of an LLM agent as a writing\nassistant, which uses web search to complete scientific publication abstracts.\nWe show that the temporality of search engine translates into tool-dependent\nagent performance but can be alleviated with base model choice and explicit\nreasoning instructions such as chain-of-thought prompting. Our results indicate\nthat agent design and evaluations should take a dynamical view and implement\nmeasures to account for the temporal influence of external resources to ensure\nreliability."
                },
                "authors": [
                    {
                        "name": "R. Patrick Xian"
                    },
                    {
                        "name": "Qiming Cui"
                    },
                    {
                        "name": "Stefan Bauer"
                    },
                    {
                        "name": "Reza Abbasi-Asl"
                    }
                ],
                "author_detail": {
                    "name": "Reza Abbasi-Asl"
                },
                "author": "Reza Abbasi-Asl",
                "arxiv_comment": "under review, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02807v1",
                "updated": "2025-04-03T17:52:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    52,
                    7,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:52:07Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    52,
                    7,
                    3,
                    93,
                    0
                ],
                "title": "MegaMath: Pushing the Limits of Open Math Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaMath: Pushing the Limits of Open Math Corpora"
                },
                "summary": "Mathematical reasoning is a cornerstone of human intelligence and a key\nbenchmark for advanced capabilities in large language models (LLMs). However,\nthe research community still lacks an open, large-scale, high-quality corpus\ntailored to the demands of math-centric LLM pre-training. We present MegaMath,\nan open dataset curated from diverse, math-focused sources through following\npractices: (1) Revisiting web data: We re-extracted mathematical documents from\nCommon Crawl with math-oriented HTML optimizations, fasttext-based filtering\nand deduplication, all for acquiring higher-quality data on the Internet. (2)\nRecalling Math-related code data: We identified high quality math-related code\nfrom large code training corpus, Stack-V2, further enhancing data diversity.\n(3) Exploring Synthetic data: We synthesized QA-style text, math-related code,\nand interleaved text-code blocks from web data or code data. By integrating\nthese strategies and validating their effectiveness through extensive\nablations, MegaMath delivers 371B tokens with the largest quantity and top\nquality among existing open math pre-training datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning is a cornerstone of human intelligence and a key\nbenchmark for advanced capabilities in large language models (LLMs). However,\nthe research community still lacks an open, large-scale, high-quality corpus\ntailored to the demands of math-centric LLM pre-training. We present MegaMath,\nan open dataset curated from diverse, math-focused sources through following\npractices: (1) Revisiting web data: We re-extracted mathematical documents from\nCommon Crawl with math-oriented HTML optimizations, fasttext-based filtering\nand deduplication, all for acquiring higher-quality data on the Internet. (2)\nRecalling Math-related code data: We identified high quality math-related code\nfrom large code training corpus, Stack-V2, further enhancing data diversity.\n(3) Exploring Synthetic data: We synthesized QA-style text, math-related code,\nand interleaved text-code blocks from web data or code data. By integrating\nthese strategies and validating their effectiveness through extensive\nablations, MegaMath delivers 371B tokens with the largest quantity and top\nquality among existing open math pre-training datasets."
                },
                "authors": [
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Zengzhi Wang"
                    },
                    {
                        "name": "Nikhil Ranjan"
                    },
                    {
                        "name": "Zhoujun Cheng"
                    },
                    {
                        "name": "Liping Tang"
                    },
                    {
                        "name": "Guowei He"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Eric P. Xing"
                    }
                ],
                "author_detail": {
                    "name": "Eric P. Xing"
                },
                "author": "Eric P. Xing",
                "arxiv_comment": "26 pages, 15 figures, 22 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02800v2",
                "updated": "2025-04-04T02:07:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    2,
                    7,
                    59,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-03T17:43:14Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    43,
                    14,
                    3,
                    93,
                    0
                ],
                "title": "A Survey of Large Language Models in Mental Health Disorder Detection on\n  Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Models in Mental Health Disorder Detection on\n  Social Media"
                },
                "summary": "The detection and intervention of mental health issues represent a critical\nglobal research focus, and social media data has been recognized as an\nimportant resource for mental health research. However, how to utilize Large\nLanguage Models (LLMs) for mental health problem detection on social media\nposes significant challenges. Hence, this paper aims to explore the potential\nof LLM applications in social media data analysis, focusing not only on the\nmost common psychological disorders such as depression and anxiety but also\nincorporating psychotic disorders and externalizing disorders, summarizing the\napplication methods of LLM from different dimensions, such as text data\nanalysis and detection of mental disorders, and revealing the major challenges\nand shortcomings of current research. In addition, the paper provides an\noverview of popular datasets, and evaluation metrics. The survey in this paper\nprovides a comprehensive frame of reference for researchers in the field of\nmental health, while demonstrating the great potential of LLMs in mental health\ndetection to facilitate the further application of LLMs in future mental health\ninterventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection and intervention of mental health issues represent a critical\nglobal research focus, and social media data has been recognized as an\nimportant resource for mental health research. However, how to utilize Large\nLanguage Models (LLMs) for mental health problem detection on social media\nposes significant challenges. Hence, this paper aims to explore the potential\nof LLM applications in social media data analysis, focusing not only on the\nmost common psychological disorders such as depression and anxiety but also\nincorporating psychotic disorders and externalizing disorders, summarizing the\napplication methods of LLM from different dimensions, such as text data\nanalysis and detection of mental disorders, and revealing the major challenges\nand shortcomings of current research. In addition, the paper provides an\noverview of popular datasets, and evaluation metrics. The survey in this paper\nprovides a comprehensive frame of reference for researchers in the field of\nmental health, while demonstrating the great potential of LLMs in mental health\ndetection to facilitate the further application of LLMs in future mental health\ninterventions."
                },
                "authors": [
                    {
                        "name": "Zhuohan Ge"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Shihao Qi"
                    },
                    {
                        "name": "Yuming Xu"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jason Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jason Zhang"
                },
                "author": "Jason Zhang",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.3; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12494v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12494v3",
                "updated": "2025-04-03T17:41:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    41,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2024-04-18T20:17:23Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    20,
                    17,
                    23,
                    3,
                    109,
                    0
                ],
                "title": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language\n  Models"
                },
                "summary": "Predictive models often need to work with incomplete information in\nreal-world tasks. Consequently, they must provide reliable probability or\nconfidence estimation, especially in large-scale decision-making and planning\ntasks. Current large language models (LLMs) are insufficient for accurate\nestimations, but they can generate relevant factors that may affect the\nprobabilities, produce coarse-grained probabilities when the information is\nmore complete, and help determine which factors are relevant to specific\ndownstream contexts. In this paper, we make use of these capabilities of LLMs\nto provide a significantly more accurate probabilistic estimation. We propose\nBIRD, a novel probabilistic inference framework that aligns a Bayesian network\nwith LLM abductions and then estimates more accurate probabilities in a\ndeduction step. We show BIRD provides reliable probability estimations that are\n30% better than those provided directly by LLM baselines. These estimates\nfurther contribute to better and more trustworthy decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive models often need to work with incomplete information in\nreal-world tasks. Consequently, they must provide reliable probability or\nconfidence estimation, especially in large-scale decision-making and planning\ntasks. Current large language models (LLMs) are insufficient for accurate\nestimations, but they can generate relevant factors that may affect the\nprobabilities, produce coarse-grained probabilities when the information is\nmore complete, and help determine which factors are relevant to specific\ndownstream contexts. In this paper, we make use of these capabilities of LLMs\nto provide a significantly more accurate probabilistic estimation. We propose\nBIRD, a novel probabilistic inference framework that aligns a Bayesian network\nwith LLM abductions and then estimates more accurate probabilities in a\ndeduction step. We show BIRD provides reliable probability estimations that are\n30% better than those provided directly by LLM baselines. These estimates\nfurther contribute to better and more trustworthy decision making."
                },
                "authors": [
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Ben Zhou"
                    },
                    {
                        "name": "Weidong Lin"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "arxiv_journal_ref": "ICLR 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12494v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12494v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18770v2",
                "updated": "2025-04-03T17:40:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    40,
                    58,
                    3,
                    93,
                    0
                ],
                "published": "2024-06-26T21:42:50Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    21,
                    42,
                    50,
                    2,
                    178,
                    0
                ],
                "title": "ADO-LLM: Analog Design Bayesian Optimization with In-Context Learning of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADO-LLM: Analog Design Bayesian Optimization with In-Context Learning of\n  Large Language Models"
                },
                "summary": "Analog circuit design requires substantial human expertise and involvement,\nwhich is a significant roadblock to design productivity. Bayesian Optimization\n(BO), a popular machine learning based optimization strategy, has been\nleveraged to automate analog design given its applicability across various\ncircuit topologies and technologies. Traditional BO methods employ black box\nGaussian Process surrogate models and optimized labeled data queries to find\noptimization solutions by trading off between exploration and exploitation.\nHowever, the search for the optimal design solution in BO can be expensive from\nboth a computational and data usage point of view, particularly for high\ndimensional optimization problems. This paper presents ADO-LLM, the first work\nintegrating large language models (LLMs) with Bayesian Optimization for analog\ndesign optimization. ADO-LLM leverages the LLM's ability to infuse domain\nknowledge to rapidly generate viable design points to remedy BO's inefficiency\nin finding high value design areas specifically under the limited design space\ncoverage of the BO's probabilistic surrogate model. In the meantime, sampling\nof design points evaluated in the iterative BO process provides quality\ndemonstrations for the LLM to generate high quality design points while\nleveraging infused broad design knowledge. Furthermore, the diversity brought\nby BO's exploration enriches the contextual understanding of the LLM and allows\nit to more broadly search in the design space and prevent repetitive and\nredundant suggestions. We evaluate the proposed framework on two different\ntypes of analog circuits and demonstrate notable improvements in design\nefficiency and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog circuit design requires substantial human expertise and involvement,\nwhich is a significant roadblock to design productivity. Bayesian Optimization\n(BO), a popular machine learning based optimization strategy, has been\nleveraged to automate analog design given its applicability across various\ncircuit topologies and technologies. Traditional BO methods employ black box\nGaussian Process surrogate models and optimized labeled data queries to find\noptimization solutions by trading off between exploration and exploitation.\nHowever, the search for the optimal design solution in BO can be expensive from\nboth a computational and data usage point of view, particularly for high\ndimensional optimization problems. This paper presents ADO-LLM, the first work\nintegrating large language models (LLMs) with Bayesian Optimization for analog\ndesign optimization. ADO-LLM leverages the LLM's ability to infuse domain\nknowledge to rapidly generate viable design points to remedy BO's inefficiency\nin finding high value design areas specifically under the limited design space\ncoverage of the BO's probabilistic surrogate model. In the meantime, sampling\nof design points evaluated in the iterative BO process provides quality\ndemonstrations for the LLM to generate high quality design points while\nleveraging infused broad design knowledge. Furthermore, the diversity brought\nby BO's exploration enriches the contextual understanding of the LLM and allows\nit to more broadly search in the design space and prevent repetitive and\nredundant suggestions. We evaluate the proposed framework on two different\ntypes of analog circuits and demonstrate notable improvements in design\nefficiency and effectiveness."
                },
                "authors": [
                    {
                        "name": "Yuxuan Yin"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Boxun Xu"
                    },
                    {
                        "name": "Peng Li"
                    }
                ],
                "author_detail": {
                    "name": "Peng Li"
                },
                "author": "Peng Li",
                "arxiv_doi": "10.1145/3676536.3676816",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676816",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.18770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 3 figures",
                "arxiv_journal_ref": "ICCAD: International Conference on Computer-Aided Design, 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02789v1",
                "updated": "2025-04-03T17:35:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    35,
                    54,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:35:54Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    35,
                    54,
                    3,
                    93,
                    0
                ],
                "title": "A Framework for Robust Cognitive Evaluation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Robust Cognitive Evaluation of LLMs"
                },
                "summary": "Emergent cognitive abilities in large language models (LLMs) have been widely\nobserved, but their nature and underlying mechanisms remain poorly understood.\nA growing body of research draws on cognitive science to investigate LLM\ncognition, but standard methodologies and experimen-tal pipelines have not yet\nbeen established. To address this gap we develop CognitivEval, a framework for\nsystematically evaluating the artificial cognitive capabilities of LLMs, with a\nparticular emphasis on robustness in response collection. The key features of\nCognitivEval include: (i) automatic prompt permutations, and (ii) testing that\ngathers both generations and model probability estimates. Our experiments\ndemonstrate that these features lead to more robust experimental outcomes.\nUsing CognitivEval, we replicate five classic experiments in cognitive science,\nillustrating the framework's generalizability across various experimental tasks\nand obtaining a cognitive profile of several state of the art LLMs.\nCognitivEval will be released publicly to foster broader collaboration within\nthe cognitive science community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent cognitive abilities in large language models (LLMs) have been widely\nobserved, but their nature and underlying mechanisms remain poorly understood.\nA growing body of research draws on cognitive science to investigate LLM\ncognition, but standard methodologies and experimen-tal pipelines have not yet\nbeen established. To address this gap we develop CognitivEval, a framework for\nsystematically evaluating the artificial cognitive capabilities of LLMs, with a\nparticular emphasis on robustness in response collection. The key features of\nCognitivEval include: (i) automatic prompt permutations, and (ii) testing that\ngathers both generations and model probability estimates. Our experiments\ndemonstrate that these features lead to more robust experimental outcomes.\nUsing CognitivEval, we replicate five classic experiments in cognitive science,\nillustrating the framework's generalizability across various experimental tasks\nand obtaining a cognitive profile of several state of the art LLMs.\nCognitivEval will be released publicly to foster broader collaboration within\nthe cognitive science community."
                },
                "authors": [
                    {
                        "name": "Karin de Langis"
                    },
                    {
                        "name": "Jong Inn Park"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Khanh Chi Le"
                    },
                    {
                        "name": "Andreas Schramm"
                    },
                    {
                        "name": "Michael C. Mensink"
                    },
                    {
                        "name": "Andrew Elfenbein"
                    },
                    {
                        "name": "Dongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyeop Kang"
                },
                "author": "Dongyeop Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16879v2",
                "updated": "2025-04-03T17:31:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    31,
                    57,
                    3,
                    93,
                    0
                ],
                "published": "2024-09-25T12:44:13Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    44,
                    13,
                    2,
                    269,
                    0
                ],
                "title": "GRACE: Generating Socially Appropriate Robot Actions Leveraging LLMs and\n  Human Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRACE: Generating Socially Appropriate Robot Actions Leveraging LLMs and\n  Human Explanations"
                },
                "summary": "When operating in human environments, robots need to handle complex tasks\nwhile both adhering to social norms and accommodating individual preferences.\nFor instance, based on common sense knowledge, a household robot can predict\nthat it should avoid vacuuming during a social gathering, but it may still be\nuncertain whether it should vacuum before or after having guests. In such\ncases, integrating common-sense knowledge with human preferences, often\nconveyed through human explanations, is fundamental yet a challenge for\nexisting systems. In this paper, we introduce GRACE, a novel approach\naddressing this while generating socially appropriate robot actions. GRACE\nleverages common sense knowledge from LLMs, and it integrates this knowledge\nwith human explanations through a generative network. The bidirectional\nstructure of GRACE enables robots to refine and enhance LLM predictions by\nutilizing human explanations and makes robots capable of generating such\nexplanations for human-specified actions. Our evaluations show that integrating\nhuman explanations boosts GRACE's performance, where it outperforms several\nbaselines and provides sensible explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When operating in human environments, robots need to handle complex tasks\nwhile both adhering to social norms and accommodating individual preferences.\nFor instance, based on common sense knowledge, a household robot can predict\nthat it should avoid vacuuming during a social gathering, but it may still be\nuncertain whether it should vacuum before or after having guests. In such\ncases, integrating common-sense knowledge with human preferences, often\nconveyed through human explanations, is fundamental yet a challenge for\nexisting systems. In this paper, we introduce GRACE, a novel approach\naddressing this while generating socially appropriate robot actions. GRACE\nleverages common sense knowledge from LLMs, and it integrates this knowledge\nwith human explanations through a generative network. The bidirectional\nstructure of GRACE enables robots to refine and enhance LLM predictions by\nutilizing human explanations and makes robots capable of generating such\nexplanations for human-specified actions. Our evaluations show that integrating\nhuman explanations boosts GRACE's performance, where it outperforms several\nbaselines and provides sensible explanations."
                },
                "authors": [
                    {
                        "name": "Fethiye Irmak Dogan"
                    },
                    {
                        "name": "Umut Ozyurt"
                    },
                    {
                        "name": "Gizem Cinar"
                    },
                    {
                        "name": "Hatice Gunes"
                    }
                ],
                "author_detail": {
                    "name": "Hatice Gunes"
                },
                "author": "Hatice Gunes",
                "arxiv_comment": "2025 IEEE International Conference on Robotics & Automation (ICRA),\n  Supplementary video: https://youtu.be/GTNCC1GkiQ4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01551v2",
                "updated": "2025-04-03T17:25:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    25,
                    41,
                    3,
                    93,
                    0
                ],
                "published": "2024-04-02T01:30:41Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    1,
                    30,
                    41,
                    1,
                    93,
                    0
                ],
                "title": "Safety-Aware Multi-Agent Learning for Dynamic Network Bridging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety-Aware Multi-Agent Learning for Dynamic Network Bridging"
                },
                "summary": "Addressing complex cooperative tasks in safety-critical environments poses\nsignificant challenges for multi-agent systems, especially under conditions of\npartial observability. We focus on a dynamic network bridging task, where\nagents must learn to maintain a communication path between two moving targets.\nTo ensure safety during training and deployment, we integrate a\ncontrol-theoretic safety filter that enforces collision avoidance through local\nsetpoint updates. We develop and evaluate multi-agent reinforcement learning\nsafety-informed message passing, showing that encoding safety filter\nactivations as edge-level features improves coordination. The results suggest\nthat local safety enforcement and decentralized learning can be effectively\ncombined in distributed multi-agent tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing complex cooperative tasks in safety-critical environments poses\nsignificant challenges for multi-agent systems, especially under conditions of\npartial observability. We focus on a dynamic network bridging task, where\nagents must learn to maintain a communication path between two moving targets.\nTo ensure safety during training and deployment, we integrate a\ncontrol-theoretic safety filter that enforces collision avoidance through local\nsetpoint updates. We develop and evaluate multi-agent reinforcement learning\nsafety-informed message passing, showing that encoding safety filter\nactivations as edge-level features improves coordination. The results suggest\nthat local safety enforcement and decentralized learning can be effectively\ncombined in distributed multi-agent tasks."
                },
                "authors": [
                    {
                        "name": "Raffaele Galliera"
                    },
                    {
                        "name": "Konstantinos Mitsopoulos"
                    },
                    {
                        "name": "Niranjan Suri"
                    },
                    {
                        "name": "Raffaele Romagnoli"
                    }
                ],
                "author_detail": {
                    "name": "Raffaele Romagnoli"
                },
                "author": "Raffaele Romagnoli",
                "arxiv_comment": "8 pages, 18 equations, 4 figures, 1 algorithm, and 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02780v1",
                "updated": "2025-04-03T17:20:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    20,
                    36,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:20:36Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    20,
                    36,
                    3,
                    93,
                    0
                ],
                "title": "From Consumption to Collaboration: Measuring Interaction Patterns to\n  Augment Human Cognition in Open-Ended Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Consumption to Collaboration: Measuring Interaction Patterns to\n  Augment Human Cognition in Open-Ended Tasks"
                },
                "summary": "The rise of Generative AI, and Large Language Models (LLMs) in particular, is\nfundamentally changing cognitive processes in knowledge work, raising critical\nquestions about their impact on human reasoning and problem-solving\ncapabilities. As these AI systems become increasingly integrated into\nworkflows, they offer unprecedented opportunities for augmenting human thinking\nwhile simultaneously risking cognitive erosion through passive consumption of\ngenerated answers. This tension is particularly pronounced in open-ended tasks,\nwhere effective solutions require deep contextualization and integration of\ndomain knowledge. Unlike structured tasks with established metrics, measuring\nthe quality of human-LLM interaction in such open-ended tasks poses significant\nchallenges due to the absence of ground truth and the iterative nature of\nsolution development. To address this, we present a framework that analyzes\ninteraction patterns along two dimensions: cognitive activity mode (exploration\nvs. exploitation) and cognitive engagement mode (constructive vs. detrimental).\nThis framework provides systematic measurements to evaluate when LLMs are\neffective tools for thought rather than substitutes for human cognition,\nadvancing theoretical understanding and practical guidance for developing AI\nsystems that protect and augment human cognitive capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Generative AI, and Large Language Models (LLMs) in particular, is\nfundamentally changing cognitive processes in knowledge work, raising critical\nquestions about their impact on human reasoning and problem-solving\ncapabilities. As these AI systems become increasingly integrated into\nworkflows, they offer unprecedented opportunities for augmenting human thinking\nwhile simultaneously risking cognitive erosion through passive consumption of\ngenerated answers. This tension is particularly pronounced in open-ended tasks,\nwhere effective solutions require deep contextualization and integration of\ndomain knowledge. Unlike structured tasks with established metrics, measuring\nthe quality of human-LLM interaction in such open-ended tasks poses significant\nchallenges due to the absence of ground truth and the iterative nature of\nsolution development. To address this, we present a framework that analyzes\ninteraction patterns along two dimensions: cognitive activity mode (exploration\nvs. exploitation) and cognitive engagement mode (constructive vs. detrimental).\nThis framework provides systematic measurements to evaluate when LLMs are\neffective tools for thought rather than substitutes for human cognition,\nadvancing theoretical understanding and practical guidance for developing AI\nsystems that protect and augment human cognitive capabilities."
                },
                "authors": [
                    {
                        "name": "Joshua Holstein"
                    },
                    {
                        "name": "Moritz Diener"
                    },
                    {
                        "name": "Philipp Spitzer"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Spitzer"
                },
                "author": "Philipp Spitzer",
                "arxiv_comment": "Accepted at Tools for Thought Workshop (CHI'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02779v1",
                "updated": "2025-04-03T17:19:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    19,
                    52,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:19:52Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    19,
                    52,
                    3,
                    93,
                    0
                ],
                "title": "BT-ACTION: A Test-Driven Approach for Modular Understanding of User\n  Instruction Leveraging Behaviour Trees and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BT-ACTION: A Test-Driven Approach for Modular Understanding of User\n  Instruction Leveraging Behaviour Trees and LLMs"
                },
                "summary": "Natural language instructions are often abstract and complex, requiring\nrobots to execute multiple subtasks even for seemingly simple queries. For\nexample, when a user asks a robot to prepare avocado toast, the task involves\nseveral sequential steps. Moreover, such instructions can be ambiguous or\ninfeasible for the robot or may exceed the robot's existing knowledge. While\nLarge Language Models (LLMs) offer strong language reasoning capabilities to\nhandle these challenges, effectively integrating them into robotic systems\nremains a key challenge. To address this, we propose BT-ACTION, a test-driven\napproach that combines the modular structure of Behavior Trees (BT) with LLMs\nto generate coherent sequences of robot actions for following complex user\ninstructions, specifically in the context of preparing recipes in a\nkitchen-assistance setting. We evaluated BT-ACTION in a comprehensive user\nstudy with 45 participants, comparing its performance to direct LLM prompting.\nResults demonstrate that the modular design of BT-ACTION helped the robot make\nfewer mistakes and increased user trust, and participants showed a significant\npreference for the robot leveraging BT-ACTION. The code is publicly available\nat https://github.com/1Eggbert7/BT_LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language instructions are often abstract and complex, requiring\nrobots to execute multiple subtasks even for seemingly simple queries. For\nexample, when a user asks a robot to prepare avocado toast, the task involves\nseveral sequential steps. Moreover, such instructions can be ambiguous or\ninfeasible for the robot or may exceed the robot's existing knowledge. While\nLarge Language Models (LLMs) offer strong language reasoning capabilities to\nhandle these challenges, effectively integrating them into robotic systems\nremains a key challenge. To address this, we propose BT-ACTION, a test-driven\napproach that combines the modular structure of Behavior Trees (BT) with LLMs\nto generate coherent sequences of robot actions for following complex user\ninstructions, specifically in the context of preparing recipes in a\nkitchen-assistance setting. We evaluated BT-ACTION in a comprehensive user\nstudy with 45 participants, comparing its performance to direct LLM prompting.\nResults demonstrate that the modular design of BT-ACTION helped the robot make\nfewer mistakes and increased user trust, and participants showed a significant\npreference for the robot leveraging BT-ACTION. The code is publicly available\nat https://github.com/1Eggbert7/BT_LLM."
                },
                "authors": [
                    {
                        "name": "Alexander Leszczynski"
                    },
                    {
                        "name": "Sarah Gillet"
                    },
                    {
                        "name": "Iolanda Leite"
                    },
                    {
                        "name": "Fethiye Irmak Dogan"
                    }
                ],
                "author_detail": {
                    "name": "Fethiye Irmak Dogan"
                },
                "author": "Fethiye Irmak Dogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02768v1",
                "updated": "2025-04-03T17:05:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    5,
                    50,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:05:50Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    5,
                    50,
                    3,
                    93,
                    0
                ],
                "title": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal\n  Pairs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal\n  Pairs"
                },
                "summary": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic\nminimal pairs, covering 101 languages, 6 linguistic phenomena and containing\nmore than 125,000 minimal pairs. Our minimal pairs are created using a fully\nautomated pipeline, leveraging the large-scale linguistic resources of\nUniversal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs\nat an unprecedented multilingual scale, and highlights the shortcomings of the\ncurrent state-of-the-art in modelling low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic\nminimal pairs, covering 101 languages, 6 linguistic phenomena and containing\nmore than 125,000 minimal pairs. Our minimal pairs are created using a fully\nautomated pipeline, leveraging the large-scale linguistic resources of\nUniversal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs\nat an unprecedented multilingual scale, and highlights the shortcomings of the\ncurrent state-of-the-art in modelling low-resource languages."
                },
                "authors": [
                    {
                        "name": "Jaap Jumelet"
                    },
                    {
                        "name": "Leonie Weissweiler"
                    },
                    {
                        "name": "Arianna Bisazza"
                    }
                ],
                "author_detail": {
                    "name": "Arianna Bisazza"
                },
                "author": "Arianna Bisazza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02767v1",
                "updated": "2025-04-03T17:04:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    4,
                    56,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:04:56Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    4,
                    56,
                    3,
                    93,
                    0
                ],
                "title": "How Deep Do Large Language Models Internalize Scientific Literature and\n  Citation Practices?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Deep Do Large Language Models Internalize Scientific Literature and\n  Citation Practices?"
                },
                "summary": "The spread of scientific knowledge depends on how researchers discover and\ncite previous work. The adoption of large language models (LLMs) in the\nscientific research process introduces a new layer to these citation practices.\nHowever, it remains unclear to what extent LLMs align with human citation\npractices, how they perform across domains, and may influence citation\ndynamics. Here, we show that LLMs systematically reinforce the Matthew effect\nin citations by consistently favoring highly cited papers when generating\nreferences. This pattern persists across scientific domains despite significant\nfield-specific variations in existence rates, which refer to the proportion of\ngenerated references that match existing records in external bibliometric\ndatabases. Analyzing 274,951 references generated by GPT-4o for 10,000 papers,\nwe find that LLM recommendations diverge from traditional citation patterns by\npreferring more recent references with shorter titles and fewer authors.\nEmphasizing their content-level relevance, the generated references are\nsemantically aligned with the content of each paper at levels comparable to the\nground truth references and display similar network effects while reducing\nauthor self-citations. These findings illustrate how LLMs may reshape citation\npractices and influence the trajectory of scientific discovery by reflecting\nand amplifying established trends. As LLMs become more integrated into the\nscientific research process, it is important to understand their role in\nshaping how scientific communities discover and build upon prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spread of scientific knowledge depends on how researchers discover and\ncite previous work. The adoption of large language models (LLMs) in the\nscientific research process introduces a new layer to these citation practices.\nHowever, it remains unclear to what extent LLMs align with human citation\npractices, how they perform across domains, and may influence citation\ndynamics. Here, we show that LLMs systematically reinforce the Matthew effect\nin citations by consistently favoring highly cited papers when generating\nreferences. This pattern persists across scientific domains despite significant\nfield-specific variations in existence rates, which refer to the proportion of\ngenerated references that match existing records in external bibliometric\ndatabases. Analyzing 274,951 references generated by GPT-4o for 10,000 papers,\nwe find that LLM recommendations diverge from traditional citation patterns by\npreferring more recent references with shorter titles and fewer authors.\nEmphasizing their content-level relevance, the generated references are\nsemantically aligned with the content of each paper at levels comparable to the\nground truth references and display similar network effects while reducing\nauthor self-citations. These findings illustrate how LLMs may reshape citation\npractices and influence the trajectory of scientific discovery by reflecting\nand amplifying established trends. As LLMs become more integrated into the\nscientific research process, it is important to understand their role in\nshaping how scientific communities discover and build upon prior work."
                },
                "authors": [
                    {
                        "name": "Andres Algaba"
                    },
                    {
                        "name": "Vincent Holst"
                    },
                    {
                        "name": "Floriano Tori"
                    },
                    {
                        "name": "Melika Mobini"
                    },
                    {
                        "name": "Brecht Verbeken"
                    },
                    {
                        "name": "Sylvia Wenmackers"
                    },
                    {
                        "name": "Vincent Ginis"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Ginis"
                },
                "author": "Vincent Ginis",
                "arxiv_comment": "32 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00164v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00164v2",
                "updated": "2025-04-03T16:54:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    54,
                    12,
                    3,
                    93,
                    0
                ],
                "published": "2024-12-30T22:15:57Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    22,
                    15,
                    57,
                    0,
                    365,
                    0
                ],
                "title": "Measuring Large Language Models Capacity to Annotate Journalistic\n  Sourcing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Large Language Models Capacity to Annotate Journalistic\n  Sourcing"
                },
                "summary": "Since the launch of ChatGPT in late 2022, the capacities of Large Language\nModels and their evaluation have been in constant discussion and evaluation\nboth in academic research and in the industry. Scenarios and benchmarks have\nbeen developed in several areas such as law, medicine and math (Bommasani et\nal., 2023) and there is continuous evaluation of model variants. One area that\nhas not received sufficient scenario development attention is journalism, and\nin particular journalistic sourcing and ethics. Journalism is a crucial\ntruth-determination function in democracy (Vincent, 2023), and sourcing is a\ncrucial pillar to all original journalistic output. Evaluating the capacities\nof LLMs to annotate stories for the different signals of sourcing and how\nreporters justify them is a crucial scenario that warrants a benchmark\napproach. It offers potential to build automated systems to contrast more\ntransparent and ethically rigorous forms of journalism with everyday fare. In\nthis paper we lay out a scenario to evaluate LLM performance on identifying and\nannotating sourcing in news stories on a five-category schema inspired from\njournalism studies (Gans, 2004). We offer the use case, our dataset and metrics\nand as the first step towards systematic benchmarking. Our accuracy findings\nindicate LLM-based approaches have more catching to do in identifying all the\nsourced statements in a story, and equally, in matching the type of sources. An\neven harder task is spotting source justifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the launch of ChatGPT in late 2022, the capacities of Large Language\nModels and their evaluation have been in constant discussion and evaluation\nboth in academic research and in the industry. Scenarios and benchmarks have\nbeen developed in several areas such as law, medicine and math (Bommasani et\nal., 2023) and there is continuous evaluation of model variants. One area that\nhas not received sufficient scenario development attention is journalism, and\nin particular journalistic sourcing and ethics. Journalism is a crucial\ntruth-determination function in democracy (Vincent, 2023), and sourcing is a\ncrucial pillar to all original journalistic output. Evaluating the capacities\nof LLMs to annotate stories for the different signals of sourcing and how\nreporters justify them is a crucial scenario that warrants a benchmark\napproach. It offers potential to build automated systems to contrast more\ntransparent and ethically rigorous forms of journalism with everyday fare. In\nthis paper we lay out a scenario to evaluate LLM performance on identifying and\nannotating sourcing in news stories on a five-category schema inspired from\njournalism studies (Gans, 2004). We offer the use case, our dataset and metrics\nand as the first step towards systematic benchmarking. Our accuracy findings\nindicate LLM-based approaches have more catching to do in identifying all the\nsourced statements in a story, and equally, in matching the type of sources. An\neven harder task is spotting source justifications."
                },
                "authors": [
                    {
                        "name": "Subramaniam Vincent"
                    },
                    {
                        "name": "Phoebe Wang"
                    },
                    {
                        "name": "Zhan Shi"
                    },
                    {
                        "name": "Sahas Koka"
                    },
                    {
                        "name": "Yi Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Fang"
                },
                "author": "Yi Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00164v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00164v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02733v1",
                "updated": "2025-04-03T16:17:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    17,
                    56,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T16:17:56Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    17,
                    56,
                    3,
                    93,
                    0
                ],
                "title": "Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study"
                },
                "summary": "Large Language Models (LLMs) are highly vulnerable to input perturbations, as\neven a small prompt change may result in a substantially different output.\nExisting methods to enhance LLM robustness are primarily focused on perturbed\ndata samples, whereas improving resiliency to perturbations of task-level\ninstructions has remained relatively underexplored. In this work, we focus on\ncharacter- and word-level edits of task-specific instructions, which\nsubstantially degrade downstream performance. We experiment with a variety of\ntechniques to enhance the robustness of LLMs, including self-denoising and\nrepresentation alignment, testing different models (Llama 3 and Flan-T5),\ndatasets (CoLA, QNLI, SST-2) and instructions (both task-oriented and\nrole-oriented). We find that, on average, self-denoising -- whether performed\nby a frozen LLM or a fine-tuned model -- achieves substantially higher\nperformance gains than alternative strategies, including more complex baselines\nsuch as ensembling and supervised methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are highly vulnerable to input perturbations, as\neven a small prompt change may result in a substantially different output.\nExisting methods to enhance LLM robustness are primarily focused on perturbed\ndata samples, whereas improving resiliency to perturbations of task-level\ninstructions has remained relatively underexplored. In this work, we focus on\ncharacter- and word-level edits of task-specific instructions, which\nsubstantially degrade downstream performance. We experiment with a variety of\ntechniques to enhance the robustness of LLMs, including self-denoising and\nrepresentation alignment, testing different models (Llama 3 and Flan-T5),\ndatasets (CoLA, QNLI, SST-2) and instructions (both task-oriented and\nrole-oriented). We find that, on average, self-denoising -- whether performed\nby a frozen LLM or a fine-tuned model -- achieves substantially higher\nperformance gains than alternative strategies, including more complex baselines\nsuch as ensembling and supervised methods."
                },
                "authors": [
                    {
                        "name": "Aryan Agrawal"
                    },
                    {
                        "name": "Lisa Alazraki"
                    },
                    {
                        "name": "Shahin Honarvar"
                    },
                    {
                        "name": "Marek Rei"
                    }
                ],
                "author_detail": {
                    "name": "Marek Rei"
                },
                "author": "Marek Rei",
                "arxiv_comment": "Building Trust Workshop, ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02732v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02732v2",
                "updated": "2025-04-04T07:41:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    7,
                    41,
                    19,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-03T16:17:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    17,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Why do LLMs attend to the first token?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why do LLMs attend to the first token?"
                },
                "summary": "Large Language Models (LLMs) tend to attend heavily to the first token in the\nsequence -- creating a so-called attention sink. Many works have studied this\nphenomenon in detail, proposing various ways to either leverage or alleviate\nit. Attention sinks have been connected to quantisation difficulties, security\nissues, and streaming attention. Yet, while many works have provided conditions\nin which they occur or not, a critical question remains shallowly answered: Why\ndo LLMs learn such patterns and how are they being used? In this work, we argue\ntheoretically and empirically that this mechanism provides a method for LLMs to\navoid over-mixing, connecting this to existing lines of work that study\nmathematically how information propagates in Transformers. We conduct\nexperiments to validate our theoretical intuitions and show how choices such as\ncontext length, depth, and data packing influence the sink behaviour. We hope\nthat this study provides a new practical perspective on why attention sinks are\nuseful in LLMs, leading to a better understanding of the attention patterns\nthat form during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) tend to attend heavily to the first token in the\nsequence -- creating a so-called attention sink. Many works have studied this\nphenomenon in detail, proposing various ways to either leverage or alleviate\nit. Attention sinks have been connected to quantisation difficulties, security\nissues, and streaming attention. Yet, while many works have provided conditions\nin which they occur or not, a critical question remains shallowly answered: Why\ndo LLMs learn such patterns and how are they being used? In this work, we argue\ntheoretically and empirically that this mechanism provides a method for LLMs to\navoid over-mixing, connecting this to existing lines of work that study\nmathematically how information propagates in Transformers. We conduct\nexperiments to validate our theoretical intuitions and show how choices such as\ncontext length, depth, and data packing influence the sink behaviour. We hope\nthat this study provides a new practical perspective on why attention sinks are\nuseful in LLMs, leading to a better understanding of the attention patterns\nthat form during training."
                },
                "authors": [
                    {
                        "name": "Federico Barbero"
                    },
                    {
                        "name": "Álvaro Arroyo"
                    },
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Christos Perivolaropoulos"
                    },
                    {
                        "name": "Michael Bronstein"
                    },
                    {
                        "name": "Petar Veličković"
                    },
                    {
                        "name": "Razvan Pascanu"
                    }
                ],
                "author_detail": {
                    "name": "Razvan Pascanu"
                },
                "author": "Razvan Pascanu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02732v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02732v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02725v1",
                "updated": "2025-04-03T16:07:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    7,
                    38,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T16:07:38Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    7,
                    38,
                    3,
                    93,
                    0
                ],
                "title": "ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference\n  Optimization"
                },
                "summary": "Recent advancements in large language models (LLMs) have accelerated progress\ntoward artificial general intelligence, yet their potential to generate harmful\ncontent poses critical safety challenges. Existing alignment methods often\nstruggle to cover diverse safety scenarios and remain vulnerable to adversarial\nattacks. In this work, we propose Ex-Ante Reasoning Preference Optimization\n(ERPO), a novel safety alignment framework that equips LLMs with explicit\npreemptive reasoning through Chain-of-Thought and provides clear evidence for\nsafety judgments by embedding predefined safety rules. Specifically, our\napproach consists of three stages: first, equipping the model with Ex-Ante\nreasoning through supervised fine-tuning (SFT) using a constructed reasoning\nmodule; second, enhancing safety, usefulness, and efficiency via Direct\nPreference Optimization (DPO); and third, mitigating inference latency with a\nlength-controlled iterative preference optimization strategy. Experiments on\nmultiple open-source LLMs demonstrate that ERPO significantly enhances safety\nperformance while maintaining response efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have accelerated progress\ntoward artificial general intelligence, yet their potential to generate harmful\ncontent poses critical safety challenges. Existing alignment methods often\nstruggle to cover diverse safety scenarios and remain vulnerable to adversarial\nattacks. In this work, we propose Ex-Ante Reasoning Preference Optimization\n(ERPO), a novel safety alignment framework that equips LLMs with explicit\npreemptive reasoning through Chain-of-Thought and provides clear evidence for\nsafety judgments by embedding predefined safety rules. Specifically, our\napproach consists of three stages: first, equipping the model with Ex-Ante\nreasoning through supervised fine-tuning (SFT) using a constructed reasoning\nmodule; second, enhancing safety, usefulness, and efficiency via Direct\nPreference Optimization (DPO); and third, mitigating inference latency with a\nlength-controlled iterative preference optimization strategy. Experiments on\nmultiple open-source LLMs demonstrate that ERPO significantly enhances safety\nperformance while maintaining response efficiency."
                },
                "authors": [
                    {
                        "name": "Kehua Feng"
                    },
                    {
                        "name": "Keyan Ding"
                    },
                    {
                        "name": "Jing Yu"
                    },
                    {
                        "name": "Menghan Li"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Xinda Wang"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "18 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02719v1",
                "updated": "2025-04-03T16:02:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    2,
                    46,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T16:02:46Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    2,
                    46,
                    3,
                    93,
                    0
                ],
                "title": "The Myth of Immutability: A Multivocal Review on Smart Contract\n  Upgradeability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Myth of Immutability: A Multivocal Review on Smart Contract\n  Upgradeability"
                },
                "summary": "The immutability of smart contracts on blockchain platforms like Ethereum\npromotes security and trustworthiness but presents challenges for updates, bug\nfixes, or adding new features post-deployment. These limitations can lead to\nvulnerabilities and outdated functionality, impeding the evolution and\nmaintenance of decentralized applications. Despite various upgrade mechanisms\nproposed in academic research and industry, a comprehensive analysis of their\ntrade-offs and practical implications is lacking. This study aims to\nsystematically identify, classify, and evaluate existing smart contract upgrade\nmechanisms, bridging the gap between theoretical concepts and practical\nimplementations. It introduces standardized terminology and evaluates the\ntrade-offs of different approaches using software quality attributes. We\nconducted a Multivocal Literature Review (MLR) to analyze upgrade mechanisms\nfrom both academic research and industry practice. We first establish a unified\ndefinition of smart contract upgradeability and identify core components\nessential for understanding the upgrade process. Based on this definition, we\nclassify existing methods into full upgrade and partial upgrade approaches,\nintroducing standardized terminology to harmonize the diverse terms used in the\nliterature. We then characterize each approach and assess its benefits and\nlimitations using software quality attributes such as complexity, flexibility,\nsecurity, and usability. The analysis highlights significant trade-offs among\nupgrade mechanisms, providing valuable insights into the benefits and\nlimitations of each approach. These findings guide developers and researchers\nin selecting mechanisms tailored to specific project requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The immutability of smart contracts on blockchain platforms like Ethereum\npromotes security and trustworthiness but presents challenges for updates, bug\nfixes, or adding new features post-deployment. These limitations can lead to\nvulnerabilities and outdated functionality, impeding the evolution and\nmaintenance of decentralized applications. Despite various upgrade mechanisms\nproposed in academic research and industry, a comprehensive analysis of their\ntrade-offs and practical implications is lacking. This study aims to\nsystematically identify, classify, and evaluate existing smart contract upgrade\nmechanisms, bridging the gap between theoretical concepts and practical\nimplementations. It introduces standardized terminology and evaluates the\ntrade-offs of different approaches using software quality attributes. We\nconducted a Multivocal Literature Review (MLR) to analyze upgrade mechanisms\nfrom both academic research and industry practice. We first establish a unified\ndefinition of smart contract upgradeability and identify core components\nessential for understanding the upgrade process. Based on this definition, we\nclassify existing methods into full upgrade and partial upgrade approaches,\nintroducing standardized terminology to harmonize the diverse terms used in the\nliterature. We then characterize each approach and assess its benefits and\nlimitations using software quality attributes such as complexity, flexibility,\nsecurity, and usability. The analysis highlights significant trade-offs among\nupgrade mechanisms, providing valuable insights into the benefits and\nlimitations of each approach. These findings guide developers and researchers\nin selecting mechanisms tailored to specific project requirements."
                },
                "authors": [
                    {
                        "name": "Ilham Qasse"
                    },
                    {
                        "name": "Isra M. Ali"
                    },
                    {
                        "name": "Nafisa Ahmed"
                    },
                    {
                        "name": "Mohammad Hamdaqa"
                    },
                    {
                        "name": "Björn Þór Jónsson"
                    }
                ],
                "author_detail": {
                    "name": "Björn Þór Jónsson"
                },
                "author": "Björn Þór Jónsson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02712v1",
                "updated": "2025-04-03T15:52:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    52,
                    20,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T15:52:20Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    52,
                    20,
                    3,
                    93,
                    0
                ],
                "title": "TeleMoM: Consensus-Driven Telecom Intelligence via Mixture of Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeleMoM: Consensus-Driven Telecom Intelligence via Mixture of Models"
                },
                "summary": "Large language models (LLMs) face significant challenges in specialized\ndomains like telecommunication (Telecom) due to technical complexity,\nspecialized terminology, and rapidly evolving knowledge. Traditional methods,\nsuch as scaling model parameters or retraining on domain-specific corpora, are\ncomputationally expensive and yield diminishing returns, while existing\napproaches like retrieval-augmented generation, mixture of experts, and\nfine-tuning struggle with accuracy, efficiency, and coordination. To address\nthis issue, we propose Telecom mixture of models (TeleMoM), a consensus-driven\nensemble framework that integrates multiple LLMs for enhanced decision-making\nin Telecom. TeleMoM employs a two-stage process: proponent models generate\njustified responses, and an adjudicator finalizes decisions, supported by a\nquality-checking mechanism. This approach leverages strengths of diverse models\nto improve accuracy, reduce biases, and handle domain-specific complexities\neffectively. Evaluation results demonstrate that TeleMoM achieves a 9.7\\%\nincrease in answer accuracy, highlighting its effectiveness in Telecom\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face significant challenges in specialized\ndomains like telecommunication (Telecom) due to technical complexity,\nspecialized terminology, and rapidly evolving knowledge. Traditional methods,\nsuch as scaling model parameters or retraining on domain-specific corpora, are\ncomputationally expensive and yield diminishing returns, while existing\napproaches like retrieval-augmented generation, mixture of experts, and\nfine-tuning struggle with accuracy, efficiency, and coordination. To address\nthis issue, we propose Telecom mixture of models (TeleMoM), a consensus-driven\nensemble framework that integrates multiple LLMs for enhanced decision-making\nin Telecom. TeleMoM employs a two-stage process: proponent models generate\njustified responses, and an adjudicator finalizes decisions, supported by a\nquality-checking mechanism. This approach leverages strengths of diverse models\nto improve accuracy, reduce biases, and handle domain-specific complexities\neffectively. Evaluation results demonstrate that TeleMoM achieves a 9.7\\%\nincrease in answer accuracy, highlighting its effectiveness in Telecom\napplications."
                },
                "authors": [
                    {
                        "name": "Xinquan Wang"
                    },
                    {
                        "name": "Fenghao Zhu"
                    },
                    {
                        "name": "Chongwen Huang"
                    },
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Sami Muhaidat"
                    },
                    {
                        "name": "Chau Yuen"
                    },
                    {
                        "name": "Mérouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Mérouane Debbah"
                },
                "author": "Mérouane Debbah",
                "arxiv_comment": "6 pages; submitted to 2025 IEEE VTC Fall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02708v1",
                "updated": "2025-04-03T15:46:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    46,
                    46,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T15:46:46Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    46,
                    46,
                    3,
                    93,
                    0
                ],
                "title": "The Hidden Space of Safety: Understanding Preference-Tuned LLMs in\n  Multilingual context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Space of Safety: Understanding Preference-Tuned LLMs in\n  Multilingual context"
                },
                "summary": "Alignment tuning has enabled large language models to excel in reasoning,\ninstruction-following, and minimizing harmful generations. However, despite\ntheir widespread deployment, these models exhibit a monolingual bias, raising\nconcerns about the effectiveness of alignment across languages. Current\nalignment methods predominantly focus on English, leaving it unclear how\nalignment mechanism generalize to multilingual settings. To address this, we\nconduct a systematic analysis of distributional shifts in the embedding space\nof LLMs before and after alignment, uncovering its impact on model behavior\nacross diverse languages. We leverage the alignment-induced separation in\nsafety space as a quantitative tool to measure how alignment enforces safety\nconstraints. Our study evaluates seven LLMs using balanced toxicity datasets\nand parallel text-detoxification benchmarks, revealing substantial disparities\nin the latent representation space between high-resource and low-resource\nlanguages. These findings underscore the need for language-specific fine-tuning\nto ensure fair, reliable and robust multilingual alignment. Our insights\nprovide a foundation for developing truly safe multilingual LLMs, emphasizing\nthe urgency of addressing alignment gaps in underrepresented languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment tuning has enabled large language models to excel in reasoning,\ninstruction-following, and minimizing harmful generations. However, despite\ntheir widespread deployment, these models exhibit a monolingual bias, raising\nconcerns about the effectiveness of alignment across languages. Current\nalignment methods predominantly focus on English, leaving it unclear how\nalignment mechanism generalize to multilingual settings. To address this, we\nconduct a systematic analysis of distributional shifts in the embedding space\nof LLMs before and after alignment, uncovering its impact on model behavior\nacross diverse languages. We leverage the alignment-induced separation in\nsafety space as a quantitative tool to measure how alignment enforces safety\nconstraints. Our study evaluates seven LLMs using balanced toxicity datasets\nand parallel text-detoxification benchmarks, revealing substantial disparities\nin the latent representation space between high-resource and low-resource\nlanguages. These findings underscore the need for language-specific fine-tuning\nto ensure fair, reliable and robust multilingual alignment. Our insights\nprovide a foundation for developing truly safe multilingual LLMs, emphasizing\nthe urgency of addressing alignment gaps in underrepresented languages."
                },
                "authors": [
                    {
                        "name": "Nikhil Verma"
                    },
                    {
                        "name": "Manasa Bharadwaj"
                    }
                ],
                "author_detail": {
                    "name": "Manasa Bharadwaj"
                },
                "author": "Manasa Bharadwaj",
                "arxiv_comment": "14 pages, 11 Figures, 2 Tables, currently under review at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02671v1",
                "updated": "2025-04-03T15:13:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    13,
                    36,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T15:13:36Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    13,
                    36,
                    3,
                    93,
                    0
                ],
                "title": "LLM for Complex Reasoning Task: An Exploratory Study in Fermi Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM for Complex Reasoning Task: An Exploratory Study in Fermi Problems"
                },
                "summary": "Fermi Problems (FPs) are mathematical reasoning tasks that require human-like\nlogic and numerical reasoning. Unlike other reasoning questions, FPs often\ninvolve real-world impracticalities or ambiguous concepts, making them\nchallenging even for humans to solve. Despite advancements in AI, particularly\nwith large language models (LLMs) in various reasoning tasks, FPs remain\nrelatively under-explored. This work conducted an exploratory study to examine\nthe capabilities and limitations of LLMs in solving FPs. We first evaluated the\noverall performance of three advanced LLMs using a publicly available FP\ndataset. We designed prompts according to the recently proposed TELeR taxonomy,\nincluding a zero-shot scenario. Results indicated that all three LLMs achieved\na fp_score (range between 0 - 1) below 0.5, underscoring the inherent\ndifficulty of these reasoning tasks. To further investigate, we categorized FPs\ninto standard and specific questions, hypothesizing that LLMs would perform\nbetter on standard questions, which are characterized by clarity and\nconciseness, than on specific ones. Comparative experiments confirmed this\nhypothesis, demonstrating that LLMs performed better on standard FPs in terms\nof both accuracy and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fermi Problems (FPs) are mathematical reasoning tasks that require human-like\nlogic and numerical reasoning. Unlike other reasoning questions, FPs often\ninvolve real-world impracticalities or ambiguous concepts, making them\nchallenging even for humans to solve. Despite advancements in AI, particularly\nwith large language models (LLMs) in various reasoning tasks, FPs remain\nrelatively under-explored. This work conducted an exploratory study to examine\nthe capabilities and limitations of LLMs in solving FPs. We first evaluated the\noverall performance of three advanced LLMs using a publicly available FP\ndataset. We designed prompts according to the recently proposed TELeR taxonomy,\nincluding a zero-shot scenario. Results indicated that all three LLMs achieved\na fp_score (range between 0 - 1) below 0.5, underscoring the inherent\ndifficulty of these reasoning tasks. To further investigate, we categorized FPs\ninto standard and specific questions, hypothesizing that LLMs would perform\nbetter on standard questions, which are characterized by clarity and\nconciseness, than on specific ones. Comparative experiments confirmed this\nhypothesis, demonstrating that LLMs performed better on standard FPs in terms\nof both accuracy and efficiency."
                },
                "authors": [
                    {
                        "name": "Zishuo Liu"
                    },
                    {
                        "name": "Carlos Rabat Villarreal"
                    },
                    {
                        "name": "Mostafa Rahgouy"
                    },
                    {
                        "name": "Amit Das"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Chang Ren"
                    },
                    {
                        "name": "Dongji Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dongji Feng"
                },
                "author": "Dongji Feng",
                "arxiv_comment": "7 pages,7 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02670v1",
                "updated": "2025-04-03T15:11:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    11,
                    55,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T15:11:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    11,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Affordable AI Assistants with Knowledge Graph of Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordable AI Assistants with Knowledge Graph of Thoughts"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose the Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively. For example, KGoT achieves a 29%\nimprovement in task success rates on the GAIA benchmark compared to Hugging\nFace Agents with GPT-4o mini, while reducing costs by over 36x compared to\nGPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and\n37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a\nscalable, affordable, and high-performing solution for AI assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose the Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively. For example, KGoT achieves a 29%\nimprovement in task success rates on the GAIA benchmark compared to Hugging\nFace Agents with GPT-4o mini, while reducing costs by over 36x compared to\nGPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and\n37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a\nscalable, affordable, and high-performing solution for AI assistants."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Lorenzo Paleari"
                    },
                    {
                        "name": "Jia Hao Andrea Jiang"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Diana Khimey"
                    },
                    {
                        "name": "Jón Gunnar Hannesson"
                    },
                    {
                        "name": "Grzegorz Kwaśniewski"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22879v2",
                "updated": "2025-04-03T15:04:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    4,
                    19,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-28T21:10:39Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    10,
                    39,
                    4,
                    87,
                    0
                ],
                "title": "Quamba2: A Robust and Scalable Post-training Quantization Framework for\n  Selective State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quamba2: A Robust and Scalable Post-training Quantization Framework for\n  Selective State Space Models"
                },
                "summary": "State Space Models (SSMs) are emerging as a compelling alternative to\nTransformers because of their consistent memory usage and high performance.\nDespite this, scaling up SSMs on cloud services or limited-resource devices is\nchallenging due to their storage requirements and computational power. To\novercome this, quantizing SSMs with low bit-width data formats can reduce model\nsize and benefit from hardware acceleration. As SSMs are prone to\nquantization-induced errors, recent efforts have focused on optimizing a\nparticular model or bit-width for efficiency without sacrificing performance.\nHowever, distinct bit-width configurations are essential for different\nscenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for\nenhancing generation speed in short prompt applications for a single user. To\nthis end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both\nMamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment\non various platforms. Based on the channel order preserving and activation\npersistence of SSMs, we propose an offline approach to quantize inputs of a\nlinear recurrence in 8-bit by sorting and clustering for input $x$, combined\nwith a per-state-group quantization for input-dependent parameters $B$ and $C$.\nTo ensure compute-invariance in the SSM output, we rearrange weights offline\naccording to the clustering sequence. The experiments show that Quamba2-8B\noutperforms several state-of-the-art SSM quantization methods and delivers\n1.3$\\times$ and 3$\\times$ speed-ups in the pre-filling and generation stages,\nrespectively, while offering 4$\\times$ memory reduction with only a $1.6\\%$\naverage accuracy drop. The evaluation on MMLU shows the generalizability and\nrobustness of our framework. The code and quantized models will be released at:\nhttps://github.com/enyac-group/Quamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Space Models (SSMs) are emerging as a compelling alternative to\nTransformers because of their consistent memory usage and high performance.\nDespite this, scaling up SSMs on cloud services or limited-resource devices is\nchallenging due to their storage requirements and computational power. To\novercome this, quantizing SSMs with low bit-width data formats can reduce model\nsize and benefit from hardware acceleration. As SSMs are prone to\nquantization-induced errors, recent efforts have focused on optimizing a\nparticular model or bit-width for efficiency without sacrificing performance.\nHowever, distinct bit-width configurations are essential for different\nscenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for\nenhancing generation speed in short prompt applications for a single user. To\nthis end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both\nMamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment\non various platforms. Based on the channel order preserving and activation\npersistence of SSMs, we propose an offline approach to quantize inputs of a\nlinear recurrence in 8-bit by sorting and clustering for input $x$, combined\nwith a per-state-group quantization for input-dependent parameters $B$ and $C$.\nTo ensure compute-invariance in the SSM output, we rearrange weights offline\naccording to the clustering sequence. The experiments show that Quamba2-8B\noutperforms several state-of-the-art SSM quantization methods and delivers\n1.3$\\times$ and 3$\\times$ speed-ups in the pre-filling and generation stages,\nrespectively, while offering 4$\\times$ memory reduction with only a $1.6\\%$\naverage accuracy drop. The evaluation on MMLU shows the generalizability and\nrobustness of our framework. The code and quantized models will be released at:\nhttps://github.com/enyac-group/Quamba."
                },
                "authors": [
                    {
                        "name": "Hung-Yueh Chiang"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Natalia Frumkin"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Diana Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Diana Marculescu"
                },
                "author": "Diana Marculescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14846v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14846v4",
                "updated": "2025-04-03T14:45:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    45,
                    58,
                    3,
                    93,
                    0
                ],
                "published": "2025-01-24T10:49:45Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    49,
                    45,
                    4,
                    24,
                    0
                ],
                "title": "Wormhole Memory: A Rubik's Cube for Cross-Dialogue Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wormhole Memory: A Rubik's Cube for Cross-Dialogue Retrieval"
                },
                "summary": "In view of the gap in the current large language model in sharing memory\nacross dialogues, this research proposes a wormhole memory module (WMM) to\nrealize memory as a Rubik's cube that can be arbitrarily retrieved between\ndifferent dialogues. Through simulation experiments, the researcher built an\nexperimental framework based on the Python environment and used setting memory\nbarriers to simulate the current situation where memories between LLMs\ndialogues are difficult to share. The CoQA development data set was imported\ninto the experiment, and the feasibility of its cross-dialogue memory retrieval\nfunction was verified for WMM's nonlinear indexing and dynamic retrieval, and a\ncomparative analysis was conducted with the capabilities of Titans and MemGPT\nmemory modules. Experimental results show that WMM demonstrated the ability to\nretrieve memory across dialogues and the stability of quantitative indicators\nin eight experiments. It contributes new technical approaches to the\noptimization of memory management of LLMs and provides experience for the\npractical application in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In view of the gap in the current large language model in sharing memory\nacross dialogues, this research proposes a wormhole memory module (WMM) to\nrealize memory as a Rubik's cube that can be arbitrarily retrieved between\ndifferent dialogues. Through simulation experiments, the researcher built an\nexperimental framework based on the Python environment and used setting memory\nbarriers to simulate the current situation where memories between LLMs\ndialogues are difficult to share. The CoQA development data set was imported\ninto the experiment, and the feasibility of its cross-dialogue memory retrieval\nfunction was verified for WMM's nonlinear indexing and dynamic retrieval, and a\ncomparative analysis was conducted with the capabilities of Titans and MemGPT\nmemory modules. Experimental results show that WMM demonstrated the ability to\nretrieve memory across dialogues and the stability of quantitative indicators\nin eight experiments. It contributes new technical approaches to the\noptimization of memory management of LLMs and provides experience for the\npractical application in the future."
                },
                "authors": [
                    {
                        "name": "Libo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Libo Wang"
                },
                "author": "Libo Wang",
                "arxiv_comment": "The experimental process and code have been uploaded to the Github\n  repository, the link is:\n  https://github.com/brucewang123456789/GeniusTrail/tree/main/Wormhole%20Memory%20Module",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14846v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14846v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02646v1",
                "updated": "2025-04-03T14:40:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    40,
                    40,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:40:40Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    40,
                    40,
                    3,
                    93,
                    0
                ],
                "title": "Prompt Optimization with Logged Bandit Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Optimization with Logged Bandit Data"
                },
                "summary": "We study how to use naturally available user feedback, such as clicks, to\noptimize large language model (LLM) pipelines for generating personalized\nsentences using prompts. Naive approaches, which estimate the policy gradient\nin the prompt space, suffer either from variance caused by the large action\nspace of prompts or bias caused by inaccurate reward predictions. To circumvent\nthese challenges, we propose a novel kernel-based off-policy gradient method,\nwhich estimates the policy gradient by leveraging similarity among generated\nsentences, substantially reducing variance while suppressing the bias.\nEmpirical results on our newly established suite of benchmarks demonstrate the\neffectiveness of the proposed approach in generating personalized descriptions\nfor movie recommendations, particularly when the number of candidate prompts is\nlarge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study how to use naturally available user feedback, such as clicks, to\noptimize large language model (LLM) pipelines for generating personalized\nsentences using prompts. Naive approaches, which estimate the policy gradient\nin the prompt space, suffer either from variance caused by the large action\nspace of prompts or bias caused by inaccurate reward predictions. To circumvent\nthese challenges, we propose a novel kernel-based off-policy gradient method,\nwhich estimates the policy gradient by leveraging similarity among generated\nsentences, substantially reducing variance while suppressing the bias.\nEmpirical results on our newly established suite of benchmarks demonstrate the\neffectiveness of the proposed approach in generating personalized descriptions\nfor movie recommendations, particularly when the number of candidate prompts is\nlarge."
                },
                "authors": [
                    {
                        "name": "Haruka Kiyohara"
                    },
                    {
                        "name": "Daniel Yiming Cao"
                    },
                    {
                        "name": "Yuta Saito"
                    },
                    {
                        "name": "Thorsten Joachims"
                    }
                ],
                "author_detail": {
                    "name": "Thorsten Joachims"
                },
                "author": "Thorsten Joachims",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03804v2",
                "updated": "2025-04-03T14:35:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    35,
                    1,
                    3,
                    93,
                    0
                ],
                "published": "2024-10-04T10:25:52Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    10,
                    25,
                    52,
                    4,
                    278,
                    0
                ],
                "title": "Mixture of Attentions For Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Attentions For Speculative Decoding"
                },
                "summary": "The growth in the number of parameters of Large Language Models (LLMs) has\nled to a significant surge in computational requirements, making them\nchallenging and costly to deploy. Speculative decoding (SD) leverages smaller\nmodels to efficiently propose future tokens, which are then verified by the LLM\nin parallel. Small models that utilise activations from the LLM currently\nachieve the fastest decoding speeds. However, we identify several limitations\nof SD models including the lack of on-policyness during training and partial\nobservability. To address these shortcomings, we propose a more grounded\narchitecture for small models by introducing a Mixture of Attentions for SD.\nOur novel architecture can be applied in two scenarios: a conventional single\ndevice deployment and a novel client-server deployment where the small model is\nhosted on a consumer device and the LLM on a server. In a single-device\nscenario, we demonstrate state-of-the-art speedups improving EAGLE-2 by 9.5%\nand its acceptance length by 25%. In a client-server setting, our experiments\ndemonstrate: 1) state-of-the-art latencies with minimal calls to the server for\ndifferent network conditions, and 2) in the event of a complete disconnection,\nour approach can maintain higher accuracy compared to other SD methods and\ndemonstrates advantages over API calls to LLMs, which would otherwise be unable\nto continue the generation process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth in the number of parameters of Large Language Models (LLMs) has\nled to a significant surge in computational requirements, making them\nchallenging and costly to deploy. Speculative decoding (SD) leverages smaller\nmodels to efficiently propose future tokens, which are then verified by the LLM\nin parallel. Small models that utilise activations from the LLM currently\nachieve the fastest decoding speeds. However, we identify several limitations\nof SD models including the lack of on-policyness during training and partial\nobservability. To address these shortcomings, we propose a more grounded\narchitecture for small models by introducing a Mixture of Attentions for SD.\nOur novel architecture can be applied in two scenarios: a conventional single\ndevice deployment and a novel client-server deployment where the small model is\nhosted on a consumer device and the LLM on a server. In a single-device\nscenario, we demonstrate state-of-the-art speedups improving EAGLE-2 by 9.5%\nand its acceptance length by 25%. In a client-server setting, our experiments\ndemonstrate: 1) state-of-the-art latencies with minimal calls to the server for\ndifferent network conditions, and 2) in the event of a complete disconnection,\nour approach can maintain higher accuracy compared to other SD methods and\ndemonstrates advantages over API calls to LLMs, which would otherwise be unable\nto continue the generation process."
                },
                "authors": [
                    {
                        "name": "Matthieu Zimmer"
                    },
                    {
                        "name": "Milan Gritta"
                    },
                    {
                        "name": "Gerasimos Lampouras"
                    },
                    {
                        "name": "Haitham Bou Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "Accepted at International Conference on Learning Representations\n  (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23037v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23037v2",
                "updated": "2025-04-03T14:32:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    32,
                    44,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-29T11:02:20Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    11,
                    2,
                    20,
                    5,
                    88,
                    0
                ],
                "title": "Agentic Large Language Models, a survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Large Language Models, a survey"
                },
                "summary": "There is great interest in agentic LLMs, large language models that act as\nagents. We review the growing body of work in this area and provide a research\nagenda. Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We\norganize the literature according to these three categories. The research in\nthe first category focuses on reasoning, reflection, and retrieval, aiming to\nimprove decision making; the second category focuses on action models, robots,\nand tools, aiming for agents that act as useful assistants; the third category\nfocuses on multi-agent systems, aiming for collaborative task solving and\nsimulating interaction to study emergent social behavior. We find that works\nmutually benefit from results in other categories: retrieval enables tool use,\nreflection improves multi-agent collaboration, and reasoning benefits all\ncategories. We discuss applications of agentic LLMs and provide an agenda for\nfurther research. Important applications are in medical diagnosis, logistics\nand financial market analysis. Meanwhile, self-reflective agents playing roles\nand interacting with one another augment the process of scientific research\nitself. Further, agentic LLMs may provide a solution for the problem of LLMs\nrunning out of training data: inference-time behavior generates new training\nstates, such that LLMs can keep learning without needing ever larger datasets.\nWe note that there is risk associated with LLM assistants taking action in the\nreal world, while agentic LLMs are also likely to benefit society.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is great interest in agentic LLMs, large language models that act as\nagents. We review the growing body of work in this area and provide a research\nagenda. Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We\norganize the literature according to these three categories. The research in\nthe first category focuses on reasoning, reflection, and retrieval, aiming to\nimprove decision making; the second category focuses on action models, robots,\nand tools, aiming for agents that act as useful assistants; the third category\nfocuses on multi-agent systems, aiming for collaborative task solving and\nsimulating interaction to study emergent social behavior. We find that works\nmutually benefit from results in other categories: retrieval enables tool use,\nreflection improves multi-agent collaboration, and reasoning benefits all\ncategories. We discuss applications of agentic LLMs and provide an agenda for\nfurther research. Important applications are in medical diagnosis, logistics\nand financial market analysis. Meanwhile, self-reflective agents playing roles\nand interacting with one another augment the process of scientific research\nitself. Further, agentic LLMs may provide a solution for the problem of LLMs\nrunning out of training data: inference-time behavior generates new training\nstates, such that LLMs can keep learning without needing ever larger datasets.\nWe note that there is risk associated with LLM assistants taking action in the\nreal world, while agentic LLMs are also likely to benefit society."
                },
                "authors": [
                    {
                        "name": "Aske Plaat"
                    },
                    {
                        "name": "Max van Duijn"
                    },
                    {
                        "name": "Niki van Stein"
                    },
                    {
                        "name": "Mike Preuss"
                    },
                    {
                        "name": "Peter van der Putten"
                    },
                    {
                        "name": "Kees Joost Batenburg"
                    }
                ],
                "author_detail": {
                    "name": "Kees Joost Batenburg"
                },
                "author": "Kees Joost Batenburg",
                "arxiv_comment": "Website: https://askeplaat.github.io/agentic-llm-survey-site/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23037v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23037v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02633v1",
                "updated": "2025-04-03T14:31:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    31,
                    20,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:31:20Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    31,
                    20,
                    3,
                    93,
                    0
                ],
                "title": "Data-Driven Design of 3GPP Handover Parameters with Bayesian\n  Optimization and Transfer Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Driven Design of 3GPP Handover Parameters with Bayesian\n  Optimization and Transfer Learning"
                },
                "summary": "Mobility management in dense cellular networks is challenging due to varying\nuser speeds and deployment conditions. Traditional 3GPP handover (HO) schemes,\nrelying on fixed A3-offset and time-to-trigger (TTT) parameters, struggle to\nbalance radio link failures (RLFs) and ping-pongs. We propose a data-driven HO\noptimization framework based on high-dimensional Bayesian optimization (HD-BO)\nand enhanced with transfer learning to reduce training time and improve\ngeneralization across different user speeds. Evaluations on a real-world\ndeployment show that HD-BO outperforms 3GPP set-1 and set-5 benchmarks, while\ntransfer learning enables rapid adaptation without loss in performance. This\nhighlights the potential of data-driven, site-specific mobility management in\nlarge-scale networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobility management in dense cellular networks is challenging due to varying\nuser speeds and deployment conditions. Traditional 3GPP handover (HO) schemes,\nrelying on fixed A3-offset and time-to-trigger (TTT) parameters, struggle to\nbalance radio link failures (RLFs) and ping-pongs. We propose a data-driven HO\noptimization framework based on high-dimensional Bayesian optimization (HD-BO)\nand enhanced with transfer learning to reduce training time and improve\ngeneralization across different user speeds. Evaluations on a real-world\ndeployment show that HD-BO outperforms 3GPP set-1 and set-5 benchmarks, while\ntransfer learning enables rapid adaptation without loss in performance. This\nhighlights the potential of data-driven, site-specific mobility management in\nlarge-scale networks."
                },
                "authors": [
                    {
                        "name": "Mohamed Benzaghta"
                    },
                    {
                        "name": "Sahar Ammar"
                    },
                    {
                        "name": "David López-Pérez"
                    },
                    {
                        "name": "Basem Shihada"
                    },
                    {
                        "name": "Giovanni Geraci"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Geraci"
                },
                "author": "Giovanni Geraci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02624v1",
                "updated": "2025-04-03T14:21:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    21,
                    35,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:21:35Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    21,
                    35,
                    3,
                    93,
                    0
                ],
                "title": "EmbodiedSense: Understanding Embodied Activities with Earphones",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmbodiedSense: Understanding Embodied Activities with Earphones"
                },
                "summary": "In this paper, we propose EmbodiedSense, a sensing system based on commercial\nearphones, which enables fine-grained activity logs using existing sensors. The\nactivity logs record both user activities and the scenario in which the\nactivities took place, benefiting detailed behavior understanding. By\nunderstanding both the user and the environment, EmbodiedSense addresses three\nmain challenges: the limited recognition capability caused by\ninformation-hungry configurations (i.e., limited sensors available), the\nineffective fusion to extract ambient information such as contextual scenarios,\nand the interference from ambient noise. Specifically, EmbodiedSense consists\nof a context-aware scenario recognition module and spatial-aware activity\ndetection, which is further integrated with other attributes by expert\nknowledge. We implement our system on commercial earphones equipped with\nbinaural microphones and an Inertial Measurement Unit (IMU). By distinguishing\nusage scenarios and identifying the source of sounds, EmbodiedSense enables\nfine-grained activity logs in a zero-shot manner (evaluated with up to 41\ncategories) and outperforms strong baselines like ImageBind-LLM by 38%\nF1-score. Extensive evaluations demonstrate that EmbodiedSense is a promising\nsolution for long-term and short-term activity logs and provides significant\nbenefits in monitoring the wearer's daily life.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose EmbodiedSense, a sensing system based on commercial\nearphones, which enables fine-grained activity logs using existing sensors. The\nactivity logs record both user activities and the scenario in which the\nactivities took place, benefiting detailed behavior understanding. By\nunderstanding both the user and the environment, EmbodiedSense addresses three\nmain challenges: the limited recognition capability caused by\ninformation-hungry configurations (i.e., limited sensors available), the\nineffective fusion to extract ambient information such as contextual scenarios,\nand the interference from ambient noise. Specifically, EmbodiedSense consists\nof a context-aware scenario recognition module and spatial-aware activity\ndetection, which is further integrated with other attributes by expert\nknowledge. We implement our system on commercial earphones equipped with\nbinaural microphones and an Inertial Measurement Unit (IMU). By distinguishing\nusage scenarios and identifying the source of sounds, EmbodiedSense enables\nfine-grained activity logs in a zero-shot manner (evaluated with up to 41\ncategories) and outperforms strong baselines like ImageBind-LLM by 38%\nF1-score. Extensive evaluations demonstrate that EmbodiedSense is a promising\nsolution for long-term and short-term activity logs and provides significant\nbenefits in monitoring the wearer's daily life."
                },
                "authors": [
                    {
                        "name": "Lixing He"
                    },
                    {
                        "name": "Bufang Yang"
                    },
                    {
                        "name": "Di Duan"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    }
                ],
                "author_detail": {
                    "name": "Guoliang Xing"
                },
                "author": "Guoliang Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02623v1",
                "updated": "2025-04-03T14:21:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    21,
                    33,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:21:33Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    21,
                    33,
                    3,
                    93,
                    0
                ],
                "title": "Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents\n  through Related and Dynamic Missions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents\n  through Related and Dynamic Missions"
                },
                "summary": "Large language models (LLMs) demonstrate strong potential as agents for tool\ninvocation due to their advanced comprehension and planning capabilities. Users\nincreasingly rely on LLM-based agents to solve complex missions through\niterative interactions. However, existing benchmarks predominantly access\nagents in single-mission scenarios, failing to capture real-world complexity.\nTo bridge this gap, we propose the Multi-Mission Tool Bench. In the benchmark,\neach test case comprises multiple interrelated missions. This design requires\nagents to dynamically adapt to evolving demands. Moreover, the proposed\nbenchmark explores all possible mission-switching patterns within a fixed\nmission number. Specifically, we propose a multi-agent data generation\nframework to construct the benchmark. We also propose a novel method to\nevaluate the accuracy and efficiency of agent decisions with dynamic decision\ntrees. Experiments on diverse open-source and closed-source LLMs reveal\ncritical factors influencing agent robustness and provide actionable insights\nto the tool invocation society.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate strong potential as agents for tool\ninvocation due to their advanced comprehension and planning capabilities. Users\nincreasingly rely on LLM-based agents to solve complex missions through\niterative interactions. However, existing benchmarks predominantly access\nagents in single-mission scenarios, failing to capture real-world complexity.\nTo bridge this gap, we propose the Multi-Mission Tool Bench. In the benchmark,\neach test case comprises multiple interrelated missions. This design requires\nagents to dynamically adapt to evolving demands. Moreover, the proposed\nbenchmark explores all possible mission-switching patterns within a fixed\nmission number. Specifically, we propose a multi-agent data generation\nframework to construct the benchmark. We also propose a novel method to\nevaluate the accuracy and efficiency of agent decisions with dynamic decision\ntrees. Experiments on diverse open-source and closed-source LLMs reveal\ncritical factors influencing agent robustness and provide actionable insights\nto the tool invocation society."
                },
                "authors": [
                    {
                        "name": "PeiJie Yu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Haorui Wang"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Feng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhang"
                },
                "author": "Feng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02622v1",
                "updated": "2025-04-03T14:21:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    21,
                    9,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:21:09Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    21,
                    9,
                    3,
                    93,
                    0
                ],
                "title": "Exploring undercurrents of learning tensions in an LLM-enhanced\n  landscape: A student-centered qualitative perspective on LLM vs Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring undercurrents of learning tensions in an LLM-enhanced\n  landscape: A student-centered qualitative perspective on LLM vs Search"
                },
                "summary": "Large language models (LLMs) are transforming how students learn by providing\nreadily available tools that can quickly augment or complete various learning\nactivities with non-trivial performance. Similar paradigm shifts have occurred\nin the past with the introduction of search engines and Wikipedia, which\nreplaced or supplemented traditional information sources such as libraries and\nbooks. This study investigates the potential for LLMs to represent the next\nshift in learning, focusing on their role in information discovery and\nsynthesis compared to existing technologies, such as search engines. Using a\nwithin-subjects, counterbalanced design, participants learned new topics using\na search engine (Google) and an LLM (ChatGPT). Post-task follow-up interviews\nexplored students' reflections, preferences, pain points, and overall\nperceptions. We present analysis of their responses that show nuanced insights\ninto when, why, and how students prefer LLMs over search engines, offering\nimplications for educators, policymakers, and technology developers navigating\nthe evolving educational landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are transforming how students learn by providing\nreadily available tools that can quickly augment or complete various learning\nactivities with non-trivial performance. Similar paradigm shifts have occurred\nin the past with the introduction of search engines and Wikipedia, which\nreplaced or supplemented traditional information sources such as libraries and\nbooks. This study investigates the potential for LLMs to represent the next\nshift in learning, focusing on their role in information discovery and\nsynthesis compared to existing technologies, such as search engines. Using a\nwithin-subjects, counterbalanced design, participants learned new topics using\na search engine (Google) and an LLM (ChatGPT). Post-task follow-up interviews\nexplored students' reflections, preferences, pain points, and overall\nperceptions. We present analysis of their responses that show nuanced insights\ninto when, why, and how students prefer LLMs over search engines, offering\nimplications for educators, policymakers, and technology developers navigating\nthe evolving educational landscape."
                },
                "authors": [
                    {
                        "name": "Rahul R. Divekar"
                    },
                    {
                        "name": "Sophia Guerra"
                    },
                    {
                        "name": "Lisette Gonzalez"
                    },
                    {
                        "name": "Natasha Boos"
                    },
                    {
                        "name": "Helen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Helen Zhou"
                },
                "author": "Helen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02620v1",
                "updated": "2025-04-03T14:20:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    20,
                    6,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:20:06Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    20,
                    6,
                    3,
                    93,
                    0
                ],
                "title": "Efficient Model Editing with Task-Localized Sparse Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Model Editing with Task-Localized Sparse Fine-tuning"
                },
                "summary": "Task arithmetic has emerged as a promising approach for editing models by\nrepresenting task-specific knowledge as composable task vectors. However,\nexisting methods rely on network linearization to derive task vectors, leading\nto computational bottlenecks during training and inference. Moreover,\nlinearization alone does not ensure weight disentanglement, the key property\nthat enables conflict-free composition of task vectors. To address this, we\npropose TaLoS which allows to build sparse task vectors with minimal\ninterference without requiring explicit linearization and sharing information\nacross tasks. We find that pre-trained models contain a subset of parameters\nwith consistently low gradient sensitivity across tasks, and that sparsely\nupdating only these parameters allows for promoting weight disentanglement\nduring fine-tuning. Our experiments prove that TaLoS improves training and\ninference efficiency while outperforming current methods in task addition and\nnegation. By enabling modular parameter editing, our approach fosters practical\ndeployment of adaptable foundation models in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task arithmetic has emerged as a promising approach for editing models by\nrepresenting task-specific knowledge as composable task vectors. However,\nexisting methods rely on network linearization to derive task vectors, leading\nto computational bottlenecks during training and inference. Moreover,\nlinearization alone does not ensure weight disentanglement, the key property\nthat enables conflict-free composition of task vectors. To address this, we\npropose TaLoS which allows to build sparse task vectors with minimal\ninterference without requiring explicit linearization and sharing information\nacross tasks. We find that pre-trained models contain a subset of parameters\nwith consistently low gradient sensitivity across tasks, and that sparsely\nupdating only these parameters allows for promoting weight disentanglement\nduring fine-tuning. Our experiments prove that TaLoS improves training and\ninference efficiency while outperforming current methods in task addition and\nnegation. By enabling modular parameter editing, our approach fosters practical\ndeployment of adaptable foundation models in real-world applications."
                },
                "authors": [
                    {
                        "name": "Leonardo Iurada"
                    },
                    {
                        "name": "Marco Ciccone"
                    },
                    {
                        "name": "Tatiana Tommasi"
                    }
                ],
                "author_detail": {
                    "name": "Tatiana Tommasi"
                },
                "author": "Tatiana Tommasi",
                "arxiv_comment": "Accepted ICLR 2025 - https://github.com/iurada/talos-task-arithmetic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02612v1",
                "updated": "2025-04-03T14:12:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    12,
                    55,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:12:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    12,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Fine-Tuning Visual Autoregressive Models for Subject-Driven Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning Visual Autoregressive Models for Subject-Driven Generation"
                },
                "summary": "Recent advances in text-to-image generative models have enabled numerous\npractical applications, including subject-driven generation, which fine-tunes\npretrained models to capture subject semantics from only a few examples. While\ndiffusion-based models produce high-quality images, their extensive denoising\nsteps result in significant computational overhead, limiting real-world\napplicability. Visual autoregressive~(VAR) models, which predict next-scale\ntokens rather than spatially adjacent ones, offer significantly faster\ninference suitable for practical deployment. In this paper, we propose the\nfirst VAR-based approach for subject-driven generation. However, na\\\"{\\i}ve\nfine-tuning VAR leads to computational overhead, language drift, and reduced\ndiversity. To address these challenges, we introduce selective layer tuning to\nreduce complexity and prior distillation to mitigate language drift.\nAdditionally, we found that the early stages have a greater influence on the\ngeneration of subject than the latter stages, which merely synthesize local\ndetails. Based on this finding, we propose scale-wise weighted tuning, which\nprioritizes coarser resolutions for promoting the model to focus on the\nsubject-relevant information instead of local details. Extensive experiments\nvalidate that our method significantly outperforms diffusion-based baselines\nacross various metrics and demonstrates its practical usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-to-image generative models have enabled numerous\npractical applications, including subject-driven generation, which fine-tunes\npretrained models to capture subject semantics from only a few examples. While\ndiffusion-based models produce high-quality images, their extensive denoising\nsteps result in significant computational overhead, limiting real-world\napplicability. Visual autoregressive~(VAR) models, which predict next-scale\ntokens rather than spatially adjacent ones, offer significantly faster\ninference suitable for practical deployment. In this paper, we propose the\nfirst VAR-based approach for subject-driven generation. However, na\\\"{\\i}ve\nfine-tuning VAR leads to computational overhead, language drift, and reduced\ndiversity. To address these challenges, we introduce selective layer tuning to\nreduce complexity and prior distillation to mitigate language drift.\nAdditionally, we found that the early stages have a greater influence on the\ngeneration of subject than the latter stages, which merely synthesize local\ndetails. Based on this finding, we propose scale-wise weighted tuning, which\nprioritizes coarser resolutions for promoting the model to focus on the\nsubject-relevant information instead of local details. Extensive experiments\nvalidate that our method significantly outperforms diffusion-based baselines\nacross various metrics and demonstrates its practical usage."
                },
                "authors": [
                    {
                        "name": "Jiwoo Chung"
                    },
                    {
                        "name": "Sangeek Hyun"
                    },
                    {
                        "name": "Hyunjun Kim"
                    },
                    {
                        "name": "Eunseo Koh"
                    },
                    {
                        "name": "MinKyu Lee"
                    },
                    {
                        "name": "Jae-Pil Heo"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Pil Heo"
                },
                "author": "Jae-Pil Heo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02607v1",
                "updated": "2025-04-03T14:09:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    9,
                    17,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:09:17Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    9,
                    17,
                    3,
                    93,
                    0
                ],
                "title": "Learning Geometrically-Informed Lyapunov Functions with Deep\n  Diffeomorphic RBF Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Geometrically-Informed Lyapunov Functions with Deep\n  Diffeomorphic RBF Networks"
                },
                "summary": "The practical deployment of learning-based autonomous systems would greatly\nbenefit from tools that flexibly obtain safety guarantees in the form of\ncertificate functions from data. While the geometrical properties of such\ncertificate functions are well understood, synthesizing them using machine\nlearning techniques still remains a challenge. To mitigate this issue, we\npropose a diffeomorphic function learning framework where prior structural\nknowledge of the desired output is encoded in the geometry of a simple\nsurrogate function, which is subsequently augmented through an expressive,\ntopology-preserving state-space transformation. Thereby, we achieve an indirect\nfunction approximation framework that is guaranteed to remain in the desired\nhypothesis space. To this end, we introduce a novel approach to construct\ndiffeomorphic maps based on RBF networks, which facilitate precise, local\ntransformations around data. Finally, we demonstrate our approach by learning\ndiffeomorphic Lyapunov functions from real-world data and apply our method to\ndifferent attractor systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The practical deployment of learning-based autonomous systems would greatly\nbenefit from tools that flexibly obtain safety guarantees in the form of\ncertificate functions from data. While the geometrical properties of such\ncertificate functions are well understood, synthesizing them using machine\nlearning techniques still remains a challenge. To mitigate this issue, we\npropose a diffeomorphic function learning framework where prior structural\nknowledge of the desired output is encoded in the geometry of a simple\nsurrogate function, which is subsequently augmented through an expressive,\ntopology-preserving state-space transformation. Thereby, we achieve an indirect\nfunction approximation framework that is guaranteed to remain in the desired\nhypothesis space. To this end, we introduce a novel approach to construct\ndiffeomorphic maps based on RBF networks, which facilitate precise, local\ntransformations around data. Finally, we demonstrate our approach by learning\ndiffeomorphic Lyapunov functions from real-world data and apply our method to\ndifferent attractor systems."
                },
                "authors": [
                    {
                        "name": "Samuel Tesfazgi"
                    },
                    {
                        "name": "Leonhard Sprandl"
                    },
                    {
                        "name": "Sandra Hirche"
                    }
                ],
                "author_detail": {
                    "name": "Sandra Hirche"
                },
                "author": "Sandra Hirche",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02605v1",
                "updated": "2025-04-03T14:06:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    6,
                    17,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:06:17Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    6,
                    17,
                    3,
                    93,
                    0
                ],
                "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving"
                },
                "summary": "The task of issue resolving is to modify a codebase to generate a patch that\naddresses a given issue. However, existing benchmarks, such as SWE-bench, focus\nalmost exclusively on Python, making them insufficient for evaluating Large\nLanguage Models (LLMs) across diverse software ecosystems. To address this, we\nintroduce a multilingual issue-resolving benchmark, called Multi-SWE-bench,\ncovering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a\ntotal of 1,632 high-quality instances, which were carefully annotated from\n2,456 candidates by 68 expert annotators, ensuring that the benchmark can\nprovide an accurate and reliable evaluation. Based on Multi-SWE-bench, we\nevaluate a series of state-of-the-art models using three representative methods\n(Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with\nkey empirical insights. In addition, we launch a Multi-SWE-RL open-source\ncommunity, aimed at building large-scale reinforcement learning (RL) training\ndatasets for issue-resolving tasks. As an initial contribution, we release a\nset of 4,723 well-structured instances spanning seven programming languages,\nlaying a solid foundation for RL research in this domain. More importantly, we\nopen-source our entire data production pipeline, along with detailed tutorials,\nencouraging the open-source community to continuously contribute and expand the\ndataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL\ncommunity as catalysts for advancing RL toward its full potential, bringing us\none step closer to the dawn of AGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of issue resolving is to modify a codebase to generate a patch that\naddresses a given issue. However, existing benchmarks, such as SWE-bench, focus\nalmost exclusively on Python, making them insufficient for evaluating Large\nLanguage Models (LLMs) across diverse software ecosystems. To address this, we\nintroduce a multilingual issue-resolving benchmark, called Multi-SWE-bench,\ncovering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a\ntotal of 1,632 high-quality instances, which were carefully annotated from\n2,456 candidates by 68 expert annotators, ensuring that the benchmark can\nprovide an accurate and reliable evaluation. Based on Multi-SWE-bench, we\nevaluate a series of state-of-the-art models using three representative methods\n(Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with\nkey empirical insights. In addition, we launch a Multi-SWE-RL open-source\ncommunity, aimed at building large-scale reinforcement learning (RL) training\ndatasets for issue-resolving tasks. As an initial contribution, we release a\nset of 4,723 well-structured instances spanning seven programming languages,\nlaying a solid foundation for RL research in this domain. More importantly, we\nopen-source our entire data production pipeline, along with detailed tutorials,\nencouraging the open-source community to continuously contribute and expand the\ndataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL\ncommunity as catalysts for advancing RL toward its full potential, bringing us\none step closer to the dawn of AGI."
                },
                "authors": [
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Zhirong Huang"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Hanwu Chen"
                    },
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Shulin Xin"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Xiaojian Zhong"
                    },
                    {
                        "name": "Aoyan Li"
                    },
                    {
                        "name": "Siyao Liu"
                    },
                    {
                        "name": "Yongsheng Xiao"
                    },
                    {
                        "name": "Liangqiang Chen"
                    },
                    {
                        "name": "Yuyu Zhang"
                    },
                    {
                        "name": "Jing Su"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Rui Long"
                    },
                    {
                        "name": "Kai Shen"
                    },
                    {
                        "name": "Liang Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Xiang"
                },
                "author": "Liang Xiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02599v1",
                "updated": "2025-04-03T14:01:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    1,
                    32,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T14:01:32Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    1,
                    32,
                    3,
                    93,
                    0
                ],
                "title": "Technical Overview of Recent Developments in Small Modular Reactors in\n  the United States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technical Overview of Recent Developments in Small Modular Reactors in\n  the United States"
                },
                "summary": "Small modular reactors (SMRs) are a class of advanced nuclear fission\nreactors characterized by their compact core size (typically <300 MWe) and\npassive safety systems. Their modular design enables on-site assembly, making\nthem suitable for deployment in locations inaccessible to conventional\nlarge-scale reactors. With rising global energy demand, particularly driven by\nthe growth of AI, SMRs have recently gained attention as a potential solution\nfor powering data centers. This technical review aims to provide the public and\nrelevant stakeholders with a foundational understanding of SMR technology. It\nbegins with an overview of SMR concepts, historical context, and their current\nrole in the U.S. energy mix. Detailed technical summaries of nine selected SMR\ndesigns are then presented, covering core design, fuel systems, reactivity\ncontrol, and safety features. The report also outlines key regulatory\nframeworks, including 10 CFR Part 50, Part 52, and the technology-inclusive,\nrisk-informed, and performance-based framework currently under development.\nFinally, major U.S. programs and legislative efforts supporting SMR deployment\nover the past decade are summarized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small modular reactors (SMRs) are a class of advanced nuclear fission\nreactors characterized by their compact core size (typically <300 MWe) and\npassive safety systems. Their modular design enables on-site assembly, making\nthem suitable for deployment in locations inaccessible to conventional\nlarge-scale reactors. With rising global energy demand, particularly driven by\nthe growth of AI, SMRs have recently gained attention as a potential solution\nfor powering data centers. This technical review aims to provide the public and\nrelevant stakeholders with a foundational understanding of SMR technology. It\nbegins with an overview of SMR concepts, historical context, and their current\nrole in the U.S. energy mix. Detailed technical summaries of nine selected SMR\ndesigns are then presented, covering core design, fuel systems, reactivity\ncontrol, and safety features. The report also outlines key regulatory\nframeworks, including 10 CFR Part 50, Part 52, and the technology-inclusive,\nrisk-informed, and performance-based framework currently under development.\nFinally, major U.S. programs and legislative efforts supporting SMR deployment\nover the past decade are summarized."
                },
                "authors": [
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Ken Kurosaki"
                    }
                ],
                "author_detail": {
                    "name": "Ken Kurosaki"
                },
                "author": "Ken Kurosaki",
                "arxiv_comment": "42 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21279v2",
                "updated": "2025-04-03T13:57:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    57,
                    23,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-27T08:59:30Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    8,
                    59,
                    30,
                    3,
                    86,
                    0
                ],
                "title": "Asynchronous BFT Consensus Made Wireless",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asynchronous BFT Consensus Made Wireless"
                },
                "summary": "Asynchronous Byzantine fault-tolerant (BFT) consensus protocols, known for\ntheir robustness in unpredictable environments without relying on timing\nassumptions, are becoming increasingly vital for wireless applications. While\nthese protocols have proven effective in wired networks, their adaptation to\nwireless environments presents significant challenges. Asynchronous BFT\nconsensus, characterized by its N parallel consensus components (e.g.,\nasynchronous Byzantine agreement, reliable broadcast), suffers from high\nmessage complexity, leading to network congestion and inefficiency, especially\nin resource-constrained wireless networks. Asynchronous Byzantine agreement\n(ABA) protocols, a foundational component of asynchronous BFT, require careful\nbalancing of message complexity and cryptographic overhead to achieve efficient\nimplementation in wireless settings. Additionally, the absence of dedicated\ntestbeds for asynchronous wireless BFT consensus protocols hinders development\nand performance evaluation. To address these challenges, we propose a consensus\nbatching protocol (ConsensusBatcher), which supports both vertical and\nhorizontal batching of multiple parallel consensus components. We leverage\nConsensusBatcher to adapt three asynchronous BFT consensus protocols\n(HoneyBadgerBFT, BEAT, and Dumbo) from wired networks to resource-constrained\nwireless networks. To evaluate the performance of ConsensusBatcher-enabled\nconsensus protocols in wireless environments, we develop and open-source a\ntestbed for deployment and performance assessment of these protocols. Using\nthis testbed, we demonstrate that ConsensusBatcher-based consensus reduces\nlatency by 48% to 59% and increases throughput by 48% to 62% compared to\nbaseline consensus protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asynchronous Byzantine fault-tolerant (BFT) consensus protocols, known for\ntheir robustness in unpredictable environments without relying on timing\nassumptions, are becoming increasingly vital for wireless applications. While\nthese protocols have proven effective in wired networks, their adaptation to\nwireless environments presents significant challenges. Asynchronous BFT\nconsensus, characterized by its N parallel consensus components (e.g.,\nasynchronous Byzantine agreement, reliable broadcast), suffers from high\nmessage complexity, leading to network congestion and inefficiency, especially\nin resource-constrained wireless networks. Asynchronous Byzantine agreement\n(ABA) protocols, a foundational component of asynchronous BFT, require careful\nbalancing of message complexity and cryptographic overhead to achieve efficient\nimplementation in wireless settings. Additionally, the absence of dedicated\ntestbeds for asynchronous wireless BFT consensus protocols hinders development\nand performance evaluation. To address these challenges, we propose a consensus\nbatching protocol (ConsensusBatcher), which supports both vertical and\nhorizontal batching of multiple parallel consensus components. We leverage\nConsensusBatcher to adapt three asynchronous BFT consensus protocols\n(HoneyBadgerBFT, BEAT, and Dumbo) from wired networks to resource-constrained\nwireless networks. To evaluate the performance of ConsensusBatcher-enabled\nconsensus protocols in wireless environments, we develop and open-source a\ntestbed for deployment and performance assessment of these protocols. Using\nthis testbed, we demonstrate that ConsensusBatcher-based consensus reduces\nlatency by 48% to 59% and increases throughput by 48% to 62% compared to\nbaseline consensus protocols."
                },
                "authors": [
                    {
                        "name": "Shuo Liu"
                    },
                    {
                        "name": "Minghui Xu"
                    },
                    {
                        "name": "Tianyi Sun"
                    },
                    {
                        "name": "Xiuzhen Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xiuzhen Cheng"
                },
                "author": "Xiuzhen Cheng",
                "arxiv_comment": "Accepted to IEEE ICDCS 2025, 11 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15316v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15316v3",
                "updated": "2025-04-04T08:29:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    8,
                    29,
                    19,
                    4,
                    94,
                    0
                ],
                "published": "2024-10-20T07:03:49Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    7,
                    3,
                    49,
                    6,
                    294,
                    0
                ],
                "title": "Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nbut their application to speech-based tasks remains challenging due to the\ncomplexities of integrating audio and text modalities. This paper introduces\nIchigo, a mixed-modal model that seamlessly processes interleaved sequences of\nspeech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes\nspeech into discrete tokens and employs a uniform transformer-based\narchitecture for both speech and text modalities. This method enables joint\nreasoning and generation across modalities without the need for separate\nadapters. We present a comprehensive training methodology, including\npre-training on multilingual speech recognition datasets and fine-tuning on a\ncurated instruction dataset. Ichigo demonstrates state-of-the-art performance\non speech question-answering benchmarks, outperforming existing open-source\nspeech language models and achieving comparable results to cascaded systems.\nNotably, Ichigo exhibits a latency of just 111 ms to first token generation,\nsignificantly lower than current models. Our approach not only advances the\nfield of multimodal AI but also provides a framework for smaller research teams\nto contribute effectively to open-source speech-language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing,\nbut their application to speech-based tasks remains challenging due to the\ncomplexities of integrating audio and text modalities. This paper introduces\nIchigo, a mixed-modal model that seamlessly processes interleaved sequences of\nspeech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes\nspeech into discrete tokens and employs a uniform transformer-based\narchitecture for both speech and text modalities. This method enables joint\nreasoning and generation across modalities without the need for separate\nadapters. We present a comprehensive training methodology, including\npre-training on multilingual speech recognition datasets and fine-tuning on a\ncurated instruction dataset. Ichigo demonstrates state-of-the-art performance\non speech question-answering benchmarks, outperforming existing open-source\nspeech language models and achieving comparable results to cascaded systems.\nNotably, Ichigo exhibits a latency of just 111 ms to first token generation,\nsignificantly lower than current models. Our approach not only advances the\nfield of multimodal AI but also provides a framework for smaller research teams\nto contribute effectively to open-source speech-language models."
                },
                "authors": [
                    {
                        "name": "Alan Dao"
                    },
                    {
                        "name": "Dinh Bach Vu"
                    },
                    {
                        "name": "Huy Hoang Ha"
                    }
                ],
                "author_detail": {
                    "name": "Huy Hoang Ha"
                },
                "arxiv_affiliation": "Gia Tuan Dao",
                "author": "Huy Hoang Ha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15316v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15316v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02590v1",
                "updated": "2025-04-03T13:54:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    54,
                    53,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T13:54:53Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    54,
                    53,
                    3,
                    93,
                    0
                ],
                "title": "LexPam: Legal Procedure Awareness-Guided Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LexPam: Legal Procedure Awareness-Guided Mathematical Reasoning"
                },
                "summary": "The legal mathematical reasoning ability of LLMs is crucial when applying\nthem to real-world scenarios, as it directly affects the credibility of the\nLLM. While existing legal LLMs can perform general judicial question answering,\ntheir legal mathematical reasoning capabilities have not been trained.\nOpen-domain reasoning models, though able to generate detailed calculation\nsteps, do not follow the reasoning logic required for legal scenarios.\nAdditionally, there is currently a lack of legal mathematical reasoning\ndatasets to help validate and enhance LLMs' reasoning abilities in legal\ncontexts. To address these issues, we propose the first Chinese legal\nMathematical Reasoning Dataset, LexNum, which includes three common legal\nmathematical reasoning scenarios: economic compensation, work injury\ncompensation, and traffic accident compensation. Based on LexNum, we tested the\nperformance of existing legal LLMs and reasoning LLMs, and introduced LexPam, a\nreinforcement learning algorithm guided by legal procedural awareness to train\nLLMs, enhancing their mathematical reasoning abilities in legal scenarios.\nExperiments on tasks in the three legal scenarios show that the performance of\nexisting legal LLMs and reasoning models in legal mathematical reasoning tasks\nis unsatisfactory. LexPam can enhance the LLM's ability in these tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The legal mathematical reasoning ability of LLMs is crucial when applying\nthem to real-world scenarios, as it directly affects the credibility of the\nLLM. While existing legal LLMs can perform general judicial question answering,\ntheir legal mathematical reasoning capabilities have not been trained.\nOpen-domain reasoning models, though able to generate detailed calculation\nsteps, do not follow the reasoning logic required for legal scenarios.\nAdditionally, there is currently a lack of legal mathematical reasoning\ndatasets to help validate and enhance LLMs' reasoning abilities in legal\ncontexts. To address these issues, we propose the first Chinese legal\nMathematical Reasoning Dataset, LexNum, which includes three common legal\nmathematical reasoning scenarios: economic compensation, work injury\ncompensation, and traffic accident compensation. Based on LexNum, we tested the\nperformance of existing legal LLMs and reasoning LLMs, and introduced LexPam, a\nreinforcement learning algorithm guided by legal procedural awareness to train\nLLMs, enhancing their mathematical reasoning abilities in legal scenarios.\nExperiments on tasks in the three legal scenarios show that the performance of\nexisting legal LLMs and reasoning models in legal mathematical reasoning tasks\nis unsatisfactory. LexPam can enhance the LLM's ability in these tasks."
                },
                "authors": [
                    {
                        "name": "Kepu Zhang"
                    },
                    {
                        "name": "Guofu Xie"
                    },
                    {
                        "name": "Weijie Yu"
                    },
                    {
                        "name": "Mingyue Xu"
                    },
                    {
                        "name": "Xu Tang"
                    },
                    {
                        "name": "Yaxin Li"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00441v2",
                "updated": "2025-04-03T13:34:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    34,
                    57,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-01T05:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    5,
                    46,
                    54,
                    1,
                    91,
                    0
                ],
                "title": "No Free Lunch with Guardrails",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Free Lunch with Guardrails"
                },
                "summary": "As large language models (LLMs) and generative AI become widely adopted,\nguardrails have emerged as a key tool to ensure their safe use. However, adding\nguardrails isn't without tradeoffs; stronger security measures can reduce\nusability, while more flexible systems may leave gaps for adversarial attacks.\nIn this work, we explore whether current guardrails effectively prevent misuse\nwhile maintaining practical utility. We introduce a framework to evaluate these\ntradeoffs, measuring how different guardrails balance risk, security, and\nusability, and build an efficient guardrail.\n  Our findings confirm that there is no free lunch with guardrails;\nstrengthening security often comes at the cost of usability. To address this,\nwe propose a blueprint for designing better guardrails that minimize risk while\nmaintaining usability. We evaluate various industry guardrails, including Azure\nContent Safety, Bedrock Guardrails, OpenAI's Moderation API, Guardrails AI,\nNemo Guardrails, and Enkrypt AI guardrails. Additionally, we assess how LLMs\nlike GPT-4o, Gemini 2.0-Flash, Claude 3.5-Sonnet, and Mistral Large-Latest\nrespond under different system prompts, including simple prompts, detailed\nprompts, and detailed prompts with chain-of-thought (CoT) reasoning. Our study\nprovides a clear comparison of how different guardrails perform, highlighting\nthe challenges in balancing security and usability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) and generative AI become widely adopted,\nguardrails have emerged as a key tool to ensure their safe use. However, adding\nguardrails isn't without tradeoffs; stronger security measures can reduce\nusability, while more flexible systems may leave gaps for adversarial attacks.\nIn this work, we explore whether current guardrails effectively prevent misuse\nwhile maintaining practical utility. We introduce a framework to evaluate these\ntradeoffs, measuring how different guardrails balance risk, security, and\nusability, and build an efficient guardrail.\n  Our findings confirm that there is no free lunch with guardrails;\nstrengthening security often comes at the cost of usability. To address this,\nwe propose a blueprint for designing better guardrails that minimize risk while\nmaintaining usability. We evaluate various industry guardrails, including Azure\nContent Safety, Bedrock Guardrails, OpenAI's Moderation API, Guardrails AI,\nNemo Guardrails, and Enkrypt AI guardrails. Additionally, we assess how LLMs\nlike GPT-4o, Gemini 2.0-Flash, Claude 3.5-Sonnet, and Mistral Large-Latest\nrespond under different system prompts, including simple prompts, detailed\nprompts, and detailed prompts with chain-of-thought (CoT) reasoning. Our study\nprovides a clear comparison of how different guardrails perform, highlighting\nthe challenges in balancing security and usability."
                },
                "authors": [
                    {
                        "name": "Divyanshu Kumar"
                    },
                    {
                        "name": "Nitin Aravind Birur"
                    },
                    {
                        "name": "Tanay Baswa"
                    },
                    {
                        "name": "Sahil Agarwal"
                    },
                    {
                        "name": "Prashanth Harshangi"
                    }
                ],
                "author_detail": {
                    "name": "Prashanth Harshangi"
                },
                "author": "Prashanth Harshangi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01919v2",
                "updated": "2025-04-03T13:30:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    30,
                    35,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T17:26:40Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    26,
                    40,
                    2,
                    92,
                    0
                ],
                "title": "Bridging the Linguistic Divide: A Survey on Leveraging Large Language\n  Models for Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Linguistic Divide: A Survey on Leveraging Large Language\n  Models for Machine Translation"
                },
                "summary": "The advent of Large Language Models (LLMs) has significantly reshaped the\nlandscape of machine translation (MT), particularly for low-resource languages\nand domains that lack sufficient parallel corpora, linguistic tools, and\ncomputational infrastructure. This survey presents a comprehensive overview of\nrecent progress in leveraging LLMs for MT. We analyze techniques such as\nfew-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning\nthat enable effective adaptation to under-resourced settings. The paper also\nexplores synthetic data generation strategies using LLMs, including\nback-translation and lexical augmentation. Additionally, we compare LLM-based\ntranslation with traditional encoder-decoder models across diverse language\npairs, highlighting the strengths and limitations of each. We discuss\npersistent challenges such as hallucinations, evaluation inconsistencies, and\ninherited biases while also evaluating emerging LLM-driven metrics for\ntranslation quality. This survey offers practical insights and outlines future\ndirections for building robust, inclusive, and scalable MT systems in the era\nof large-scale generative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has significantly reshaped the\nlandscape of machine translation (MT), particularly for low-resource languages\nand domains that lack sufficient parallel corpora, linguistic tools, and\ncomputational infrastructure. This survey presents a comprehensive overview of\nrecent progress in leveraging LLMs for MT. We analyze techniques such as\nfew-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning\nthat enable effective adaptation to under-resourced settings. The paper also\nexplores synthetic data generation strategies using LLMs, including\nback-translation and lexical augmentation. Additionally, we compare LLM-based\ntranslation with traditional encoder-decoder models across diverse language\npairs, highlighting the strengths and limitations of each. We discuss\npersistent challenges such as hallucinations, evaluation inconsistencies, and\ninherited biases while also evaluating emerging LLM-driven metrics for\ntranslation quality. This survey offers practical insights and outlines future\ndirections for building robust, inclusive, and scalable MT systems in the era\nof large-scale generative models."
                },
                "authors": [
                    {
                        "name": "Baban Gain"
                    },
                    {
                        "name": "Dibyanayan Bandyopadhyay"
                    },
                    {
                        "name": "Asif Ekbal"
                    }
                ],
                "author_detail": {
                    "name": "Asif Ekbal"
                },
                "author": "Asif Ekbal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v2",
                "updated": "2025-04-03T13:28:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    28,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity ."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00631v2",
                "updated": "2025-04-03T13:23:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    23,
                    35,
                    3,
                    93,
                    0
                ],
                "published": "2025-02-02T02:43:40Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    2,
                    43,
                    40,
                    6,
                    33,
                    0
                ],
                "title": "MedConv: Convolutions Beat Transformers on Long-Tailed Bone Density\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedConv: Convolutions Beat Transformers on Long-Tailed Bone Density\n  Prediction"
                },
                "summary": "Bone density prediction via CT scans to estimate T-scores is crucial,\nproviding a more precise assessment of bone health compared to traditional\nmethods like X-ray bone density tests, which lack spatial resolution and the\nability to detect localized changes. However, CT-based prediction faces two\nmajor challenges: the high computational complexity of transformer-based\narchitectures, which limits their deployment in portable and clinical settings,\nand the imbalanced, long-tailed distribution of real-world hospital data that\nskews predictions. To address these issues, we introduce MedConv, a\nconvolutional model for bone density prediction that outperforms transformer\nmodels with lower computational demands. We also adapt Bal-CE loss and post-hoc\nlogit adjustment to improve class balance. Extensive experiments on our\nAustinSpine dataset shows that our approach achieves up to 21% improvement in\naccuracy and 20% in ROC AUC over previous state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bone density prediction via CT scans to estimate T-scores is crucial,\nproviding a more precise assessment of bone health compared to traditional\nmethods like X-ray bone density tests, which lack spatial resolution and the\nability to detect localized changes. However, CT-based prediction faces two\nmajor challenges: the high computational complexity of transformer-based\narchitectures, which limits their deployment in portable and clinical settings,\nand the imbalanced, long-tailed distribution of real-world hospital data that\nskews predictions. To address these issues, we introduce MedConv, a\nconvolutional model for bone density prediction that outperforms transformer\nmodels with lower computational demands. We also adapt Bal-CE loss and post-hoc\nlogit adjustment to improve class balance. Extensive experiments on our\nAustinSpine dataset shows that our approach achieves up to 21% improvement in\naccuracy and 20% in ROC AUC over previous state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Xuyin Qi"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Huazhan Zheng"
                    },
                    {
                        "name": "Mingxi Chen"
                    },
                    {
                        "name": "Numan Kutaiba"
                    },
                    {
                        "name": "Ruth Lim"
                    },
                    {
                        "name": "Cherie Chiang"
                    },
                    {
                        "name": "Zi En Tham"
                    },
                    {
                        "name": "Xuan Ren"
                    },
                    {
                        "name": "Wenxin Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Wenbing Lv"
                    },
                    {
                        "name": "Guangzhen Yao"
                    },
                    {
                        "name": "Renda Han"
                    },
                    {
                        "name": "Kangsheng Wang"
                    },
                    {
                        "name": "Mingyuan Li"
                    },
                    {
                        "name": "Hongtao Mao"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Zhibin Liao"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Minh-Son To"
                    }
                ],
                "author_detail": {
                    "name": "Minh-Son To"
                },
                "author": "Minh-Son To",
                "arxiv_comment": "Accepted to IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02559v1",
                "updated": "2025-04-03T13:15:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    15,
                    18,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T13:15:18Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    15,
                    18,
                    3,
                    93,
                    0
                ],
                "title": "Leveraging LLM For Synchronizing Information Across Multilingual Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLM For Synchronizing Information Across Multilingual Tables"
                },
                "summary": "The vast amount of online information today poses challenges for non-English\nspeakers, as much of it is concentrated in high-resource languages such as\nEnglish and French. Wikipedia reflects this imbalance, with content in\nlow-resource languages frequently outdated or incomplete. Recent research has\nsought to improve cross-language synchronization of Wikipedia tables using\nrule-based methods. These approaches can be effective, but they struggle with\ncomplexity and generalization. This paper explores large language models (LLMs)\nfor multilingual information synchronization, using zero-shot prompting as a\nscalable solution. We introduce the Information Updation dataset, simulating\nthe real-world process of updating outdated Wikipedia tables, and evaluate LLM\nperformance. Our findings reveal that single-prompt approaches often produce\nsuboptimal results, prompting us to introduce a task decomposition strategy\nthat enhances coherence and accuracy. Our proposed method outperforms existing\nbaselines, particularly in Information Updation (1.79%) and Information\nAddition (20.58%), highlighting the model strength in dynamically updating and\nenriching data across architectures",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vast amount of online information today poses challenges for non-English\nspeakers, as much of it is concentrated in high-resource languages such as\nEnglish and French. Wikipedia reflects this imbalance, with content in\nlow-resource languages frequently outdated or incomplete. Recent research has\nsought to improve cross-language synchronization of Wikipedia tables using\nrule-based methods. These approaches can be effective, but they struggle with\ncomplexity and generalization. This paper explores large language models (LLMs)\nfor multilingual information synchronization, using zero-shot prompting as a\nscalable solution. We introduce the Information Updation dataset, simulating\nthe real-world process of updating outdated Wikipedia tables, and evaluate LLM\nperformance. Our findings reveal that single-prompt approaches often produce\nsuboptimal results, prompting us to introduce a task decomposition strategy\nthat enhances coherence and accuracy. Our proposed method outperforms existing\nbaselines, particularly in Information Updation (1.79%) and Information\nAddition (20.58%), highlighting the model strength in dynamically updating and\nenriching data across architectures"
                },
                "authors": [
                    {
                        "name": "Siddharth Khincha"
                    },
                    {
                        "name": "Tushar Kataria"
                    },
                    {
                        "name": "Ankita Anand"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Vivek Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Gupta"
                },
                "author": "Vivek Gupta",
                "arxiv_comment": "17 Pages, 11 Tables, 2 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02553v1",
                "updated": "2025-04-03T13:07:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    7,
                    4,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T13:07:04Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    7,
                    4,
                    3,
                    93,
                    0
                ],
                "title": "Exploring Individual Factors in the Adoption of LLMs for Specific\n  Software Engineering Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Individual Factors in the Adoption of LLMs for Specific\n  Software Engineering Tasks"
                },
                "summary": "The advent of Large Language Models (LLMs) is transforming software\ndevelopment, significantly enhancing software engineering processes. Research\nhas explored their role within development teams, focusing on specific tasks\nsuch as artifact generation, decision-making support, and information\nretrieval. Despite the growing body of work on LLMs in software engineering,\nmost studies have centered on broad adoption trends, neglecting the nuanced\nrelationship between individual cognitive and behavioral factors and their\nimpact on task-specific adoption. While factors such as perceived effort and\nperformance expectancy have been explored at a general level, their influence\non distinct software engineering tasks remains underexamined. This gap hinders\nthe development of tailored LLM-based systems (e.g., Generative AI Agents) that\nalign with engineers' specific needs and limits the ability of team leaders to\ndevise effective strategies for fostering LLM adoption in targeted workflows.\nThis study bridges this gap by surveying N=188 software engineers to test the\nrelationship between individual attributes related to technology adoption and\nLLM adoption across five key tasks, using structural equation modeling (SEM).\nThe Unified Theory of Acceptance and Use of Technology (UTAUT2) was applied to\ncharacterize individual adoption behaviors. The findings reveal that\ntask-specific adoption is influenced by distinct factors, some of which\nnegatively impact adoption when considered in isolation, underscoring the\ncomplexity of LLM integration in software engineering. To support effective\nadoption, this article provides actionable recommendations, such as seamlessly\nintegrating LLMs into existing development environments and encouraging\npeer-driven knowledge sharing to enhance information retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) is transforming software\ndevelopment, significantly enhancing software engineering processes. Research\nhas explored their role within development teams, focusing on specific tasks\nsuch as artifact generation, decision-making support, and information\nretrieval. Despite the growing body of work on LLMs in software engineering,\nmost studies have centered on broad adoption trends, neglecting the nuanced\nrelationship between individual cognitive and behavioral factors and their\nimpact on task-specific adoption. While factors such as perceived effort and\nperformance expectancy have been explored at a general level, their influence\non distinct software engineering tasks remains underexamined. This gap hinders\nthe development of tailored LLM-based systems (e.g., Generative AI Agents) that\nalign with engineers' specific needs and limits the ability of team leaders to\ndevise effective strategies for fostering LLM adoption in targeted workflows.\nThis study bridges this gap by surveying N=188 software engineers to test the\nrelationship between individual attributes related to technology adoption and\nLLM adoption across five key tasks, using structural equation modeling (SEM).\nThe Unified Theory of Acceptance and Use of Technology (UTAUT2) was applied to\ncharacterize individual adoption behaviors. The findings reveal that\ntask-specific adoption is influenced by distinct factors, some of which\nnegatively impact adoption when considered in isolation, underscoring the\ncomplexity of LLM integration in software engineering. To support effective\nadoption, this article provides actionable recommendations, such as seamlessly\nintegrating LLMs into existing development environments and encouraging\npeer-driven knowledge sharing to enhance information retrieval."
                },
                "authors": [
                    {
                        "name": "Stefano Lambiase"
                    },
                    {
                        "name": "Gemma Catolino"
                    },
                    {
                        "name": "Fabio Palomba"
                    },
                    {
                        "name": "Filomena Ferrucci"
                    },
                    {
                        "name": "Daniel Russo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Russo"
                },
                "author": "Daniel Russo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00030v2",
                "updated": "2025-04-03T12:31:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    31,
                    40,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-28T23:41:55Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    23,
                    41,
                    55,
                    4,
                    87,
                    0
                ],
                "title": "Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative\n  Decoding"
                },
                "summary": "Speculative decoding accelerates large language model (LLM) inference by\nusing a smaller draft model to propose tokens, which are then verified by a\nlarger target model. However, selecting an optimal speculation length is\ncritical for maximizing speedup while minimizing wasted computation. We\nintroduce \\textit{GammaTune} and \\textit{GammaTune+}, training-free adaptive\nalgorithms that dynamically adjust speculation length based on token acceptance\nrates using a heuristic-based switching mechanism. Evaluated on SpecBench\nacross multiple tasks and model pairs, our method outperforms other\nheuristic-based approaches and fixed-length speculative decoding, achieving an\naverage speedup of 15\\% ($\\pm$5\\%) with \\textit{GammaTune} and 16\\% ($\\pm$3\\%)\nwith \\textit{GammaTune+}, while reducing performance variance. This makes\n\\textit{GammaTune} a robust and efficient solution for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding accelerates large language model (LLM) inference by\nusing a smaller draft model to propose tokens, which are then verified by a\nlarger target model. However, selecting an optimal speculation length is\ncritical for maximizing speedup while minimizing wasted computation. We\nintroduce \\textit{GammaTune} and \\textit{GammaTune+}, training-free adaptive\nalgorithms that dynamically adjust speculation length based on token acceptance\nrates using a heuristic-based switching mechanism. Evaluated on SpecBench\nacross multiple tasks and model pairs, our method outperforms other\nheuristic-based approaches and fixed-length speculative decoding, achieving an\naverage speedup of 15\\% ($\\pm$5\\%) with \\textit{GammaTune} and 16\\% ($\\pm$3\\%)\nwith \\textit{GammaTune+}, while reducing performance variance. This makes\n\\textit{GammaTune} a robust and efficient solution for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Aayush Gautam"
                    },
                    {
                        "name": "Susav Shrestha"
                    },
                    {
                        "name": "Narasimha Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Narasimha Reddy"
                },
                "author": "Narasimha Reddy",
                "arxiv_comment": "6 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02521v1",
                "updated": "2025-04-03T12:18:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    18,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T12:18:51Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    18,
                    51,
                    3,
                    93,
                    0
                ],
                "title": "UNDO: Understanding Distillation as Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNDO: Understanding Distillation as Optimization"
                },
                "summary": "Knowledge distillation has emerged as an effective strategy for compressing\nlarge language models' (LLMs) knowledge into smaller, more efficient student\nmodels. However, standard one-shot distillation methods often produce\nsuboptimal results due to a mismatch between teacher-generated rationales and\nthe student's specific learning requirements. In this paper, we introduce the\nUNDO: UNderstanding Distillation as Optimization framework, designed to bridge\nthis gap by iteratively identifying the student's errors and prompting the\nteacher to refine its explanations accordingly. Each iteration directly targets\nthe student's learning deficiencies, motivating the teacher to provide tailored\nand enhanced rationales that specifically address these weaknesses. Empirical\nevaluations on various challenging mathematical and commonsense reasoning tasks\ndemonstrate that our iterative distillation method, UNDO, significantly\noutperforms standard one-step distillation methods, achieving performance gains\nof up to 20%. Additionally, we show that teacher-generated data refined through\nour iterative process remains effective even when applied to different student\nmodels, underscoring the broad applicability of our approach. Our work\nfundamentally reframes knowledge distillation as an iterative teacher-student\ninteraction, effectively leveraging dynamic refinement by the teacher for\nbetter knowledge distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation has emerged as an effective strategy for compressing\nlarge language models' (LLMs) knowledge into smaller, more efficient student\nmodels. However, standard one-shot distillation methods often produce\nsuboptimal results due to a mismatch between teacher-generated rationales and\nthe student's specific learning requirements. In this paper, we introduce the\nUNDO: UNderstanding Distillation as Optimization framework, designed to bridge\nthis gap by iteratively identifying the student's errors and prompting the\nteacher to refine its explanations accordingly. Each iteration directly targets\nthe student's learning deficiencies, motivating the teacher to provide tailored\nand enhanced rationales that specifically address these weaknesses. Empirical\nevaluations on various challenging mathematical and commonsense reasoning tasks\ndemonstrate that our iterative distillation method, UNDO, significantly\noutperforms standard one-step distillation methods, achieving performance gains\nof up to 20%. Additionally, we show that teacher-generated data refined through\nour iterative process remains effective even when applied to different student\nmodels, underscoring the broad applicability of our approach. Our work\nfundamentally reframes knowledge distillation as an iterative teacher-student\ninteraction, effectively leveraging dynamic refinement by the teacher for\nbetter knowledge distillation."
                },
                "authors": [
                    {
                        "name": "Kushal Jain"
                    },
                    {
                        "name": "Piyushi Goyal"
                    },
                    {
                        "name": "Kumar Shridhar"
                    }
                ],
                "author_detail": {
                    "name": "Kumar Shridhar"
                },
                "author": "Kumar Shridhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02509v1",
                "updated": "2025-04-03T11:50:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    50,
                    29,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T11:50:29Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    50,
                    29,
                    3,
                    93,
                    0
                ],
                "title": "A Memory-Augmented LLM-Driven Method for Autonomous Merging of 3D\n  Printing Work Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Memory-Augmented LLM-Driven Method for Autonomous Merging of 3D\n  Printing Work Orders"
                },
                "summary": "With the rapid development of 3D printing, the demand for personalized and\ncustomized production on the manufacturing line is steadily increasing.\nEfficient merging of printing workpieces can significantly enhance the\nprocessing efficiency of the production line. Addressing the challenge, a Large\nLanguage Model (LLM)-driven method is established in this paper for the\nautonomous merging of 3D printing work orders, integrated with a\nmemory-augmented learning strategy. In industrial scenarios, both device and\norder features are modeled into LLM-readable natural language prompt templates,\nand develop an order-device matching tool along with a merging interference\nchecking module. By incorporating a self-memory learning strategy, an\nintelligent agent for autonomous order merging is constructed, resulting in\nimproved accuracy and precision in order allocation. The proposed method\neffectively leverages the strengths of LLMs in industrial applications while\nreducing hallucination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of 3D printing, the demand for personalized and\ncustomized production on the manufacturing line is steadily increasing.\nEfficient merging of printing workpieces can significantly enhance the\nprocessing efficiency of the production line. Addressing the challenge, a Large\nLanguage Model (LLM)-driven method is established in this paper for the\nautonomous merging of 3D printing work orders, integrated with a\nmemory-augmented learning strategy. In industrial scenarios, both device and\norder features are modeled into LLM-readable natural language prompt templates,\nand develop an order-device matching tool along with a merging interference\nchecking module. By incorporating a self-memory learning strategy, an\nintelligent agent for autonomous order merging is constructed, resulting in\nimproved accuracy and precision in order allocation. The proposed method\neffectively leverages the strengths of LLMs in industrial applications while\nreducing hallucination."
                },
                "authors": [
                    {
                        "name": "Yuhao Liu"
                    },
                    {
                        "name": "Maolin Yang"
                    },
                    {
                        "name": "Pingyu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Pingyu Jiang"
                },
                "author": "Pingyu Jiang",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02508v1",
                "updated": "2025-04-03T11:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    48,
                    56,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T11:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    48,
                    56,
                    3,
                    93,
                    0
                ],
                "title": "APHQ-ViT: Post-Training Quantization with Average Perturbation Hessian\n  Based Reconstruction for Vision Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APHQ-ViT: Post-Training Quantization with Average Perturbation Hessian\n  Based Reconstruction for Vision Transformers"
                },
                "summary": "Vision Transformers (ViTs) have become one of the most commonly used\nbackbones for vision tasks. Despite their remarkable performance, they often\nsuffer significant accuracy drops when quantized for practical deployment,\nparticularly by post-training quantization (PTQ) under ultra-low bits.\nRecently, reconstruction-based PTQ methods have shown promising performance in\nquantizing Convolutional Neural Networks (CNNs). However, they fail when\napplied to ViTs, primarily due to the inaccurate estimation of output\nimportance and the substantial accuracy degradation in quantizing post-GELU\nactivations. To address these issues, we propose \\textbf{APHQ-ViT}, a novel PTQ\napproach based on importance estimation with Average Perturbation Hessian\n(APH). Specifically, we first thoroughly analyze the current approximation\napproaches with Hessian loss, and propose an improved average perturbation\nHessian loss. To deal with the quantization of the post-GELU activations, we\ndesign an MLP Reconstruction (MR) method by replacing the GELU function in MLP\nwith ReLU and reconstructing it by the APH loss on a small unlabeled\ncalibration set. Extensive experiments demonstrate that APHQ-ViT using linear\nquantizers outperforms existing PTQ methods by substantial margins in 3-bit and\n4-bit across different vision tasks. The source code is available at\nhttps://github.com/GoatWu/APHQ-ViT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) have become one of the most commonly used\nbackbones for vision tasks. Despite their remarkable performance, they often\nsuffer significant accuracy drops when quantized for practical deployment,\nparticularly by post-training quantization (PTQ) under ultra-low bits.\nRecently, reconstruction-based PTQ methods have shown promising performance in\nquantizing Convolutional Neural Networks (CNNs). However, they fail when\napplied to ViTs, primarily due to the inaccurate estimation of output\nimportance and the substantial accuracy degradation in quantizing post-GELU\nactivations. To address these issues, we propose \\textbf{APHQ-ViT}, a novel PTQ\napproach based on importance estimation with Average Perturbation Hessian\n(APH). Specifically, we first thoroughly analyze the current approximation\napproaches with Hessian loss, and propose an improved average perturbation\nHessian loss. To deal with the quantization of the post-GELU activations, we\ndesign an MLP Reconstruction (MR) method by replacing the GELU function in MLP\nwith ReLU and reconstructing it by the APH loss on a small unlabeled\ncalibration set. Extensive experiments demonstrate that APHQ-ViT using linear\nquantizers outperforms existing PTQ methods by substantial margins in 3-bit and\n4-bit across different vision tasks. The source code is available at\nhttps://github.com/GoatWu/APHQ-ViT."
                },
                "authors": [
                    {
                        "name": "Zhuguanyu Wu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Jiaxin Chen"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Yunhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhong Wang"
                },
                "author": "Yunhong Wang",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09184v2",
                "updated": "2025-04-03T11:45:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    45,
                    18,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-12T09:24:31Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    24,
                    31,
                    2,
                    71,
                    0
                ],
                "title": "Exploiting Unstructured Sparsity in Fully Homomorphic Encrypted DNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Unstructured Sparsity in Fully Homomorphic Encrypted DNNs"
                },
                "summary": "The deployment of deep neural networks (DNNs) in privacy-sensitive\nenvironments is constrained by computational overheads in fully homomorphic\nencryption (FHE). This paper explores unstructured sparsity in FHE matrix\nmultiplication schemes as a means of reducing this burden while maintaining\nmodel accuracy requirements. We demonstrate that sparsity can be exploited in\narbitrary matrix multiplication, providing runtime benefits compared to a\nbaseline naive algorithm at all sparsity levels. This is a notable departure\nfrom the plaintext domain, where there is a trade-off between sparsity and the\noverhead of the sparse multiplication algorithm. In addition, we propose three\nsparse multiplication schemes in FHE based on common plaintext sparse\nencodings. We demonstrate the performance gain is scheme-invariant; however,\nsome sparse schemes vastly reduce the memory storage requirements of the\nencrypted matrix at high sparsity values. Our proposed sparse schemes yield an\naverage performance gain of 2.5x at 50% unstructured sparsity, with our\nmulti-threading scheme providing a 32.5x performance increase over the\nequivalent single-threaded sparse computation when utilizing 64 cores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of deep neural networks (DNNs) in privacy-sensitive\nenvironments is constrained by computational overheads in fully homomorphic\nencryption (FHE). This paper explores unstructured sparsity in FHE matrix\nmultiplication schemes as a means of reducing this burden while maintaining\nmodel accuracy requirements. We demonstrate that sparsity can be exploited in\narbitrary matrix multiplication, providing runtime benefits compared to a\nbaseline naive algorithm at all sparsity levels. This is a notable departure\nfrom the plaintext domain, where there is a trade-off between sparsity and the\noverhead of the sparse multiplication algorithm. In addition, we propose three\nsparse multiplication schemes in FHE based on common plaintext sparse\nencodings. We demonstrate the performance gain is scheme-invariant; however,\nsome sparse schemes vastly reduce the memory storage requirements of the\nencrypted matrix at high sparsity values. Our proposed sparse schemes yield an\naverage performance gain of 2.5x at 50% unstructured sparsity, with our\nmulti-threading scheme providing a 32.5x performance increase over the\nequivalent single-threaded sparse computation when utilizing 64 cores."
                },
                "authors": [
                    {
                        "name": "Aidan Ferguson"
                    },
                    {
                        "name": "Perry Gibson"
                    },
                    {
                        "name": "Lara D'Agata"
                    },
                    {
                        "name": "Parker McLeod"
                    },
                    {
                        "name": "Ferhat Yaman"
                    },
                    {
                        "name": "Amitabh Das"
                    },
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "José Cano"
                    }
                ],
                "author_detail": {
                    "name": "José Cano"
                },
                "author": "José Cano",
                "arxiv_comment": "Accepted to 5th Workshop on Machine Learning and Systems (EuroMLSys)\n  co-located with EuroSys '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02507v1",
                "updated": "2025-04-03T11:41:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    41,
                    55,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T11:41:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    41,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "ZClip: Adaptive Spike Mitigation for LLM Pre-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZClip: Adaptive Spike Mitigation for LLM Pre-Training"
                },
                "summary": "Training large language models (LLMs) presents numerous challenges, including\ngradient instability and loss spikes. These phenomena can lead to catastrophic\ndivergence, requiring costly checkpoint restoration and data batch skipping.\nTraditional gradient clipping techniques, such as constant or norm-based\nmethods, fail to address these issues effectively due to their reliance on\nfixed thresholds or heuristics, leading to inefficient learning and requiring\nfrequent manual intervention. In this work, we propose ZClip, an adaptive\ngradient clipping algorithm that dynamically adjusts the clipping threshold\nbased on statistical properties of gradient norms over time. Unlike prior\nreactive strategies, ZClip proactively adapts to training dynamics without\nmaking any prior assumptions on the scale and the temporal evolution of\ngradient norms. At its core, it leverages z-score-based anomaly detection to\nidentify and mitigate large gradient spikes, preventing malignant loss spikes\nwhile not interfering with convergence otherwise. Our code is available at:\nhttps://github.com/bluorion-com/ZClip.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) presents numerous challenges, including\ngradient instability and loss spikes. These phenomena can lead to catastrophic\ndivergence, requiring costly checkpoint restoration and data batch skipping.\nTraditional gradient clipping techniques, such as constant or norm-based\nmethods, fail to address these issues effectively due to their reliance on\nfixed thresholds or heuristics, leading to inefficient learning and requiring\nfrequent manual intervention. In this work, we propose ZClip, an adaptive\ngradient clipping algorithm that dynamically adjusts the clipping threshold\nbased on statistical properties of gradient norms over time. Unlike prior\nreactive strategies, ZClip proactively adapts to training dynamics without\nmaking any prior assumptions on the scale and the temporal evolution of\ngradient norms. At its core, it leverages z-score-based anomaly detection to\nidentify and mitigate large gradient spikes, preventing malignant loss spikes\nwhile not interfering with convergence otherwise. Our code is available at:\nhttps://github.com/bluorion-com/ZClip."
                },
                "authors": [
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Louis Owen"
                    },
                    {
                        "name": "Nilabhra Roy Chowdhury"
                    },
                    {
                        "name": "Fabian Güra"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Güra"
                },
                "author": "Fabian Güra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01667v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01667v2",
                "updated": "2025-04-03T11:39:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    39,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T12:16:14Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    16,
                    14,
                    2,
                    92,
                    0
                ],
                "title": "Testing Low-Resource Language Support in LLMs Using Language Proficiency\n  Exams: the Case of Luxembourgish",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing Low-Resource Language Support in LLMs Using Language Proficiency\n  Exams: the Case of Luxembourgish"
                },
                "summary": "Large Language Models (LLMs) have become an increasingly important tool in\nresearch and society at large. While LLMs are regularly used all over the world\nby experts and lay-people alike, they are predominantly developed with\nEnglish-speaking users in mind, performing well in English and other\nwide-spread languages while less-resourced languages such as Luxembourgish are\nseen as a lower priority. This lack of attention is also reflected in the\nsparsity of available evaluation tools and datasets. In this study, we\ninvestigate the viability of language proficiency exams as such evaluation\ntools for the Luxembourgish language. We find that large models such as\nChatGPT, Claude and DeepSeek-R1 typically achieve high scores, while smaller\nmodels show weak performances. We also find that the performances in such\nlanguage exams can be used to predict performances in other NLP tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become an increasingly important tool in\nresearch and society at large. While LLMs are regularly used all over the world\nby experts and lay-people alike, they are predominantly developed with\nEnglish-speaking users in mind, performing well in English and other\nwide-spread languages while less-resourced languages such as Luxembourgish are\nseen as a lower priority. This lack of attention is also reflected in the\nsparsity of available evaluation tools and datasets. In this study, we\ninvestigate the viability of language proficiency exams as such evaluation\ntools for the Luxembourgish language. We find that large models such as\nChatGPT, Claude and DeepSeek-R1 typically achieve high scores, while smaller\nmodels show weak performances. We also find that the performances in such\nlanguage exams can be used to predict performances in other NLP tasks."
                },
                "authors": [
                    {
                        "name": "Cedric Lothritz"
                    },
                    {
                        "name": "Jordi Cabot"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Cabot"
                },
                "author": "Jordi Cabot",
                "arxiv_comment": "18 pages, 2 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01667v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01667v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02498v1",
                "updated": "2025-04-03T11:20:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    20,
                    49,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T11:20:49Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    20,
                    49,
                    3,
                    93,
                    0
                ],
                "title": "VISTA: Unsupervised 2D Temporal Dependency Representations for Time\n  Series Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VISTA: Unsupervised 2D Temporal Dependency Representations for Time\n  Series Anomaly Detection"
                },
                "summary": "Time Series Anomaly Detection (TSAD) is essential for uncovering rare and\npotentially harmful events in unlabeled time series data. Existing methods are\nhighly dependent on clean, high-quality inputs, making them susceptible to\nnoise and real-world imperfections. Additionally, intricate temporal\nrelationships in time series data are often inadequately captured in\ntraditional 1D representations, leading to suboptimal modeling of dependencies.\nWe introduce VISTA, a training-free, unsupervised TSAD algorithm designed to\novercome these challenges. VISTA features three core modules: 1) Time Series\nDecomposition using Seasonal and Trend Decomposition via Loess (STL) to\ndecompose noisy time series into trend, seasonal, and residual components; 2)\nTemporal Self-Attention, which transforms 1D time series into 2D temporal\ncorrelation matrices for richer dependency modeling and anomaly detection; and\n3) Multivariate Temporal Aggregation, which uses a pretrained feature extractor\nto integrate cross-variable information into a unified, memory-efficient\nrepresentation. VISTA's training-free approach enables rapid deployment and\neasy hyperparameter tuning, making it suitable for industrial applications. It\nachieves state-of-the-art performance on five multivariate TSAD benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Series Anomaly Detection (TSAD) is essential for uncovering rare and\npotentially harmful events in unlabeled time series data. Existing methods are\nhighly dependent on clean, high-quality inputs, making them susceptible to\nnoise and real-world imperfections. Additionally, intricate temporal\nrelationships in time series data are often inadequately captured in\ntraditional 1D representations, leading to suboptimal modeling of dependencies.\nWe introduce VISTA, a training-free, unsupervised TSAD algorithm designed to\novercome these challenges. VISTA features three core modules: 1) Time Series\nDecomposition using Seasonal and Trend Decomposition via Loess (STL) to\ndecompose noisy time series into trend, seasonal, and residual components; 2)\nTemporal Self-Attention, which transforms 1D time series into 2D temporal\ncorrelation matrices for richer dependency modeling and anomaly detection; and\n3) Multivariate Temporal Aggregation, which uses a pretrained feature extractor\nto integrate cross-variable information into a unified, memory-efficient\nrepresentation. VISTA's training-free approach enables rapid deployment and\neasy hyperparameter tuning, making it suitable for industrial applications. It\nachieves state-of-the-art performance on five multivariate TSAD benchmarks."
                },
                "authors": [
                    {
                        "name": "Sinchee Chin"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Xiaochen Yang"
                    },
                    {
                        "name": "Jing-Hao Xue"
                    },
                    {
                        "name": "Wenming Yang"
                    },
                    {
                        "name": "Peng Jia"
                    },
                    {
                        "name": "Guijin Wang"
                    },
                    {
                        "name": "Luo Yingqun"
                    }
                ],
                "author_detail": {
                    "name": "Luo Yingqun"
                },
                "author": "Luo Yingqun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02495v1",
                "updated": "2025-04-03T11:19:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    19,
                    49,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T11:19:49Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    11,
                    19,
                    49,
                    3,
                    93,
                    0
                ],
                "title": "Inference-Time Scaling for Generalist Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Scaling for Generalist Reward Modeling"
                },
                "summary": "Reinforcement learning (RL) has been widely adopted in post-training for\nlarge language models (LLMs) at scale. Recently, the incentivization of\nreasoning capabilities in LLMs from RL indicates that $\\textit{proper learning\nmethods could enable effective inference-time scalability}$. A key challenge of\nRL is to obtain accurate reward signals for LLMs in various domains beyond\nverifiable questions or artificial rules. In this work, we investigate how to\nimprove reward modeling (RM) with more inference compute for general queries,\ni.e. the $\\textbf{inference-time scalability of generalist RM}$, and further,\nhow to improve the effectiveness of performance-compute scaling with proper\nlearning methods. For the RM approach, we adopt pointwise generative reward\nmodeling (GRM) to enable flexibility for different input types and potential\nfor inference-time scaling. For the learning method, we propose Self-Principled\nCritique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs\nthrough online RL, to generate principles adaptively and critiques accurately,\nresulting in $\\textbf{DeepSeek-GRM}$ models. Furthermore, for effective\ninference-time scaling, we use parallel sampling to expand compute usage, and\nintroduce a meta RM to guide voting process for better scaling performance.\nEmpirically, we show that SPCT significantly improves the quality and\nscalability of GRMs, outperforming existing methods and models in various RM\nbenchmarks without severe biases, and could achieve better performance compared\nto training-time scaling. DeepSeek-GRM still meets challenges in some tasks,\nwhich we believe can be addressed by future efforts in generalist reward\nsystems. The models will be released and open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has been widely adopted in post-training for\nlarge language models (LLMs) at scale. Recently, the incentivization of\nreasoning capabilities in LLMs from RL indicates that $\\textit{proper learning\nmethods could enable effective inference-time scalability}$. A key challenge of\nRL is to obtain accurate reward signals for LLMs in various domains beyond\nverifiable questions or artificial rules. In this work, we investigate how to\nimprove reward modeling (RM) with more inference compute for general queries,\ni.e. the $\\textbf{inference-time scalability of generalist RM}$, and further,\nhow to improve the effectiveness of performance-compute scaling with proper\nlearning methods. For the RM approach, we adopt pointwise generative reward\nmodeling (GRM) to enable flexibility for different input types and potential\nfor inference-time scaling. For the learning method, we propose Self-Principled\nCritique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs\nthrough online RL, to generate principles adaptively and critiques accurately,\nresulting in $\\textbf{DeepSeek-GRM}$ models. Furthermore, for effective\ninference-time scaling, we use parallel sampling to expand compute usage, and\nintroduce a meta RM to guide voting process for better scaling performance.\nEmpirically, we show that SPCT significantly improves the quality and\nscalability of GRMs, outperforming existing methods and models in various RM\nbenchmarks without severe biases, and could achieve better performance compared\nto training-time scaling. DeepSeek-GRM still meets challenges in some tasks,\nwhich we believe can be addressed by future efforts in generalist reward\nsystems. The models will be released and open-sourced."
                },
                "authors": [
                    {
                        "name": "Zijun Liu"
                    },
                    {
                        "name": "Peiyi Wang"
                    },
                    {
                        "name": "Runxin Xu"
                    },
                    {
                        "name": "Shirong Ma"
                    },
                    {
                        "name": "Chong Ruan"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wu"
                },
                "author": "Yu Wu",
                "arxiv_comment": "Preprint, under review. 42 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02477v1",
                "updated": "2025-04-03T10:53:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    53,
                    7,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T10:53:07Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    53,
                    7,
                    3,
                    93,
                    0
                ],
                "title": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision"
                },
                "summary": "Robot vision has greatly benefited from advancements in multimodal fusion\ntechniques and vision-language models (VLMs). We systematically review the\napplications of multimodal fusion in key robotic vision tasks, including\nsemantic scene understanding, simultaneous localization and mapping (SLAM), 3D\nobject detection, navigation and localization, and robot manipulation. We\ncompare VLMs based on large language models (LLMs) with traditional multimodal\nfusion methods, analyzing their advantages, limitations, and synergies.\nAdditionally, we conduct an in-depth analysis of commonly used datasets,\nevaluating their applicability and challenges in real-world robotic scenarios.\nFurthermore, we identify critical research challenges such as cross-modal\nalignment, efficient fusion strategies, real-time deployment, and domain\nadaptation, and propose future research directions, including self-supervised\nlearning for robust multimodal representations, transformer-based fusion\narchitectures, and scalable multimodal frameworks. Through a comprehensive\nreview, comparative analysis, and forward-looking discussion, we provide a\nvaluable reference for advancing multimodal perception and interaction in\nrobotic vision. A comprehensive list of studies in this survey is available at\nhttps://github.com/Xiaofeng-Han-Res/MF-RV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot vision has greatly benefited from advancements in multimodal fusion\ntechniques and vision-language models (VLMs). We systematically review the\napplications of multimodal fusion in key robotic vision tasks, including\nsemantic scene understanding, simultaneous localization and mapping (SLAM), 3D\nobject detection, navigation and localization, and robot manipulation. We\ncompare VLMs based on large language models (LLMs) with traditional multimodal\nfusion methods, analyzing their advantages, limitations, and synergies.\nAdditionally, we conduct an in-depth analysis of commonly used datasets,\nevaluating their applicability and challenges in real-world robotic scenarios.\nFurthermore, we identify critical research challenges such as cross-modal\nalignment, efficient fusion strategies, real-time deployment, and domain\nadaptation, and propose future research directions, including self-supervised\nlearning for robust multimodal representations, transformer-based fusion\narchitectures, and scalable multimodal frameworks. Through a comprehensive\nreview, comparative analysis, and forward-looking discussion, we provide a\nvaluable reference for advancing multimodal perception and interaction in\nrobotic vision. A comprehensive list of studies in this survey is available at\nhttps://github.com/Xiaofeng-Han-Res/MF-RV."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Han"
                    },
                    {
                        "name": "Shunpeng Chen"
                    },
                    {
                        "name": "Zenghuang Fu"
                    },
                    {
                        "name": "Zhe Feng"
                    },
                    {
                        "name": "Lue Fan"
                    },
                    {
                        "name": "Dong An"
                    },
                    {
                        "name": "Changwei Wang"
                    },
                    {
                        "name": "Li Guo"
                    },
                    {
                        "name": "Weiliang Meng"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    },
                    {
                        "name": "Rongtao Xu"
                    },
                    {
                        "name": "Shibiao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Shibiao Xu"
                },
                "author": "Shibiao Xu",
                "arxiv_comment": "27 pages, 11 figures, survey paper submitted to Information Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02458v1",
                "updated": "2025-04-03T10:22:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    22,
                    30,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T10:22:30Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    22,
                    30,
                    3,
                    93,
                    0
                ],
                "title": "Retrieval-Augmented Purifier for Robust LLM-Empowered Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Purifier for Robust LLM-Empowered Recommendation"
                },
                "summary": "Recently, Large Language Model (LLM)-empowered recommender systems have\nrevolutionized personalized recommendation frameworks and attracted extensive\nattention. Despite the remarkable success, existing LLM-empowered RecSys have\nbeen demonstrated to be highly vulnerable to minor perturbations. To mitigate\nthe negative impact of such vulnerabilities, one potential solution is to\nemploy collaborative signals based on item-item co-occurrence to purify the\nmalicious collaborative knowledge from the user's historical interactions\ninserted by attackers. On the other hand, due to the capabilities to expand\ninsufficient internal knowledge of LLMs, Retrieval-Augmented Generation (RAG)\ntechniques provide unprecedented opportunities to enhance the robustness of\nLLM-empowered recommender systems by introducing external collaborative\nknowledge. Therefore, in this paper, we propose a novel framework (RETURN) by\nretrieving external collaborative signals to purify the poisoned user profiles\nand enhance the robustness of LLM-empowered RecSys in a plug-and-play manner.\nSpecifically, retrieval-augmented perturbation positioning is proposed to\nidentify potential perturbations within the users' historical sequences by\nretrieving external knowledge from collaborative item graphs. After that, we\nfurther retrieve the collaborative knowledge to cleanse the perturbations by\nusing either deletion or replacement strategies and introduce a robust ensemble\nrecommendation strategy to generate final robust predictions. Extensive\nexperiments on three real-world datasets demonstrate the effectiveness of the\nproposed RETURN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Model (LLM)-empowered recommender systems have\nrevolutionized personalized recommendation frameworks and attracted extensive\nattention. Despite the remarkable success, existing LLM-empowered RecSys have\nbeen demonstrated to be highly vulnerable to minor perturbations. To mitigate\nthe negative impact of such vulnerabilities, one potential solution is to\nemploy collaborative signals based on item-item co-occurrence to purify the\nmalicious collaborative knowledge from the user's historical interactions\ninserted by attackers. On the other hand, due to the capabilities to expand\ninsufficient internal knowledge of LLMs, Retrieval-Augmented Generation (RAG)\ntechniques provide unprecedented opportunities to enhance the robustness of\nLLM-empowered recommender systems by introducing external collaborative\nknowledge. Therefore, in this paper, we propose a novel framework (RETURN) by\nretrieving external collaborative signals to purify the poisoned user profiles\nand enhance the robustness of LLM-empowered RecSys in a plug-and-play manner.\nSpecifically, retrieval-augmented perturbation positioning is proposed to\nidentify potential perturbations within the users' historical sequences by\nretrieving external knowledge from collaborative item graphs. After that, we\nfurther retrieve the collaborative knowledge to cleanse the perturbations by\nusing either deletion or replacement strategies and introduce a robust ensemble\nrecommendation strategy to generate final robust predictions. Extensive\nexperiments on three real-world datasets demonstrate the effectiveness of the\nproposed RETURN."
                },
                "authors": [
                    {
                        "name": "Liangbo Ning"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02455v1",
                "updated": "2025-04-03T10:20:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    20,
                    16,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T10:20:16Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    20,
                    16,
                    3,
                    93,
                    0
                ],
                "title": "QPanda3: A High-Performance Software-Hardware Collaborative Framework\n  for Large-Scale Quantum-Classical Computing Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QPanda3: A High-Performance Software-Hardware Collaborative Framework\n  for Large-Scale Quantum-Classical Computing Integration"
                },
                "summary": "QPanda3 is a high-performance quantum programming framework that enhances\nquantum computing efficiency through optimized circuit compilation, an advanced\ninstruction stream format (OriginBIS), and hardware-aware execution strategies.\nThese engineering optimizations significantly improve both processing speed and\nsystem performance, addressing key challenges in the NISQ era. A core\ninnovation, OriginBIS, accelerates encoding speeds by up to 86.9x compared to\nOpenQASM 2.0, while decoding is 35.6x faster, leading to more efficient data\nhandling, reduced memory overhead, and improved communication efficiency. This\ndirectly enhances the execution of quantum circuits, making large-scale quantum\nsimulations more feasible. Comprehensive benchmarking demonstrates QPanda3's\nsuperior performance: quantum circuit construction is 20.7x faster, execution\nspeeds improve by 3.4x, and transpilation efficiency increases by 14.97x over\nQiskit. Notably, in compiling a 118-qubit W-state circuit on a 2D-grid\ntopology, QPanda3 achieves an unprecedented 869.9x speedup, underscoring its\nability to handle complex quantum workloads at scale. By combining high-speed\nquantum processing with a modular and extensible software architecture, QPanda3\nprovides a practical bridge between today's NISQ devices and future\nfault-tolerant quantum computing. It facilitates real-world applications in\nfinancial modeling, materials science, and combinatorial optimization, while\nits robust and scalable design supports industrial adoption and cloud-based\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QPanda3 is a high-performance quantum programming framework that enhances\nquantum computing efficiency through optimized circuit compilation, an advanced\ninstruction stream format (OriginBIS), and hardware-aware execution strategies.\nThese engineering optimizations significantly improve both processing speed and\nsystem performance, addressing key challenges in the NISQ era. A core\ninnovation, OriginBIS, accelerates encoding speeds by up to 86.9x compared to\nOpenQASM 2.0, while decoding is 35.6x faster, leading to more efficient data\nhandling, reduced memory overhead, and improved communication efficiency. This\ndirectly enhances the execution of quantum circuits, making large-scale quantum\nsimulations more feasible. Comprehensive benchmarking demonstrates QPanda3's\nsuperior performance: quantum circuit construction is 20.7x faster, execution\nspeeds improve by 3.4x, and transpilation efficiency increases by 14.97x over\nQiskit. Notably, in compiling a 118-qubit W-state circuit on a 2D-grid\ntopology, QPanda3 achieves an unprecedented 869.9x speedup, underscoring its\nability to handle complex quantum workloads at scale. By combining high-speed\nquantum processing with a modular and extensible software architecture, QPanda3\nprovides a practical bridge between today's NISQ devices and future\nfault-tolerant quantum computing. It facilitates real-world applications in\nfinancial modeling, materials science, and combinatorial optimization, while\nits robust and scalable design supports industrial adoption and cloud-based\ndeployment."
                },
                "authors": [
                    {
                        "name": "Tianrui Zou"
                    },
                    {
                        "name": "Yuan Fang"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Menghan Dou"
                    },
                    {
                        "name": "Jun Fu"
                    },
                    {
                        "name": "ZiQiang Zhao"
                    },
                    {
                        "name": "ShuBin Zhao"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Dongyi Zhao"
                    },
                    {
                        "name": "Zhaoyun Chen"
                    },
                    {
                        "name": "Guoping Guo"
                    }
                ],
                "author_detail": {
                    "name": "Guoping Guo"
                },
                "author": "Guoping Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05445v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05445v2",
                "updated": "2025-04-03T10:16:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    16,
                    53,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-07T14:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    16,
                    48,
                    4,
                    66,
                    0
                ],
                "title": "ToxicSQL: Migrating SQL Injection Threats into Text-to-SQL Models via\n  Backdoor Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToxicSQL: Migrating SQL Injection Threats into Text-to-SQL Models via\n  Backdoor Attack"
                },
                "summary": "Large language models (LLMs) have shown state-of-the-art results in\ntranslating natural language questions into SQL queries (Text-to-SQL), a\nlong-standing challenge within the database community. However, security\nconcerns remain largely unexplored, particularly the threat of backdoor\nattacks, which can introduce malicious behaviors into models through\nfine-tuning with poisoned datasets. In this work, we systematically investigate\nthe vulnerabilities of LLM-based Text-to-SQL models and present ToxicSQL, a\nnovel backdoor attack framework. Our approach leverages stealthy {semantic and\ncharacter-level triggers} to make backdoors difficult to detect and remove,\nensuring that malicious behaviors remain covert while maintaining high model\naccuracy on benign inputs. Furthermore, we propose leveraging SQL injection\npayloads as backdoor targets, enabling the generation of malicious yet\nexecutable SQL queries, which pose severe security and privacy risks in\nlanguage model-based SQL development. We demonstrate that injecting only 0.44%\nof poisoned data can result in an attack success rate of 79.41%, posing a\nsignificant risk to database security. Additionally, we propose detection and\nmitigation strategies to enhance model reliability. Our findings highlight the\nurgent need for security-aware Text-to-SQL development, emphasizing the\nimportance of robust defenses against backdoor threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown state-of-the-art results in\ntranslating natural language questions into SQL queries (Text-to-SQL), a\nlong-standing challenge within the database community. However, security\nconcerns remain largely unexplored, particularly the threat of backdoor\nattacks, which can introduce malicious behaviors into models through\nfine-tuning with poisoned datasets. In this work, we systematically investigate\nthe vulnerabilities of LLM-based Text-to-SQL models and present ToxicSQL, a\nnovel backdoor attack framework. Our approach leverages stealthy {semantic and\ncharacter-level triggers} to make backdoors difficult to detect and remove,\nensuring that malicious behaviors remain covert while maintaining high model\naccuracy on benign inputs. Furthermore, we propose leveraging SQL injection\npayloads as backdoor targets, enabling the generation of malicious yet\nexecutable SQL queries, which pose severe security and privacy risks in\nlanguage model-based SQL development. We demonstrate that injecting only 0.44%\nof poisoned data can result in an attack success rate of 79.41%, posing a\nsignificant risk to database security. Additionally, we propose detection and\nmitigation strategies to enhance model reliability. Our findings highlight the\nurgent need for security-aware Text-to-SQL development, emphasizing the\nimportance of robust defenses against backdoor threats."
                },
                "authors": [
                    {
                        "name": "Meiyu Lin"
                    },
                    {
                        "name": "Haichuan Zhang"
                    },
                    {
                        "name": "Jiale Lao"
                    },
                    {
                        "name": "Renyuan Li"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    },
                    {
                        "name": "Carl Yang"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Mingjie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Mingjie Tang"
                },
                "author": "Mingjie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05445v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05445v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13734v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13734v2",
                "updated": "2025-04-03T10:04:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    4,
                    54,
                    3,
                    93,
                    0
                ],
                "published": "2025-02-19T14:02:00Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    2,
                    0,
                    2,
                    50,
                    0
                ],
                "title": "CARE: Confidence-Aware Regression Estimation of building density\n  fine-tuning EO Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARE: Confidence-Aware Regression Estimation of building density\n  fine-tuning EO Foundation Models"
                },
                "summary": "Performing accurate confidence quantification and assessment in pixel-wise\nregression tasks, which are downstream applications of AI Foundation Models for\nEarth Observation (EO), is important for deep neural networks to predict their\nfailures, improve their performance and enhance their capabilities in\nreal-world applications, for their practical deployment. For pixel-wise\nregression tasks, specifically utilizing remote sensing data from satellite\nimagery in EO Foundation Models, confidence quantification is a critical\nchallenge. The focus of this research work is on developing a Foundation Model\nusing EO satellite data that computes and assigns a confidence metric alongside\nregression outputs to improve the reliability and interpretability of\npredictions generated by deep neural networks. To this end, we develop, train\nand evaluate the proposed Confidence-Aware Regression Estimation (CARE)\nFoundation Model. Our model CARE computes and assigns confidence to regression\nresults as downstream tasks of a Foundation Model for EO data, and performs a\nconfidence-aware self-corrective learning method for the low-confidence\nregions. We evaluate the model CARE, and experimental results on multi-spectral\ndata from the Copernicus Sentinel-2 satellite constellation to estimate the\nbuilding density (i.e. monitoring urban growth), show that the proposed method\ncan be successfully applied to important regression problems in EO and remote\nsensing. We also show that our model CARE outperforms other baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performing accurate confidence quantification and assessment in pixel-wise\nregression tasks, which are downstream applications of AI Foundation Models for\nEarth Observation (EO), is important for deep neural networks to predict their\nfailures, improve their performance and enhance their capabilities in\nreal-world applications, for their practical deployment. For pixel-wise\nregression tasks, specifically utilizing remote sensing data from satellite\nimagery in EO Foundation Models, confidence quantification is a critical\nchallenge. The focus of this research work is on developing a Foundation Model\nusing EO satellite data that computes and assigns a confidence metric alongside\nregression outputs to improve the reliability and interpretability of\npredictions generated by deep neural networks. To this end, we develop, train\nand evaluate the proposed Confidence-Aware Regression Estimation (CARE)\nFoundation Model. Our model CARE computes and assigns confidence to regression\nresults as downstream tasks of a Foundation Model for EO data, and performs a\nconfidence-aware self-corrective learning method for the low-confidence\nregions. We evaluate the model CARE, and experimental results on multi-spectral\ndata from the Copernicus Sentinel-2 satellite constellation to estimate the\nbuilding density (i.e. monitoring urban growth), show that the proposed method\ncan be successfully applied to important regression problems in EO and remote\nsensing. We also show that our model CARE outperforms other baseline methods."
                },
                "authors": [
                    {
                        "name": "Nikolaos Dionelis"
                    },
                    {
                        "name": "Jente Bosmans"
                    },
                    {
                        "name": "Nicolas Longépé"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Longépé"
                },
                "author": "Nicolas Longépé",
                "arxiv_comment": "7 pages, 4 figures, Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13734v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13734v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v1",
                "updated": "2025-04-03T09:58:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15289v2",
                "updated": "2025-04-03T09:56:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    56,
                    4,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-19T15:09:39Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    9,
                    39,
                    2,
                    78,
                    0
                ],
                "title": "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence\n  Tracing and Relationship Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence\n  Tracing and Relationship Classification"
                },
                "summary": "LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains such as healthcare, law, and news, it is\ncrucial to understand where and how the content is created. To address this, we\nintroduce the Text pROVEnance (TROVE) challenge, designed to trace each\nsentence of a target text back to specific source sentences within potentially\nlengthy or multi-document inputs. Beyond identifying sources, TROVE annotates\nthe fine-grained relationships (quotation, compression, inference, and others),\nproviding a deep understanding of how each target sentence is formed. To\nbenchmark TROVE, we construct our dataset by leveraging three public datasets\ncovering 11 diverse scenarios (e.g., QA and summarization) in English and\nChinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+),\nemphasizing the multi-document and long-document settings essential for\nprovenance. To ensure high-quality data, we employ a three-stage annotation\nprocess: sentence retrieval, GPT provenance, and human provenance. We evaluate\n11 LLMs under direct prompting and retrieval-augmented paradigms, revealing\nthat retrieval is essential for robust performance, larger models perform\nbetter in complex relationship classification, and closed-source models often\nlead, yet open-source models show significant promise, particularly with\nretrieval augmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains such as healthcare, law, and news, it is\ncrucial to understand where and how the content is created. To address this, we\nintroduce the Text pROVEnance (TROVE) challenge, designed to trace each\nsentence of a target text back to specific source sentences within potentially\nlengthy or multi-document inputs. Beyond identifying sources, TROVE annotates\nthe fine-grained relationships (quotation, compression, inference, and others),\nproviding a deep understanding of how each target sentence is formed. To\nbenchmark TROVE, we construct our dataset by leveraging three public datasets\ncovering 11 diverse scenarios (e.g., QA and summarization) in English and\nChinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+),\nemphasizing the multi-document and long-document settings essential for\nprovenance. To ensure high-quality data, we employ a three-stage annotation\nprocess: sentence retrieval, GPT provenance, and human provenance. We evaluate\n11 LLMs under direct prompting and retrieval-augmented paradigms, revealing\nthat retrieval is essential for robust performance, larger models perform\nbetter in complex relationship classification, and closed-source models often\nlead, yet open-source models show significant promise, particularly with\nretrieval augmentation."
                },
                "authors": [
                    {
                        "name": "Junnan Zhu"
                    },
                    {
                        "name": "Min Xiao"
                    },
                    {
                        "name": "Yining Wang"
                    },
                    {
                        "name": "Feifei Zhai"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Chengqing Zong"
                    }
                ],
                "author_detail": {
                    "name": "Chengqing Zong"
                },
                "author": "Chengqing Zong",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11167v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11167v3",
                "updated": "2025-04-03T09:54:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    54,
                    20,
                    3,
                    93,
                    0
                ],
                "published": "2025-02-16T15:38:19Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    15,
                    38,
                    19,
                    6,
                    47,
                    0
                ],
                "title": "SURGE: On the Potential of Large Language Models as General-Purpose\n  Surrogate Code Executors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SURGE: On the Potential of Large Language Models as General-Purpose\n  Surrogate Code Executors"
                },
                "summary": "Neural surrogate models have emerged as powerful and efficient tools in data\nmining. Meanwhile, large language models (LLMs) have demonstrated remarkable\ncapabilities in code-related tasks. We investigate a novel application: using\nLLMs as surrogate models for code execution prediction. Given LLMs' unique\nability to understand and process diverse programs, they present a promising\ndirection for building general-purpose surrogate models. To systematically\ninvestigate this capability, we introduce SURGE, a comprehensive benchmark with\n$1160$ problems covering $8$ key aspects: multi-language programming tasks,\ncompetition-level programming problems, repository-level code analysis,\nhigh-cost scientific computing, time-complexity-intensive algorithms, buggy\ncode analysis, programs dependent on specific compilers or execution\nenvironments, and formal mathematical proof verification. Through extensive\nempirical analysis of $21$ open-source and proprietary LLMs, we examine scaling\nlaws, data efficiency, and predictive accuracy. Our findings reveal important\ninsights about the feasibility of LLMs as efficient surrogates for\ncomputational processes, with implications for automated software testing,\nprogram analysis, and computational resource optimization in data mining\napplications. Code and dataset are released at\nhttps://github.com/Imbernoulli/SURGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural surrogate models have emerged as powerful and efficient tools in data\nmining. Meanwhile, large language models (LLMs) have demonstrated remarkable\ncapabilities in code-related tasks. We investigate a novel application: using\nLLMs as surrogate models for code execution prediction. Given LLMs' unique\nability to understand and process diverse programs, they present a promising\ndirection for building general-purpose surrogate models. To systematically\ninvestigate this capability, we introduce SURGE, a comprehensive benchmark with\n$1160$ problems covering $8$ key aspects: multi-language programming tasks,\ncompetition-level programming problems, repository-level code analysis,\nhigh-cost scientific computing, time-complexity-intensive algorithms, buggy\ncode analysis, programs dependent on specific compilers or execution\nenvironments, and formal mathematical proof verification. Through extensive\nempirical analysis of $21$ open-source and proprietary LLMs, we examine scaling\nlaws, data efficiency, and predictive accuracy. Our findings reveal important\ninsights about the feasibility of LLMs as efficient surrogates for\ncomputational processes, with implications for automated software testing,\nprogram analysis, and computational resource optimization in data mining\napplications. Code and dataset are released at\nhttps://github.com/Imbernoulli/SURGE."
                },
                "authors": [
                    {
                        "name": "Bohan Lyu"
                    },
                    {
                        "name": "Siqiao Huang"
                    },
                    {
                        "name": "Zichen Liang"
                    },
                    {
                        "name": "Qi-An Sun"
                    },
                    {
                        "name": "Jiaming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaming Zhang"
                },
                "author": "Jiaming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11167v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11167v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02429v1",
                "updated": "2025-04-03T09:35:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    35,
                    7,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:35:07Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    35,
                    7,
                    3,
                    93,
                    0
                ],
                "title": "A Multi-Level Sentiment Analysis Framework for Financial Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Level Sentiment Analysis Framework for Financial Texts"
                },
                "summary": "Existing financial sentiment analysis methods often fail to capture the\nmulti-faceted nature of risk in bond markets due to their single-level approach\nand neglect of temporal dynamics. We propose Multi-Level Sentiment Analysis\nbased on pre-trained language models (PLMs) and large language models (LLMs), a\nnovel framework that systematically integrates firm-specific micro-level\nsentiment, industry-specific meso-level sentiment, and duration-aware smoothing\nto model the latency and persistence of textual impact. Applying our framework\nto the comprehensive Chinese bond market corpus constructed by us (2013-2023,\n1.39M texts), we extracted a daily composite sentiment index. Empirical results\nshow statistically measurable improvements in credit spread forecasting when\nincorporating sentiment (3.25% MAE and 10.96% MAPE reduction), with sentiment\nshifts closely correlating with major social risk events and firm-specific\ncrises. This framework provides a more nuanced understanding of sentiment\nacross different market levels while accounting for the temporal evolution of\nsentiment effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing financial sentiment analysis methods often fail to capture the\nmulti-faceted nature of risk in bond markets due to their single-level approach\nand neglect of temporal dynamics. We propose Multi-Level Sentiment Analysis\nbased on pre-trained language models (PLMs) and large language models (LLMs), a\nnovel framework that systematically integrates firm-specific micro-level\nsentiment, industry-specific meso-level sentiment, and duration-aware smoothing\nto model the latency and persistence of textual impact. Applying our framework\nto the comprehensive Chinese bond market corpus constructed by us (2013-2023,\n1.39M texts), we extracted a daily composite sentiment index. Empirical results\nshow statistically measurable improvements in credit spread forecasting when\nincorporating sentiment (3.25% MAE and 10.96% MAPE reduction), with sentiment\nshifts closely correlating with major social risk events and firm-specific\ncrises. This framework provides a more nuanced understanding of sentiment\nacross different market levels while accounting for the temporal evolution of\nsentiment effects."
                },
                "authors": [
                    {
                        "name": "Yiwei Liu"
                    },
                    {
                        "name": "Junbo Wang"
                    },
                    {
                        "name": "Lei Long"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Ruiting Ma"
                    },
                    {
                        "name": "Yuankai Wu"
                    },
                    {
                        "name": "Xuebin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xuebin Chen"
                },
                "author": "Xuebin Chen",
                "arxiv_comment": "14pages, 11 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02426v1",
                "updated": "2025-04-03T09:31:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    31,
                    7,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:31:07Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    31,
                    7,
                    3,
                    93,
                    0
                ],
                "title": "Narrative Studio: Visual narrative exploration using LLMs and Monte\n  Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Narrative Studio: Visual narrative exploration using LLMs and Monte\n  Carlo Tree Search"
                },
                "summary": "Interactive storytelling benefits from planning and exploring multiple 'what\nif' scenarios. Modern LLMs are useful tools for ideation and exploration, but\ncurrent chat-based user interfaces restrict users to a single linear flow. To\naddress this limitation, we propose Narrative Studio -- a novel in-browser\nnarrative exploration environment featuring a tree-like interface that allows\nbranching exploration from user-defined points in a story. Each branch is\nextended via iterative LLM inference guided by system and user-defined prompts.\nAdditionally, we employ Monte Carlo Tree Search (MCTS) to automatically expand\npromising narrative paths based on user-specified criteria, enabling more\ndiverse and robust story development. We also allow users to enhance narrative\ncoherence by grounding the generated text in an entity graph that represents\nthe actors and environment of the story.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive storytelling benefits from planning and exploring multiple 'what\nif' scenarios. Modern LLMs are useful tools for ideation and exploration, but\ncurrent chat-based user interfaces restrict users to a single linear flow. To\naddress this limitation, we propose Narrative Studio -- a novel in-browser\nnarrative exploration environment featuring a tree-like interface that allows\nbranching exploration from user-defined points in a story. Each branch is\nextended via iterative LLM inference guided by system and user-defined prompts.\nAdditionally, we employ Monte Carlo Tree Search (MCTS) to automatically expand\npromising narrative paths based on user-specified criteria, enabling more\ndiverse and robust story development. We also allow users to enhance narrative\ncoherence by grounding the generated text in an entity graph that represents\nthe actors and environment of the story."
                },
                "authors": [
                    {
                        "name": "Parsa Ghaffari"
                    },
                    {
                        "name": "Chris Hokamp"
                    }
                ],
                "author_detail": {
                    "name": "Chris Hokamp"
                },
                "author": "Chris Hokamp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02420v1",
                "updated": "2025-04-03T09:21:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    21,
                    48,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:21:48Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    21,
                    48,
                    3,
                    93,
                    0
                ],
                "title": "On learning racing policies with reinforcement learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On learning racing policies with reinforcement learning"
                },
                "summary": "Fully autonomous vehicles promise enhanced safety and efficiency. However,\nensuring reliable operation in challenging corner cases requires control\nalgorithms capable of performing at the vehicle limits. We address this\nrequirement by considering the task of autonomous racing and propose solving it\nby learning a racing policy using Reinforcement Learning (RL). Our approach\nleverages domain randomization, actuator dynamics modeling, and policy\narchitecture design to enable reliable and safe zero-shot deployment on a real\nplatform. Evaluated on the F1TENTH race car, our RL policy not only surpasses a\nstate-of-the-art Model Predictive Control (MPC), but, to the best of our\nknowledge, also represents the first instance of an RL policy outperforming\nexpert human drivers in RC racing. This work identifies the key factors driving\nthis performance improvement, providing critical insights for the design of\nrobust RL-based control strategies for autonomous vehicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully autonomous vehicles promise enhanced safety and efficiency. However,\nensuring reliable operation in challenging corner cases requires control\nalgorithms capable of performing at the vehicle limits. We address this\nrequirement by considering the task of autonomous racing and propose solving it\nby learning a racing policy using Reinforcement Learning (RL). Our approach\nleverages domain randomization, actuator dynamics modeling, and policy\narchitecture design to enable reliable and safe zero-shot deployment on a real\nplatform. Evaluated on the F1TENTH race car, our RL policy not only surpasses a\nstate-of-the-art Model Predictive Control (MPC), but, to the best of our\nknowledge, also represents the first instance of an RL policy outperforming\nexpert human drivers in RC racing. This work identifies the key factors driving\nthis performance improvement, providing critical insights for the design of\nrobust RL-based control strategies for autonomous vehicles."
                },
                "authors": [
                    {
                        "name": "Grzegorz Czechmanowski"
                    },
                    {
                        "name": "Jan Węgrzynowski"
                    },
                    {
                        "name": "Piotr Kicki"
                    },
                    {
                        "name": "Krzysztof Walas"
                    }
                ],
                "author_detail": {
                    "name": "Krzysztof Walas"
                },
                "author": "Krzysztof Walas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22036v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22036v2",
                "updated": "2025-04-03T09:08:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    8,
                    48,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-27T23:06:30Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    23,
                    6,
                    30,
                    3,
                    86,
                    0
                ],
                "title": "Cognitive Prompts Using Guilford's Structure of Intellect Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Prompts Using Guilford's Structure of Intellect Model"
                },
                "summary": "Large language models (LLMs) demonstrate strong language generation\ncapabilities but often struggle with structured reasoning, leading to\ninconsistent or suboptimal problem-solving. To mitigate this limitation,\nGuilford's Structure of Intellect (SOI) model - a foundational framework from\nintelligence theory - is leveraged as the basis for cognitive prompt\nengineering. The SOI model categorizes cognitive operations such as pattern\nrecognition, memory retrieval, and evaluation, offering a systematic approach\nto enhancing LLM reasoning and decision-making. This position paper presents a\nnovel cognitive prompting approach for enforcing SOI-inspired reasoning for\nimproving clarity, coherence, and adaptability in model responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate strong language generation\ncapabilities but often struggle with structured reasoning, leading to\ninconsistent or suboptimal problem-solving. To mitigate this limitation,\nGuilford's Structure of Intellect (SOI) model - a foundational framework from\nintelligence theory - is leveraged as the basis for cognitive prompt\nengineering. The SOI model categorizes cognitive operations such as pattern\nrecognition, memory retrieval, and evaluation, offering a systematic approach\nto enhancing LLM reasoning and decision-making. This position paper presents a\nnovel cognitive prompting approach for enforcing SOI-inspired reasoning for\nimproving clarity, coherence, and adaptability in model responses."
                },
                "authors": [
                    {
                        "name": "Oliver Kramer"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Kramer"
                },
                "author": "Oliver Kramer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22036v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22036v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14614v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14614v3",
                "updated": "2025-04-03T09:07:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    7,
                    7,
                    3,
                    93,
                    0
                ],
                "published": "2025-02-20T14:52:36Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    52,
                    36,
                    3,
                    51,
                    0
                ],
                "title": "FIND: Fine-grained Information Density Guided Adaptive\n  Retrieval-Augmented Generation for Disease Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIND: Fine-grained Information Density Guided Adaptive\n  Retrieval-Augmented Generation for Disease Diagnosis"
                },
                "summary": "Retrieval-Augmented Large Language Models (LLMs), which integrate external\nknowledge into LLMs, have shown remarkable performance in various medical\ndomains, including clinical diagnosis. However, existing RAG methods struggle\nto effectively assess task difficulty to make retrieval decisions, thereby\nfailing to meet the clinical requirements for balancing efficiency and\naccuracy. So in this paper, we propose FIND (\\textbf{F}ine-grained\n\\textbf{In}formation \\textbf{D}ensity Guided Adaptive RAG), a novel framework\nthat improves the reliability of RAG in disease diagnosis scenarios. FIND\nincorporates a fine-grained adaptive control module to determine whether\nretrieval is necessary based on the information density of the input. By\noptimizing the retrieval process and implementing a knowledge filtering module,\nFIND ensures that the retrieval is better suited to clinical scenarios.\nExperiments on three Chinese electronic medical record datasets demonstrate\nthat FIND significantly outperforms various baseline methods, highlighting its\neffectiveness in clinical diagnosis tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Large Language Models (LLMs), which integrate external\nknowledge into LLMs, have shown remarkable performance in various medical\ndomains, including clinical diagnosis. However, existing RAG methods struggle\nto effectively assess task difficulty to make retrieval decisions, thereby\nfailing to meet the clinical requirements for balancing efficiency and\naccuracy. So in this paper, we propose FIND (\\textbf{F}ine-grained\n\\textbf{In}formation \\textbf{D}ensity Guided Adaptive RAG), a novel framework\nthat improves the reliability of RAG in disease diagnosis scenarios. FIND\nincorporates a fine-grained adaptive control module to determine whether\nretrieval is necessary based on the information density of the input. By\noptimizing the retrieval process and implementing a knowledge filtering module,\nFIND ensures that the retrieval is better suited to clinical scenarios.\nExperiments on three Chinese electronic medical record datasets demonstrate\nthat FIND significantly outperforms various baseline methods, highlighting its\neffectiveness in clinical diagnosis tasks."
                },
                "authors": [
                    {
                        "name": "Mingyi Jia"
                    },
                    {
                        "name": "Junwen Duan"
                    },
                    {
                        "name": "Yan Song"
                    },
                    {
                        "name": "Jianxin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Wang"
                },
                "author": "Jianxin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14614v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14614v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15127v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15127v3",
                "updated": "2025-04-03T09:05:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    5,
                    45,
                    3,
                    93,
                    0
                ],
                "published": "2024-09-23T15:33:38Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    33,
                    38,
                    0,
                    267,
                    0
                ],
                "title": "Pareto-Optimized Open-Source LLMs for Healthcare via Context Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pareto-Optimized Open-Source LLMs for Healthcare via Context Retrieval"
                },
                "summary": "This study leverages optimized context retrieval to enhance open-source Large\nLanguage Models (LLMs) for cost-effective, high performance healthcare AI. We\ndemonstrate that this approach achieves state-of-the-art accuracy on medical\nquestion answering at a fraction of the cost of proprietary models,\nsignificantly improving the cost-accuracy Pareto frontier on the MedQA\nbenchmark. Key contributions include: (1) OpenMedQA, a novel benchmark\nrevealing a performance gap in open-ended medical QA compared to\nmultiple-choice formats; (2) a practical, reproducible pipeline for context\nretrieval optimization; and (3) open-source resources (Prompt Engine,\nCoT/ToT/Thinking databases) to empower healthcare AI development. By advancing\nretrieval techniques and QA evaluation, we enable more affordable and reliable\nLLM solutions for healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study leverages optimized context retrieval to enhance open-source Large\nLanguage Models (LLMs) for cost-effective, high performance healthcare AI. We\ndemonstrate that this approach achieves state-of-the-art accuracy on medical\nquestion answering at a fraction of the cost of proprietary models,\nsignificantly improving the cost-accuracy Pareto frontier on the MedQA\nbenchmark. Key contributions include: (1) OpenMedQA, a novel benchmark\nrevealing a performance gap in open-ended medical QA compared to\nmultiple-choice formats; (2) a practical, reproducible pipeline for context\nretrieval optimization; and (3) open-source resources (Prompt Engine,\nCoT/ToT/Thinking databases) to empower healthcare AI development. By advancing\nretrieval techniques and QA evaluation, we enable more affordable and reliable\nLLM solutions for healthcare."
                },
                "authors": [
                    {
                        "name": "Jordi Bayarri-Planas"
                    },
                    {
                        "name": "Ashwin Kumar Gururajan"
                    },
                    {
                        "name": "Dario Garcia-Gasulla"
                    }
                ],
                "author_detail": {
                    "name": "Dario Garcia-Gasulla"
                },
                "author": "Dario Garcia-Gasulla",
                "arxiv_comment": "14 pages, 3 figures, 5 tables, Accepted for publication at the 21st\n  International Conference on Artificial Intelligence Applications and\n  Innovations (AIAI 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15127v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15127v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02411v1",
                "updated": "2025-04-03T09:03:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    3,
                    40,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:03:40Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    3,
                    40,
                    3,
                    93,
                    0
                ],
                "title": "Adapting Large Language Models for Multi-Domain\n  Retrieval-Augmented-Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Large Language Models for Multi-Domain\n  Retrieval-Augmented-Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enhances LLM factuality, but\nmulti-domain applications face challenges like lack of diverse benchmarks and\npoor out-of-domain generalization. The first contribution of this work is to\nintroduce a diverse benchmark comprising a variety of question-answering tasks\nfrom 8 sources and covering 13 domains. Our second contribution consists in\nsystematically testing out-of-domain generalization for typical RAG tuning\nstrategies. While our findings reveal that standard fine-tuning fails to\ngeneralize effectively, we show that sequence-level distillation with\nteacher-generated labels improves out-of-domain performance by providing more\ncoherent supervision. Our findings highlight key strategies for improving\nmulti-domain RAG robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enhances LLM factuality, but\nmulti-domain applications face challenges like lack of diverse benchmarks and\npoor out-of-domain generalization. The first contribution of this work is to\nintroduce a diverse benchmark comprising a variety of question-answering tasks\nfrom 8 sources and covering 13 domains. Our second contribution consists in\nsystematically testing out-of-domain generalization for typical RAG tuning\nstrategies. While our findings reveal that standard fine-tuning fails to\ngeneralize effectively, we show that sequence-level distillation with\nteacher-generated labels improves out-of-domain performance by providing more\ncoherent supervision. Our findings highlight key strategies for improving\nmulti-domain RAG robustness."
                },
                "authors": [
                    {
                        "name": "Alexandre Misrahi"
                    },
                    {
                        "name": "Nadezhda Chirkova"
                    },
                    {
                        "name": "Maxime Louis"
                    },
                    {
                        "name": "Vassilina Nikoulina"
                    }
                ],
                "author_detail": {
                    "name": "Vassilina Nikoulina"
                },
                "author": "Vassilina Nikoulina",
                "arxiv_comment": "25 pages, 8 figures, 21 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02404v1",
                "updated": "2025-04-03T08:54:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    54,
                    23,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T08:54:23Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    54,
                    23,
                    3,
                    93,
                    0
                ],
                "title": "AnesBench: Multi-Dimensional Evaluation of LLM Reasoning in\n  Anesthesiology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnesBench: Multi-Dimensional Evaluation of LLM Reasoning in\n  Anesthesiology"
                },
                "summary": "The application of large language models (LLMs) in the medical field has\ngained significant attention, yet their reasoning capabilities in more\nspecialized domains like anesthesiology remain underexplored. In this paper, we\nsystematically evaluate the reasoning capabilities of LLMs in anesthesiology\nand analyze key factors influencing their performance. To this end, we\nintroduce AnesBench, a cross-lingual benchmark designed to assess\nanesthesiology-related reasoning across three levels: factual retrieval (System\n1), hybrid reasoning (System 1.x), and complex decision-making (System 2).\nThrough extensive experiments, we first explore how model characteristics,\nincluding model scale, Chain of Thought (CoT) length, and language\ntransferability, affect reasoning performance. Then, we further evaluate the\neffectiveness of different training strategies, leveraging our curated\nanesthesiology-related dataset, including continuous pre-training (CPT) and\nsupervised fine-tuning (SFT). Additionally, we also investigate how the\ntest-time reasoning techniques, such as Best-of-N sampling and beam search,\ninfluence reasoning performance, and assess the impact of reasoning-enhanced\nmodel distillation, specifically DeepSeek-R1. We will publicly release\nAnesBench, along with our CPT and SFT training datasets and evaluation code at\nhttps://github.com/MiliLab/AnesBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of large language models (LLMs) in the medical field has\ngained significant attention, yet their reasoning capabilities in more\nspecialized domains like anesthesiology remain underexplored. In this paper, we\nsystematically evaluate the reasoning capabilities of LLMs in anesthesiology\nand analyze key factors influencing their performance. To this end, we\nintroduce AnesBench, a cross-lingual benchmark designed to assess\nanesthesiology-related reasoning across three levels: factual retrieval (System\n1), hybrid reasoning (System 1.x), and complex decision-making (System 2).\nThrough extensive experiments, we first explore how model characteristics,\nincluding model scale, Chain of Thought (CoT) length, and language\ntransferability, affect reasoning performance. Then, we further evaluate the\neffectiveness of different training strategies, leveraging our curated\nanesthesiology-related dataset, including continuous pre-training (CPT) and\nsupervised fine-tuning (SFT). Additionally, we also investigate how the\ntest-time reasoning techniques, such as Best-of-N sampling and beam search,\ninfluence reasoning performance, and assess the impact of reasoning-enhanced\nmodel distillation, specifically DeepSeek-R1. We will publicly release\nAnesBench, along with our CPT and SFT training datasets and evaluation code at\nhttps://github.com/MiliLab/AnesBench."
                },
                "authors": [
                    {
                        "name": "Xiang Feng"
                    },
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Yong Luo"
                    },
                    {
                        "name": "Pingbo Xu"
                    },
                    {
                        "name": "Baosheng Yu"
                    },
                    {
                        "name": "Hua Jin"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang",
                "arxiv_comment": "23 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01707v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01707v2",
                "updated": "2025-04-03T08:53:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    53,
                    6,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T13:15:44Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    13,
                    15,
                    44,
                    2,
                    92,
                    0
                ],
                "title": "InfiniteICL: Breaking the Limit of Context Window Size via Long\n  Short-term Memory Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteICL: Breaking the Limit of Context Window Size via Long\n  Short-term Memory Transformation"
                },
                "summary": "In-context learning (ICL) is critical for large language models (LLMs), but\nits effectiveness is constrained by finite context windows, particularly in\nultra-long contexts. To overcome this, we introduce InfiniteICL, a framework\nthat parallels context and parameters in LLMs with short- and long-term memory\nin human cognitive systems, focusing on transforming temporary context\nknowledge into permanent parameter updates. This approach significantly reduces\nmemory usage, maintains robust performance across varying input lengths, and\ntheoretically enables infinite context integration through the principles of\ncontext knowledge elicitation, selection, and consolidation. Evaluations\ndemonstrate that our method reduces context length by 90% while achieving 103%\naverage performance of full-context prompting across fact recall, grounded\nreasoning, and skill acquisition tasks. When conducting sequential multi-turn\ntransformations on complex, real-world contexts (with length up to 2M tokens),\nour approach surpasses full-context prompting while using only 0.4% of the\noriginal contexts. These findings highlight InfiniteICL's potential to enhance\nthe scalability and efficiency of LLMs by breaking the limitations of\nconventional context window sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) is critical for large language models (LLMs), but\nits effectiveness is constrained by finite context windows, particularly in\nultra-long contexts. To overcome this, we introduce InfiniteICL, a framework\nthat parallels context and parameters in LLMs with short- and long-term memory\nin human cognitive systems, focusing on transforming temporary context\nknowledge into permanent parameter updates. This approach significantly reduces\nmemory usage, maintains robust performance across varying input lengths, and\ntheoretically enables infinite context integration through the principles of\ncontext knowledge elicitation, selection, and consolidation. Evaluations\ndemonstrate that our method reduces context length by 90% while achieving 103%\naverage performance of full-context prompting across fact recall, grounded\nreasoning, and skill acquisition tasks. When conducting sequential multi-turn\ntransformations on complex, real-world contexts (with length up to 2M tokens),\nour approach surpasses full-context prompting while using only 0.4% of the\noriginal contexts. These findings highlight InfiniteICL's potential to enhance\nthe scalability and efficiency of LLMs by breaking the limitations of\nconventional context window sizes."
                },
                "authors": [
                    {
                        "name": "Bowen Cao"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01707v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01707v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02403v1",
                "updated": "2025-04-03T08:52:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    52,
                    42,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T08:52:42Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    52,
                    42,
                    3,
                    93,
                    0
                ],
                "title": "DaKultur: Evaluating the Cultural Awareness of Language Models for\n  Danish with Native Speakers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DaKultur: Evaluating the Cultural Awareness of Language Models for\n  Danish with Native Speakers"
                },
                "summary": "Large Language Models (LLMs) have seen widespread societal adoption. However,\nwhile they are able to interact with users in languages beyond English, they\nhave been shown to lack cultural awareness, providing anglocentric or\ninappropriate responses for underrepresented language communities. To\ninvestigate this gap and disentangle linguistic versus cultural proficiency, we\nconduct the first cultural evaluation study for the mid-resource language of\nDanish, in which native speakers prompt different models to solve tasks\nrequiring cultural awareness. Our analysis of the resulting 1,038 interactions\nfrom 63 demographically diverse participants highlights open challenges to\ncultural adaptation: Particularly, how currently employed automatically\ntranslated data are insufficient to train or measure cultural adaptation, and\nhow training on native-speaker data can more than double response acceptance\nrates. We release our study data as DaKultur - the first native Danish cultural\nawareness dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have seen widespread societal adoption. However,\nwhile they are able to interact with users in languages beyond English, they\nhave been shown to lack cultural awareness, providing anglocentric or\ninappropriate responses for underrepresented language communities. To\ninvestigate this gap and disentangle linguistic versus cultural proficiency, we\nconduct the first cultural evaluation study for the mid-resource language of\nDanish, in which native speakers prompt different models to solve tasks\nrequiring cultural awareness. Our analysis of the resulting 1,038 interactions\nfrom 63 demographically diverse participants highlights open challenges to\ncultural adaptation: Particularly, how currently employed automatically\ntranslated data are insufficient to train or measure cultural adaptation, and\nhow training on native-speaker data can more than double response acceptance\nrates. We release our study data as DaKultur - the first native Danish cultural\nawareness dataset."
                },
                "authors": [
                    {
                        "name": "Max Müller-Eberstein"
                    },
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Elisa Bassignana"
                    },
                    {
                        "name": "Peter Brunsgaard Trolle"
                    },
                    {
                        "name": "Rob van der Goot"
                    }
                ],
                "author_detail": {
                    "name": "Rob van der Goot"
                },
                "author": "Rob van der Goot",
                "arxiv_comment": "Accepted at C3NLP at NAACL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02395v1",
                "updated": "2025-04-03T08:41:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    41,
                    26,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T08:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    41,
                    26,
                    3,
                    93,
                    0
                ],
                "title": "The quasi-semantic competence of LLMs: a case study on the part-whole\n  relation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quasi-semantic competence of LLMs: a case study on the part-whole\n  relation"
                },
                "summary": "Understanding the extent and depth of the semantic competence of \\emph{Large\nLanguage Models} (LLMs) is at the center of the current scientific agenda in\nArtificial Intelligence (AI) and Computational Linguistics (CL). We contribute\nto this endeavor by investigating their knowledge of the \\emph{part-whole}\nrelation, a.k.a. \\emph{meronymy}, which plays a crucial role in lexical\norganization, but it is significantly understudied. We used data from\nConceptNet relations \\citep{speer2016conceptnet} and human-generated semantic\nfeature norms \\citep{McRae:2005} to explore the abilities of LLMs to deal with\n\\textit{part-whole} relations. We employed several methods based on three\nlevels of analysis: i.) \\textbf{behavioral} testing via prompting, where we\ndirectly queried the models on their knowledge of meronymy, ii.) sentence\n\\textbf{probability} scoring, where we tested models' abilities to discriminate\ncorrect (real) and incorrect (asymmetric counterfactual) \\textit{part-whole}\nrelations, and iii.) \\textbf{concept representation} analysis in vector space,\nwhere we proved the linear organization of the \\textit{part-whole} concept in\nthe embedding and unembedding spaces. These analyses present a complex picture\nthat reveals that the LLMs' knowledge of this relation is only partial. They\nhave just a ``\\emph{quasi}-semantic'' competence and still fall short of\ncapturing deep inferential properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the extent and depth of the semantic competence of \\emph{Large\nLanguage Models} (LLMs) is at the center of the current scientific agenda in\nArtificial Intelligence (AI) and Computational Linguistics (CL). We contribute\nto this endeavor by investigating their knowledge of the \\emph{part-whole}\nrelation, a.k.a. \\emph{meronymy}, which plays a crucial role in lexical\norganization, but it is significantly understudied. We used data from\nConceptNet relations \\citep{speer2016conceptnet} and human-generated semantic\nfeature norms \\citep{McRae:2005} to explore the abilities of LLMs to deal with\n\\textit{part-whole} relations. We employed several methods based on three\nlevels of analysis: i.) \\textbf{behavioral} testing via prompting, where we\ndirectly queried the models on their knowledge of meronymy, ii.) sentence\n\\textbf{probability} scoring, where we tested models' abilities to discriminate\ncorrect (real) and incorrect (asymmetric counterfactual) \\textit{part-whole}\nrelations, and iii.) \\textbf{concept representation} analysis in vector space,\nwhere we proved the linear organization of the \\textit{part-whole} concept in\nthe embedding and unembedding spaces. These analyses present a complex picture\nthat reveals that the LLMs' knowledge of this relation is only partial. They\nhave just a ``\\emph{quasi}-semantic'' competence and still fall short of\ncapturing deep inferential properties."
                },
                "authors": [
                    {
                        "name": "Mattia Proietti"
                    },
                    {
                        "name": "Alessandro Lenci"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Lenci"
                },
                "author": "Alessandro Lenci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17233v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17233v3",
                "updated": "2025-04-03T08:27:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    27,
                    46,
                    3,
                    93,
                    0
                ],
                "published": "2024-10-22T17:53:34Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    17,
                    53,
                    34,
                    1,
                    296,
                    0
                ],
                "title": "ICPL: Few-shot In-context Preference Learning via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICPL: Few-shot In-context Preference Learning via LLMs"
                },
                "summary": "Preference-based reinforcement learning is an effective way to handle tasks\nwhere rewards are hard to specify but can be exceedingly inefficient as\npreference learning is often tabula rasa. We demonstrate that Large Language\nModels (LLMs) have native preference-learning capabilities that allow them to\nachieve sample-efficient preference learning, addressing this challenge. We\npropose In-Context Preference Learning (ICPL), which uses in-context learning\ncapabilities of LLMs to reduce human query inefficiency. ICPL uses the task\ndescription and basic environment code to create sets of reward functions which\nare iteratively refined by placing human feedback over videos of the resultant\npolicies into the context of an LLM and then requesting better rewards. We\nfirst demonstrate ICPL's effectiveness through a synthetic preference study,\nproviding quantitative evidence that it significantly outperforms baseline\npreference-based methods with much higher performance and orders of magnitude\ngreater efficiency. We observe that these improvements are not solely coming\nfrom LLM grounding in the task but that the quality of the rewards improves\nover time, indicating preference learning capabilities. Additionally, we\nperform a series of real human preference-learning trials and observe that ICPL\nextends beyond synthetic settings and can work effectively with\nhumans-in-the-loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference-based reinforcement learning is an effective way to handle tasks\nwhere rewards are hard to specify but can be exceedingly inefficient as\npreference learning is often tabula rasa. We demonstrate that Large Language\nModels (LLMs) have native preference-learning capabilities that allow them to\nachieve sample-efficient preference learning, addressing this challenge. We\npropose In-Context Preference Learning (ICPL), which uses in-context learning\ncapabilities of LLMs to reduce human query inefficiency. ICPL uses the task\ndescription and basic environment code to create sets of reward functions which\nare iteratively refined by placing human feedback over videos of the resultant\npolicies into the context of an LLM and then requesting better rewards. We\nfirst demonstrate ICPL's effectiveness through a synthetic preference study,\nproviding quantitative evidence that it significantly outperforms baseline\npreference-based methods with much higher performance and orders of magnitude\ngreater efficiency. We observe that these improvements are not solely coming\nfrom LLM grounding in the task but that the quality of the rewards improves\nover time, indicating preference learning capabilities. Additionally, we\nperform a series of real human preference-learning trials and observe that ICPL\nextends beyond synthetic settings and can work effectively with\nhumans-in-the-loop."
                },
                "authors": [
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Qixin Tan"
                    },
                    {
                        "name": "Hong Lu"
                    },
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Xinting Yang"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Eugene Vinitsky"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Vinitsky"
                },
                "author": "Eugene Vinitsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17233v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17233v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11377v2",
                "updated": "2025-04-03T08:22:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    22,
                    27,
                    3,
                    93,
                    0
                ],
                "published": "2024-10-15T08:16:43Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    16,
                    43,
                    1,
                    289,
                    0
                ],
                "title": "A Framework for Adapting Human-Robot Interaction to Diverse User Groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Adapting Human-Robot Interaction to Diverse User Groups"
                },
                "summary": "To facilitate natural and intuitive interactions with diverse user groups in\nreal-world settings, social robots must be capable of addressing the varying\nrequirements and expectations of these groups while adapting their behavior\nbased on user feedback. While previous research often focuses on specific\ndemographics, we present a novel framework for adaptive Human-Robot Interaction\n(HRI) that tailors interactions to different user groups and enables individual\nusers to modulate interactions through both minor and major interruptions. Our\nprimary contributions include the development of an adaptive, ROS-based HRI\nframework with an open-source code base. This framework supports natural\ninteractions through advanced speech recognition and voice activity detection,\nand leverages a large language model (LLM) as a dialogue bridge. We validate\nthe efficiency of our framework through module tests and system trials,\ndemonstrating its high accuracy in age recognition and its robustness to\nrepeated user inputs and plan changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To facilitate natural and intuitive interactions with diverse user groups in\nreal-world settings, social robots must be capable of addressing the varying\nrequirements and expectations of these groups while adapting their behavior\nbased on user feedback. While previous research often focuses on specific\ndemographics, we present a novel framework for adaptive Human-Robot Interaction\n(HRI) that tailors interactions to different user groups and enables individual\nusers to modulate interactions through both minor and major interruptions. Our\nprimary contributions include the development of an adaptive, ROS-based HRI\nframework with an open-source code base. This framework supports natural\ninteractions through advanced speech recognition and voice activity detection,\nand leverages a large language model (LLM) as a dialogue bridge. We validate\nthe efficiency of our framework through module tests and system trials,\ndemonstrating its high accuracy in age recognition and its robustness to\nrepeated user inputs and plan changes."
                },
                "authors": [
                    {
                        "name": "Theresa Pekarek Rosin"
                    },
                    {
                        "name": "Vanessa Hassouna"
                    },
                    {
                        "name": "Xiaowen Sun"
                    },
                    {
                        "name": "Luca Krohm"
                    },
                    {
                        "name": "Henri-Leon Kordt"
                    },
                    {
                        "name": "Michael Beetz"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_doi": "10.1007/978-981-96-3525-2_3",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-981-96-3525-2_3",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.11377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the Proceedings of the 16th International Conference on\n  Social Robotics (ICSR) 2024",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02357v1",
                "updated": "2025-04-03T07:45:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    7,
                    45,
                    9,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T07:45:09Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    7,
                    45,
                    9,
                    3,
                    93,
                    0
                ],
                "title": "ReuseDroid: A VLM-empowered Android UI Test Migrator Boosted by Active\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReuseDroid: A VLM-empowered Android UI Test Migrator Boosted by Active\n  Feedback"
                },
                "summary": "GUI testing is an essential quality assurance process in mobile app\ndevelopment. However, the creation and maintenance of GUI tests for mobile apps\nare resource-intensive and costly. Recognizing that many apps share similar\nfunctionalities, researchers have proposed various techniques to migrate GUI\ntests from one app to another with similar features. For example, some\ntechniques employ mapping-based approaches to align the GUI elements traversed\nby the tests of a source app to those present in the target app. Other test\nmigration techniques have also been proposed to leverage large language models\n(LLMs) by adapting the GUI tasks in source tests. However, these techniques are\nineffective in dealing with different operational logic between the source and\ntarget apps. The semantics of GUI elements may not be correctly inferred due to\nthe missing analysis of these flows. In this work, we propose REUSEDROID, a\nnovel multiagent framework for GUI test migration empowered by Large\nVision-Language Models (VLMs). REUSEDROID is powered by multiple VLM-based\nagents, each tackling a stage of the test migration process by leveraging the\nrelevant visual and textual information embedded in GUI pages. An insight of\nREUSEDROID is to migrate tests based only on the core logic shared across\nsimilar apps, while their entire operational logic could differ. We evaluate\nREUSEDROID on LinPro, a new test migration dataset that consists of 578\nmigration tasks for 39 popular apps across 4 categories. The experimental\nresult shows that REUSEDROID can successfully migrate 90.3% of the migration\ntasks, outperforming the best mapping-based and LLM-based baselines by 318.1%\nand 109.1%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI testing is an essential quality assurance process in mobile app\ndevelopment. However, the creation and maintenance of GUI tests for mobile apps\nare resource-intensive and costly. Recognizing that many apps share similar\nfunctionalities, researchers have proposed various techniques to migrate GUI\ntests from one app to another with similar features. For example, some\ntechniques employ mapping-based approaches to align the GUI elements traversed\nby the tests of a source app to those present in the target app. Other test\nmigration techniques have also been proposed to leverage large language models\n(LLMs) by adapting the GUI tasks in source tests. However, these techniques are\nineffective in dealing with different operational logic between the source and\ntarget apps. The semantics of GUI elements may not be correctly inferred due to\nthe missing analysis of these flows. In this work, we propose REUSEDROID, a\nnovel multiagent framework for GUI test migration empowered by Large\nVision-Language Models (VLMs). REUSEDROID is powered by multiple VLM-based\nagents, each tackling a stage of the test migration process by leveraging the\nrelevant visual and textual information embedded in GUI pages. An insight of\nREUSEDROID is to migrate tests based only on the core logic shared across\nsimilar apps, while their entire operational logic could differ. We evaluate\nREUSEDROID on LinPro, a new test migration dataset that consists of 578\nmigration tasks for 39 popular apps across 4 categories. The experimental\nresult shows that REUSEDROID can successfully migrate 90.3% of the migration\ntasks, outperforming the best mapping-based and LLM-based baselines by 318.1%\nand 109.1%, respectively."
                },
                "authors": [
                    {
                        "name": "Xiaolei Li"
                    },
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Yepang Liu"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    },
                    {
                        "name": "Hailong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hailong Wang"
                },
                "author": "Hailong Wang",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02351v1",
                "updated": "2025-04-03T07:38:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    7,
                    38,
                    9,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T07:38:09Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    7,
                    38,
                    9,
                    3,
                    93,
                    0
                ],
                "title": "Agglomerating Large Vision Encoders via Distillation for VFSS\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agglomerating Large Vision Encoders via Distillation for VFSS\n  Segmentation"
                },
                "summary": "The deployment of foundation models for medical imaging has demonstrated\nconsiderable success. However, their training overheads associated with\ndownstream tasks remain substantial due to the size of the image encoders\nemployed, and the inference complexity is also significantly high. Although\nlightweight variants have been obtained for these foundation models, their\nperformance is constrained by their limited model capacity and suboptimal\ntraining strategies. In order to achieve an improved tradeoff between\ncomplexity and performance, we propose a new framework to improve the\nperformance of low complexity models via knowledge distillation from multiple\nlarge medical foundation models (e.g., MedSAM, RAD-DINO, MedCLIP), each\nspecializing in different vision tasks, with the goal to effectively bridge the\nperformance gap for medical image segmentation tasks. The agglomerated model\ndemonstrates superior generalization across 12 segmentation tasks, whereas\nspecialized models require explicit training for each task. Our approach\nachieved an average performance gain of 2\\% in Dice coefficient compared to\nsimple distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of foundation models for medical imaging has demonstrated\nconsiderable success. However, their training overheads associated with\ndownstream tasks remain substantial due to the size of the image encoders\nemployed, and the inference complexity is also significantly high. Although\nlightweight variants have been obtained for these foundation models, their\nperformance is constrained by their limited model capacity and suboptimal\ntraining strategies. In order to achieve an improved tradeoff between\ncomplexity and performance, we propose a new framework to improve the\nperformance of low complexity models via knowledge distillation from multiple\nlarge medical foundation models (e.g., MedSAM, RAD-DINO, MedCLIP), each\nspecializing in different vision tasks, with the goal to effectively bridge the\nperformance gap for medical image segmentation tasks. The agglomerated model\ndemonstrates superior generalization across 12 segmentation tasks, whereas\nspecialized models require explicit training for each task. Our approach\nachieved an average performance gain of 2\\% in Dice coefficient compared to\nsimple distillation."
                },
                "authors": [
                    {
                        "name": "Chengxi Zeng"
                    },
                    {
                        "name": "Yuxuan Jiang"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Alberto Gambaruto"
                    },
                    {
                        "name": "Tilo Burghardt"
                    }
                ],
                "author_detail": {
                    "name": "Tilo Burghardt"
                },
                "author": "Tilo Burghardt",
                "arxiv_journal_ref": "IEEE / CVF Computer Vision and Pattern Recognition Conference\n  (CVPR) 2025, 2nd Efficient Large Vision Models Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09436v2",
                "updated": "2025-04-03T07:27:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    7,
                    27,
                    56,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-12T14:31:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    31,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "PromptMap: An Alternative Interaction Style for AI-Based Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptMap: An Alternative Interaction Style for AI-Based Image\n  Generation"
                },
                "summary": "Recent technological advances popularized the use of image generation among\nthe general public. Crafting effective prompts can, however, be difficult for\nnovice users. To tackle this challenge, we developed PromptMap, a new\ninteraction style for text-to-image AI that allows users to freely explore a\nvast collection of synthetic prompts through a map-like view with semantic\nzoom. PromptMap groups images visually by their semantic similarity, allowing\nusers to discover relevant examples. We evaluated PromptMap in a\nbetween-subject online study ($n=60$) and a qualitative within-subject study\n($n=12$). We found that PromptMap supported users in crafting prompts by\nproviding them with examples. We also demonstrated the feasibility of using\nLLMs to create vast example collections. Our work contributes a new interaction\nstyle that supports users unfamiliar with prompting in achieving a satisfactory\nimage output.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent technological advances popularized the use of image generation among\nthe general public. Crafting effective prompts can, however, be difficult for\nnovice users. To tackle this challenge, we developed PromptMap, a new\ninteraction style for text-to-image AI that allows users to freely explore a\nvast collection of synthetic prompts through a map-like view with semantic\nzoom. PromptMap groups images visually by their semantic similarity, allowing\nusers to discover relevant examples. We evaluated PromptMap in a\nbetween-subject online study ($n=60$) and a qualitative within-subject study\n($n=12$). We found that PromptMap supported users in crafting prompts by\nproviding them with examples. We also demonstrated the feasibility of using\nLLMs to create vast example collections. Our work contributes a new interaction\nstyle that supports users unfamiliar with prompting in achieving a satisfactory\nimage output."
                },
                "authors": [
                    {
                        "name": "Krzysztof Adamkiewicz"
                    },
                    {
                        "name": "Paweł W. Woźniak"
                    },
                    {
                        "name": "Julia Dominiak"
                    },
                    {
                        "name": "Andrzej Romanowski"
                    },
                    {
                        "name": "Jakob Karolus"
                    },
                    {
                        "name": "Stanislav Frolov"
                    }
                ],
                "author_detail": {
                    "name": "Stanislav Frolov"
                },
                "author": "Stanislav Frolov",
                "arxiv_doi": "10.1145/3708359.3712150",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708359.3712150",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.09436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to the 30th International Conference on Intelligent User\n  Interfaces (IUI '25), March 24-27, 2025, Cagliari, Italy ; Link to code\n  https://github.com/Bill2462/prompt-map",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02343v1",
                "updated": "2025-04-03T07:24:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    7,
                    24,
                    18,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T07:24:18Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    7,
                    24,
                    18,
                    3,
                    93,
                    0
                ],
                "title": "Toward General and Robust LLM-enhanced Text-attributed Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward General and Robust LLM-enhanced Text-attributed Graph Learning"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) and the proliferation of\nText-Attributed Graphs (TAGs) across various domains have positioned\nLLM-enhanced TAG learning as a critical research area. By utilizing rich graph\ndescriptions, this paradigm leverages LLMs to generate high-quality embeddings,\nthereby enhancing the representational capacity of Graph Neural Networks\n(GNNs). However, the field faces significant challenges: (1) the absence of a\nunified framework to systematize the diverse optimization perspectives arising\nfrom the complex interactions between LLMs and GNNs, and (2) the lack of a\nrobust method capable of handling real-world TAGs, which often suffer from\ntexts and edge sparsity, leading to suboptimal performance.\n  To address these challenges, we propose UltraTAG, a unified pipeline for\nLLM-enhanced TAG learning. UltraTAG provides a unified comprehensive and\ndomain-adaptive framework that not only organizes existing methodologies but\nalso paves the way for future advancements in the field. Building on this\nframework, we propose UltraTAG-S, a robust instantiation of UltraTAG designed\nto tackle the inherent sparsity issues in real-world TAGs. UltraTAG-S employs\nLLM-based text propagation and text augmentation to mitigate text sparsity,\nwhile leveraging LLM-augmented node selection techniques based on PageRank and\nedge reconfiguration strategies to address edge sparsity. Our extensive\nexperiments demonstrate that UltraTAG-S significantly outperforms existing\nbaselines, achieving improvements of 2.12\\% and 17.47\\% in ideal and sparse\nsettings, respectively. Moreover, as the data sparsity ratio increases, the\nperformance improvement of UltraTAG-S also rises, which underscores the\neffectiveness and robustness of UltraTAG-S.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) and the proliferation of\nText-Attributed Graphs (TAGs) across various domains have positioned\nLLM-enhanced TAG learning as a critical research area. By utilizing rich graph\ndescriptions, this paradigm leverages LLMs to generate high-quality embeddings,\nthereby enhancing the representational capacity of Graph Neural Networks\n(GNNs). However, the field faces significant challenges: (1) the absence of a\nunified framework to systematize the diverse optimization perspectives arising\nfrom the complex interactions between LLMs and GNNs, and (2) the lack of a\nrobust method capable of handling real-world TAGs, which often suffer from\ntexts and edge sparsity, leading to suboptimal performance.\n  To address these challenges, we propose UltraTAG, a unified pipeline for\nLLM-enhanced TAG learning. UltraTAG provides a unified comprehensive and\ndomain-adaptive framework that not only organizes existing methodologies but\nalso paves the way for future advancements in the field. Building on this\nframework, we propose UltraTAG-S, a robust instantiation of UltraTAG designed\nto tackle the inherent sparsity issues in real-world TAGs. UltraTAG-S employs\nLLM-based text propagation and text augmentation to mitigate text sparsity,\nwhile leveraging LLM-augmented node selection techniques based on PageRank and\nedge reconfiguration strategies to address edge sparsity. Our extensive\nexperiments demonstrate that UltraTAG-S significantly outperforms existing\nbaselines, achieving improvements of 2.12\\% and 17.47\\% in ideal and sparse\nsettings, respectively. Moreover, as the data sparsity ratio increases, the\nperformance improvement of UltraTAG-S also rises, which underscores the\neffectiveness and robustness of UltraTAG-S."
                },
                "authors": [
                    {
                        "name": "Zihao Zhang"
                    },
                    {
                        "name": "Xunkai Li"
                    },
                    {
                        "name": "Rong-Hua Li"
                    },
                    {
                        "name": "Bing Zhou"
                    },
                    {
                        "name": "Zhenjun Li"
                    },
                    {
                        "name": "Guoren Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoren Wang"
                },
                "author": "Guoren Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17867v2",
                "updated": "2025-04-03T07:13:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    7,
                    13,
                    30,
                    3,
                    93,
                    0
                ],
                "published": "2024-12-21T10:13:45Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    10,
                    13,
                    45,
                    5,
                    356,
                    0
                ],
                "title": "Evaluating and Enhancing LLMs for Multi-turn Text-to-SQL with Multiple\n  Question Types",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Enhancing LLMs for Multi-turn Text-to-SQL with Multiple\n  Question Types"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nadvanced text-to-SQL systems. However, most LLM-based methods often narrowly\nfocus on SQL generation, neglecting the complexities of real-world\nconversational queries. This oversight can lead to unreliable responses,\nparticularly for ambiguous questions that cannot be directly addressed with\nSQL. To bridge this gap, we propose MMSQL, a comprehensive test suite designed\nto evaluate the question classification and SQL generation capabilities of LLMs\nby simulating real-world scenarios with diverse question types and multi-turn\nQ\\&A interactions. Using MMSQL, we assessed the performance of popular LLMs,\nincluding both open-source and closed-source models, and identified key factors\nimpacting their performance in such scenarios. Moreover, we introduce an\nLLM-based multi-agent framework that employs specialized agents to identify\nquestion types and determine appropriate answering strategies. Our experiments\ndemonstrate that this approach significantly enhances the model's ability to\nnavigate the complexities of conversational dynamics, effectively handling the\ndiverse and complex nature of user queries. Our dataset and code are publicly\navailable at https://mcxiaoxiao.github.io/MMSQL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nadvanced text-to-SQL systems. However, most LLM-based methods often narrowly\nfocus on SQL generation, neglecting the complexities of real-world\nconversational queries. This oversight can lead to unreliable responses,\nparticularly for ambiguous questions that cannot be directly addressed with\nSQL. To bridge this gap, we propose MMSQL, a comprehensive test suite designed\nto evaluate the question classification and SQL generation capabilities of LLMs\nby simulating real-world scenarios with diverse question types and multi-turn\nQ\\&A interactions. Using MMSQL, we assessed the performance of popular LLMs,\nincluding both open-source and closed-source models, and identified key factors\nimpacting their performance in such scenarios. Moreover, we introduce an\nLLM-based multi-agent framework that employs specialized agents to identify\nquestion types and determine appropriate answering strategies. Our experiments\ndemonstrate that this approach significantly enhances the model's ability to\nnavigate the complexities of conversational dynamics, effectively handling the\ndiverse and complex nature of user queries. Our dataset and code are publicly\navailable at https://mcxiaoxiao.github.io/MMSQL."
                },
                "authors": [
                    {
                        "name": "Ziming Guo"
                    },
                    {
                        "name": "Chao Ma"
                    },
                    {
                        "name": "Yinggang Sun"
                    },
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Guangyao Wang"
                    },
                    {
                        "name": "Hai Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Huang"
                },
                "author": "Hai Huang",
                "arxiv_comment": "International Joint Conference on Neural Networks 2025 (IJCNN 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02327v1",
                "updated": "2025-04-03T06:59:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    6,
                    59,
                    44,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T06:59:44Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    6,
                    59,
                    44,
                    3,
                    93,
                    0
                ],
                "title": "LearNAT: Learning NL2SQL with AST-guided Task Decomposition for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LearNAT: Learning NL2SQL with AST-guided Task Decomposition for Large\n  Language Models"
                },
                "summary": "Natural Language to SQL (NL2SQL) has emerged as a critical task for enabling\nseamless interaction with databases. Recent advancements in Large Language\nModels (LLMs) have demonstrated remarkable performance in this domain. However,\nexisting NL2SQL methods predominantly rely on closed-source LLMs leveraging\nprompt engineering, while open-source models typically require fine-tuning to\nacquire domain-specific knowledge. Despite these efforts, open-source LLMs\nstruggle with complex NL2SQL tasks due to the indirect expression of user query\nobjectives and the semantic gap between user queries and database schemas.\nInspired by the application of reinforcement learning in mathematical\nproblem-solving to encourage step-by-step reasoning in LLMs, we propose LearNAT\n(Learning NL2SQL with AST-guided Task Decomposition), a novel framework that\nimproves the performance of open-source LLMs on complex NL2SQL tasks through\ntask decomposition and reinforcement learning. LearNAT introduces three key\ncomponents: (1) a Decomposition Synthesis Procedure that leverages Abstract\nSyntax Trees (ASTs) to guide efficient search and pruning strategies for task\ndecomposition, (2) Margin-aware Reinforcement Learning, which employs\nfine-grained step-level optimization via DPO with AST margins, and (3) Adaptive\nDemonstration Reasoning, a mechanism for dynamically selecting relevant\nexamples to enhance decomposition capabilities. Extensive experiments on two\nbenchmark datasets, Spider and BIRD, demonstrate that LearNAT enables a\n7B-parameter open-source LLM to achieve performance comparable to GPT-4, while\noffering improved efficiency and accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language to SQL (NL2SQL) has emerged as a critical task for enabling\nseamless interaction with databases. Recent advancements in Large Language\nModels (LLMs) have demonstrated remarkable performance in this domain. However,\nexisting NL2SQL methods predominantly rely on closed-source LLMs leveraging\nprompt engineering, while open-source models typically require fine-tuning to\nacquire domain-specific knowledge. Despite these efforts, open-source LLMs\nstruggle with complex NL2SQL tasks due to the indirect expression of user query\nobjectives and the semantic gap between user queries and database schemas.\nInspired by the application of reinforcement learning in mathematical\nproblem-solving to encourage step-by-step reasoning in LLMs, we propose LearNAT\n(Learning NL2SQL with AST-guided Task Decomposition), a novel framework that\nimproves the performance of open-source LLMs on complex NL2SQL tasks through\ntask decomposition and reinforcement learning. LearNAT introduces three key\ncomponents: (1) a Decomposition Synthesis Procedure that leverages Abstract\nSyntax Trees (ASTs) to guide efficient search and pruning strategies for task\ndecomposition, (2) Margin-aware Reinforcement Learning, which employs\nfine-grained step-level optimization via DPO with AST margins, and (3) Adaptive\nDemonstration Reasoning, a mechanism for dynamically selecting relevant\nexamples to enhance decomposition capabilities. Extensive experiments on two\nbenchmark datasets, Spider and BIRD, demonstrate that LearNAT enables a\n7B-parameter open-source LLM to achieve performance comparable to GPT-4, while\noffering improved efficiency and accessibility."
                },
                "authors": [
                    {
                        "name": "Weibin Liao"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Tianyu Jia"
                    },
                    {
                        "name": "Rihong Qiu"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Junfeng Zhao"
                    },
                    {
                        "name": "Yasha Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yasha Wang"
                },
                "author": "Yasha Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22512v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22512v2",
                "updated": "2025-04-03T06:56:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    6,
                    56,
                    58,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-28T15:15:56Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    15,
                    56,
                    4,
                    87,
                    0
                ],
                "title": "Unlocking LLM Repair Capabilities in Low-Resource Programming Languages\n  Through Cross-Language Translation and Multi-Agent Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking LLM Repair Capabilities in Low-Resource Programming Languages\n  Through Cross-Language Translation and Multi-Agent Refinement"
                },
                "summary": "Recent advances in leveraging LLMs for APR have demonstrated impressive\ncapabilities in fixing software defects. However, current LLM-based approaches\npredominantly focus on mainstream programming languages like Java and Python,\nneglecting less prevalent but emerging languages such as Rust due to expensive\ntraining resources, limited datasets, and insufficient community support. This\nnarrow focus creates a significant gap in repair capabilities across the\nprogramming language spectrum, where the full potential of LLMs for\ncomprehensive multilingual program repair remains largely unexplored. To\naddress this limitation, we introduce a novel cross-language program repair\napproach LANTERN that leverages LLMs' differential proficiency across languages\nthrough a multi-agent iterative repair paradigm. Our technique strategically\ntranslates defective code from languages where LLMs exhibit weaker repair\ncapabilities to languages where they demonstrate stronger performance, without\nrequiring additional training. A key innovation of our approach is an LLM-based\ndecision-making system that dynamically selects optimal target languages based\non bug characteristics and continuously incorporates feedback from previous\nrepair attempts. We evaluate our method on xCodeEval, a comprehensive\nmultilingual benchmark comprising 5,068 bugs across 11 programming languages.\nResults demonstrate significant enhancement in repair effectiveness,\nparticularly for underrepresented languages, with Rust showing a 22.09%\nimprovement in Pass@10 metrics. Our research provides the first empirical\nevidence that cross-language translation significantly expands the repair\ncapabilities of LLMs and effectively bridges the performance gap between\nprogramming languages with different levels of popularity, opening new avenues\nfor truly language-agnostic automated program repair.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in leveraging LLMs for APR have demonstrated impressive\ncapabilities in fixing software defects. However, current LLM-based approaches\npredominantly focus on mainstream programming languages like Java and Python,\nneglecting less prevalent but emerging languages such as Rust due to expensive\ntraining resources, limited datasets, and insufficient community support. This\nnarrow focus creates a significant gap in repair capabilities across the\nprogramming language spectrum, where the full potential of LLMs for\ncomprehensive multilingual program repair remains largely unexplored. To\naddress this limitation, we introduce a novel cross-language program repair\napproach LANTERN that leverages LLMs' differential proficiency across languages\nthrough a multi-agent iterative repair paradigm. Our technique strategically\ntranslates defective code from languages where LLMs exhibit weaker repair\ncapabilities to languages where they demonstrate stronger performance, without\nrequiring additional training. A key innovation of our approach is an LLM-based\ndecision-making system that dynamically selects optimal target languages based\non bug characteristics and continuously incorporates feedback from previous\nrepair attempts. We evaluate our method on xCodeEval, a comprehensive\nmultilingual benchmark comprising 5,068 bugs across 11 programming languages.\nResults demonstrate significant enhancement in repair effectiveness,\nparticularly for underrepresented languages, with Rust showing a 22.09%\nimprovement in Pass@10 metrics. Our research provides the first empirical\nevidence that cross-language translation significantly expands the repair\ncapabilities of LLMs and effectively bridges the performance gap between\nprogramming languages with different levels of popularity, opening new avenues\nfor truly language-agnostic automated program repair."
                },
                "authors": [
                    {
                        "name": "Wenqiang Luo"
                    },
                    {
                        "name": "Jacky Wai Keung"
                    },
                    {
                        "name": "Boyang Yang"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "Tegawende F. Bissyande"
                    },
                    {
                        "name": "Haoye Tian"
                    },
                    {
                        "name": "Bach Le"
                    }
                ],
                "author_detail": {
                    "name": "Bach Le"
                },
                "author": "Bach Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22512v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22512v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02323v1",
                "updated": "2025-04-03T06:53:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    6,
                    53,
                    34,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T06:53:34Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    6,
                    53,
                    34,
                    3,
                    93,
                    0
                ],
                "title": "CoTAL: Human-in-the-Loop Prompt Engineering, Chain-of-Thought Reasoning,\n  and Active Learning for Generalizable Formative Assessment Scoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoTAL: Human-in-the-Loop Prompt Engineering, Chain-of-Thought Reasoning,\n  and Active Learning for Generalizable Formative Assessment Scoring"
                },
                "summary": "Large language models (LLMs) have created new opportunities to assist\nteachers and support student learning. Methods such as chain-of-thought (CoT)\nprompting enable LLMs to grade formative assessments in science, providing\nscores and relevant feedback to students. However, the extent to which these\nmethods generalize across curricula in multiple domains (such as science,\ncomputing, and engineering) remains largely untested. In this paper, we\nintroduce Chain-of-Thought Prompting + Active Learning (CoTAL), an LLM-based\napproach to formative assessment scoring that (1) leverages Evidence-Centered\nDesign (ECD) principles to develop curriculum-aligned formative assessments and\nrubrics, (2) applies human-in-the-loop prompt engineering to automate response\nscoring, and (3) incorporates teacher and student feedback to iteratively\nrefine assessment questions, grading rubrics, and LLM prompts for automated\ngrading. Our findings demonstrate that CoTAL improves GPT-4's scoring\nperformance, achieving gains of up to 24.5% over a non-prompt-engineered\nbaseline. Both teachers and students view CoTAL as effective in scoring and\nexplaining student responses, each providing valuable refinements to enhance\ngrading accuracy and explanation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have created new opportunities to assist\nteachers and support student learning. Methods such as chain-of-thought (CoT)\nprompting enable LLMs to grade formative assessments in science, providing\nscores and relevant feedback to students. However, the extent to which these\nmethods generalize across curricula in multiple domains (such as science,\ncomputing, and engineering) remains largely untested. In this paper, we\nintroduce Chain-of-Thought Prompting + Active Learning (CoTAL), an LLM-based\napproach to formative assessment scoring that (1) leverages Evidence-Centered\nDesign (ECD) principles to develop curriculum-aligned formative assessments and\nrubrics, (2) applies human-in-the-loop prompt engineering to automate response\nscoring, and (3) incorporates teacher and student feedback to iteratively\nrefine assessment questions, grading rubrics, and LLM prompts for automated\ngrading. Our findings demonstrate that CoTAL improves GPT-4's scoring\nperformance, achieving gains of up to 24.5% over a non-prompt-engineered\nbaseline. Both teachers and students view CoTAL as effective in scoring and\nexplaining student responses, each providing valuable refinements to enhance\ngrading accuracy and explanation quality."
                },
                "authors": [
                    {
                        "name": "Clayton Cohn"
                    },
                    {
                        "name": "Nicole Hutchins"
                    },
                    {
                        "name": "Ashwin T S"
                    },
                    {
                        "name": "Gautam Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Gautam Biswas"
                },
                "author": "Gautam Biswas",
                "arxiv_comment": "Submitted to IEEE Transactions on Learning Technologies. Currently\n  under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02310v1",
                "updated": "2025-04-03T06:37:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    6,
                    37,
                    55,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T06:37:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    6,
                    37,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Improving Harmful Text Detection with Joint Retrieval and External\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Harmful Text Detection with Joint Retrieval and External\n  Knowledge"
                },
                "summary": "Harmful text detection has become a crucial task in the development and\ndeployment of large language models, especially as AI-generated content\ncontinues to expand across digital platforms. This study proposes a joint\nretrieval framework that integrates pre-trained language models with knowledge\ngraphs to improve the accuracy and robustness of harmful text detection.\nExperimental results demonstrate that the joint retrieval approach\nsignificantly outperforms single-model baselines, particularly in low-resource\ntraining scenarios and multilingual environments. The proposed method\neffectively captures nuanced harmful content by leveraging external contextual\ninformation, addressing the limitations of traditional detection models. Future\nresearch should focus on optimizing computational efficiency, enhancing model\ninterpretability, and expanding multimodal detection capabilities to better\ntackle evolving harmful content patterns. This work contributes to the\nadvancement of AI safety, ensuring more trustworthy and reliable content\nmoderation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmful text detection has become a crucial task in the development and\ndeployment of large language models, especially as AI-generated content\ncontinues to expand across digital platforms. This study proposes a joint\nretrieval framework that integrates pre-trained language models with knowledge\ngraphs to improve the accuracy and robustness of harmful text detection.\nExperimental results demonstrate that the joint retrieval approach\nsignificantly outperforms single-model baselines, particularly in low-resource\ntraining scenarios and multilingual environments. The proposed method\neffectively captures nuanced harmful content by leveraging external contextual\ninformation, addressing the limitations of traditional detection models. Future\nresearch should focus on optimizing computational efficiency, enhancing model\ninterpretability, and expanding multimodal detection capabilities to better\ntackle evolving harmful content patterns. This work contributes to the\nadvancement of AI safety, ensuring more trustworthy and reliable content\nmoderation systems."
                },
                "authors": [
                    {
                        "name": "Zidong Yu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Weiqiang Huang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Junliang Du"
                    }
                ],
                "author_detail": {
                    "name": "Junliang Du"
                },
                "author": "Junliang Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02304v1",
                "updated": "2025-04-03T06:22:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    6,
                    22,
                    19,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T06:22:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    6,
                    22,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Measurement of LLM's Philosophies of Human Nature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of LLM's Philosophies of Human Nature"
                },
                "summary": "The widespread application of artificial intelligence (AI) in various tasks,\nalong with frequent reports of conflicts or violations involving AI, has\nsparked societal concerns about interactions with AI systems. Based on\nWrightsman's Philosophies of Human Nature Scale (PHNS), a scale empirically\nvalidated over decades to effectively assess individuals' attitudes toward\nhuman nature, we design the standardized psychological scale specifically\ntargeting large language models (LLM), named the Machine-based Philosophies of\nHuman Nature Scale (M-PHNS). By evaluating LLMs' attitudes toward human nature\nacross six dimensions, we reveal that current LLMs exhibit a systemic lack of\ntrust in humans, and there is a significant negative correlation between the\nmodel's intelligence level and its trust in humans. Furthermore, we propose a\nmental loop learning framework, which enables LLM to continuously optimize its\nvalue system during virtual interactions by constructing moral scenarios,\nthereby improving its attitude toward human nature. Experiments demonstrate\nthat mental loop learning significantly enhances their trust in humans compared\nto persona or instruction prompts. This finding highlights the potential of\nhuman-based psychological assessments for LLM, which can not only diagnose\ncognitive biases but also provide a potential solution for ethical learning in\nartificial intelligence. We release the M-PHNS evaluation code and data at\nhttps://github.com/kodenii/M-PHNS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread application of artificial intelligence (AI) in various tasks,\nalong with frequent reports of conflicts or violations involving AI, has\nsparked societal concerns about interactions with AI systems. Based on\nWrightsman's Philosophies of Human Nature Scale (PHNS), a scale empirically\nvalidated over decades to effectively assess individuals' attitudes toward\nhuman nature, we design the standardized psychological scale specifically\ntargeting large language models (LLM), named the Machine-based Philosophies of\nHuman Nature Scale (M-PHNS). By evaluating LLMs' attitudes toward human nature\nacross six dimensions, we reveal that current LLMs exhibit a systemic lack of\ntrust in humans, and there is a significant negative correlation between the\nmodel's intelligence level and its trust in humans. Furthermore, we propose a\nmental loop learning framework, which enables LLM to continuously optimize its\nvalue system during virtual interactions by constructing moral scenarios,\nthereby improving its attitude toward human nature. Experiments demonstrate\nthat mental loop learning significantly enhances their trust in humans compared\nto persona or instruction prompts. This finding highlights the potential of\nhuman-based psychological assessments for LLM, which can not only diagnose\ncognitive biases but also provide a potential solution for ethical learning in\nartificial intelligence. We release the M-PHNS evaluation code and data at\nhttps://github.com/kodenii/M-PHNS."
                },
                "authors": [
                    {
                        "name": "Minheng Ni"
                    },
                    {
                        "name": "Ennan Wu"
                    },
                    {
                        "name": "Zidong Gong"
                    },
                    {
                        "name": "Zhengyuan Yang"
                    },
                    {
                        "name": "Linjie Li"
                    },
                    {
                        "name": "Chung-Ching Lin"
                    },
                    {
                        "name": "Kevin Lin"
                    },
                    {
                        "name": "Lijuan Wang"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Wangmeng Zuo"
                },
                "author": "Wangmeng Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02293v1",
                "updated": "2025-04-03T05:47:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    5,
                    47,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T05:47:51Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    5,
                    47,
                    51,
                    3,
                    93,
                    0
                ],
                "title": "State-of-the-Art Translation of Text-to-Gloss using mBART : A case study\n  of Bangla",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-Art Translation of Text-to-Gloss using mBART : A case study\n  of Bangla"
                },
                "summary": "Despite a large deaf and dumb population of 1.7 million, Bangla Sign Language\n(BdSL) remains a understudied domain. Specifically, there are no works on\nBangla text-to-gloss translation task. To address this gap, we begin by\naddressing the dataset problem. We take inspiration from grammatical rule based\ngloss generation used in Germany and American sign langauage (ASL) and adapt it\nfor BdSL. We also leverage LLM to generate synthetic data and use\nback-translation, text generation for data augmentation. With dataset prepared,\nwe started experimentation. We fine-tuned pretrained mBART-50 and\nmBERT-multiclass-uncased model on our dataset. We also trained GRU, RNN and a\nnovel seq-to-seq model with multi-head attention. We observe significant high\nperformance (ScareBLEU=79.53) with fine-tuning pretrained mBART-50 multilingual\nmodel from Facebook. We then explored why we observe such high performance with\nmBART. We soon notice an interesting property of mBART -- it was trained on\nshuffled and masked text data. And as we know, gloss form has shuffling\nproperty. So we hypothesize that mBART is inherently good at text-to-gloss\ntasks. To find support against this hypothesis, we trained mBART-50 on\nPHOENIX-14T benchmark and evaluated it with existing literature. Our mBART-50\nfinetune demonstrated State-of-the-Art performance on PHOENIX-14T benchmark,\nfar outperforming existing models in all 6 metrics (ScareBLEU = 63.89, BLEU-1 =\n55.14, BLEU-2 = 38.07, BLEU-3 = 27.13, BLEU-4 = 20.68, COMET = 0.624). Based on\nthe results, this study proposes a new paradigm for text-to-gloss task using\nmBART models. Additionally, our results show that BdSL text-to-gloss task can\ngreatly benefit from rule-based synthetic dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite a large deaf and dumb population of 1.7 million, Bangla Sign Language\n(BdSL) remains a understudied domain. Specifically, there are no works on\nBangla text-to-gloss translation task. To address this gap, we begin by\naddressing the dataset problem. We take inspiration from grammatical rule based\ngloss generation used in Germany and American sign langauage (ASL) and adapt it\nfor BdSL. We also leverage LLM to generate synthetic data and use\nback-translation, text generation for data augmentation. With dataset prepared,\nwe started experimentation. We fine-tuned pretrained mBART-50 and\nmBERT-multiclass-uncased model on our dataset. We also trained GRU, RNN and a\nnovel seq-to-seq model with multi-head attention. We observe significant high\nperformance (ScareBLEU=79.53) with fine-tuning pretrained mBART-50 multilingual\nmodel from Facebook. We then explored why we observe such high performance with\nmBART. We soon notice an interesting property of mBART -- it was trained on\nshuffled and masked text data. And as we know, gloss form has shuffling\nproperty. So we hypothesize that mBART is inherently good at text-to-gloss\ntasks. To find support against this hypothesis, we trained mBART-50 on\nPHOENIX-14T benchmark and evaluated it with existing literature. Our mBART-50\nfinetune demonstrated State-of-the-Art performance on PHOENIX-14T benchmark,\nfar outperforming existing models in all 6 metrics (ScareBLEU = 63.89, BLEU-1 =\n55.14, BLEU-2 = 38.07, BLEU-3 = 27.13, BLEU-4 = 20.68, COMET = 0.624). Based on\nthe results, this study proposes a new paradigm for text-to-gloss task using\nmBART models. Additionally, our results show that BdSL text-to-gloss task can\ngreatly benefit from rule-based synthetic dataset."
                },
                "authors": [
                    {
                        "name": "Sharif Md. Abdullah"
                    },
                    {
                        "name": "Abhijit Paul"
                    },
                    {
                        "name": "Shebuti Rayana"
                    },
                    {
                        "name": "Ahmedul Kabir"
                    },
                    {
                        "name": "Zarif Masud"
                    }
                ],
                "author_detail": {
                    "name": "Zarif Masud"
                },
                "author": "Zarif Masud",
                "arxiv_comment": "Initial Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10347v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10347v4",
                "updated": "2025-04-03T05:41:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    5,
                    41,
                    14,
                    3,
                    93,
                    0
                ],
                "published": "2024-05-16T02:00:44Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    2,
                    0,
                    44,
                    3,
                    137,
                    0
                ],
                "title": "Networking Systems for Video Anomaly Detection: A Tutorial and Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Networking Systems for Video Anomaly Detection: A Tutorial and Survey"
                },
                "summary": "The increasing utilization of surveillance cameras in smart cities, coupled\nwith the surge of online video applications, has heightened concerns regarding\npublic security and privacy protection, which propelled automated Video Anomaly\nDetection (VAD) into a fundamental research task within the Artificial\nIntelligence (AI) community. With the advancements in deep learning and edge\ncomputing, VAD has made significant progress and advances synergized with\nemerging applications in smart cities and video internet, which has moved\nbeyond the conventional research scope of algorithm engineering to deployable\nNetworking Systems for VAD (NSVAD), a practical hotspot for intersection\nexploration in the AI, IoVT, and computing fields. In this article, we\ndelineate the foundational assumptions, learning frameworks, and applicable\nscenarios of various deep learning-driven VAD routes, offering an exhaustive\ntutorial for novices in NSVAD. In addition, this article elucidates core\nconcepts by reviewing recent advances and typical solutions and aggregating\navailable research resources accessible at https://github.com/fdjingliu/NSVAD.\nLastly, this article projects future development trends and discusses how the\nintegration of AI and computing technologies can address existing research\nchallenges and promote open opportunities, serving as an insightful guide for\nprospective researchers and engineers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing utilization of surveillance cameras in smart cities, coupled\nwith the surge of online video applications, has heightened concerns regarding\npublic security and privacy protection, which propelled automated Video Anomaly\nDetection (VAD) into a fundamental research task within the Artificial\nIntelligence (AI) community. With the advancements in deep learning and edge\ncomputing, VAD has made significant progress and advances synergized with\nemerging applications in smart cities and video internet, which has moved\nbeyond the conventional research scope of algorithm engineering to deployable\nNetworking Systems for VAD (NSVAD), a practical hotspot for intersection\nexploration in the AI, IoVT, and computing fields. In this article, we\ndelineate the foundational assumptions, learning frameworks, and applicable\nscenarios of various deep learning-driven VAD routes, offering an exhaustive\ntutorial for novices in NSVAD. In addition, this article elucidates core\nconcepts by reviewing recent advances and typical solutions and aggregating\navailable research resources accessible at https://github.com/fdjingliu/NSVAD.\nLastly, this article projects future development trends and discusses how the\nintegration of AI and computing technologies can address existing research\nchallenges and promote open opportunities, serving as an insightful guide for\nprospective researchers and engineers."
                },
                "authors": [
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jieyu Lin"
                    },
                    {
                        "name": "Jielin Li"
                    },
                    {
                        "name": "Liang Cao"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Bo Hu"
                    },
                    {
                        "name": "Liang Song"
                    },
                    {
                        "name": "Azzedine Boukerche"
                    },
                    {
                        "name": "Victor C. M. Leung"
                    }
                ],
                "author_detail": {
                    "name": "Victor C. M. Leung"
                },
                "author": "Victor C. M. Leung",
                "arxiv_comment": "Accepted to ACM Computing Surveys. For more information and\n  supplementary material, please visit https://github.com/fdjingliu/NSVAD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10347v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10347v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02285v1",
                "updated": "2025-04-03T05:16:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    5,
                    16,
                    9,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T05:16:09Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    5,
                    16,
                    9,
                    3,
                    93,
                    0
                ],
                "title": "Tree-based Models for Vertical Federated Learning: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree-based Models for Vertical Federated Learning: A Survey"
                },
                "summary": "Tree-based models have achieved great success in a wide range of real-world\napplications due to their effectiveness, robustness, and interpretability,\nwhich inspired people to apply them in vertical federated learning (VFL)\nscenarios in recent years. In this paper, we conduct a comprehensive study to\ngive an overall picture of applying tree-based models in VFL, from the\nperspective of their communication and computation protocols. We categorize\ntree-based models in VFL into two types, i.e., feature-gathering models and\nlabel-scattering models, and provide a detailed discussion regarding their\ncharacteristics, advantages, privacy protection mechanisms, and applications.\nThis study also focuses on the implementation of tree-based models in VFL,\nsummarizing several design principles for better satisfying various\nrequirements from both academic research and industrial deployment. We conduct\na series of experiments to provide empirical observations on the differences\nand advances of different types of tree-based models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree-based models have achieved great success in a wide range of real-world\napplications due to their effectiveness, robustness, and interpretability,\nwhich inspired people to apply them in vertical federated learning (VFL)\nscenarios in recent years. In this paper, we conduct a comprehensive study to\ngive an overall picture of applying tree-based models in VFL, from the\nperspective of their communication and computation protocols. We categorize\ntree-based models in VFL into two types, i.e., feature-gathering models and\nlabel-scattering models, and provide a detailed discussion regarding their\ncharacteristics, advantages, privacy protection mechanisms, and applications.\nThis study also focuses on the implementation of tree-based models in VFL,\nsummarizing several design principles for better satisfying various\nrequirements from both academic research and industrial deployment. We conduct\na series of experiments to provide empirical observations on the differences\nand advances of different types of tree-based models."
                },
                "authors": [
                    {
                        "name": "Bingchen Qian"
                    },
                    {
                        "name": "Yuexiang Xie"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "Accepted by ACM Computing Surveys (CSUR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02281v1",
                "updated": "2025-04-03T05:08:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    5,
                    8,
                    4,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T05:08:04Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    5,
                    8,
                    4,
                    3,
                    93,
                    0
                ],
                "title": "Parallel Market Environments for FinRL Contests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Market Environments for FinRL Contests"
                },
                "summary": "Reinforcement learning has shown great potential in finance. We have\norganized the FinRL Contests 2023-2025 featuring different financial tasks.\nLarge language models have a strong capability to process financial texts.\nIntegrating LLM-generated signals into FinRL is a new task, enabling agents to\nuse both structured market data and unstructured financial text. To address the\nsampling bottleneck during training, we introduce GPU-based parallel market\nenvironments to improve sampling speed. In this paper, we summarize the\nparallel market environments used in FinRL Contests 2023-2025. Two new\nenvironments incorporate LLM-generated signals and support massively parallel\nsimulation. Contestants utilize these environments to train agents for stock\nand cryptocurrency trading tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning has shown great potential in finance. We have\norganized the FinRL Contests 2023-2025 featuring different financial tasks.\nLarge language models have a strong capability to process financial texts.\nIntegrating LLM-generated signals into FinRL is a new task, enabling agents to\nuse both structured market data and unstructured financial text. To address the\nsampling bottleneck during training, we introduce GPU-based parallel market\nenvironments to improve sampling speed. In this paper, we summarize the\nparallel market environments used in FinRL Contests 2023-2025. Two new\nenvironments incorporate LLM-generated signals and support massively parallel\nsimulation. Contestants utilize these environments to train agents for stock\nand cryptocurrency trading tasks."
                },
                "authors": [
                    {
                        "name": "Keyi Wang"
                    },
                    {
                        "name": "Kairong Xiao"
                    },
                    {
                        "name": "Xiao-Yang Liu Yanglet"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Yang Liu Yanglet"
                },
                "author": "Xiao-Yang Liu Yanglet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02280v1",
                "updated": "2025-04-03T05:06:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    5,
                    6,
                    6,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T05:06:06Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    5,
                    6,
                    6,
                    3,
                    93,
                    0
                ],
                "title": "LLM-Guided Evolution: An Autonomous Model Optimization for Object\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Guided Evolution: An Autonomous Model Optimization for Object\n  Detection"
                },
                "summary": "In machine learning, Neural Architecture Search (NAS) requires domain\nknowledge of model design and a large amount of trial-and-error to achieve\npromising performance. Meanwhile, evolutionary algorithms have traditionally\nrelied on fixed rules and pre-defined building blocks. The Large Language Model\n(LLM)-Guided Evolution (GE) framework transformed this approach by\nincorporating LLMs to directly modify model source code for image\nclassification algorithms on CIFAR data and intelligently guide mutations and\ncrossovers. A key element of LLM-GE is the \"Evolution of Thought\" (EoT)\ntechnique, which establishes feedback loops, allowing LLMs to refine their\ndecisions iteratively based on how previous operations performed. In this\nstudy, we perform NAS for object detection by improving LLM-GE to modify the\narchitecture of You Only Look Once (YOLO) models to enhance performance on the\nKITTI dataset. Our approach intelligently adjusts the design and settings of\nYOLO to find the optimal algorithms against objective such as detection\naccuracy and speed. We show that LLM-GE produced variants with significant\nperformance improvements, such as an increase in Mean Average Precision from\n92.5% to 94.5%. This result highlights the flexibility and effectiveness of\nLLM-GE on real-world challenges, offering a novel paradigm for automated\nmachine learning that combines LLM-driven reasoning with evolutionary\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In machine learning, Neural Architecture Search (NAS) requires domain\nknowledge of model design and a large amount of trial-and-error to achieve\npromising performance. Meanwhile, evolutionary algorithms have traditionally\nrelied on fixed rules and pre-defined building blocks. The Large Language Model\n(LLM)-Guided Evolution (GE) framework transformed this approach by\nincorporating LLMs to directly modify model source code for image\nclassification algorithms on CIFAR data and intelligently guide mutations and\ncrossovers. A key element of LLM-GE is the \"Evolution of Thought\" (EoT)\ntechnique, which establishes feedback loops, allowing LLMs to refine their\ndecisions iteratively based on how previous operations performed. In this\nstudy, we perform NAS for object detection by improving LLM-GE to modify the\narchitecture of You Only Look Once (YOLO) models to enhance performance on the\nKITTI dataset. Our approach intelligently adjusts the design and settings of\nYOLO to find the optimal algorithms against objective such as detection\naccuracy and speed. We show that LLM-GE produced variants with significant\nperformance improvements, such as an increase in Mean Average Precision from\n92.5% to 94.5%. This result highlights the flexibility and effectiveness of\nLLM-GE on real-world challenges, offering a novel paradigm for automated\nmachine learning that combines LLM-driven reasoning with evolutionary\nstrategies."
                },
                "authors": [
                    {
                        "name": "YiMing Yu"
                    },
                    {
                        "name": "Jason Zutty"
                    }
                ],
                "author_detail": {
                    "name": "Jason Zutty"
                },
                "author": "Jason Zutty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02273v1",
                "updated": "2025-04-03T04:46:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    46,
                    17,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T04:46:17Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    46,
                    17,
                    3,
                    93,
                    0
                ],
                "title": "Reasoning Under 1 Billion: Memory-Augmented Reinforcement Learning for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Under 1 Billion: Memory-Augmented Reinforcement Learning for\n  Large Language Models"
                },
                "summary": "Recent advances in fine-tuning large language models (LLMs) with\nreinforcement learning (RL) have shown promising improvements in complex\nreasoning tasks, particularly when paired with chain-of-thought (CoT)\nprompting. However, these successes have been largely demonstrated on\nlarge-scale models with billions of parameters, where a strong pretraining\nfoundation ensures effective initial exploration. In contrast, RL remains\nchallenging for tiny LLMs with 1 billion parameters or fewer because they lack\nthe necessary pretraining strength to explore effectively, often leading to\nsuboptimal reasoning patterns. This work introduces a novel intrinsic\nmotivation approach that leverages episodic memory to address this challenge,\nimproving tiny LLMs in CoT reasoning tasks. Inspired by human memory-driven\nlearning, our method leverages successful reasoning patterns stored in memory\nwhile allowing for controlled exploration to generate novel responses.\nIntrinsic rewards are computed efficiently using a kNN-based episodic memory,\nallowing the model to discover new reasoning strategies while quickly adapting\nto effective past solutions. Experiments on fine-tuning GSM8K and AI-MO\ndatasets demonstrate that our approach significantly enhances smaller LLMs'\nsample efficiency and generalization capability, making RL-based reasoning\nimprovements more accessible in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in fine-tuning large language models (LLMs) with\nreinforcement learning (RL) have shown promising improvements in complex\nreasoning tasks, particularly when paired with chain-of-thought (CoT)\nprompting. However, these successes have been largely demonstrated on\nlarge-scale models with billions of parameters, where a strong pretraining\nfoundation ensures effective initial exploration. In contrast, RL remains\nchallenging for tiny LLMs with 1 billion parameters or fewer because they lack\nthe necessary pretraining strength to explore effectively, often leading to\nsuboptimal reasoning patterns. This work introduces a novel intrinsic\nmotivation approach that leverages episodic memory to address this challenge,\nimproving tiny LLMs in CoT reasoning tasks. Inspired by human memory-driven\nlearning, our method leverages successful reasoning patterns stored in memory\nwhile allowing for controlled exploration to generate novel responses.\nIntrinsic rewards are computed efficiently using a kNN-based episodic memory,\nallowing the model to discover new reasoning strategies while quickly adapting\nto effective past solutions. Experiments on fine-tuning GSM8K and AI-MO\ndatasets demonstrate that our approach significantly enhances smaller LLMs'\nsample efficiency and generalization capability, making RL-based reasoning\nimprovements more accessible in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Hung Le"
                    },
                    {
                        "name": "Dai Do"
                    },
                    {
                        "name": "Dung Nguyen"
                    },
                    {
                        "name": "Svetha Venkatesh"
                    }
                ],
                "author_detail": {
                    "name": "Svetha Venkatesh"
                },
                "author": "Svetha Venkatesh",
                "arxiv_comment": "preprint,20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02270v1",
                "updated": "2025-04-03T04:31:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    31,
                    56,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T04:31:56Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    31,
                    56,
                    3,
                    93,
                    0
                ],
                "title": "MinkOcc: Towards real-time label-efficient semantic occupancy prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MinkOcc: Towards real-time label-efficient semantic occupancy prediction"
                },
                "summary": "Developing 3D semantic occupancy prediction models often relies on dense 3D\nannotations for supervised learning, a process that is both labor and\nresource-intensive, underscoring the need for label-efficient or even\nlabel-free approaches. To address this, we introduce MinkOcc, a multi-modal 3D\nsemantic occupancy prediction framework for cameras and LiDARs that proposes a\ntwo-step semi-supervised training procedure. Here, a small dataset of\nexplicitly 3D annotations warm-starts the training process; then, the\nsupervision is continued by simpler-to-annotate accumulated LiDAR sweeps and\nimages -- semantically labelled through vision foundational models. MinkOcc\neffectively utilizes these sensor-rich supervisory cues and reduces reliance on\nmanual labeling by 90\\% while maintaining competitive accuracy. In addition,\nthe proposed model incorporates information from LiDAR and camera data through\nearly fusion and leverages sparse convolution networks for real-time\nprediction. With its efficiency in both supervision and computation, we aim to\nextend MinkOcc beyond curated datasets, enabling broader real-world deployment\nof 3D semantic occupancy prediction in autonomous driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing 3D semantic occupancy prediction models often relies on dense 3D\nannotations for supervised learning, a process that is both labor and\nresource-intensive, underscoring the need for label-efficient or even\nlabel-free approaches. To address this, we introduce MinkOcc, a multi-modal 3D\nsemantic occupancy prediction framework for cameras and LiDARs that proposes a\ntwo-step semi-supervised training procedure. Here, a small dataset of\nexplicitly 3D annotations warm-starts the training process; then, the\nsupervision is continued by simpler-to-annotate accumulated LiDAR sweeps and\nimages -- semantically labelled through vision foundational models. MinkOcc\neffectively utilizes these sensor-rich supervisory cues and reduces reliance on\nmanual labeling by 90\\% while maintaining competitive accuracy. In addition,\nthe proposed model incorporates information from LiDAR and camera data through\nearly fusion and leverages sparse convolution networks for real-time\nprediction. With its efficiency in both supervision and computation, we aim to\nextend MinkOcc beyond curated datasets, enabling broader real-world deployment\nof 3D semantic occupancy prediction in autonomous driving."
                },
                "authors": [
                    {
                        "name": "Samuel Sze"
                    },
                    {
                        "name": "Daniele De Martini"
                    },
                    {
                        "name": "Lars Kunze"
                    }
                ],
                "author_detail": {
                    "name": "Lars Kunze"
                },
                "author": "Lars Kunze",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02269v1",
                "updated": "2025-04-03T04:30:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    30,
                    10,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T04:30:10Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    30,
                    10,
                    3,
                    93,
                    0
                ],
                "title": "Engineering Artificial Intelligence: Framework, Challenges, and Future\n  Direction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering Artificial Intelligence: Framework, Challenges, and Future\n  Direction"
                },
                "summary": "Over the past ten years, the application of artificial intelligence (AI) and\nmachine learning (ML) in engineering domains has gained significant popularity,\nshowcasing their potential in data-driven contexts. However, the complexity and\ndiversity of engineering problems often require the development of\ndomain-specific AI approaches, which are frequently hindered by a lack of\nsystematic methodologies, scalability, and robustness during the development\nprocess. To address this gap, this paper introduces the \"ABCDE\" as the key\nelements of Engineering AI and proposes a unified, systematic engineering AI\necosystem framework, including eight essential layers, along with attributes,\ngoals, and applications, to guide the development and deployment of AI\nsolutions for specific engineering needs. Additionally, key challenges are\nexamined, and nine future research directions are highlighted. By providing a\ncomprehensive perspective, this paper aims to advance the strategic\nimplementation of AI, fostering the development of next-generation engineering\nAI solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past ten years, the application of artificial intelligence (AI) and\nmachine learning (ML) in engineering domains has gained significant popularity,\nshowcasing their potential in data-driven contexts. However, the complexity and\ndiversity of engineering problems often require the development of\ndomain-specific AI approaches, which are frequently hindered by a lack of\nsystematic methodologies, scalability, and robustness during the development\nprocess. To address this gap, this paper introduces the \"ABCDE\" as the key\nelements of Engineering AI and proposes a unified, systematic engineering AI\necosystem framework, including eight essential layers, along with attributes,\ngoals, and applications, to guide the development and deployment of AI\nsolutions for specific engineering needs. Additionally, key challenges are\nexamined, and nine future research directions are highlighted. By providing a\ncomprehensive perspective, this paper aims to advance the strategic\nimplementation of AI, fostering the development of next-generation engineering\nAI solutions."
                },
                "authors": [
                    {
                        "name": "Jay Lee"
                    },
                    {
                        "name": "Hanqi Su"
                    },
                    {
                        "name": "Dai-Yan Ji"
                    },
                    {
                        "name": "Takanobu Minami"
                    }
                ],
                "author_detail": {
                    "name": "Takanobu Minami"
                },
                "author": "Takanobu Minami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02268v1",
                "updated": "2025-04-03T04:27:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T04:27:02Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "title": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data"
                },
                "summary": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Justin Cechmanek"
                    },
                    {
                        "name": "Tyler Hutcherson"
                    },
                    {
                        "name": "Srijith Rajamohan"
                    },
                    {
                        "name": "Jen Agarwal"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    },
                    {
                        "name": "Manvinder Singh"
                    },
                    {
                        "name": "Benoit Dion"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Dion"
                },
                "arxiv_affiliation": "Redis",
                "author": "Benoit Dion",
                "arxiv_comment": "Initial study on embedding fine tuning for semantic cache. It also\n  explores synthetic data. Total pages are 12, including refrences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02263v1",
                "updated": "2025-04-03T04:20:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    20,
                    44,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T04:20:44Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    20,
                    44,
                    3,
                    93,
                    0
                ],
                "title": "MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated\n  Expert Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated\n  Expert Parallelism"
                },
                "summary": "Mixture-of-Experts (MoE) showcases tremendous potential to scale large\nlanguage models (LLMs) with enhanced performance and reduced computational\ncomplexity. However, its sparsely activated architecture shifts feed-forward\nnetworks (FFNs) from being compute-intensive to memory-intensive during\ninference, leading to substantially lower GPU utilization and increased\noperational costs. We present MegaScale-Infer, an efficient and cost-effective\nsystem for serving large-scale MoE models. MegaScale-Infer disaggregates\nattention and FFN modules within each model layer, enabling independent\nscaling, tailored parallelism strategies, and heterogeneous deployment for both\nmodules. To fully exploit disaggregation in the presence of MoE's sparsity,\nMegaScale-Infer introduces ping-pong pipeline parallelism, which partitions a\nrequest batch into micro-batches and shuttles them between attention and FFNs\nfor inference. Combined with distinct model parallelism for each module,\nMegaScale-Infer effectively hides communication overhead and maximizes GPU\nutilization. To adapt to disaggregated attention and FFN modules and minimize\ndata transmission overhead (e.g., token dispatch), MegaScale-Infer provides a\nhigh-performance M2N communication library that eliminates unnecessary\nGPU-to-CPU data copies, group initialization overhead, and GPU synchronization.\nExperimental results indicate that MegaScale-Infer achieves up to 1.90x higher\nper-GPU throughput than state-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) showcases tremendous potential to scale large\nlanguage models (LLMs) with enhanced performance and reduced computational\ncomplexity. However, its sparsely activated architecture shifts feed-forward\nnetworks (FFNs) from being compute-intensive to memory-intensive during\ninference, leading to substantially lower GPU utilization and increased\noperational costs. We present MegaScale-Infer, an efficient and cost-effective\nsystem for serving large-scale MoE models. MegaScale-Infer disaggregates\nattention and FFN modules within each model layer, enabling independent\nscaling, tailored parallelism strategies, and heterogeneous deployment for both\nmodules. To fully exploit disaggregation in the presence of MoE's sparsity,\nMegaScale-Infer introduces ping-pong pipeline parallelism, which partitions a\nrequest batch into micro-batches and shuttles them between attention and FFNs\nfor inference. Combined with distinct model parallelism for each module,\nMegaScale-Infer effectively hides communication overhead and maximizes GPU\nutilization. To adapt to disaggregated attention and FFN modules and minimize\ndata transmission overhead (e.g., token dispatch), MegaScale-Infer provides a\nhigh-performance M2N communication library that eliminates unnecessary\nGPU-to-CPU data copies, group initialization overhead, and GPU synchronization.\nExperimental results indicate that MegaScale-Infer achieves up to 1.90x higher\nper-GPU throughput than state-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Ruidong Zhu"
                    },
                    {
                        "name": "Ziheng Jiang"
                    },
                    {
                        "name": "Chao Jin"
                    },
                    {
                        "name": "Peng Wu"
                    },
                    {
                        "name": "Cesar A. Stuardo"
                    },
                    {
                        "name": "Dongyang Wang"
                    },
                    {
                        "name": "Xinlei Zhang"
                    },
                    {
                        "name": "Huaping Zhou"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Yang Cheng"
                    },
                    {
                        "name": "Jianzhe Xiao"
                    },
                    {
                        "name": "Xinyi Zhang"
                    },
                    {
                        "name": "Lingjun Liu"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Jianxi Ye"
                    },
                    {
                        "name": "Xiao Yu"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.01479v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.01479v7",
                "updated": "2025-04-03T04:16:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    16,
                    58,
                    3,
                    93,
                    0
                ],
                "published": "2023-11-02T05:18:28Z",
                "published_parsed": [
                    2023,
                    11,
                    2,
                    5,
                    18,
                    28,
                    3,
                    306,
                    0
                ],
                "title": "Detecting Out-of-Distribution Through the Lens of Neural Collapse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Out-of-Distribution Through the Lens of Neural Collapse"
                },
                "summary": "Out-of-Distribution (OOD) detection is critical for safe deployment; however,\nexisting detectors often struggle to generalize across datasets of varying\nscales and model architectures, and some can incur high computational costs in\nreal-world applications. Inspired by the phenomenon of Neural Collapse, we\npropose a versatile and efficient OOD detection method. Specifically, we\nre-characterize prior observations that in-distribution (ID) samples form\nclusters, demonstrating that, with appropriate centering, these clusters align\nclosely with model weight vectors. Additionally, we reveal that ID features\ntend to expand into a simplex Equiangular Tight Frame, explaining the common\nobservation that ID features are situated farther from the origin than OOD\nfeatures. Incorporating both insights from Neural Collapse, our OOD detector\nleverages feature proximity to weight vectors and complements this approach by\nusing feature norms to effectively filter out OOD samples. Extensive\nexperiments on off-the-shelf models demonstrate the robustness of our OOD\ndetector across diverse scenarios, mitigating generalization discrepancies and\nenhancing overall performance, with inference latency comparable to that of the\nbasic softmax-confidence detector. Code is available here:\nhttps://github.com/litianliu/NCI-OOD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-Distribution (OOD) detection is critical for safe deployment; however,\nexisting detectors often struggle to generalize across datasets of varying\nscales and model architectures, and some can incur high computational costs in\nreal-world applications. Inspired by the phenomenon of Neural Collapse, we\npropose a versatile and efficient OOD detection method. Specifically, we\nre-characterize prior observations that in-distribution (ID) samples form\nclusters, demonstrating that, with appropriate centering, these clusters align\nclosely with model weight vectors. Additionally, we reveal that ID features\ntend to expand into a simplex Equiangular Tight Frame, explaining the common\nobservation that ID features are situated farther from the origin than OOD\nfeatures. Incorporating both insights from Neural Collapse, our OOD detector\nleverages feature proximity to weight vectors and complements this approach by\nusing feature norms to effectively filter out OOD samples. Extensive\nexperiments on off-the-shelf models demonstrate the robustness of our OOD\ndetector across diverse scenarios, mitigating generalization discrepancies and\nenhancing overall performance, with inference latency comparable to that of the\nbasic softmax-confidence detector. Code is available here:\nhttps://github.com/litianliu/NCI-OOD."
                },
                "authors": [
                    {
                        "name": "Litian Liu"
                    },
                    {
                        "name": "Yao Qin"
                    }
                ],
                "author_detail": {
                    "name": "Yao Qin"
                },
                "author": "Yao Qin",
                "arxiv_comment": "CVPR 2025 main conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.01479v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.01479v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06249v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06249v3",
                "updated": "2025-04-03T04:15:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    15,
                    55,
                    3,
                    93,
                    0
                ],
                "published": "2024-07-08T17:55:04Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    17,
                    55,
                    4,
                    0,
                    190,
                    0
                ],
                "title": "CodeUpdateArena: Benchmarking Knowledge Editing on API Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeUpdateArena: Benchmarking Knowledge Editing on API Updates"
                },
                "summary": "Large language models (LLMs) are increasingly being used to synthesize and\nreason about source code. However, the static nature of these models' knowledge\ndoes not reflect the fact that libraries and API functions they invoke are\ncontinuously evolving, with functionality being added or changing. While\nnumerous benchmarks evaluate how LLMs can generate code, no prior work has\nstudied how an LLMs' knowledge about code API functions can be updated. To fill\nthis gap, we present CodeUpdateArena, a benchmark for knowledge editing in the\ncode domain. An instance in our benchmark consists of a synthetic API function\nupdate paired with a program synthesis example that uses the updated\nfunctionality; our goal is to update an LLM to be able to solve this program\nsynthesis example without providing documentation of the update at inference\ntime. Compared to knowledge editing for facts encoded in text, success here is\nmore challenging: a code LLM must correctly reason about the semantics of the\nmodified function rather than just reproduce its syntax. Our dataset is\nconstructed by first prompting GPT-4 to generate atomic and executable function\nupdates. Then, for each update, we generate program synthesis examples whose\ncode solutions are prone to use the update. Our benchmark covers updates of\nvarious types to 54 functions from seven diverse Python packages, with a total\nof 670 program synthesis examples. Our experiments show that prepending\ndocumentation of the update to open-source code LLMs (i.e., DeepSeek,\nCodeLlama) does not allow them to incorporate changes for problem solving, and\nexisting knowledge editing techniques also have substantial room for\nimprovement. We hope our benchmark will inspire new methods for knowledge\nupdating in code LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being used to synthesize and\nreason about source code. However, the static nature of these models' knowledge\ndoes not reflect the fact that libraries and API functions they invoke are\ncontinuously evolving, with functionality being added or changing. While\nnumerous benchmarks evaluate how LLMs can generate code, no prior work has\nstudied how an LLMs' knowledge about code API functions can be updated. To fill\nthis gap, we present CodeUpdateArena, a benchmark for knowledge editing in the\ncode domain. An instance in our benchmark consists of a synthetic API function\nupdate paired with a program synthesis example that uses the updated\nfunctionality; our goal is to update an LLM to be able to solve this program\nsynthesis example without providing documentation of the update at inference\ntime. Compared to knowledge editing for facts encoded in text, success here is\nmore challenging: a code LLM must correctly reason about the semantics of the\nmodified function rather than just reproduce its syntax. Our dataset is\nconstructed by first prompting GPT-4 to generate atomic and executable function\nupdates. Then, for each update, we generate program synthesis examples whose\ncode solutions are prone to use the update. Our benchmark covers updates of\nvarious types to 54 functions from seven diverse Python packages, with a total\nof 670 program synthesis examples. Our experiments show that prepending\ndocumentation of the update to open-source code LLMs (i.e., DeepSeek,\nCodeLlama) does not allow them to incorporate changes for problem solving, and\nexisting knowledge editing techniques also have substantial room for\nimprovement. We hope our benchmark will inspire new methods for knowledge\nupdating in code LLMs."
                },
                "authors": [
                    {
                        "name": "Zeyu Leo Liu"
                    },
                    {
                        "name": "Shrey Pandit"
                    },
                    {
                        "name": "Xi Ye"
                    },
                    {
                        "name": "Eunsol Choi"
                    },
                    {
                        "name": "Greg Durrett"
                    }
                ],
                "author_detail": {
                    "name": "Greg Durrett"
                },
                "author": "Greg Durrett",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06249v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06249v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10512v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10512v3",
                "updated": "2025-04-03T04:00:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    0,
                    6,
                    3,
                    93,
                    0
                ],
                "published": "2024-02-16T08:57:31Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    8,
                    57,
                    31,
                    4,
                    47,
                    0
                ],
                "title": "A Novel Computing Paradigm for MobileNetV3 using Memristor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Computing Paradigm for MobileNetV3 using Memristor"
                },
                "summary": "The increasing computational demands of deep learning models pose significant\nchallenges for edge devices. To address this, we propose a memristor-based\ncircuit design for MobileNetV3, specifically for image classification tasks.\nOur design leverages the low power consumption and high integration density of\nmemristors, making it suitable for edge computing. The architecture includes\noptimized memristive convolutional modules, batch normalization modules,\nactivation function modules, global average pooling modules, and fully\nconnected modules. Experimental results on the CIFAR-10 dataset show that our\nmemristor-based MobileNetV3 achieves over 90% accuracy while significantly\nreducing inference time and energy consumption compared to traditional\nimplementations. This work demonstrates the potential of memristor-based\ndesigns for efficient deployment of deep learning models in\nresource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing computational demands of deep learning models pose significant\nchallenges for edge devices. To address this, we propose a memristor-based\ncircuit design for MobileNetV3, specifically for image classification tasks.\nOur design leverages the low power consumption and high integration density of\nmemristors, making it suitable for edge computing. The architecture includes\noptimized memristive convolutional modules, batch normalization modules,\nactivation function modules, global average pooling modules, and fully\nconnected modules. Experimental results on the CIFAR-10 dataset show that our\nmemristor-based MobileNetV3 achieves over 90% accuracy while significantly\nreducing inference time and energy consumption compared to traditional\nimplementations. This work demonstrates the potential of memristor-based\ndesigns for efficient deployment of deep learning models in\nresource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Jiale Li"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Sean Longyu Ma"
                    },
                    {
                        "name": "Chiu-Wing Sham"
                    },
                    {
                        "name": "Chong Fu"
                    }
                ],
                "author_detail": {
                    "name": "Chong Fu"
                },
                "author": "Chong Fu",
                "arxiv_comment": "Published at the 2025 International Joint Conference on Neural\n  Networks (IJCNN 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10512v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10512v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17651v2",
                "updated": "2025-04-03T03:58:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    3,
                    58,
                    45,
                    3,
                    93,
                    0
                ],
                "published": "2024-03-26T12:31:58Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    12,
                    31,
                    58,
                    1,
                    86,
                    0
                ],
                "title": "Exploring Dynamic Transformer for Efficient Object Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Dynamic Transformer for Efficient Object Tracking"
                },
                "summary": "The speed-precision trade-off is a critical problem for visual object\ntracking which usually requires low latency and deployment on constrained\nresources. Existing solutions for efficient tracking mainly focus on adopting\nlight-weight backbones or modules, which nevertheless come at the cost of a\nsacrifice in precision. In this paper, inspired by dynamic network routing, we\npropose DyTrack, a dynamic transformer framework for efficient tracking.\nReal-world tracking scenarios exhibit diverse levels of complexity. We argue\nthat a simple network is sufficient for easy frames in video sequences, while\nmore computation could be assigned to difficult ones. DyTrack automatically\nlearns to configure proper reasoning routes for various inputs, gaining better\nutilization of the available computational budget. Thus, it can achieve higher\nperformance with the same running speed. We formulate instance-specific\ntracking as a sequential decision problem and attach terminating branches to\nintermediate layers of the entire model. Especially, to fully utilize the\ncomputations, we introduce the feature recycling mechanism to reuse the outputs\nof predecessors. Furthermore, a target-aware self-distillation strategy is\ndesigned to enhance the discriminating capabilities of early predictions by\neffectively mimicking the representation pattern of the deep model. Extensive\nexperiments on multiple benchmarks demonstrate that DyTrack achieves promising\nspeed-precision trade-offs with only a single model. For instance, DyTrack\nobtains 64.9% AUC on LaSOT with a speed of 256 fps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The speed-precision trade-off is a critical problem for visual object\ntracking which usually requires low latency and deployment on constrained\nresources. Existing solutions for efficient tracking mainly focus on adopting\nlight-weight backbones or modules, which nevertheless come at the cost of a\nsacrifice in precision. In this paper, inspired by dynamic network routing, we\npropose DyTrack, a dynamic transformer framework for efficient tracking.\nReal-world tracking scenarios exhibit diverse levels of complexity. We argue\nthat a simple network is sufficient for easy frames in video sequences, while\nmore computation could be assigned to difficult ones. DyTrack automatically\nlearns to configure proper reasoning routes for various inputs, gaining better\nutilization of the available computational budget. Thus, it can achieve higher\nperformance with the same running speed. We formulate instance-specific\ntracking as a sequential decision problem and attach terminating branches to\nintermediate layers of the entire model. Especially, to fully utilize the\ncomputations, we introduce the feature recycling mechanism to reuse the outputs\nof predecessors. Furthermore, a target-aware self-distillation strategy is\ndesigned to enhance the discriminating capabilities of early predictions by\neffectively mimicking the representation pattern of the deep model. Extensive\nexperiments on multiple benchmarks demonstrate that DyTrack achieves promising\nspeed-precision trade-offs with only a single model. For instance, DyTrack\nobtains 64.9% AUC on LaSOT with a speed of 256 fps."
                },
                "authors": [
                    {
                        "name": "Jiawen Zhu"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Haiwen Diao"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Jun-Yan He"
                    },
                    {
                        "name": "Chenyang Li"
                    },
                    {
                        "name": "Bin Luo"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Huchuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Huchuan Lu"
                },
                "author": "Huchuan Lu",
                "arxiv_doi": "10.1109/TNNLS.2025.3545752",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TNNLS.2025.3545752",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.17651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by TNNLS",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]