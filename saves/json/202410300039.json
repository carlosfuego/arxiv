[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.21266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21266v1",
                "updated": "2024-10-28T17:57:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:57:40Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "title": "Online Weighted Paging with Unknown Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Weighted Paging with Unknown Weights"
                },
                "summary": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling."
                },
                "authors": [
                    {
                        "name": "Orin Levy"
                    },
                    {
                        "name": "Noam Touitou"
                    },
                    {
                        "name": "Aviv Rosenberg"
                    }
                ],
                "author_detail": {
                    "name": "Aviv Rosenberg"
                },
                "author": "Aviv Rosenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v2",
                "updated": "2024-10-28T16:42:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    42,
                    11,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe)."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21142v1",
                "updated": "2024-10-28T15:43:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    33,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)"
                },
                "summary": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient."
                },
                "authors": [
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Hua Lu"
                    },
                    {
                        "name": "Christian S. Jensen"
                    }
                ],
                "author_detail": {
                    "name": "Christian S. Jensen"
                },
                "author": "Christian S. Jensen",
                "arxiv_comment": "Accepted at TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v2",
                "updated": "2024-10-28T14:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    44,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21073v1",
                "updated": "2024-10-28T14:35:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices"
                },
                "summary": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board."
                },
                "authors": [
                    {
                        "name": "Hiroki Matsutani"
                    },
                    {
                        "name": "Masaaki Kondo"
                    },
                    {
                        "name": "Kazuki Sunaga"
                    },
                    {
                        "name": "Radu Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Marculescu"
                },
                "author": "Radu Marculescu",
                "arxiv_comment": "ASP-DAC 2025 (accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v1",
                "updated": "2024-10-28T13:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v1",
                "updated": "2024-10-28T07:13:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01847v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01847v3",
                "updated": "2024-10-27T14:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    14,
                    40,
                    8,
                    6,
                    301,
                    0
                ],
                "published": "2024-04-02T11:12:42Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    11,
                    12,
                    42,
                    1,
                    93,
                    0
                ],
                "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Transformer Pre-training with 2:4 Sparsity"
                },
                "summary": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain."
                },
                "authors": [
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Weiyu Huang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024), in Proceedings of Machine Learning Research 235:19531-19543",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01847v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20337v1",
                "updated": "2024-10-27T04:31:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "published": "2024-10-27T04:31:35Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "title": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms"
                },
                "summary": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided."
                },
                "authors": [
                    {
                        "name": "Lorenzo De Stefani"
                    },
                    {
                        "name": "Vedant Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vedant Gupta"
                },
                "author": "Vedant Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04216v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04216v3",
                "updated": "2024-10-26T22:19:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    22,
                    19,
                    4,
                    5,
                    300,
                    0
                ],
                "published": "2024-02-06T18:17:02Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    18,
                    17,
                    2,
                    1,
                    37,
                    0
                ],
                "title": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks"
                },
                "summary": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines."
                },
                "authors": [
                    {
                        "name": "Md Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in IEEE Transactions on\n  Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04216v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04216v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20149v1",
                "updated": "2024-10-26T11:20:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "published": "2024-10-26T11:20:02Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "title": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models"
                },
                "summary": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}."
                },
                "authors": [
                    {
                        "name": "Yabin Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "NIPS 2024 Camera Ready, Codes are available at\n  \\url{https://github.com/YBZh/OpenOOD-VLM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20004v1",
                "updated": "2024-10-25T23:17:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T23:17:56Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "title": "Lightweight, Secure and Stateful Serverless Computing with PSL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight, Secure and Stateful Serverless Computing with PSL"
                },
                "summary": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks."
                },
                "authors": [
                    {
                        "name": "Alexander Thomas"
                    },
                    {
                        "name": "Shubham Mishra"
                    },
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "John Kubiatowicz"
                    }
                ],
                "author_detail": {
                    "name": "John Kubiatowicz"
                },
                "author": "John Kubiatowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05317v2",
                "updated": "2024-10-25T21:09:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    21,
                    9,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-08T01:35:11Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    1,
                    35,
                    11,
                    5,
                    160,
                    0
                ],
                "title": "LoCoCo: Dropping In Convolutions for Long Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoCoCo: Dropping In Convolutions for Long Context Compression"
                },
                "summary": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v2",
                "updated": "2024-10-25T19:45:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    45,
                    33,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19937v1",
                "updated": "2024-10-25T19:18:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T19:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "title": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction"
                },
                "summary": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Zian Wang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Changjiang Li"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18248v2",
                "updated": "2024-10-25T19:18:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    0,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T19:53:30Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    19,
                    53,
                    30,
                    2,
                    297,
                    0
                ],
                "title": "Fast Inference for Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Inference for Augmented Large Language Models"
                },
                "summary": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM."
                },
                "authors": [
                    {
                        "name": "Rana Shahout"
                    },
                    {
                        "name": "Cong Liang"
                    },
                    {
                        "name": "Shiji Xin"
                    },
                    {
                        "name": "Qianru Lao"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Minlan Yu"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18079v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18079v5",
                "updated": "2024-10-25T18:29:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    18,
                    29,
                    43,
                    4,
                    299,
                    0
                ],
                "published": "2024-01-31T18:58:14Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    18,
                    58,
                    14,
                    2,
                    31,
                    0
                ],
                "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization"
                },
                "summary": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18079v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18079v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v1",
                "updated": "2024-10-25T07:24:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v1",
                "updated": "2024-10-25T03:01:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v2",
                "updated": "2024-10-28T19:32:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    32,
                    23,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark."
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19123v1",
                "updated": "2024-10-24T19:48:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T19:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "title": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design"
                },
                "summary": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yeonju Ro"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v2",
                "updated": "2024-10-24T16:40:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    40,
                    10,
                    3,
                    298,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "João Monteiro"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Pierre-André Noël"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vázquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v1",
                "updated": "2024-10-24T10:36:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a Base Station (BS) associated with local cache,\nwhich in turn is connected to the backend server and users. The contents get\ncontinuously updated at the backend server, and the BS has a local copy of the\nsubset of the contents. Upon receiving a request from the user, the BS can\neither fetch a fresh version or, serve the local copy or can wait for\nadditional requests before serving. Fetching content from the BS incurs a fixed\nfetching cost, serving it locally incurs an aging cost, and for each request\nwaiting at the BS, there will be a waiting for cost per unit time. The aging\ncost relies on the freshness of the content, which is measured by a metric age\nof version (AoV). We aim to minimize the average cost subject to cache capacity\nconstraints. We pose the problem as Restless Multi-armed Bandits Problem (RMAB)\nand propose a Whittle index-based policy that performs very close to the\noptimal policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a Base Station (BS) associated with local cache,\nwhich in turn is connected to the backend server and users. The contents get\ncontinuously updated at the backend server, and the BS has a local copy of the\nsubset of the contents. Upon receiving a request from the user, the BS can\neither fetch a fresh version or, serve the local copy or can wait for\nadditional requests before serving. Fetching content from the BS incurs a fixed\nfetching cost, serving it locally incurs an aging cost, and for each request\nwaiting at the BS, there will be a waiting for cost per unit time. The aging\ncost relies on the freshness of the content, which is measured by a metric age\nof version (AoV). We aim to minimize the average cost subject to cache capacity\nconstraints. We pose the problem as Restless Multi-armed Bandits Problem (RMAB)\nand propose a Whittle index-based policy that performs very close to the\noptimal policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18517v1",
                "updated": "2024-10-24T08:06:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T08:06:41Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing"
                },
                "summary": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory."
                },
                "authors": [
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Dongjie Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "arxiv_comment": "Under Review by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18441v1",
                "updated": "2024-10-24T05:29:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T05:29:20Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "title": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI"
                },
                "summary": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings."
                },
                "authors": [
                    {
                        "name": "Fulu Li"
                    }
                ],
                "author_detail": {
                    "name": "Fulu Li"
                },
                "author": "Fulu Li",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v1",
                "updated": "2024-10-23T16:25:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhe Chen"
                },
                "author": "Mingzhe Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08437v2",
                "updated": "2024-10-23T15:44:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    44,
                    9,
                    2,
                    297,
                    0
                ],
                "published": "2023-10-12T16:01:46Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    16,
                    1,
                    46,
                    3,
                    285,
                    0
                ],
                "title": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions"
                },
                "summary": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions."
                },
                "authors": [
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1145/3700875",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3700875",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in ACM Computing Survey,\n  2024",
                "arxiv_journal_ref": "ACM Computing Surveys 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17954v1",
                "updated": "2024-10-23T15:24:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:54Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference"
                },
                "summary": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Shunkang Zhang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Ivor Tsang"
                    },
                    {
                        "name": "Ong Yew Soon"
                    }
                ],
                "author_detail": {
                    "name": "Ong Yew Soon"
                },
                "author": "Ong Yew Soon",
                "arxiv_comment": "Mixture-of-Experts, Inference, Offloading",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v1",
                "updated": "2024-10-23T14:15:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v3",
                "updated": "2024-10-23T10:39:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    39,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v1",
                "updated": "2024-10-23T07:53:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Minpeng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Minpeng Liao"
                },
                "author": "Minpeng Liao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v5",
                "updated": "2024-10-23T05:55:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    5,
                    55,
                    31,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14740v2",
                "updated": "2024-10-23T01:08:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    1,
                    8,
                    59,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-17T08:33:39Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    33,
                    39,
                    3,
                    291,
                    0
                ],
                "title": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD."
                },
                "authors": [
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Zhang Cao"
                    },
                    {
                        "name": "Huaizhi Qu"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Yanyong Zhang"
                    },
                    {
                        "name": "Zhichao Cao"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11724v2",
                "updated": "2024-10-22T19:07:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    22,
                    19,
                    7,
                    8,
                    1,
                    296,
                    0
                ],
                "published": "2024-05-20T01:57:34Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    1,
                    57,
                    34,
                    0,
                    141,
                    0
                ],
                "title": "Token-wise Influential Training Data Retrieval for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-wise Influential Training Data Retrieval for Large Language Models"
                },
                "summary": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn."
                },
                "authors": [
                    {
                        "name": "Huawei Lin"
                    },
                    {
                        "name": "Jikai Long"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Weijie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weijie Zhao"
                },
                "author": "Weijie Zhao",
                "arxiv_comment": "Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v5",
                "updated": "2024-10-21T22:56:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    22,
                    56,
                    6,
                    0,
                    295,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16218v1",
                "updated": "2024-10-21T17:23:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T17:23:03Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "title": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire"
                },
                "summary": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress."
                },
                "authors": [
                    {
                        "name": "Md Tahmidul Alam"
                    },
                    {
                        "name": "Swarnav Mukhopadhyay"
                    },
                    {
                        "name": "Md Mobinul Haque"
                    },
                    {
                        "name": "Shubhra S. Pasayat"
                    },
                    {
                        "name": "Chirag Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Gupta"
                },
                "author": "Chirag Gupta",
                "arxiv_comment": "4 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v2",
                "updated": "2024-10-21T15:59:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    15,
                    59,
                    18,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v1",
                "updated": "2024-10-21T11:29:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14142v2",
                "updated": "2024-10-21T07:24:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    24,
                    53,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-18T03:30:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    30,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels"
                },
                "summary": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed."
                },
                "authors": [
                    {
                        "name": "Tianqing Zhou"
                    },
                    {
                        "name": "Bobo Wang"
                    },
                    {
                        "name": "Dong Qin"
                    },
                    {
                        "name": "Xuefang Nie"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15704v1",
                "updated": "2024-10-21T07:20:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T07:20:41Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "title": "Residual vector quantization for KV cache compression in large language\n  model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual vector quantization for KV cache compression in large language\n  model"
                },
                "summary": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision."
                },
                "authors": [
                    {
                        "name": "Ankur Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ankur Kumar"
                },
                "author": "Ankur Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v2",
                "updated": "2024-10-21T05:06:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    5,
                    6,
                    1,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v2",
                "updated": "2024-10-21T02:35:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    2,
                    35,
                    8,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04053v2",
                "updated": "2024-10-20T13:37:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    13,
                    37,
                    46,
                    6,
                    294,
                    0
                ],
                "published": "2024-07-04T16:51:17Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    16,
                    51,
                    17,
                    3,
                    186,
                    0
                ],
                "title": "Edge AI: A Taxonomy, Systematic Review and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI: A Taxonomy, Systematic Review and Future Directions"
                },
                "summary": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions."
                },
                "authors": [
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Junhui Du"
                    },
                    {
                        "name": "Huaming Wu"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Subramaniam Subramanian Murugesan"
                    },
                    {
                        "name": "Babar Ali"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Prabal Verma"
                    },
                    {
                        "name": "Surendra Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1007/s10586-024-04686-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-024-04686-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.04053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in Springer Cluster\n  Computing, 2024",
                "arxiv_journal_ref": "Springer Cluster Computing, Volume 28, article number 18, pages\n  11953 - 11981, (2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15344v1",
                "updated": "2024-10-20T09:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T09:37:07Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "title": "LLC Intra-set Write Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLC Intra-set Write Balancing"
                },
                "summary": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance."
                },
                "authors": [
                    {
                        "name": "Keshav Krishna"
                    },
                    {
                        "name": "Ayush Verma"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Verma"
                },
                "author": "Ayush Verma",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v1",
                "updated": "2024-10-20T08:42:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15252v1",
                "updated": "2024-10-20T02:17:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T02:17:35Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "title": "Lossless KV Cache Compression to 2%",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless KV Cache Compression to 2%"
                },
                "summary": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "J. N. Han"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanhui Kang"
                },
                "author": "Zhanhui Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.05579v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.05579v4",
                "updated": "2024-10-19T12:15:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    12,
                    15,
                    50,
                    5,
                    293,
                    0
                ],
                "published": "2022-06-11T17:52:10Z",
                "published_parsed": [
                    2022,
                    6,
                    11,
                    17,
                    52,
                    10,
                    5,
                    162,
                    0
                ],
                "title": "Online Paging with Heterogeneous Cache Slots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Paging with Heterogeneous Cache Slots"
                },
                "summary": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized)."
                },
                "authors": [
                    {
                        "name": "Marek Chrobak"
                    },
                    {
                        "name": "Samuel Haney"
                    },
                    {
                        "name": "Mehraneh Liaee"
                    },
                    {
                        "name": "Debmalya Panigrahi"
                    },
                    {
                        "name": "Rajmohan Rajaraman"
                    },
                    {
                        "name": "Ravi Sundaram"
                    },
                    {
                        "name": "Neal E. Young"
                    }
                ],
                "author_detail": {
                    "name": "Neal E. Young"
                },
                "author": "Neal E. Young",
                "arxiv_doi": "10.1007/s00453-024-01270-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00453-024-01270-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2206.05579v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.05579v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "conference and journal versions appear in STACS 2023 and Algorithmica\n  (2004)",
                "arxiv_journal_ref": "Algorithmica (2004)",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0; F.1.2; C.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v2",
                "updated": "2024-10-19T08:45:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    8,
                    45,
                    11,
                    5,
                    293,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v3",
                "updated": "2024-10-18T19:30:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    19,
                    30,
                    35,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "4th NeurIPS Efficient Natural Language and Speech Processing Workshop\n  (ENLSP-IV 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v2",
                "updated": "2024-10-18T13:59:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    59,
                    54,
                    4,
                    292,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v1",
                "updated": "2024-10-18T13:01:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10859v2",
                "updated": "2024-10-18T10:02:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    2,
                    3,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-07T13:46:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "FAME: Towards Factual Multi-Task Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAME: Towards Factual Multi-Task Model Editing"
                },
                "summary": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality."
                },
                "authors": [
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Yingyu Shan"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "9 pages, 3 figures. This paper has been accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v1",
                "updated": "2024-10-17T20:11:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_comment": "RTSS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v1",
                "updated": "2024-10-17T17:58:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction"
                },
                "summary": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v4",
                "updated": "2024-10-17T15:27:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    15,
                    27,
                    30,
                    3,
                    291,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07979v2",
                "updated": "2024-10-17T08:54:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    54,
                    37,
                    3,
                    291,
                    0
                ],
                "published": "2024-04-11T17:57:22Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    17,
                    57,
                    22,
                    3,
                    102,
                    0
                ],
                "title": "LLoCO: Learning Long Contexts Offline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLoCO: Learning Long Contexts Offline"
                },
                "summary": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco."
                },
                "authors": [
                    {
                        "name": "Sijun Tan"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Shishir Patil"
                    },
                    {
                        "name": "Ziyang Wu"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Raluca Ada Popa"
                    }
                ],
                "author_detail": {
                    "name": "Raluca Ada Popa"
                },
                "author": "Raluca Ada Popa",
                "arxiv_comment": "EMNLP 2024. The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18126v1",
                "updated": "2024-10-17T04:37:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:37:43Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "title": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers"
                },
                "summary": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shubham"
                    },
                    {
                        "name": "Keichi Takahashi"
                    },
                    {
                        "name": "Hiroyuki Takizawa"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Takizawa"
                },
                "author": "Hiroyuki Takizawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13212v1",
                "updated": "2024-10-17T04:35:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:35:57Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "title": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations"
                },
                "summary": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters."
                },
                "authors": [
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v3",
                "updated": "2024-10-16T17:54:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    54,
                    15,
                    2,
                    290,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Accepted to PVLDB Volume 18",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12749v1",
                "updated": "2024-10-16T17:10:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:10:48Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "title": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM"
                },
                "summary": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads."
                },
                "authors": [
                    {
                        "name": "Juechu Dong"
                    },
                    {
                        "name": "Jonah Rosenblum"
                    },
                    {
                        "name": "Satish Narayanasamy"
                    }
                ],
                "author_detail": {
                    "name": "Satish Narayanasamy"
                },
                "author": "Satish Narayanasamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12605v1",
                "updated": "2024-10-16T14:24:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:24:16Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "title": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques"
                },
                "summary": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Rishal Ravikesh Chand"
                    },
                    {
                        "name": "Neeraj Anand Sharma"
                    },
                    {
                        "name": "Muhammad Ashad Kabir"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ashad Kabir"
                },
                "author": "Muhammad Ashad Kabir",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v1",
                "updated": "2024-10-16T12:45:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "17 pages, 6 figures, Submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12423v1",
                "updated": "2024-10-16T10:06:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:06:22Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "title": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors"
                },
                "summary": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency."
                },
                "authors": [
                    {
                        "name": "Qinghang Zhao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Yixi Ji"
                    },
                    {
                        "name": "Jinjian Wu"
                    },
                    {
                        "name": "Guangming Shi"
                    }
                ],
                "author_detail": {
                    "name": "Guangming Shi"
                },
                "author": "Guangming Shi",
                "arxiv_doi": "10.1145/3676536.3676710",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676710",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.12423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14731v1",
                "updated": "2024-10-16T08:34:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T08:34:51Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "title": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection"
                },
                "summary": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base."
                },
                "authors": [
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Zipeng Xiao"
                    },
                    {
                        "name": "Siqi Kou"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Xiaofeng Gao"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12168v1",
                "updated": "2024-10-16T02:16:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T02:16:53Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "title": "COMET: Towards Partical W4A4KV4 LLMs Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMET: Towards Partical W4A4KV4 LLMs Serving"
                },
                "summary": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective."
                },
                "authors": [
                    {
                        "name": "Lian Liu"
                    },
                    {
                        "name": "Haimeng Ren"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Zhaohui Xu"
                    },
                    {
                        "name": "Yudong Pan"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Xiaowei Li"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Ying Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wang"
                },
                "author": "Ying Wang",
                "arxiv_comment": "14 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02536v2",
                "updated": "2024-10-15T15:58:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    58,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-04T17:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    55,
                    38,
                    1,
                    156,
                    0
                ],
                "title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11417v1",
                "updated": "2024-10-15T09:07:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T09:07:25Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "title": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models"
                },
                "summary": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaohan Lan"
                    },
                    {
                        "name": "Yitian Yuan"
                    },
                    {
                        "name": "Zequn Jie"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09297v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09297v3",
                "updated": "2024-10-15T08:45:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    45,
                    18,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-13T16:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    16,
                    33,
                    44,
                    3,
                    165,
                    0
                ],
                "title": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding"
                },
                "summary": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv"
                },
                "authors": [
                    {
                        "name": "Zayd Muhammad Kawakibi Zuhri"
                    },
                    {
                        "name": "Muhammad Farid Adilazuarda"
                    },
                    {
                        "name": "Ayu Purwarianti"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09297v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09297v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v2",
                "updated": "2024-10-15T06:09:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    6,
                    9,
                    35,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v1",
                "updated": "2024-10-15T05:57:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v3",
                "updated": "2024-10-15T05:34:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    34,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11260v1",
                "updated": "2024-10-15T04:35:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T04:35:49Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "title": "A Zoned Storage Optimized Flash Cache on ZNS SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zoned Storage Optimized Flash Cache on ZNS SSDs"
                },
                "summary": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme."
                },
                "authors": [
                    {
                        "name": "Chongzhuo Yang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v4",
                "updated": "2024-10-14T19:12:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    19,
                    12,
                    48,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10819v1",
                "updated": "2024-10-14T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads"
                },
                "summary": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention."
                },
                "authors": [
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Jingwei Zuo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v1",
                "updated": "2024-10-14T17:50:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10511v1",
                "updated": "2024-10-14T13:49:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:49:06Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "title": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling"
                },
                "summary": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities."
                },
                "authors": [
                    {
                        "name": "Wenze Liu"
                    },
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Sheng Xia"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "19 pages, 17 figures, 8 tables, github repo:\n  https://github.com/poppuppy/SAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v2",
                "updated": "2024-10-14T09:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    35,
                    35,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13378v2",
                "updated": "2024-10-14T07:58:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    7,
                    58,
                    39,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-22T06:19:43Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    6,
                    19,
                    43,
                    2,
                    143,
                    0
                ],
                "title": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation"
                },
                "summary": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency."
                },
                "authors": [
                    {
                        "name": "Quyang Pan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zhiyuan Wu"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Bo Gao"
                    },
                    {
                        "name": "Jingyuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyuan Wang"
                },
                "author": "Jingyuan Wang",
                "arxiv_comment": "17 pages, 7 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10157v1",
                "updated": "2024-10-14T04:49:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:49:22Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "title": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI"
                },
                "summary": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI."
                },
                "authors": [
                    {
                        "name": "Meng Gao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Huafu Li"
                    },
                    {
                        "name": "Junqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Junqi Guo"
                },
                "author": "Junqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10149v1",
                "updated": "2024-10-14T04:30:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:30:38Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "title": "Fast and Accurate Neural Rendering Using Semi-Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Accurate Neural Rendering Using Semi-Gradients"
                },
                "summary": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss."
                },
                "authors": [
                    {
                        "name": "In-Young Cho"
                    },
                    {
                        "name": "Jaewoong Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoong Cho"
                },
                "author": "Jaewoong Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10071v1",
                "updated": "2024-10-14T01:25:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T01:25:56Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "title": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning"
                },
                "summary": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jinjin Shen"
                    },
                    {
                        "name": "Yan Lin"
                    },
                    {
                        "name": "Yijin Zhang"
                    },
                    {
                        "name": "Weibin Zhang"
                    },
                    {
                        "name": "Feng Shu"
                    },
                    {
                        "name": "Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Jun Li"
                },
                "author": "Jun Li",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09533v1",
                "updated": "2024-10-12T13:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T13:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "title": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence"
                },
                "summary": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24"
                },
                "authors": [
                    {
                        "name": "Felipe Cadar"
                    },
                    {
                        "name": "Guilherme Potje"
                    },
                    {
                        "name": "Renato Martins"
                    },
                    {
                        "name": "Cédric Demonceaux"
                    },
                    {
                        "name": "Erickson R. Nascimento"
                    }
                ],
                "author_detail": {
                    "name": "Erickson R. Nascimento"
                },
                "author": "Erickson R. Nascimento",
                "arxiv_comment": "Accepted in ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v1",
                "updated": "2024-10-12T10:38:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09397v1",
                "updated": "2024-10-12T07:01:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T07:01:30Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "title": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yufa Zhou"
                },
                "author": "Yufa Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v3",
                "updated": "2024-10-12T02:11:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    2,
                    11,
                    14,
                    5,
                    286,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS'24 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09237v1",
                "updated": "2024-10-11T20:23:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T20:23:00Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "title": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor"
                },
                "summary": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}."
                },
                "authors": [
                    {
                        "name": "Sahar Ahmadi"
                    },
                    {
                        "name": "Ali Cheraghian"
                    },
                    {
                        "name": "Morteza Saberi"
                    },
                    {
                        "name": "Md. Towsif Abir"
                    },
                    {
                        "name": "Hamidreza Dastmalchi"
                    },
                    {
                        "name": "Farookh Hussain"
                    },
                    {
                        "name": "Shafin Rahman"
                    }
                ],
                "author_detail": {
                    "name": "Shafin Rahman"
                },
                "author": "Shafin Rahman",
                "arxiv_comment": "ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08895v1",
                "updated": "2024-10-11T15:12:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:12:30Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "title": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation"
                },
                "summary": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Kun Ding"
                    },
                    {
                        "name": "Qiang Yu"
                    },
                    {
                        "name": "Haojian Zhang"
                    },
                    {
                        "name": "Gaofeng Meng"
                    },
                    {
                        "name": "Shiming Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Shiming Xiang"
                },
                "author": "Shiming Xiang",
                "arxiv_comment": "submitted to IJCV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v1",
                "updated": "2024-10-11T12:19:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtárik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtárik"
                },
                "author": "Peter Richtárik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08618v1",
                "updated": "2024-10-11T08:33:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T08:33:58Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "title": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination"
                },
                "summary": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Qiulin Tian"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Tong Xin"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v1",
                "updated": "2024-10-11T07:24:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03462v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03462v3",
                "updated": "2024-10-11T02:18:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    2,
                    18,
                    24,
                    4,
                    285,
                    0
                ],
                "published": "2024-01-07T11:57:40Z",
                "published_parsed": [
                    2024,
                    1,
                    7,
                    11,
                    57,
                    40,
                    6,
                    7,
                    0
                ],
                "title": "Long Context Compression with Activation Beacon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Compression with Activation Beacon"
                },
                "summary": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}."
                },
                "authors": [
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Ninglu Shao"
                    },
                    {
                        "name": "Qiwei Ye"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "Newer version of Activation Beacon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.03462v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03462v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08391v1",
                "updated": "2024-10-10T21:55:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    21,
                    55,
                    11,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T21:55:11Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    21,
                    55,
                    11,
                    3,
                    284,
                    0
                ],
                "title": "KV Prediction for Improved Time to First Token",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Prediction for Improved Time to First Token"
                },
                "summary": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of $15\\%-50\\%$ across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to $30\\%$ on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of $15\\%-50\\%$ across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to $30\\%$ on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction ."
                },
                "authors": [
                    {
                        "name": "Maxwell Horton"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Chenfan Sun"
                    },
                    {
                        "name": "Yanzi Jin"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Moin Nabi"
                    }
                ],
                "author_detail": {
                    "name": "Moin Nabi"
                },
                "author": "Moin Nabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11284v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11284v3",
                "updated": "2024-10-10T16:57:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    57,
                    34,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-17T11:48:14Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    48,
                    14,
                    2,
                    108,
                    0
                ],
                "title": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations"
                },
                "summary": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "F. Nisa Bostanci"
                    },
                    {
                        "name": "Ataberk Olgun"
                    },
                    {
                        "name": "A. Giray Yaglikci"
                    },
                    {
                        "name": "Ismail Emir Yuksel"
                    },
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Zulal Bingol"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11284v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12850v1",
                "updated": "2024-10-10T15:24:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    24,
                    12,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T15:24:12Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    24,
                    12,
                    3,
                    284,
                    0
                ],
                "title": "RecurFormer: Not All Transformer Heads Need Self-Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecurFormer: Not All Transformer Heads Need Self-Attention"
                },
                "summary": "Transformer-based large language models (LLMs) excel in modeling complex\nlanguage patterns but face significant computational costs during inference,\nespecially with long inputs due to the attention mechanism's memory overhead.\nWe observe that certain attention heads exhibit a distribution where the\nattention weights concentrate on tokens near the query token, termed as recency\naware, which focuses on local and short-range dependencies. Leveraging this\ninsight, we propose RecurFormer, a novel architecture that replaces these\nattention heads with linear recurrent neural networks (RNNs), specifically the\nMamba architecture. This replacement reduces the cache size without evicting\ntokens, thus maintaining generation quality. RecurFormer retains the ability to\nmodel long-range dependencies through the remaining attention heads and allows\nfor reusing pre-trained Transformer-based LLMs weights with continual training.\nExperiments demonstrate that RecurFormer matches the original model's\nperformance while significantly enhancing inference efficiency. Our approach\nprovides a practical solution to the computational challenges of\nTransformer-based LLMs inference, making it highly attractive for tasks\ninvolving long inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) excel in modeling complex\nlanguage patterns but face significant computational costs during inference,\nespecially with long inputs due to the attention mechanism's memory overhead.\nWe observe that certain attention heads exhibit a distribution where the\nattention weights concentrate on tokens near the query token, termed as recency\naware, which focuses on local and short-range dependencies. Leveraging this\ninsight, we propose RecurFormer, a novel architecture that replaces these\nattention heads with linear recurrent neural networks (RNNs), specifically the\nMamba architecture. This replacement reduces the cache size without evicting\ntokens, thus maintaining generation quality. RecurFormer retains the ability to\nmodel long-range dependencies through the remaining attention heads and allows\nfor reusing pre-trained Transformer-based LLMs weights with continual training.\nExperiments demonstrate that RecurFormer matches the original model's\nperformance while significantly enhancing inference efficiency. Our approach\nprovides a practical solution to the computational challenges of\nTransformer-based LLMs inference, making it highly attractive for tasks\ninvolving long inputs."
                },
                "authors": [
                    {
                        "name": "Ruiqing Yan"
                    },
                    {
                        "name": "Linghan Zheng"
                    },
                    {
                        "name": "Xingbo Du"
                    },
                    {
                        "name": "Han Zou"
                    },
                    {
                        "name": "Yufeng Guo"
                    },
                    {
                        "name": "Jianfei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Yang"
                },
                "author": "Jianfei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01195v2",
                "updated": "2024-10-10T11:01:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    1,
                    44,
                    3,
                    284,
                    0
                ],
                "published": "2024-06-03T10:58:32Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    10,
                    58,
                    32,
                    0,
                    155,
                    0
                ],
                "title": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping"
                },
                "summary": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Qijie Ge"
                    },
                    {
                        "name": "Lulu Suo"
                    },
                    {
                        "name": "Weijie Tang"
                    },
                    {
                        "name": "Zhengyu Wei"
                    },
                    {
                        "name": "Longxiang Huang"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04793v2",
                "updated": "2024-10-10T05:11:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    5,
                    11,
                    52,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-07T03:08:14Z",
                "published_parsed": [
                    2024,
                    4,
                    7,
                    3,
                    8,
                    14,
                    6,
                    98,
                    0
                ],
                "title": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget"
                },
                "summary": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention."
                },
                "authors": [
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Shaoduo Gan"
                    }
                ],
                "author_detail": {
                    "name": "Shaoduo Gan"
                },
                "author": "Shaoduo Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07590v1",
                "updated": "2024-10-10T03:52:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:52:54Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text"
                },
                "summary": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems."
                },
                "authors": [
                    {
                        "name": "Songshuo Lu"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Yutian Rong"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07579v1",
                "updated": "2024-10-10T03:28:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:28:46Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "title": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching"
                },
                "summary": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy."
                },
                "authors": [
                    {
                        "name": "Ruonan Yu"
                    },
                    {
                        "name": "Songhua Liu"
                    },
                    {
                        "name": "Jingwen Ye"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted by ECCV2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19519v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19519v3",
                "updated": "2024-10-09T15:57:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    57,
                    3,
                    2,
                    283,
                    0
                ],
                "published": "2024-03-28T15:52:15Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    15,
                    52,
                    15,
                    3,
                    88,
                    0
                ],
                "title": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage"
                },
                "summary": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser."
                },
                "authors": [
                    {
                        "name": "Philip Wykeham Bradford"
                    },
                    {
                        "name": "Valeria Ospina-Bohorquez"
                    },
                    {
                        "name": "Michael Ehret"
                    },
                    {
                        "name": "Jose-Luis Henares"
                    },
                    {
                        "name": "Pilar Puyuelo-Valdes"
                    },
                    {
                        "name": "Tomasz Chodukowski"
                    },
                    {
                        "name": "Tadeusz Pisarczyk"
                    },
                    {
                        "name": "Zofia Rusiniak"
                    },
                    {
                        "name": "Carlos Salgado-Lopez"
                    },
                    {
                        "name": "Christos Vlachos"
                    },
                    {
                        "name": "Massimiliano Sciscio"
                    },
                    {
                        "name": "Martina Salvadori"
                    },
                    {
                        "name": "Claudio Verona"
                    },
                    {
                        "name": "George Hicks"
                    },
                    {
                        "name": "Oliver Ettlinger"
                    },
                    {
                        "name": "Zulfikar Najmudin"
                    },
                    {
                        "name": "Jean-Raphael Marques"
                    },
                    {
                        "name": "Laurent Gremillet"
                    },
                    {
                        "name": "Joao Jorge Santos"
                    },
                    {
                        "name": "Fabrizio Consoli"
                    },
                    {
                        "name": "Vladimir Tikhonchuk"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Tikhonchuk"
                },
                "author": "Vladimir Tikhonchuk",
                "arxiv_comment": "18 pages (total), 12 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19519v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19519v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06934v1",
                "updated": "2024-10-09T14:28:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:28:59Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "title": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks"
                },
                "summary": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies."
                },
                "authors": [
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Xiaolong Xu"
                    },
                    {
                        "name": "Muhammad Bilal"
                    },
                    {
                        "name": "Xiangwei Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Siyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Siyu Wu"
                },
                "author": "Siyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.21272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21272v1",
                "updated": "2024-10-28T17:59:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    59,
                    6,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:59:06Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    59,
                    6,
                    0,
                    302,
                    0
                ],
                "title": "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of\n  Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of\n  Heuristics"
                },
                "summary": "Do large language models (LLMs) solve reasoning tasks by learning robust\ngeneralizable algorithms, or do they memorize training data? To investigate\nthis question, we use arithmetic reasoning as a representative task. Using\ncausal analysis, we identify a subset of the model (a circuit) that explains\nmost of the model's behavior for basic arithmetic logic and examine its\nfunctionality. By zooming in on the level of individual circuit neurons, we\ndiscover a sparse set of important neurons that implement simple heuristics.\nEach heuristic identifies a numerical input pattern and outputs corresponding\nanswers. We hypothesize that the combination of these heuristic neurons is the\nmechanism used to produce correct arithmetic answers. To test this, we\ncategorize each neuron into several heuristic types-such as neurons that\nactivate when an operand falls within a certain range-and find that the\nunordered combination of these heuristic types is the mechanism that explains\nmost of the model's accuracy on arithmetic prompts. Finally, we demonstrate\nthat this mechanism appears as the main source of arithmetic accuracy early in\ntraining. Overall, our experimental results across several LLMs show that LLMs\nperform arithmetic using neither robust algorithms nor memorization; rather,\nthey rely on a \"bag of heuristics\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do large language models (LLMs) solve reasoning tasks by learning robust\ngeneralizable algorithms, or do they memorize training data? To investigate\nthis question, we use arithmetic reasoning as a representative task. Using\ncausal analysis, we identify a subset of the model (a circuit) that explains\nmost of the model's behavior for basic arithmetic logic and examine its\nfunctionality. By zooming in on the level of individual circuit neurons, we\ndiscover a sparse set of important neurons that implement simple heuristics.\nEach heuristic identifies a numerical input pattern and outputs corresponding\nanswers. We hypothesize that the combination of these heuristic neurons is the\nmechanism used to produce correct arithmetic answers. To test this, we\ncategorize each neuron into several heuristic types-such as neurons that\nactivate when an operand falls within a certain range-and find that the\nunordered combination of these heuristic types is the mechanism that explains\nmost of the model's accuracy on arithmetic prompts. Finally, we demonstrate\nthat this mechanism appears as the main source of arithmetic accuracy early in\ntraining. Overall, our experimental results across several LLMs show that LLMs\nperform arithmetic using neither robust algorithms nor memorization; rather,\nthey rely on a \"bag of heuristics\"."
                },
                "authors": [
                    {
                        "name": "Yaniv Nikankin"
                    },
                    {
                        "name": "Anja Reusch"
                    },
                    {
                        "name": "Aaron Mueller"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21271v1",
                "updated": "2024-10-28T17:59:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    59,
                    3,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:59:03Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    59,
                    3,
                    0,
                    302,
                    0
                ],
                "title": "EoRA: Training-free Compensation for Compressed LLM with Eigenspace\n  Low-Rank Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EoRA: Training-free Compensation for Compressed LLM with Eigenspace\n  Low-Rank Approximation"
                },
                "summary": "In this work, we re-formulate the model compression problem into the\ncustomized compensation problem: Given a compressed model, we aim to introduce\nresidual low-rank paths to compensate for compression errors under customized\nrequirements from users (e.g., tasks, compression ratios), resulting in greater\nflexibility in adjusting overall capacity without being constrained by specific\ncompression formats. However, naively applying SVD to derive residual paths\ncauses suboptimal utilization of the low-rank representation capacity. Instead,\nwe propose Training-free Eigenspace Low-Rank Approximation (EoRA), a method\nthat directly minimizes compression-induced errors without requiring\ngradient-based training, achieving fast optimization in minutes using a small\namount of calibration data. EoRA projects compression errors into the\neigenspace of input activations, leveraging eigenvalues to effectively\nprioritize the reconstruction of high-importance error components. Moreover,\nEoRA can be seamlessly integrated with fine-tuning and quantization to further\nimprove effectiveness and efficiency. EoRA consistently outperforms previous\nmethods in compensating errors for compressed LLaMA2/3 models on various tasks,\nsuch as language generation, commonsense reasoning, and math reasoning tasks\n(e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and\nMathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4\nsparsity). EoRA offers a scalable, training-free solution to compensate for\ncompression errors, making it a powerful tool to deploy LLMs in various\ncapacity and efficiency requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we re-formulate the model compression problem into the\ncustomized compensation problem: Given a compressed model, we aim to introduce\nresidual low-rank paths to compensate for compression errors under customized\nrequirements from users (e.g., tasks, compression ratios), resulting in greater\nflexibility in adjusting overall capacity without being constrained by specific\ncompression formats. However, naively applying SVD to derive residual paths\ncauses suboptimal utilization of the low-rank representation capacity. Instead,\nwe propose Training-free Eigenspace Low-Rank Approximation (EoRA), a method\nthat directly minimizes compression-induced errors without requiring\ngradient-based training, achieving fast optimization in minutes using a small\namount of calibration data. EoRA projects compression errors into the\neigenspace of input activations, leveraging eigenvalues to effectively\nprioritize the reconstruction of high-importance error components. Moreover,\nEoRA can be seamlessly integrated with fine-tuning and quantization to further\nimprove effectiveness and efficiency. EoRA consistently outperforms previous\nmethods in compensating errors for compressed LLaMA2/3 models on various tasks,\nsuch as language generation, commonsense reasoning, and math reasoning tasks\n(e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and\nMathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4\nsparsity). EoRA offers a scalable, training-free solution to compensate for\ncompression errors, making it a powerful tool to deploy LLMs in various\ncapacity and efficiency requirements."
                },
                "authors": [
                    {
                        "name": "Shih-Yang Liu"
                    },
                    {
                        "name": "Huck Yang"
                    },
                    {
                        "name": "Chein-Yi Wang"
                    },
                    {
                        "name": "Nai Chit Fung"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Charbel Sakr"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Min-Hung Chen"
                    }
                ],
                "author_detail": {
                    "name": "Min-Hung Chen"
                },
                "author": "Min-Hung Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21264v1",
                "updated": "2024-10-28T17:57:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    7,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:57:07Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    7,
                    0,
                    302,
                    0
                ],
                "title": "LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior"
                },
                "summary": "We present LARP, a novel video tokenizer designed to overcome limitations in\ncurrent video tokenization methods for autoregressive (AR) generative models.\nUnlike traditional patchwise tokenizers that directly encode local visual\npatches into discrete tokens, LARP introduces a holistic tokenization scheme\nthat gathers information from the visual content using a set of learned\nholistic queries. This design allows LARP to capture more global and semantic\nrepresentations, rather than being limited to local patch-level information.\nFurthermore, it offers flexibility by supporting an arbitrary number of\ndiscrete tokens, enabling adaptive and efficient tokenization based on the\nspecific requirements of the task. To align the discrete token space with\ndownstream AR generation tasks, LARP integrates a lightweight AR transformer as\na training-time prior model that predicts the next token on its discrete latent\nspace. By incorporating the prior model during training, LARP learns a latent\nspace that is not only optimized for video reconstruction but is also\nstructured in a way that is more conducive to autoregressive generation.\nMoreover, this process defines a sequential order for the discrete tokens,\nprogressively pushing them toward an optimal configuration during training,\nensuring smoother and more accurate AR generation at inference time.\nComprehensive experiments demonstrate LARP's strong performance, achieving\nstate-of-the-art FVD on the UCF101 class-conditional video generation\nbenchmark. LARP enhances the compatibility of AR models with videos and opens\nup the potential to build unified high-fidelity multimodal large language\nmodels (MLLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LARP, a novel video tokenizer designed to overcome limitations in\ncurrent video tokenization methods for autoregressive (AR) generative models.\nUnlike traditional patchwise tokenizers that directly encode local visual\npatches into discrete tokens, LARP introduces a holistic tokenization scheme\nthat gathers information from the visual content using a set of learned\nholistic queries. This design allows LARP to capture more global and semantic\nrepresentations, rather than being limited to local patch-level information.\nFurthermore, it offers flexibility by supporting an arbitrary number of\ndiscrete tokens, enabling adaptive and efficient tokenization based on the\nspecific requirements of the task. To align the discrete token space with\ndownstream AR generation tasks, LARP integrates a lightweight AR transformer as\na training-time prior model that predicts the next token on its discrete latent\nspace. By incorporating the prior model during training, LARP learns a latent\nspace that is not only optimized for video reconstruction but is also\nstructured in a way that is more conducive to autoregressive generation.\nMoreover, this process defines a sequential order for the discrete tokens,\nprogressively pushing them toward an optimal configuration during training,\nensuring smoother and more accurate AR generation at inference time.\nComprehensive experiments demonstrate LARP's strong performance, achieving\nstate-of-the-art FVD on the UCF101 class-conditional video generation\nbenchmark. LARP enhances the compatibility of AR models with videos and opens\nup the potential to build unified high-fidelity multimodal large language\nmodels (MLLMs)."
                },
                "authors": [
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Saksham Suri"
                    },
                    {
                        "name": "Yixuan Ren"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Shrivastava"
                },
                "author": "Abhinav Shrivastava",
                "arxiv_comment": "Project page: https://hywang66.github.io/larp/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21262v1",
                "updated": "2024-10-28T17:56:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    56,
                    18,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:56:18Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    56,
                    18,
                    0,
                    302,
                    0
                ],
                "title": "BLAST: Block-Level Adaptive Structured Matrices for Efficient Deep\n  Neural Network Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAST: Block-Level Adaptive Structured Matrices for Efficient Deep\n  Neural Network Inference"
                },
                "summary": "Large-scale foundation models have demonstrated exceptional performance in\nlanguage and vision tasks. However, the numerous dense matrix-vector operations\ninvolved in these large networks pose significant computational challenges\nduring inference. To address these challenges, we introduce the Block-Level\nAdaptive STructured (BLAST) matrix, designed to learn and leverage efficient\nstructures prevalent in the weight matrices of linear layers within deep\nlearning models. Compared to existing structured matrices, the BLAST matrix\noffers substantial flexibility, as it can represent various types of structures\nthat are either learned from data or computed from pre-existing weight\nmatrices. We demonstrate the efficiency of using the BLAST matrix for\ncompressing both language and vision tasks, showing that (i) for medium-sized\nmodels such as ViT and GPT-2, training with BLAST weights boosts performance\nwhile reducing complexity by 70\\% and 40\\%, respectively; and (ii) for large\nfoundation models such as Llama-7B and DiT-XL, the BLAST matrix achieves a 2x\ncompression while exhibiting the lowest performance degradation among all\ntested structured matrices. Our code is available at\n\\url{https://github.com/changwoolee/BLAST}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale foundation models have demonstrated exceptional performance in\nlanguage and vision tasks. However, the numerous dense matrix-vector operations\ninvolved in these large networks pose significant computational challenges\nduring inference. To address these challenges, we introduce the Block-Level\nAdaptive STructured (BLAST) matrix, designed to learn and leverage efficient\nstructures prevalent in the weight matrices of linear layers within deep\nlearning models. Compared to existing structured matrices, the BLAST matrix\noffers substantial flexibility, as it can represent various types of structures\nthat are either learned from data or computed from pre-existing weight\nmatrices. We demonstrate the efficiency of using the BLAST matrix for\ncompressing both language and vision tasks, showing that (i) for medium-sized\nmodels such as ViT and GPT-2, training with BLAST weights boosts performance\nwhile reducing complexity by 70\\% and 40\\%, respectively; and (ii) for large\nfoundation models such as Llama-7B and DiT-XL, the BLAST matrix achieves a 2x\ncompression while exhibiting the lowest performance degradation among all\ntested structured matrices. Our code is available at\n\\url{https://github.com/changwoolee/BLAST}."
                },
                "authors": [
                    {
                        "name": "Changwoo Lee"
                    },
                    {
                        "name": "Soo Min Kwon"
                    },
                    {
                        "name": "Qing Qu"
                    },
                    {
                        "name": "Hun-Seok Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hun-Seok Kim"
                },
                "author": "Hun-Seok Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14392v2",
                "updated": "2024-10-28T17:55:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    55,
                    9,
                    0,
                    302,
                    0
                ],
                "published": "2024-05-23T10:08:19Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    10,
                    8,
                    19,
                    3,
                    144,
                    0
                ],
                "title": "Markovian Flow Matching: Accelerating MCMC with Continuous Normalizing\n  Flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markovian Flow Matching: Accelerating MCMC with Continuous Normalizing\n  Flows"
                },
                "summary": "Continuous normalizing flows (CNFs) learn the probability path between a\nreference distribution and a target distribution by modeling the vector field\ngenerating said path using neural networks. Recently, Lipman et al. (2022)\nintroduced a simple and inexpensive method for training CNFs in generative\nmodeling, termed flow matching (FM). In this paper, we repurpose this method\nfor probabilistic inference by incorporating Markovian sampling methods in\nevaluating the FM objective, and using the learned CNF to improve Monte Carlo\nsampling. Specifically, we propose an adaptive Markov chain Monte Carlo (MCMC)\nalgorithm, which combines a local Markov transition kernel with a non-local,\nflow-informed transition kernel, defined using a CNF. This CNF is adapted\non-the-fly using samples from the Markov chain, which are used to specify the\nprobability path for the FM objective. Our method also includes an adaptive\ntempering mechanism that allows the discovery of multiple modes in the target\ndistribution. Under mild assumptions, we establish convergence of our method to\na local optimum of the FM objective. We then benchmark our approach on several\nsynthetic and real-world examples, achieving similar performance to other\nstate-of-the-art methods, but often at a significantly lower computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous normalizing flows (CNFs) learn the probability path between a\nreference distribution and a target distribution by modeling the vector field\ngenerating said path using neural networks. Recently, Lipman et al. (2022)\nintroduced a simple and inexpensive method for training CNFs in generative\nmodeling, termed flow matching (FM). In this paper, we repurpose this method\nfor probabilistic inference by incorporating Markovian sampling methods in\nevaluating the FM objective, and using the learned CNF to improve Monte Carlo\nsampling. Specifically, we propose an adaptive Markov chain Monte Carlo (MCMC)\nalgorithm, which combines a local Markov transition kernel with a non-local,\nflow-informed transition kernel, defined using a CNF. This CNF is adapted\non-the-fly using samples from the Markov chain, which are used to specify the\nprobability path for the FM objective. Our method also includes an adaptive\ntempering mechanism that allows the discovery of multiple modes in the target\ndistribution. Under mild assumptions, we establish convergence of our method to\na local optimum of the FM objective. We then benchmark our approach on several\nsynthetic and real-world examples, achieving similar performance to other\nstate-of-the-art methods, but often at a significantly lower computational\ncost."
                },
                "authors": [
                    {
                        "name": "Alberto Cabezas"
                    },
                    {
                        "name": "Louis Sharrock"
                    },
                    {
                        "name": "Christopher Nemeth"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Nemeth"
                },
                "author": "Christopher Nemeth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21257v1",
                "updated": "2024-10-28T17:54:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    54,
                    31,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:54:31Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    54,
                    31,
                    0,
                    302,
                    0
                ],
                "title": "One-Step Diffusion Policy: Fast Visuomotor Policies via Diffusion\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Step Diffusion Policy: Fast Visuomotor Policies via Diffusion\n  Distillation"
                },
                "summary": "Diffusion models, praised for their success in generative tasks, are\nincreasingly being applied to robotics, demonstrating exceptional performance\nin behavior cloning. However, their slow generation process stemming from\niterative denoising steps poses a challenge for real-time applications in\nresource-constrained robotics setups and dynamically changing environments. In\nthis paper, we introduce the One-Step Diffusion Policy (OneDP), a novel\napproach that distills knowledge from pre-trained diffusion policies into a\nsingle-step action generator, significantly accelerating response times for\nrobotic control tasks. We ensure the distilled generator closely aligns with\nthe original policy distribution by minimizing the Kullback-Leibler (KL)\ndivergence along the diffusion chain, requiring only $2\\%$-$10\\%$ additional\npre-training cost for convergence. We evaluated OneDP on 6 challenging\nsimulation tasks as well as 4 self-designed real-world tasks using the Franka\nrobot. The results demonstrate that OneDP not only achieves state-of-the-art\nsuccess rates but also delivers an order-of-magnitude improvement in inference\nspeed, boosting action prediction frequency from 1.5 Hz to 62 Hz, establishing\nits potential for dynamic and computationally constrained robotic applications.\nWe share the project page at https://research.nvidia.com/labs/dir/onedp/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models, praised for their success in generative tasks, are\nincreasingly being applied to robotics, demonstrating exceptional performance\nin behavior cloning. However, their slow generation process stemming from\niterative denoising steps poses a challenge for real-time applications in\nresource-constrained robotics setups and dynamically changing environments. In\nthis paper, we introduce the One-Step Diffusion Policy (OneDP), a novel\napproach that distills knowledge from pre-trained diffusion policies into a\nsingle-step action generator, significantly accelerating response times for\nrobotic control tasks. We ensure the distilled generator closely aligns with\nthe original policy distribution by minimizing the Kullback-Leibler (KL)\ndivergence along the diffusion chain, requiring only $2\\%$-$10\\%$ additional\npre-training cost for convergence. We evaluated OneDP on 6 challenging\nsimulation tasks as well as 4 self-designed real-world tasks using the Franka\nrobot. The results demonstrate that OneDP not only achieves state-of-the-art\nsuccess rates but also delivers an order-of-magnitude improvement in inference\nspeed, boosting action prediction frequency from 1.5 Hz to 62 Hz, establishing\nits potential for dynamic and computationally constrained robotic applications.\nWe share the project page at https://research.nvidia.com/labs/dir/onedp/."
                },
                "authors": [
                    {
                        "name": "Zhendong Wang"
                    },
                    {
                        "name": "Zhaoshuo Li"
                    },
                    {
                        "name": "Ajay Mandlekar"
                    },
                    {
                        "name": "Zhenjia Xu"
                    },
                    {
                        "name": "Jiaojiao Fan"
                    },
                    {
                        "name": "Yashraj Narang"
                    },
                    {
                        "name": "Linxi Fan"
                    },
                    {
                        "name": "Yuke Zhu"
                    },
                    {
                        "name": "Yogesh Balaji"
                    },
                    {
                        "name": "Mingyuan Zhou"
                    },
                    {
                        "name": "Ming-Yu Liu"
                    },
                    {
                        "name": "Yu Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zeng"
                },
                "author": "Yu Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21254v1",
                "updated": "2024-10-28T17:52:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    52,
                    15,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:52:15Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    52,
                    15,
                    0,
                    302,
                    0
                ],
                "title": "Are BabyLMs Second Language Learners?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are BabyLMs Second Language Learners?"
                },
                "summary": "This paper describes a linguistically-motivated approach to the 2024 edition\nof the BabyLM Challenge (Warstadt et al. 2023). Rather than pursuing a first\nlanguage learning (L1) paradigm, we approach the challenge from a second\nlanguage (L2) learning perspective. In L2 learning, there is a stronger focus\non learning explicit linguistic information, such as grammatical notions,\ndefinitions of words or different ways of expressing a meaning. This makes L2\nlearning potentially more efficient and concise. We approximate this using data\nfrom Wiktionary, grammar examples either generated by an LLM or sourced from\ngrammar books, and paraphrase data. We find that explicit information about\nword meaning (in our case, Wiktionary) does not boost model performance, while\ngrammatical information can give a small improvement. The most impactful data\ningredient is sentence paraphrases, with our two best models being trained on\n1) a mix of paraphrase data and data from the BabyLM pretraining dataset, and\n2) exclusively paraphrase data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes a linguistically-motivated approach to the 2024 edition\nof the BabyLM Challenge (Warstadt et al. 2023). Rather than pursuing a first\nlanguage learning (L1) paradigm, we approach the challenge from a second\nlanguage (L2) learning perspective. In L2 learning, there is a stronger focus\non learning explicit linguistic information, such as grammatical notions,\ndefinitions of words or different ways of expressing a meaning. This makes L2\nlearning potentially more efficient and concise. We approximate this using data\nfrom Wiktionary, grammar examples either generated by an LLM or sourced from\ngrammar books, and paraphrase data. We find that explicit information about\nword meaning (in our case, Wiktionary) does not boost model performance, while\ngrammatical information can give a small improvement. The most impactful data\ningredient is sentence paraphrases, with our two best models being trained on\n1) a mix of paraphrase data and data from the BabyLM pretraining dataset, and\n2) exclusively paraphrase data."
                },
                "authors": [
                    {
                        "name": "Lukas Edman"
                    },
                    {
                        "name": "Lisa Bylinina"
                    },
                    {
                        "name": "Faeze Ghorbanpour"
                    },
                    {
                        "name": "Alexander Fraser"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Fraser"
                },
                "author": "Alexander Fraser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21252v1",
                "updated": "2024-10-28T17:50:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    50,
                    42,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:50:42Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    50,
                    42,
                    0,
                    302,
                    0
                ],
                "title": "LongReward: Improving Long-context Large Language Models with AI\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongReward: Improving Long-context Large Language Models with AI\n  Feedback"
                },
                "summary": "Though significant advancements have been achieved in developing long-context\nlarge language models (LLMs), the compromised quality of LLM-synthesized data\nfor supervised fine-tuning (SFT) often affects the long-context performance of\nSFT models and leads to inherent limitations. In principle, reinforcement\nlearning (RL) with appropriate reward signals can further enhance models'\ncapacities. However, how to obtain reliable rewards in long-context scenarios\nremains unexplored. To this end, we propose LongReward, a novel method that\nutilizes an off-the-shelf LLM to provide rewards for long-context model\nresponses from four human-valued dimensions: helpfulness, logicality,\nfaithfulness, and completeness, each with a carefully designed assessment\npipeline. By combining LongReward and offline RL algorithm DPO, we are able to\neffectively improve long-context SFT models. Our experiments indicate that\nLongReward not only significantly improves models' long-context performance but\nalso enhances their ability to follow short instructions. We also find that\nlong-context DPO with LongReward and conventional short-context DPO can be used\ntogether without hurting either one's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though significant advancements have been achieved in developing long-context\nlarge language models (LLMs), the compromised quality of LLM-synthesized data\nfor supervised fine-tuning (SFT) often affects the long-context performance of\nSFT models and leads to inherent limitations. In principle, reinforcement\nlearning (RL) with appropriate reward signals can further enhance models'\ncapacities. However, how to obtain reliable rewards in long-context scenarios\nremains unexplored. To this end, we propose LongReward, a novel method that\nutilizes an off-the-shelf LLM to provide rewards for long-context model\nresponses from four human-valued dimensions: helpfulness, logicality,\nfaithfulness, and completeness, each with a carefully designed assessment\npipeline. By combining LongReward and offline RL algorithm DPO, we are able to\neffectively improve long-context SFT models. Our experiments indicate that\nLongReward not only significantly improves models' long-context performance but\nalso enhances their ability to follow short instructions. We also find that\nlong-context DPO with LongReward and conventional short-context DPO can be used\ntogether without hurting either one's performance."
                },
                "authors": [
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Zhongni Hou"
                    },
                    {
                        "name": "Xin Lv"
                    },
                    {
                        "name": "Shulin Cao"
                    },
                    {
                        "name": "Zhenyu Hou"
                    },
                    {
                        "name": "Yilin Niu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Ling Feng"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04151v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04151v2",
                "updated": "2024-10-28T17:48:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    48,
                    44,
                    0,
                    302,
                    0
                ],
                "published": "2024-07-04T20:57:06Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    20,
                    57,
                    6,
                    3,
                    186,
                    0
                ],
                "title": "Securing Multi-turn Conversational Language Models From Distributed\n  Backdoor Triggers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Multi-turn Conversational Language Models From Distributed\n  Backdoor Triggers"
                },
                "summary": "Large language models (LLMs) have acquired the ability to handle longer\ncontext lengths and understand nuances in text, expanding their dialogue\ncapabilities beyond a single utterance. A popular user-facing application of\nLLMs is the multi-turn chat setting. Though longer chat memory and better\nunderstanding may seemingly benefit users, our paper exposes a vulnerability\nthat leverages the multi-turn feature and strong learning ability of LLMs to\nharm the end-user: the backdoor. We demonstrate that LLMs can capture the\ncombinational backdoor representation. Only upon presentation of triggers\ntogether does the backdoor activate. We also verify empirically that this\nrepresentation is invariant to the position of the trigger utterance.\nSubsequently, inserting a single extra token into two utterances of 5%of the\ndata can cause over 99% Attack Success Rate (ASR). Our results with 3 triggers\ndemonstrate that this framework is generalizable, compatible with any trigger\nin an adversary's toolbox in a plug-and-play manner. Defending the backdoor can\nbe challenging in the chat setting because of the large input and output space.\nOur analysis indicates that the distributed backdoor exacerbates the current\nchallenges by polynomially increasing the dimension of the attacked input\nspace. Canonical textual defenses like ONION and BKI leverage auxiliary model\nforward passes over individual tokens, scaling exponentially with the input\nsequence length and struggling to maintain computational feasibility. To this\nend, we propose a decoding time defense - decayed contrastive decoding - that\nscales linearly with assistant response sequence length and reduces the\nbackdoor to as low as 0.35%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have acquired the ability to handle longer\ncontext lengths and understand nuances in text, expanding their dialogue\ncapabilities beyond a single utterance. A popular user-facing application of\nLLMs is the multi-turn chat setting. Though longer chat memory and better\nunderstanding may seemingly benefit users, our paper exposes a vulnerability\nthat leverages the multi-turn feature and strong learning ability of LLMs to\nharm the end-user: the backdoor. We demonstrate that LLMs can capture the\ncombinational backdoor representation. Only upon presentation of triggers\ntogether does the backdoor activate. We also verify empirically that this\nrepresentation is invariant to the position of the trigger utterance.\nSubsequently, inserting a single extra token into two utterances of 5%of the\ndata can cause over 99% Attack Success Rate (ASR). Our results with 3 triggers\ndemonstrate that this framework is generalizable, compatible with any trigger\nin an adversary's toolbox in a plug-and-play manner. Defending the backdoor can\nbe challenging in the chat setting because of the large input and output space.\nOur analysis indicates that the distributed backdoor exacerbates the current\nchallenges by polynomially increasing the dimension of the attacked input\nspace. Canonical textual defenses like ONION and BKI leverage auxiliary model\nforward passes over individual tokens, scaling exponentially with the input\nsequence length and struggling to maintain computational feasibility. To this\nend, we propose a decoding time defense - decayed contrastive decoding - that\nscales linearly with assistant response sequence length and reduces the\nbackdoor to as low as 0.35%."
                },
                "authors": [
                    {
                        "name": "Terry Tong"
                    },
                    {
                        "name": "Jiashu Xu"
                    },
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Muhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Muhao Chen"
                },
                "author": "Muhao Chen",
                "arxiv_comment": "Findings of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04151v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04151v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21242v1",
                "updated": "2024-10-28T17:40:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    40,
                    40,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:40:40Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    40,
                    40,
                    0,
                    302,
                    0
                ],
                "title": "Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback"
                },
                "summary": "Building effective dense retrieval systems remains difficult when relevance\nsupervision is not available. Recent work has looked to overcome this challenge\nby using a Large Language Model (LLM) to generate hypothetical documents that\ncan be used to find the closest real document. However, this approach relies\nsolely on the LLM to have domain-specific knowledge relevant to the query,\nwhich may not be practical. Furthermore, generating hypothetical documents can\nbe inefficient as it requires the LLM to generate a large number of tokens for\neach query. To address these challenges, we introduce Real Document Embeddings\nfrom Relevance Feedback (ReDE-RF). Inspired by relevance feedback, ReDE-RF\nproposes to re-frame hypothetical document generation as a relevance estimation\ntask, using an LLM to select which documents should be used for nearest\nneighbor search. Through this re-framing, the LLM no longer needs\ndomain-specific knowledge but only needs to judge what is relevant.\nAdditionally, relevance estimation only requires the LLM to output a single\ntoken, thereby improving search latency. Our experiments show that ReDE-RF\nconsistently surpasses state-of-the-art zero-shot dense retrieval methods\nacross a wide range of low-resource retrieval datasets while also making\nsignificant improvements in latency per-query.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building effective dense retrieval systems remains difficult when relevance\nsupervision is not available. Recent work has looked to overcome this challenge\nby using a Large Language Model (LLM) to generate hypothetical documents that\ncan be used to find the closest real document. However, this approach relies\nsolely on the LLM to have domain-specific knowledge relevant to the query,\nwhich may not be practical. Furthermore, generating hypothetical documents can\nbe inefficient as it requires the LLM to generate a large number of tokens for\neach query. To address these challenges, we introduce Real Document Embeddings\nfrom Relevance Feedback (ReDE-RF). Inspired by relevance feedback, ReDE-RF\nproposes to re-frame hypothetical document generation as a relevance estimation\ntask, using an LLM to select which documents should be used for nearest\nneighbor search. Through this re-framing, the LLM no longer needs\ndomain-specific knowledge but only needs to judge what is relevant.\nAdditionally, relevance estimation only requires the LLM to output a single\ntoken, thereby improving search latency. Our experiments show that ReDE-RF\nconsistently surpasses state-of-the-art zero-shot dense retrieval methods\nacross a wide range of low-resource retrieval datasets while also making\nsignificant improvements in latency per-query."
                },
                "authors": [
                    {
                        "name": "Nour Jedidi"
                    },
                    {
                        "name": "Yung-Sung Chuang"
                    },
                    {
                        "name": "Leslie Shing"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21237v1",
                "updated": "2024-10-28T17:34:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    34,
                    5,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:34:05Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    34,
                    5,
                    0,
                    302,
                    0
                ],
                "title": "Hierarchical Knowledge Graph Construction from Images for Scalable\n  E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Knowledge Graph Construction from Images for Scalable\n  E-Commerce"
                },
                "summary": "Knowledge Graph (KG) is playing an increasingly important role in various AI\nsystems. For e-commerce, an efficient and low-cost automated knowledge graph\nconstruction method is the foundation of enabling various successful downstream\napplications. In this paper, we propose a novel method for constructing\nstructured product knowledge graphs from raw product images. The method\ncooperatively leverages recent advances in the vision-language model (VLM) and\nlarge language model (LLM), fully automating the process and allowing timely\ngraph updates. We also present a human-annotated e-commerce product dataset for\nbenchmarking product property extraction in knowledge graph construction. Our\nmethod outperforms our baseline in all metrics and evaluated properties,\ndemonstrating its effectiveness and bright usage potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph (KG) is playing an increasingly important role in various AI\nsystems. For e-commerce, an efficient and low-cost automated knowledge graph\nconstruction method is the foundation of enabling various successful downstream\napplications. In this paper, we propose a novel method for constructing\nstructured product knowledge graphs from raw product images. The method\ncooperatively leverages recent advances in the vision-language model (VLM) and\nlarge language model (LLM), fully automating the process and allowing timely\ngraph updates. We also present a human-annotated e-commerce product dataset for\nbenchmarking product property extraction in knowledge graph construction. Our\nmethod outperforms our baseline in all metrics and evaluated properties,\ndemonstrating its effectiveness and bright usage potential."
                },
                "authors": [
                    {
                        "name": "Zhantao Yang"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Fangyi Chen"
                    },
                    {
                        "name": "Anudeepsekhar Bolimera"
                    },
                    {
                        "name": "Marios Savvides"
                    }
                ],
                "author_detail": {
                    "name": "Marios Savvides"
                },
                "author": "Marios Savvides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17134v2",
                "updated": "2024-10-28T17:33:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    33,
                    27,
                    0,
                    302,
                    0
                ],
                "published": "2024-03-25T19:17:43Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    19,
                    17,
                    43,
                    0,
                    85,
                    0
                ],
                "title": "RepairAgent: An Autonomous, LLM-Based Agent for Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepairAgent: An Autonomous, LLM-Based Agent for Program Repair"
                },
                "summary": "Automated program repair has emerged as a powerful technique to mitigate the\nimpact of software bugs on system reliability and user experience. This paper\nintroduces RepairAgent, the first work to address the program repair challenge\nthrough an autonomous agent based on a large language model (LLM). Unlike\nexisting deep learning-based approaches, which prompt a model with a fixed\nprompt or in a fixed feedback loop, our work treats the LLM as an agent capable\nof autonomously planning and executing actions to fix bugs by invoking suitable\ntools. RepairAgent freely interleaves gathering information about the bug,\ngathering repair ingredients, and validating fixes, while deciding which tools\nto invoke based on the gathered information and feedback from previous fix\nattempts. Key contributions that enable RepairAgent include a set of tools that\nare useful for program repair, a dynamically updated prompt format that allows\nthe LLM to interact with these tools, and a finite state machine that guides\nthe agent in invoking the tools. Our evaluation on the popular Defects4J\ndataset demonstrates RepairAgent's effectiveness in autonomously repairing 164\nbugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM\nimposes an average cost of 270,000 tokens per bug, which, under the current\npricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To\nthe best of our knowledge, this work is the first to present an autonomous,\nLLM-based agent for program repair, paving the way for future agent-based\ntechniques in software engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated program repair has emerged as a powerful technique to mitigate the\nimpact of software bugs on system reliability and user experience. This paper\nintroduces RepairAgent, the first work to address the program repair challenge\nthrough an autonomous agent based on a large language model (LLM). Unlike\nexisting deep learning-based approaches, which prompt a model with a fixed\nprompt or in a fixed feedback loop, our work treats the LLM as an agent capable\nof autonomously planning and executing actions to fix bugs by invoking suitable\ntools. RepairAgent freely interleaves gathering information about the bug,\ngathering repair ingredients, and validating fixes, while deciding which tools\nto invoke based on the gathered information and feedback from previous fix\nattempts. Key contributions that enable RepairAgent include a set of tools that\nare useful for program repair, a dynamically updated prompt format that allows\nthe LLM to interact with these tools, and a finite state machine that guides\nthe agent in invoking the tools. Our evaluation on the popular Defects4J\ndataset demonstrates RepairAgent's effectiveness in autonomously repairing 164\nbugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM\nimposes an average cost of 270,000 tokens per bug, which, under the current\npricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To\nthe best of our knowledge, this work is the first to present an autonomous,\nLLM-based agent for program repair, paving the way for future agent-based\ntechniques in software engineering."
                },
                "authors": [
                    {
                        "name": "Islem Bouzenia"
                    },
                    {
                        "name": "Premkumar Devanbu"
                    },
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11801v2",
                "updated": "2024-10-28T17:30:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    30,
                    58,
                    0,
                    302,
                    0
                ],
                "published": "2024-06-17T17:48:13Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    17,
                    48,
                    13,
                    0,
                    169,
                    0
                ],
                "title": "Safety Arithmetic: A Framework for Test-time Safety Alignment of\n  Language Models by Steering Parameters and Activations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety Arithmetic: A Framework for Test-time Safety Alignment of\n  Language Models by Steering Parameters and Activations"
                },
                "summary": "Ensuring the safe alignment of large language models (LLMs) with human values\nis critical as they become integral to applications like translation and\nquestion answering. Current alignment methods struggle with dynamic user\nintentions and complex objectives, making models vulnerable to generating\nharmful content. We propose Safety Arithmetic, a training-free framework\nenhancing LLM safety across different scenarios: Base models, Supervised\nfine-tuned models (SFT), and Edited models. Safety Arithmetic involves Harm\nDirection Removal to avoid harmful content and Safety Alignment to promote safe\nresponses. Additionally, we present NoIntentEdit, a dataset highlighting edit\ninstances that could compromise model safety if used unintentionally. Our\nexperiments show that Safety Arithmetic significantly improves safety measures,\nreduces over-safety, and maintains model utility, outperforming existing\nmethods in ensuring safe content generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the safe alignment of large language models (LLMs) with human values\nis critical as they become integral to applications like translation and\nquestion answering. Current alignment methods struggle with dynamic user\nintentions and complex objectives, making models vulnerable to generating\nharmful content. We propose Safety Arithmetic, a training-free framework\nenhancing LLM safety across different scenarios: Base models, Supervised\nfine-tuned models (SFT), and Edited models. Safety Arithmetic involves Harm\nDirection Removal to avoid harmful content and Safety Alignment to promote safe\nresponses. Additionally, we present NoIntentEdit, a dataset highlighting edit\ninstances that could compromise model safety if used unintentionally. Our\nexperiments show that Safety Arithmetic significantly improves safety measures,\nreduces over-safety, and maintains model utility, outperforming existing\nmethods in ensuring safe content generation."
                },
                "authors": [
                    {
                        "name": "Rima Hazra"
                    },
                    {
                        "name": "Sayan Layek"
                    },
                    {
                        "name": "Somnath Banerjee"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "arxiv_comment": "EMNLP 2024 Main. Codes are available at:\n  https://github.com/declare-lab/safety-arithmetic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21236v1",
                "updated": "2024-10-28T17:30:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    30,
                    1,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:30:01Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    30,
                    1,
                    0,
                    302,
                    0
                ],
                "title": "Flaming-hot Initiation with Regular Execution Sampling for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flaming-hot Initiation with Regular Execution Sampling for Large\n  Language Models"
                },
                "summary": "Since the release of ChatGPT, large language models (LLMs) have demonstrated\nremarkable capabilities across various domains. A key challenge in developing\nthese general capabilities is efficiently sourcing diverse, high-quality data.\nThis becomes especially critical in reasoning-related tasks with sandbox\ncheckers, such as math or code, where the goal is to generate correct solutions\nto specific problems with higher probability. In this work, we introduce\nFlaming-hot Initiation with Regular Execution (FIRE) sampling, a simple yet\nhighly effective method to efficiently find good responses. Our empirical\nfindings show that FIRE sampling enhances inference-time generation quality and\nalso benefits training in the alignment stage. Furthermore, we explore how FIRE\nsampling improves performance by promoting diversity and analyze the impact of\nemploying FIRE at different positions within a response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the release of ChatGPT, large language models (LLMs) have demonstrated\nremarkable capabilities across various domains. A key challenge in developing\nthese general capabilities is efficiently sourcing diverse, high-quality data.\nThis becomes especially critical in reasoning-related tasks with sandbox\ncheckers, such as math or code, where the goal is to generate correct solutions\nto specific problems with higher probability. In this work, we introduce\nFlaming-hot Initiation with Regular Execution (FIRE) sampling, a simple yet\nhighly effective method to efficiently find good responses. Our empirical\nfindings show that FIRE sampling enhances inference-time generation quality and\nalso benefits training in the alignment stage. Furthermore, we explore how FIRE\nsampling improves performance by promoting diversity and analyze the impact of\nemploying FIRE at different positions within a response."
                },
                "authors": [
                    {
                        "name": "Weizhe Chen"
                    },
                    {
                        "name": "Zhicheng Zhang"
                    },
                    {
                        "name": "Guanlin Liu"
                    },
                    {
                        "name": "Renjie Zheng"
                    },
                    {
                        "name": "Wenlei Shi"
                    },
                    {
                        "name": "Chen Dun"
                    },
                    {
                        "name": "Zheng Wu"
                    },
                    {
                        "name": "Xing Jin"
                    },
                    {
                        "name": "Lin Yan"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yan"
                },
                "author": "Lin Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17195v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17195v3",
                "updated": "2024-10-28T17:28:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    28,
                    51,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-22T17:13:38Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    17,
                    13,
                    38,
                    1,
                    296,
                    0
                ],
                "title": "Non-myopic Generation of Language Models for Reasoning and Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-myopic Generation of Language Models for Reasoning and Planning"
                },
                "summary": "Large Language Models have demonstrated remarkable abilities in reasoning and\nplanning by breaking down complex problems into sequential steps. Despite their\nsuccess in various domains like mathematical problem-solving and coding, LLMs\nface challenges in ensuring reliable and optimal planning due to their inherent\nmyopic nature of autoregressive decoding. This paper revisits LLM reasoning\nfrom an optimal-control perspective, proposing a novel method,\nPredictive-Decoding, that leverages Model Predictive Control to enhance\nplanning accuracy. By re-weighting LLM distributions based on foresight\ntrajectories, Predictive-Decoding aims to mitigate early errors and promote\nnon-myopic planning. Our experiments show significant improvements in a wide\nrange of tasks for math, coding, and agents. Furthermore, Predictive-Decoding\ndemonstrates computational efficiency, outperforming search baselines with\nreduced computational resources. This study provides insights into optimizing\nLLM planning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have demonstrated remarkable abilities in reasoning and\nplanning by breaking down complex problems into sequential steps. Despite their\nsuccess in various domains like mathematical problem-solving and coding, LLMs\nface challenges in ensuring reliable and optimal planning due to their inherent\nmyopic nature of autoregressive decoding. This paper revisits LLM reasoning\nfrom an optimal-control perspective, proposing a novel method,\nPredictive-Decoding, that leverages Model Predictive Control to enhance\nplanning accuracy. By re-weighting LLM distributions based on foresight\ntrajectories, Predictive-Decoding aims to mitigate early errors and promote\nnon-myopic planning. Our experiments show significant improvements in a wide\nrange of tasks for math, coding, and agents. Furthermore, Predictive-Decoding\ndemonstrates computational efficiency, outperforming search baselines with\nreduced computational resources. This study provides insights into optimizing\nLLM planning capabilities."
                },
                "authors": [
                    {
                        "name": "Chang Ma"
                    },
                    {
                        "name": "Haiteng Zhao"
                    },
                    {
                        "name": "Junlei Zhang"
                    },
                    {
                        "name": "Junxian He"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17195v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17195v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21233v1",
                "updated": "2024-10-28T17:24:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    24,
                    37,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:24:37Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    24,
                    37,
                    0,
                    302,
                    0
                ],
                "title": "ST-ITO: Controlling Audio Effects for Style Transfer with Inference-Time\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST-ITO: Controlling Audio Effects for Style Transfer with Inference-Time\n  Optimization"
                },
                "summary": "Audio production style transfer is the task of processing an input to impart\nstylistic elements from a reference recording. Existing approaches often train\na neural network to estimate control parameters for a set of audio effects.\nHowever, these approaches are limited in that they can only control a fixed set\nof effects, where the effects must be differentiable or otherwise employ\nspecialized training techniques. In this work, we introduce ST-ITO, Style\nTransfer with Inference-Time Optimization, an approach that instead searches\nthe parameter space of an audio effect chain at inference. This method enables\ncontrol of arbitrary audio effect chains, including unseen and\nnon-differentiable effects. Our approach employs a learned metric of audio\nproduction style, which we train through a simple and scalable self-supervised\npretraining strategy, along with a gradient-free optimizer. Due to the limited\nexisting evaluation methods for audio production style transfer, we introduce a\nmulti-part benchmark to evaluate audio production style metrics and style\ntransfer systems. This evaluation demonstrates that our audio representation\nbetter captures attributes related to audio production and enables expressive\nstyle transfer via control of arbitrary audio effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio production style transfer is the task of processing an input to impart\nstylistic elements from a reference recording. Existing approaches often train\na neural network to estimate control parameters for a set of audio effects.\nHowever, these approaches are limited in that they can only control a fixed set\nof effects, where the effects must be differentiable or otherwise employ\nspecialized training techniques. In this work, we introduce ST-ITO, Style\nTransfer with Inference-Time Optimization, an approach that instead searches\nthe parameter space of an audio effect chain at inference. This method enables\ncontrol of arbitrary audio effect chains, including unseen and\nnon-differentiable effects. Our approach employs a learned metric of audio\nproduction style, which we train through a simple and scalable self-supervised\npretraining strategy, along with a gradient-free optimizer. Due to the limited\nexisting evaluation methods for audio production style transfer, we introduce a\nmulti-part benchmark to evaluate audio production style metrics and style\ntransfer systems. This evaluation demonstrates that our audio representation\nbetter captures attributes related to audio production and enables expressive\nstyle transfer via control of arbitrary audio effects."
                },
                "authors": [
                    {
                        "name": "Christian J. Steinmetz"
                    },
                    {
                        "name": "Shubhr Singh"
                    },
                    {
                        "name": "Marco Comunità"
                    },
                    {
                        "name": "Ilias Ibnyahya"
                    },
                    {
                        "name": "Shanxin Yuan"
                    },
                    {
                        "name": "Emmanouil Benetos"
                    },
                    {
                        "name": "Joshua D. Reiss"
                    }
                ],
                "author_detail": {
                    "name": "Joshua D. Reiss"
                },
                "author": "Joshua D. Reiss",
                "arxiv_comment": "Accepted to ISMIR 2024. Code available\n  https://github.com/csteinmetz1/st-ito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02902v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02902v3",
                "updated": "2024-10-28T17:22:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    22,
                    43,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-03T18:48:38Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    18,
                    48,
                    38,
                    3,
                    277,
                    0
                ],
                "title": "Better Instruction-Following Through Minimum Bayes Risk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Instruction-Following Through Minimum Bayes Risk"
                },
                "summary": "General-purpose LLM judges capable of human-level evaluation provide not only\na scalable and accurate way of evaluating instruction-following LLMs but also\nnew avenues for supervising and improving their performance. One promising way\nof leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR)\ndecoding, which uses a reference-based evaluator to select a high-quality\noutput from amongst a set of candidate outputs. In the first part of this work,\nwe explore using MBR decoding as a method for improving the test-time\nperformance of instruction-following LLMs. We find that MBR decoding with\nreference-based LLM judges substantially improves over greedy decoding,\nbest-of-N decoding with reference-free judges and MBR decoding with lexical and\nembedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent\nacross LLMs with up to 70B parameters, demonstrating that smaller LLM judges\ncan be used to supervise much larger LLMs. Then, seeking to retain the\nimprovements from MBR decoding while mitigating additional test-time costs, we\nexplore iterative self-training on MBR-decoded outputs. We find that\nself-training using Direct Preference Optimisation leads to significant\nperformance gains, such that the self-trained models with greedy decoding\ngenerally match and sometimes exceed the performance of their base models with\nMBR decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-purpose LLM judges capable of human-level evaluation provide not only\na scalable and accurate way of evaluating instruction-following LLMs but also\nnew avenues for supervising and improving their performance. One promising way\nof leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR)\ndecoding, which uses a reference-based evaluator to select a high-quality\noutput from amongst a set of candidate outputs. In the first part of this work,\nwe explore using MBR decoding as a method for improving the test-time\nperformance of instruction-following LLMs. We find that MBR decoding with\nreference-based LLM judges substantially improves over greedy decoding,\nbest-of-N decoding with reference-free judges and MBR decoding with lexical and\nembedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent\nacross LLMs with up to 70B parameters, demonstrating that smaller LLM judges\ncan be used to supervise much larger LLMs. Then, seeking to retain the\nimprovements from MBR decoding while mitigating additional test-time costs, we\nexplore iterative self-training on MBR-decoded outputs. We find that\nself-training using Direct Preference Optimisation leads to significant\nperformance gains, such that the self-trained models with greedy decoding\ngenerally match and sometimes exceed the performance of their base models with\nMBR decoding."
                },
                "authors": [
                    {
                        "name": "Ian Wu"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Amanda Bertsch"
                    },
                    {
                        "name": "Seungone Kim"
                    },
                    {
                        "name": "Sina Pakazad"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02902v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02902v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03225v2",
                "updated": "2024-10-28T17:05:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    5,
                    27,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-04T08:24:15Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    8,
                    24,
                    15,
                    4,
                    278,
                    0
                ],
                "title": "AutoPenBench: Benchmarking Generative Agents for Penetration Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPenBench: Benchmarking Generative Agents for Penetration Testing"
                },
                "summary": "Generative AI agents, software systems powered by Large Language Models\n(LLMs), are emerging as a promising approach to automate cybersecurity tasks.\nAmong the others, penetration testing is a challenging field due to the task\ncomplexity and the diverse strategies to simulate cyber-attacks. Despite\ngrowing interest and initial studies in automating penetration testing with\ngenerative agents, there remains a significant gap in the form of a\ncomprehensive and standard framework for their evaluation and development. This\npaper introduces AutoPenBench, an open benchmark for evaluating generative\nagents in automated penetration testing. We present a comprehensive framework\nthat includes 33 tasks, each representing a vulnerable system that the agent\nhas to attack. Tasks are of increasing difficulty levels, including in-vitro\nand real-world scenarios. We assess the agent performance with generic and\nspecific milestones that allow us to compare results in a standardised manner\nand understand the limits of the agent under test. We show the benefits of\nAutoPenBench by testing two agent architectures: a fully autonomous and a\nsemi-autonomous supporting human interaction. We compare their performance and\nlimitations. For example, the fully autonomous agent performs unsatisfactorily\nachieving a 21% Success Rate (SR) across the benchmark, solving 27% of the\nsimple tasks and only one real-world task. In contrast, the assisted agent\ndemonstrates substantial improvements, with 64% of SR. AutoPenBench allows us\nalso to observe how different LLMs like GPT-4o or OpenAI o1 impact the ability\nof the agents to complete the tasks. We believe that our benchmark fills the\ngap with a standard and flexible framework to compare penetration testing\nagents on a common ground. We hope to extend AutoPenBench along with the\nresearch community by making it available under\nhttps://github.com/lucagioacchini/auto-pen-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI agents, software systems powered by Large Language Models\n(LLMs), are emerging as a promising approach to automate cybersecurity tasks.\nAmong the others, penetration testing is a challenging field due to the task\ncomplexity and the diverse strategies to simulate cyber-attacks. Despite\ngrowing interest and initial studies in automating penetration testing with\ngenerative agents, there remains a significant gap in the form of a\ncomprehensive and standard framework for their evaluation and development. This\npaper introduces AutoPenBench, an open benchmark for evaluating generative\nagents in automated penetration testing. We present a comprehensive framework\nthat includes 33 tasks, each representing a vulnerable system that the agent\nhas to attack. Tasks are of increasing difficulty levels, including in-vitro\nand real-world scenarios. We assess the agent performance with generic and\nspecific milestones that allow us to compare results in a standardised manner\nand understand the limits of the agent under test. We show the benefits of\nAutoPenBench by testing two agent architectures: a fully autonomous and a\nsemi-autonomous supporting human interaction. We compare their performance and\nlimitations. For example, the fully autonomous agent performs unsatisfactorily\nachieving a 21% Success Rate (SR) across the benchmark, solving 27% of the\nsimple tasks and only one real-world task. In contrast, the assisted agent\ndemonstrates substantial improvements, with 64% of SR. AutoPenBench allows us\nalso to observe how different LLMs like GPT-4o or OpenAI o1 impact the ability\nof the agents to complete the tasks. We believe that our benchmark fills the\ngap with a standard and flexible framework to compare penetration testing\nagents on a common ground. We hope to extend AutoPenBench along with the\nresearch community by making it available under\nhttps://github.com/lucagioacchini/auto-pen-bench."
                },
                "authors": [
                    {
                        "name": "Luca Gioacchini"
                    },
                    {
                        "name": "Marco Mellia"
                    },
                    {
                        "name": "Idilio Drago"
                    },
                    {
                        "name": "Alexander Delsanto"
                    },
                    {
                        "name": "Giuseppe Siracusano"
                    },
                    {
                        "name": "Roberto Bifulco"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Bifulco"
                },
                "author": "Roberto Bifulco",
                "arxiv_comment": "Codes for the benchmark:\n  https://github.com/lucagioacchini/auto-pen-bench Codes for the paper\n  experiments: https://github.com/lucagioacchini/genai-pentest-paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01724v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01724v3",
                "updated": "2024-10-28T17:05:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    5,
                    16,
                    0,
                    302,
                    0
                ],
                "published": "2024-07-01T18:57:35Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    18,
                    57,
                    35,
                    0,
                    183,
                    0
                ],
                "title": "Predicting DC-Link Capacitor Current Ripple in AC-DC Rectifier Circuits\n  Using Fine-Tuned Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting DC-Link Capacitor Current Ripple in AC-DC Rectifier Circuits\n  Using Fine-Tuned Large Language Models"
                },
                "summary": "Foundational Large Language Models (LLMs) such as GPT-3.5-turbo allow users\nto refine the model based on newer information, known as ``fine-tuning''. This\npaper leverages this ability to analyze AC-DC converter behaviors, focusing on\nthe ripple current in DC-link capacitors. Capacitors degrade faster under high\nripple currents, complicating life monitoring and necessitating preemptive\nreplacements. Using minimal invasive noisy hardware measurements from a full\nbridge rectifier and 90W Power Factor Correction (PFC) boost converter, an\nLLM-based models to predict ripple content in DC-link currents was developed\nwhich demonstrated the LLMs' ability for near-accurate predictions. This study\nalso highlights data requirements for precise nonlinear power electronic\ncircuit parameter predictions to predict component degradation without any\nadditional sensors. Furthermore, the proposed framework could be extended to\nany non-linear function mapping problem as well as estimating the capacitor\nEquivalent Series Resistance (ESR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundational Large Language Models (LLMs) such as GPT-3.5-turbo allow users\nto refine the model based on newer information, known as ``fine-tuning''. This\npaper leverages this ability to analyze AC-DC converter behaviors, focusing on\nthe ripple current in DC-link capacitors. Capacitors degrade faster under high\nripple currents, complicating life monitoring and necessitating preemptive\nreplacements. Using minimal invasive noisy hardware measurements from a full\nbridge rectifier and 90W Power Factor Correction (PFC) boost converter, an\nLLM-based models to predict ripple content in DC-link currents was developed\nwhich demonstrated the LLMs' ability for near-accurate predictions. This study\nalso highlights data requirements for precise nonlinear power electronic\ncircuit parameter predictions to predict component degradation without any\nadditional sensors. Furthermore, the proposed framework could be extended to\nany non-linear function mapping problem as well as estimating the capacitor\nEquivalent Series Resistance (ESR)."
                },
                "authors": [
                    {
                        "name": "Mohamed Zeid"
                    },
                    {
                        "name": "Subir Majumder"
                    },
                    {
                        "name": "Hasan Ibrahim"
                    },
                    {
                        "name": "Prasad Enjeti"
                    },
                    {
                        "name": "Le Xie"
                    },
                    {
                        "name": "Chao Tian"
                    }
                ],
                "author_detail": {
                    "name": "Chao Tian"
                },
                "author": "Chao Tian",
                "arxiv_comment": "6 pages, 12 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01724v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01724v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09243v2",
                "updated": "2024-10-28T17:02:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    2,
                    46,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-14T00:51:45Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    0,
                    51,
                    45,
                    5,
                    258,
                    0
                ],
                "title": "Unconditional Randomization Tests for Interference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unconditional Randomization Tests for Interference"
                },
                "summary": "When conducting causal inference or designing policy, researchers are often\nconcerned with the existence and extent of interference between units, which\nmay be influenced by factors such as distance, proximity, and connection\nstrength. However, complex correlations across units pose significant\nchallenges for inference. This paper introduces partial null randomization\ntests (PNRTs), a novel framework for testing interference in experimental\nsettings. PNRTs adopt a design-based approach, combining unconditional\nrandomization testing with pairwise comparisons to enable straightforward\nimplementation and ensure finite-sample validity under minimal assumptions\nabout network structure. To illustrate the method's broad applicability, this\npaper applies it to a large-scale experiment by Blattman et al. (2021) in\nBogota, Colombia, which evaluates the impact of hotspot policing on crime using\nstreet segments as units of analysis. The findings indicate that increasing\npolice patrolling time in hotspots has a significant displacement effect on\nviolent crime but not on property crime. A simulation study calibrated to this\ndataset further demonstrates the strong power properties of PNRTs and their\nsuitability for general interference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When conducting causal inference or designing policy, researchers are often\nconcerned with the existence and extent of interference between units, which\nmay be influenced by factors such as distance, proximity, and connection\nstrength. However, complex correlations across units pose significant\nchallenges for inference. This paper introduces partial null randomization\ntests (PNRTs), a novel framework for testing interference in experimental\nsettings. PNRTs adopt a design-based approach, combining unconditional\nrandomization testing with pairwise comparisons to enable straightforward\nimplementation and ensure finite-sample validity under minimal assumptions\nabout network structure. To illustrate the method's broad applicability, this\npaper applies it to a large-scale experiment by Blattman et al. (2021) in\nBogota, Colombia, which evaluates the impact of hotspot policing on crime using\nstreet segments as units of analysis. The findings indicate that increasing\npolice patrolling time in hotspots has a significant displacement effect on\nviolent crime but not on property crime. A simulation study calibrated to this\ndataset further demonstrates the strong power properties of PNRTs and their\nsuitability for general interference scenarios."
                },
                "authors": [
                    {
                        "name": "Liang Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Liang Zhong"
                },
                "author": "Liang Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01536v2",
                "updated": "2024-10-28T17:02:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    2,
                    28,
                    0,
                    302,
                    0
                ],
                "published": "2024-05-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    17,
                    59,
                    52,
                    3,
                    123,
                    0
                ],
                "title": "Customizing Text-to-Image Models with a Single Image Pair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customizing Text-to-Image Models with a Single Image Pair"
                },
                "summary": "Art reinterpretation is the practice of creating a variation of a reference\nwork, making a paired artwork that exhibits a distinct artistic style. We ask\nif such an image pair can be used to customize a generative model to capture\nthe demonstrated stylistic difference. We propose Pair Customization, a new\ncustomization method that learns stylistic difference from a single image pair\nand then applies the acquired style to the generation process. Unlike existing\nmethods that learn to mimic a single concept from a collection of images, our\nmethod captures the stylistic difference between paired images. This allows us\nto apply a stylistic change without overfitting to the specific image content\nin the examples. To address this new task, we employ a joint optimization\nmethod that explicitly separates the style and content into distinct LoRA\nweight spaces. We optimize these style and content weights to reproduce the\nstyle and content images while encouraging their orthogonality. During\ninference, we modify the diffusion process via a new style guidance based on\nour learned weights. Both qualitative and quantitative experiments show that\nour method can effectively learn style while avoiding overfitting to image\ncontent, highlighting the potential of modeling such stylistic differences from\na single image pair.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Art reinterpretation is the practice of creating a variation of a reference\nwork, making a paired artwork that exhibits a distinct artistic style. We ask\nif such an image pair can be used to customize a generative model to capture\nthe demonstrated stylistic difference. We propose Pair Customization, a new\ncustomization method that learns stylistic difference from a single image pair\nand then applies the acquired style to the generation process. Unlike existing\nmethods that learn to mimic a single concept from a collection of images, our\nmethod captures the stylistic difference between paired images. This allows us\nto apply a stylistic change without overfitting to the specific image content\nin the examples. To address this new task, we employ a joint optimization\nmethod that explicitly separates the style and content into distinct LoRA\nweight spaces. We optimize these style and content weights to reproduce the\nstyle and content images while encouraging their orthogonality. During\ninference, we modify the diffusion process via a new style guidance based on\nour learned weights. Both qualitative and quantitative experiments show that\nour method can effectively learn style while avoiding overfitting to image\ncontent, highlighting the potential of modeling such stylistic differences from\na single image pair."
                },
                "authors": [
                    {
                        "name": "Maxwell Jones"
                    },
                    {
                        "name": "Sheng-Yu Wang"
                    },
                    {
                        "name": "Nupur Kumari"
                    },
                    {
                        "name": "David Bau"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun-Yan Zhu"
                },
                "author": "Jun-Yan Zhu",
                "arxiv_comment": "project page: https://paircustomization.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21218v1",
                "updated": "2024-10-28T17:02:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    2,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:02:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    2,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Lifting the Veil on the Large Language Model Supply Chain: Composition,\n  Risks, and Mitigations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifting the Veil on the Large Language Model Supply Chain: Composition,\n  Risks, and Mitigations"
                },
                "summary": "Large language models (LLM) have sparked significant impact with regard to\nboth intelligence and productivity. In recent years, a great surge has been\nwitnessed in the introduction of both commercial and open-source LLMs. Many\nbusinesses have adopted the LLMs into their applications to solve their own\ndomain-specific tasks. However, integrating LLMs into specific business\nscenarios requires more than just utilizing the models themselves. Instead, it\nis a systematic process that involves substantial components, which are\ncollectively referred to as the LLM supply chain. The LLM supply chain\ninherently carries risks. Therefore, it is essential to understand the types of\ncomponents that may be introduced into the supply chain and the associated\nrisks, enabling different stakeholders to implement effective mitigation\nmeasures. While some literature touches on risks associated with the LLM supply\nchain, there is currently no paper that explicitly defines its scope,\nidentifies inherent risks, and examines potential mitigation strategies. As\nLLMs have become essential infrastructure in the new era, we believe that a\nthorough review of the LLM supply chain, along with its inherent risks and\nmitigation strategies, would be valuable for industry practitioners to avoid\npotential damages and losses, and enlightening for academic researchers to\nrethink existing approaches and explore new avenues of research. Our paper\nprovides a comprehensive overview of the LLM supply chain, detailing the\nstakeholders, composing artifacts, and the supplying types. We developed\ntaxonomies of risk types, risky actions, and mitigations related to various\nsupply chain stakeholders and components. In summary, our work explores the\ntechnical and operational aspects of the LLM supply chain, offering valuable\ninsights for researchers and engineers in the evolving LLM landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLM) have sparked significant impact with regard to\nboth intelligence and productivity. In recent years, a great surge has been\nwitnessed in the introduction of both commercial and open-source LLMs. Many\nbusinesses have adopted the LLMs into their applications to solve their own\ndomain-specific tasks. However, integrating LLMs into specific business\nscenarios requires more than just utilizing the models themselves. Instead, it\nis a systematic process that involves substantial components, which are\ncollectively referred to as the LLM supply chain. The LLM supply chain\ninherently carries risks. Therefore, it is essential to understand the types of\ncomponents that may be introduced into the supply chain and the associated\nrisks, enabling different stakeholders to implement effective mitigation\nmeasures. While some literature touches on risks associated with the LLM supply\nchain, there is currently no paper that explicitly defines its scope,\nidentifies inherent risks, and examines potential mitigation strategies. As\nLLMs have become essential infrastructure in the new era, we believe that a\nthorough review of the LLM supply chain, along with its inherent risks and\nmitigation strategies, would be valuable for industry practitioners to avoid\npotential damages and losses, and enlightening for academic researchers to\nrethink existing approaches and explore new avenues of research. Our paper\nprovides a comprehensive overview of the LLM supply chain, detailing the\nstakeholders, composing artifacts, and the supplying types. We developed\ntaxonomies of risk types, risky actions, and mitigations related to various\nsupply chain stakeholders and components. In summary, our work explores the\ntechnical and operational aspects of the LLM supply chain, offering valuable\ninsights for researchers and engineers in the evolving LLM landscape."
                },
                "authors": [
                    {
                        "name": "Kaifeng Huang"
                    },
                    {
                        "name": "Bihuan Chen"
                    },
                    {
                        "name": "You Lu"
                    },
                    {
                        "name": "Susheng Wu"
                    },
                    {
                        "name": "Dingji Wang"
                    },
                    {
                        "name": "Yiheng Huang"
                    },
                    {
                        "name": "Haowen Jiang"
                    },
                    {
                        "name": "Zhuotong Zhou"
                    },
                    {
                        "name": "Junming Cao"
                    },
                    {
                        "name": "Xin Peng"
                    }
                ],
                "author_detail": {
                    "name": "Xin Peng"
                },
                "author": "Xin Peng",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21216v1",
                "updated": "2024-10-28T17:01:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    1,
                    52,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:01:52Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    1,
                    52,
                    0,
                    302,
                    0
                ],
                "title": "HoPE: A Novel Positional Encoding Without Long-Term Decay for Enhanced\n  Context Awareness and Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoPE: A Novel Positional Encoding Without Long-Term Decay for Enhanced\n  Context Awareness and Extrapolation"
                },
                "summary": "Many positional encodings (PEs) are designed to exhibit long-term decay,\nbased on an entrenched and long-standing inductive opinion: tokens farther away\nfrom the current position carry less relevant information. We argue that\nlong-term decay is outdated in the era of LLMs, as LLMs are now applied to\ntasks demanding precise retrieval of in-context information from arbitrary\npositions. Firstly, we present empirical analyses on various PEs, demonstrating\nthat models inherently learn attention with only a local-decay pattern while\nforming a U-shape pattern globally, contradicting the principle of long-term\ndecay. Furthermore, we conduct a detailed analysis of rotary position encoding\n(RoPE, a prevalent relative positional encoding in LLMs), and found that the\nU-shape attention is caused by some learned components, which are also the key\nfactor limiting RoPE's expressiveness and extrapolation.Inspired by these\ninsights, we propose High-frequency rotary Position Encoding (HoPE). HoPE\nreplaces the specific components in RoPE with position-independent ones,\nretaining only high-frequency signals, which also breaks the principle of\nlong-term decay in theory. HoPE achieves two major advantages: (1) Without\nconstraints imposed by long-term decay, contradictory factors that limit\nspontaneous attention optimization and model extrapolation performance are\nremoved. (2) Components representing positions and semantics are are optimized.\nThese enhances model's context awareness and extrapolation, as validated by\nextensive experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many positional encodings (PEs) are designed to exhibit long-term decay,\nbased on an entrenched and long-standing inductive opinion: tokens farther away\nfrom the current position carry less relevant information. We argue that\nlong-term decay is outdated in the era of LLMs, as LLMs are now applied to\ntasks demanding precise retrieval of in-context information from arbitrary\npositions. Firstly, we present empirical analyses on various PEs, demonstrating\nthat models inherently learn attention with only a local-decay pattern while\nforming a U-shape pattern globally, contradicting the principle of long-term\ndecay. Furthermore, we conduct a detailed analysis of rotary position encoding\n(RoPE, a prevalent relative positional encoding in LLMs), and found that the\nU-shape attention is caused by some learned components, which are also the key\nfactor limiting RoPE's expressiveness and extrapolation.Inspired by these\ninsights, we propose High-frequency rotary Position Encoding (HoPE). HoPE\nreplaces the specific components in RoPE with position-independent ones,\nretaining only high-frequency signals, which also breaks the principle of\nlong-term decay in theory. HoPE achieves two major advantages: (1) Without\nconstraints imposed by long-term decay, contradictory factors that limit\nspontaneous attention optimization and model extrapolation performance are\nremoved. (2) Components representing positions and semantics are are optimized.\nThese enhances model's context awareness and extrapolation, as validated by\nextensive experiments."
                },
                "authors": [
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Wei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Liu"
                },
                "author": "Wei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02144v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02144v3",
                "updated": "2024-10-28T17:01:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    1,
                    23,
                    0,
                    302,
                    0
                ],
                "published": "2024-05-03T14:48:20Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    14,
                    48,
                    20,
                    4,
                    124,
                    0
                ],
                "title": "MedReadMe: A Systematic Study for Fine-grained Sentence Readability in\n  Medical Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedReadMe: A Systematic Study for Fine-grained Sentence Readability in\n  Medical Domain"
                },
                "summary": "Medical texts are notoriously challenging to read. Properly measuring their\nreadability is the first step towards making them more accessible. In this\npaper, we present a systematic study on fine-grained readability measurements\nin the medical domain at both sentence-level and span-level. We introduce a new\ndataset MedReadMe, which consists of manually annotated readability ratings and\nfine-grained complex span annotation for 4,520 sentences, featuring two novel\n\"Google-Easy\" and \"Google-Hard\" categories. It supports our quantitative\nanalysis, which covers 650 linguistic features and automatic complex word and\njargon identification. Enabled by our high-quality annotation, we benchmark and\nimprove several state-of-the-art sentence-level readability metrics for the\nmedical domain specifically, which include unsupervised, supervised, and\nprompting-based methods using recently developed large language models (LLMs).\nInformed by our fine-grained complex span annotation, we find that adding a\nsingle feature, capturing the number of jargon spans, into existing readability\nformulas can significantly improve their correlation with human judgments. The\ndata is available at tinyurl.com/medreadme-repo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical texts are notoriously challenging to read. Properly measuring their\nreadability is the first step towards making them more accessible. In this\npaper, we present a systematic study on fine-grained readability measurements\nin the medical domain at both sentence-level and span-level. We introduce a new\ndataset MedReadMe, which consists of manually annotated readability ratings and\nfine-grained complex span annotation for 4,520 sentences, featuring two novel\n\"Google-Easy\" and \"Google-Hard\" categories. It supports our quantitative\nanalysis, which covers 650 linguistic features and automatic complex word and\njargon identification. Enabled by our high-quality annotation, we benchmark and\nimprove several state-of-the-art sentence-level readability metrics for the\nmedical domain specifically, which include unsupervised, supervised, and\nprompting-based methods using recently developed large language models (LLMs).\nInformed by our fine-grained complex span annotation, we find that adding a\nsingle feature, capturing the number of jargon spans, into existing readability\nformulas can significantly improve their correlation with human judgments. The\ndata is available at tinyurl.com/medreadme-repo"
                },
                "authors": [
                    {
                        "name": "Chao Jiang"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "This paper has been accepted as oral presentation at EMNLP 2024 main\n  conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02144v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02144v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21213v1",
                "updated": "2024-10-28T16:58:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    58,
                    33,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T16:58:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    58,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Spatial causal inference in the presence of preferential sampling to\n  study the impacts of marine protected areas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial causal inference in the presence of preferential sampling to\n  study the impacts of marine protected areas"
                },
                "summary": "Marine Protected Areas (MPAs) have been established globally to conserve\nmarine resources. Given their maintenance costs and impact on commercial\nfishing, it is critical to evaluate their effectiveness to support future\nconservation. In this paper, we use data collected from the Australian coast to\nestimate the effect of MPAs on biodiversity. Environmental studies such as\nthese are often observational, and processes of interest exhibit spatial\ndependence, which presents challenges in estimating the causal effects. Spatial\ndata can also be subject to preferential sampling, where the sampling locations\nare related to the response variable, further complicating inference and\nprediction. To address these challenges, we propose a spatial causal inference\nmethod that simultaneously accounts for unmeasured spatial confounders in both\nthe sampling process and the treatment allocation. We prove the identifiability\nof key parameters in the model and the consistency of the posterior\ndistributions of those parameters. We show via simulation studies that the\ncausal effect of interest can be reliably estimated under the proposed model.\nThe proposed method is applied to assess the effect of MPAs on fish biomass. We\nfind evidence of preferential sampling and that properly accounting for this\nsource of bias impacts the estimate of the causal effect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marine Protected Areas (MPAs) have been established globally to conserve\nmarine resources. Given their maintenance costs and impact on commercial\nfishing, it is critical to evaluate their effectiveness to support future\nconservation. In this paper, we use data collected from the Australian coast to\nestimate the effect of MPAs on biodiversity. Environmental studies such as\nthese are often observational, and processes of interest exhibit spatial\ndependence, which presents challenges in estimating the causal effects. Spatial\ndata can also be subject to preferential sampling, where the sampling locations\nare related to the response variable, further complicating inference and\nprediction. To address these challenges, we propose a spatial causal inference\nmethod that simultaneously accounts for unmeasured spatial confounders in both\nthe sampling process and the treatment allocation. We prove the identifiability\nof key parameters in the model and the consistency of the posterior\ndistributions of those parameters. We show via simulation studies that the\ncausal effect of interest can be reliably estimated under the proposed model.\nThe proposed method is applied to assess the effect of MPAs on fish biomass. We\nfind evidence of preferential sampling and that properly accounting for this\nsource of bias impacts the estimate of the causal effect."
                },
                "authors": [
                    {
                        "name": "Dongjae Son"
                    },
                    {
                        "name": "Brian J. Reich"
                    },
                    {
                        "name": "Erin M. Schliep"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "David A. Gill"
                    }
                ],
                "author_detail": {
                    "name": "David A. Gill"
                },
                "author": "David A. Gill",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21211v1",
                "updated": "2024-10-28T16:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T16:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Exploring contextual modeling with linear complexity for point cloud\n  segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring contextual modeling with linear complexity for point cloud\n  segmentation"
                },
                "summary": "Point cloud segmentation is an important topic in 3D understanding that has\ntraditionally has been tackled using either the CNN or Transformer. Recently,\nMamba has emerged as a promising alternative, offering efficient long-range\ncontextual modeling capabilities without the quadratic complexity associated\nwith Transformer's attention mechanisms. However, despite Mamba's potential,\nearly efforts have all failed to achieve better performance than the best\nCNN-based and Transformer-based methods. In this work, we address this\nchallenge by identifying the key components of an effective and efficient point\ncloud segmentation architecture. Specifically, we show that: 1) Spatial\nlocality and robust contextual understanding are critical for strong\nperformance, and 2) Mamba features linear computational complexity, offering\nsuperior data and inference efficiency compared to Transformers, while still\nbeing capable of delivering strong contextual understanding. Additionally, we\nfurther enhance the standard Mamba specifically for point cloud segmentation by\nidentifying its two key shortcomings. First, the enforced causality in the\noriginal Mamba is unsuitable for processing point clouds that have no such\ndependencies. Second, its unidirectional scanning strategy imposes a\ndirectional bias, hampering its ability to capture the full context of\nunordered point clouds in a single pass. To address these issues, we carefully\nremove the causal convolutions and introduce a novel Strided Bidirectional SSM\nto enhance the model's capability to capture spatial relationships. Our efforts\nculminate in the development of a novel architecture named MEEPO, which\neffectively integrates the strengths of CNN and Mamba. MEEPO surpasses the\nprevious state-of-the-art method, PTv3, by up to +0.8 mIoU on multiple key\nbenchmark datasets, while being 42.1% faster and 5.53x more memory efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point cloud segmentation is an important topic in 3D understanding that has\ntraditionally has been tackled using either the CNN or Transformer. Recently,\nMamba has emerged as a promising alternative, offering efficient long-range\ncontextual modeling capabilities without the quadratic complexity associated\nwith Transformer's attention mechanisms. However, despite Mamba's potential,\nearly efforts have all failed to achieve better performance than the best\nCNN-based and Transformer-based methods. In this work, we address this\nchallenge by identifying the key components of an effective and efficient point\ncloud segmentation architecture. Specifically, we show that: 1) Spatial\nlocality and robust contextual understanding are critical for strong\nperformance, and 2) Mamba features linear computational complexity, offering\nsuperior data and inference efficiency compared to Transformers, while still\nbeing capable of delivering strong contextual understanding. Additionally, we\nfurther enhance the standard Mamba specifically for point cloud segmentation by\nidentifying its two key shortcomings. First, the enforced causality in the\noriginal Mamba is unsuitable for processing point clouds that have no such\ndependencies. Second, its unidirectional scanning strategy imposes a\ndirectional bias, hampering its ability to capture the full context of\nunordered point clouds in a single pass. To address these issues, we carefully\nremove the causal convolutions and introduce a novel Strided Bidirectional SSM\nto enhance the model's capability to capture spatial relationships. Our efforts\nculminate in the development of a novel architecture named MEEPO, which\neffectively integrates the strengths of CNN and Mamba. MEEPO surpasses the\nprevious state-of-the-art method, PTv3, by up to +0.8 mIoU on multiple key\nbenchmark datasets, while being 42.1% faster and 5.53x more memory efficient."
                },
                "authors": [
                    {
                        "name": "Yong Xien Chng"
                    },
                    {
                        "name": "Xuchong Qiu"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Yifan Pu"
                    },
                    {
                        "name": "Jiewei Cao"
                    },
                    {
                        "name": "Gao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Gao Huang"
                },
                "author": "Gao Huang",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14584v2",
                "updated": "2024-10-28T16:41:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    41,
                    48,
                    0,
                    302,
                    0
                ],
                "published": "2024-07-19T18:00:00Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    18,
                    0,
                    0,
                    4,
                    201,
                    0
                ],
                "title": "Echo Location: Distances to Galactic Supernovae From ASAS-SN Light\n  Echoes and 3D Dust Maps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Echo Location: Distances to Galactic Supernovae From ASAS-SN Light\n  Echoes and 3D Dust Maps"
                },
                "summary": "Light echoes occur when light from a luminous transient is scattered by dust\nback into our line of sight with a time delay due to the extra propagation\ndistance. We introduce a novel approach to estimating the distance to a source\nby combining light echoes with recent three-dimensional dust maps. We identify\nlight echoes from the historical supernovae Cassiopeia A and SN 1572 (Tycho) in\nnearly a decade of imaging from the All-Sky Automated Survey for Supernovae\n(ASAS-SN). Using these light echoes, we find distances of $3.6\\pm0.1$ kpc and\n$3.2^{+0.1}_{-0.2}$ kpc to Cas A and Tycho, respectively, which are generally\nconsistent with previous estimates but are more precise. These distance\nuncertainties are primarily dominated by the low distance resolution of the 3D\ndust maps, which will likely improve in the future. The candidate single\ndegenerate explosion donor stars B and G in Tycho are clearly foreground stars.\nFinally, the inferred reddening towards each SN agrees well with the\nintervening HI column density estimates from X-ray analyses of the remnants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light echoes occur when light from a luminous transient is scattered by dust\nback into our line of sight with a time delay due to the extra propagation\ndistance. We introduce a novel approach to estimating the distance to a source\nby combining light echoes with recent three-dimensional dust maps. We identify\nlight echoes from the historical supernovae Cassiopeia A and SN 1572 (Tycho) in\nnearly a decade of imaging from the All-Sky Automated Survey for Supernovae\n(ASAS-SN). Using these light echoes, we find distances of $3.6\\pm0.1$ kpc and\n$3.2^{+0.1}_{-0.2}$ kpc to Cas A and Tycho, respectively, which are generally\nconsistent with previous estimates but are more precise. These distance\nuncertainties are primarily dominated by the low distance resolution of the 3D\ndust maps, which will likely improve in the future. The candidate single\ndegenerate explosion donor stars B and G in Tycho are clearly foreground stars.\nFinally, the inferred reddening towards each SN agrees well with the\nintervening HI column density estimates from X-ray analyses of the remnants."
                },
                "authors": [
                    {
                        "name": "Kyle D. Neumann"
                    },
                    {
                        "name": "Michael A. Tucker"
                    },
                    {
                        "name": "Christopher S. Kochanek"
                    },
                    {
                        "name": "Benjamin J. Shappee"
                    },
                    {
                        "name": "K. Z. Stanek"
                    }
                ],
                "author_detail": {
                    "name": "K. Z. Stanek"
                },
                "author": "K. Z. Stanek",
                "arxiv_comment": "11 pages, 10 figures, 2 tables. The table containing the light echo\n  data presented in this submission is included in machine-readable format as\n  an ancillary file",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07076v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07076v3",
                "updated": "2024-10-28T16:39:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    39,
                    35,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-09T17:19:58Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    19,
                    58,
                    2,
                    283,
                    0
                ],
                "title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry\n  Scientific Hypotheses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry\n  Scientific Hypotheses"
                },
                "summary": "Scientific discovery contributes largely to human society's prosperity, and\nrecent progress shows that LLMs could potentially catalyze this process.\nHowever, it is still unclear whether LLMs can discover novel and valid\nhypotheses in chemistry. In this work, we investigate this central research\nquestion: Can LLMs automatically discover novel and valid chemistry research\nhypotheses given only a chemistry research background (consisting of a research\nquestion and/or a background survey), without limitation on the domain of the\nresearch question? After extensive discussions with chemistry experts, we\npropose an assumption that a majority of chemistry hypotheses can be resulted\nfrom a research background and several inspirations. With this key insight, we\nbreak the central question into three smaller fundamental questions. In brief,\nthey are: (1) given a background question, whether LLMs can retrieve good\ninspirations; (2) with background and inspirations, whether LLMs can lead to\nhypothesis; and (3) whether LLMs can identify good hypotheses to rank them\nhigher. To investigate these questions, we construct a benchmark consisting of\n51 chemistry papers published in Nature, Science, or a similar level in 2024\n(all papers are only available online since 2024). Every paper is divided by\nchemistry PhD students into three components: background, inspirations, and\nhypothesis. The goal is to rediscover the hypothesis, given only the background\nand a large randomly selected chemistry literature corpus consisting the ground\ntruth inspiration papers, with LLMs trained with data up to 2023. We also\ndevelop an LLM-based multi-agent framework that leverages the assumption,\nconsisting of three stages reflecting the three smaller questions. The proposed\nmethod can rediscover many hypotheses with very high similarity with the ground\ntruth ones, covering the main innovations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific discovery contributes largely to human society's prosperity, and\nrecent progress shows that LLMs could potentially catalyze this process.\nHowever, it is still unclear whether LLMs can discover novel and valid\nhypotheses in chemistry. In this work, we investigate this central research\nquestion: Can LLMs automatically discover novel and valid chemistry research\nhypotheses given only a chemistry research background (consisting of a research\nquestion and/or a background survey), without limitation on the domain of the\nresearch question? After extensive discussions with chemistry experts, we\npropose an assumption that a majority of chemistry hypotheses can be resulted\nfrom a research background and several inspirations. With this key insight, we\nbreak the central question into three smaller fundamental questions. In brief,\nthey are: (1) given a background question, whether LLMs can retrieve good\ninspirations; (2) with background and inspirations, whether LLMs can lead to\nhypothesis; and (3) whether LLMs can identify good hypotheses to rank them\nhigher. To investigate these questions, we construct a benchmark consisting of\n51 chemistry papers published in Nature, Science, or a similar level in 2024\n(all papers are only available online since 2024). Every paper is divided by\nchemistry PhD students into three components: background, inspirations, and\nhypothesis. The goal is to rediscover the hypothesis, given only the background\nand a large randomly selected chemistry literature corpus consisting the ground\ntruth inspiration papers, with LLMs trained with data up to 2023. We also\ndevelop an LLM-based multi-agent framework that leverages the assumption,\nconsisting of three stages reflecting the three smaller questions. The proposed\nmethod can rediscover many hypotheses with very high similarity with the ground\ntruth ones, covering the main innovations."
                },
                "authors": [
                    {
                        "name": "Zonglin Yang"
                    },
                    {
                        "name": "Wanhao Liu"
                    },
                    {
                        "name": "Ben Gao"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Soujanya Poria"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongzhan Zhou"
                },
                "author": "Dongzhan Zhou",
                "arxiv_comment": "Code and Benchmark are available at\n  https://github.com/ZonglinY/MOOSE-Chem.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07076v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07076v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14577v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14577v3",
                "updated": "2024-10-28T16:37:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    37,
                    6,
                    0,
                    302,
                    0
                ],
                "published": "2024-05-23T13:51:55Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    13,
                    51,
                    55,
                    3,
                    144,
                    0
                ],
                "title": "Representation noising can prevent harmful fine-tuning on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation noising can prevent harmful fine-tuning on LLMs"
                },
                "summary": "Releasing open-source large language models (LLMs) presents a dual-use risk\nsince bad actors can easily fine-tune these models for harmful purposes. Even\nwithout the open release of weights, weight stealing and fine-tuning APIs make\nclosed models vulnerable to harmful fine-tuning attacks (HFAs). While safety\nmeasures like preventing jailbreaks and improving safety guardrails are\nimportant, such measures can easily be reversed through fine-tuning. In this\nwork, we propose Representation Noising (RepNoise), a defence mechanism that is\neffective even when attackers have access to the weights. RepNoise works by\nremoving information about harmful representations such that it is difficult to\nrecover them during fine-tuning. Importantly, our defence is also able to\ngeneralize across different subsets of harm that have not been seen during the\ndefence process as long as they are drawn from the same distribution of the\nattack set. Our method does not degrade the general capability of LLMs and\nretains the ability to train the model on harmless tasks. We provide empirical\nevidence that the effectiveness of our defence lies in its \"depth\": the degree\nto which information about harmful representations is removed across all layers\nof the LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Releasing open-source large language models (LLMs) presents a dual-use risk\nsince bad actors can easily fine-tune these models for harmful purposes. Even\nwithout the open release of weights, weight stealing and fine-tuning APIs make\nclosed models vulnerable to harmful fine-tuning attacks (HFAs). While safety\nmeasures like preventing jailbreaks and improving safety guardrails are\nimportant, such measures can easily be reversed through fine-tuning. In this\nwork, we propose Representation Noising (RepNoise), a defence mechanism that is\neffective even when attackers have access to the weights. RepNoise works by\nremoving information about harmful representations such that it is difficult to\nrecover them during fine-tuning. Importantly, our defence is also able to\ngeneralize across different subsets of harm that have not been seen during the\ndefence process as long as they are drawn from the same distribution of the\nattack set. Our method does not degrade the general capability of LLMs and\nretains the ability to train the model on harmless tasks. We provide empirical\nevidence that the effectiveness of our defence lies in its \"depth\": the degree\nto which information about harmful representations is removed across all layers\nof the LLM."
                },
                "authors": [
                    {
                        "name": "Domenic Rosati"
                    },
                    {
                        "name": "Jan Wehner"
                    },
                    {
                        "name": "Kai Williams"
                    },
                    {
                        "name": "Łukasz Bartoszcze"
                    },
                    {
                        "name": "David Atanasov"
                    },
                    {
                        "name": "Robie Gonzales"
                    },
                    {
                        "name": "Subhabrata Majumdar"
                    },
                    {
                        "name": "Carsten Maple"
                    },
                    {
                        "name": "Hassan Sajjad"
                    },
                    {
                        "name": "Frank Rudzicz"
                    }
                ],
                "author_detail": {
                    "name": "Frank Rudzicz"
                },
                "author": "Frank Rudzicz",
                "arxiv_comment": "Published in NeurIPs 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14577v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14577v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01631v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01631v3",
                "updated": "2024-10-28T16:33:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    33,
                    37,
                    0,
                    302,
                    0
                ],
                "published": "2024-08-03T02:07:39Z",
                "published_parsed": [
                    2024,
                    8,
                    3,
                    2,
                    7,
                    39,
                    5,
                    216,
                    0
                ],
                "title": "A Comparative Analysis of Wealth Index Predictions in Africa between\n  three Multi-Source Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Analysis of Wealth Index Predictions in Africa between\n  three Multi-Source Inference Models"
                },
                "summary": "Poverty map inference has become a critical focus of research, utilizing both\ntraditional and modern techniques, ranging from regression models to\nconvolutional neural networks applied to tabular data, satellite imagery, and\nnetworks. While much attention has been given to validating models during the\ntraining phase, the final predictions have received less scrutiny. In this\nstudy, we analyze the International Wealth Index (IWI) predicted by Lee and\nBraithwaite (2022) and Esp\\'in-Noboa et al. (2023), alongside the Relative\nWealth Index (RWI) inferred by Chi et al. (2022), across six Sub-Saharan\nAfrican countries. Our analysis reveals trends and discrepancies in wealth\npredictions between these models. In particular, significant and unexpected\ndiscrepancies between the predictions of Lee and Braithwaite and Esp\\'in-Noboa\net al., even after accounting for differences in training data. In contrast,\nthe shape of the wealth distributions predicted by Esp\\'in-Noboa et al. and Chi\net al. are more closely aligned, suggesting similar levels of skewness. These\nfindings raise concerns about the validity of certain models and emphasize the\nimportance of rigorous audits for wealth prediction algorithms used in\npolicy-making. Continuous validation and refinement are essential to ensure the\nreliability of these models, particularly when they inform poverty alleviation\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poverty map inference has become a critical focus of research, utilizing both\ntraditional and modern techniques, ranging from regression models to\nconvolutional neural networks applied to tabular data, satellite imagery, and\nnetworks. While much attention has been given to validating models during the\ntraining phase, the final predictions have received less scrutiny. In this\nstudy, we analyze the International Wealth Index (IWI) predicted by Lee and\nBraithwaite (2022) and Esp\\'in-Noboa et al. (2023), alongside the Relative\nWealth Index (RWI) inferred by Chi et al. (2022), across six Sub-Saharan\nAfrican countries. Our analysis reveals trends and discrepancies in wealth\npredictions between these models. In particular, significant and unexpected\ndiscrepancies between the predictions of Lee and Braithwaite and Esp\\'in-Noboa\net al., even after accounting for differences in training data. In contrast,\nthe shape of the wealth distributions predicted by Esp\\'in-Noboa et al. and Chi\net al. are more closely aligned, suggesting similar levels of skewness. These\nfindings raise concerns about the validity of certain models and emphasize the\nimportance of rigorous audits for wealth prediction algorithms used in\npolicy-making. Continuous validation and refinement are essential to ensure the\nreliability of these models, particularly when they inform poverty alleviation\nstrategies."
                },
                "authors": [
                    {
                        "name": "Márton Karsai"
                    },
                    {
                        "name": "János Kertész"
                    },
                    {
                        "name": "Lisette Espín-Noboa"
                    }
                ],
                "author_detail": {
                    "name": "Lisette Espín-Noboa"
                },
                "author": "Lisette Espín-Noboa",
                "arxiv_comment": "14 pages (main + references) + 8 pages (appendix). Accepted at the\n  9th Workshop on Data Science for Social Good, SoGood 2024, held in\n  conjunction with ECML PKDD 2024, at Vilnius, Lithuania",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01631v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01631v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01119v2",
                "updated": "2024-10-28T16:32:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    32,
                    1,
                    0,
                    302,
                    0
                ],
                "published": "2024-07-01T09:28:58Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    9,
                    28,
                    58,
                    0,
                    183,
                    0
                ],
                "title": "Pron vs Prompt: Can Large Language Models already Challenge a\n  World-Class Fiction Author at Creative Text Writing?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pron vs Prompt: Can Large Language Models already Challenge a\n  World-Class Fiction Author at Creative Text Writing?"
                },
                "summary": "It has become routine to report research results where Large Language Models\n(LLMs) outperform average humans in a wide range of language-related tasks, and\ncreative text writing is no exception. It seems natural, then, to raise the\nbid: Are LLMs ready to compete in creative writing skills with a top (rather\nthan average) novelist? To provide an initial answer for this question, we have\ncarried out a contest between Patricio Pron (an awarded novelist, considered\none of the best of his generation) and GPT-4 (one of the top performing LLMs),\nin the spirit of AI-human duels such as DeepBlue vs Kasparov and AlphaGo vs Lee\nSidol. We asked Pron and GPT-4 to provide thirty titles each, and then to write\nshort stories for both their titles and their opponent's. Then, we prepared an\nevaluation rubric inspired by Boden's definition of creativity, and we\ncollected 5,400 manual assessments provided by literature critics and scholars.\nThe results of our experimentation indicate that LLMs are still far from\nchallenging a top human creative writer, and that reaching such level of\nautonomous creative writing skills probably cannot be reached simply with\nlarger language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has become routine to report research results where Large Language Models\n(LLMs) outperform average humans in a wide range of language-related tasks, and\ncreative text writing is no exception. It seems natural, then, to raise the\nbid: Are LLMs ready to compete in creative writing skills with a top (rather\nthan average) novelist? To provide an initial answer for this question, we have\ncarried out a contest between Patricio Pron (an awarded novelist, considered\none of the best of his generation) and GPT-4 (one of the top performing LLMs),\nin the spirit of AI-human duels such as DeepBlue vs Kasparov and AlphaGo vs Lee\nSidol. We asked Pron and GPT-4 to provide thirty titles each, and then to write\nshort stories for both their titles and their opponent's. Then, we prepared an\nevaluation rubric inspired by Boden's definition of creativity, and we\ncollected 5,400 manual assessments provided by literature critics and scholars.\nThe results of our experimentation indicate that LLMs are still far from\nchallenging a top human creative writer, and that reaching such level of\nautonomous creative writing skills probably cannot be reached simply with\nlarger language models."
                },
                "authors": [
                    {
                        "name": "Guillermo Marco"
                    },
                    {
                        "name": "Julio Gonzalo"
                    },
                    {
                        "name": "Ramón del Castillo"
                    },
                    {
                        "name": "María Teresa Mateo Girona"
                    }
                ],
                "author_detail": {
                    "name": "María Teresa Mateo Girona"
                },
                "author": "María Teresa Mateo Girona",
                "arxiv_comment": "9 pages 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14035v3",
                "updated": "2024-10-28T16:07:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    7,
                    26,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-17T21:17:17Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    21,
                    17,
                    17,
                    3,
                    291,
                    0
                ],
                "title": "Optimal Communication and Key Rate Region for Hierarchical Secure\n  Aggregation with User Collusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Communication and Key Rate Region for Hierarchical Secure\n  Aggregation with User Collusion"
                },
                "summary": "Secure aggregation is concerned with the task of securely uploading the\ninputs of multiple users to an aggregation server without letting the server\nknow the inputs beyond their summation. It finds broad applications in\ndistributed machine learning paradigms such as federated learning (FL) where\nmultiple clients, each having access to a proprietary dataset, periodically\nupload their locally trained models (abstracted as inputs) to a parameter\nserver which then generates an aggregate (e.g., averaged) model that is sent\nback to the clients as an initializing point for a new round of local training.\nTo enhance the data privacy of the clients, secure aggregation protocols are\ndeveloped using techniques from cryptography to ensure that the server infers\nno more information of the users' inputs beyond the desired aggregated input,\neven if the server can collude with some users. Although laying the ground for\nunderstanding the fundamental utility-security trade-off in secure aggregation,\nthe simple star client-server architecture cannot capture more complex network\narchitectures used in practical systems. Motivated by hierarchical federated\nlearning, we investigate the secure aggregation problem in a $3$-layer\nhierarchical network consisting of clustered users connecting to an aggregation\nserver through an intermediate layer of relays. Besides the conventional server\nsecurity which requires that the server learns nothing beyond the desired sum\nof inputs, relay security is also imposed so that the relays infer nothing\nabout the users' inputs and remain oblivious. For such a hierarchical secure\naggregation (HSA) problem, we characterize the optimal multifaceted trade-off\nbetween communication (in terms of user-to-relay and relay-to-server\ncommunication rates) and secret key generation efficiency (in terms of\nindividual key and source key rates).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure aggregation is concerned with the task of securely uploading the\ninputs of multiple users to an aggregation server without letting the server\nknow the inputs beyond their summation. It finds broad applications in\ndistributed machine learning paradigms such as federated learning (FL) where\nmultiple clients, each having access to a proprietary dataset, periodically\nupload their locally trained models (abstracted as inputs) to a parameter\nserver which then generates an aggregate (e.g., averaged) model that is sent\nback to the clients as an initializing point for a new round of local training.\nTo enhance the data privacy of the clients, secure aggregation protocols are\ndeveloped using techniques from cryptography to ensure that the server infers\nno more information of the users' inputs beyond the desired aggregated input,\neven if the server can collude with some users. Although laying the ground for\nunderstanding the fundamental utility-security trade-off in secure aggregation,\nthe simple star client-server architecture cannot capture more complex network\narchitectures used in practical systems. Motivated by hierarchical federated\nlearning, we investigate the secure aggregation problem in a $3$-layer\nhierarchical network consisting of clustered users connecting to an aggregation\nserver through an intermediate layer of relays. Besides the conventional server\nsecurity which requires that the server learns nothing beyond the desired sum\nof inputs, relay security is also imposed so that the relays infer nothing\nabout the users' inputs and remain oblivious. For such a hierarchical secure\naggregation (HSA) problem, we characterize the optimal multifaceted trade-off\nbetween communication (in terms of user-to-relay and relay-to-server\ncommunication rates) and secret key generation efficiency (in terms of\nindividual key and source key rates)."
                },
                "authors": [
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Shiqiang Wang"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21163v1",
                "updated": "2024-10-28T16:04:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    4,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T16:04:22Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    4,
                    22,
                    0,
                    302,
                    0
                ],
                "title": "Resilience in Knowledge Graph Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resilience in Knowledge Graph Embeddings"
                },
                "summary": "In recent years, knowledge graphs have gained interest and witnessed\nwidespread applications in various domains, such as information retrieval,\nquestion-answering, recommendation systems, amongst others. Large-scale\nknowledge graphs to this end have demonstrated their utility in effectively\nrepresenting structured knowledge. To further facilitate the application of\nmachine learning techniques, knowledge graph embedding (KGE) models have been\ndeveloped. Such models can transform entities and relationships within\nknowledge graphs into vectors. However, these embedding models often face\nchallenges related to noise, missing information, distribution shift,\nadversarial attacks, etc. This can lead to sub-optimal embeddings and incorrect\ninferences, thereby negatively impacting downstream applications. While the\nexisting literature has focused so far on adversarial attacks on KGE models,\nthe challenges related to the other critical aspects remain unexplored. In this\npaper, we, first of all, give a unified definition of resilience, encompassing\nseveral factors such as generalisation, performance consistency, distribution\nadaption, and robustness. After formalizing these concepts for machine learning\nin general, we define them in the context of knowledge graphs. To find the gap\nin the existing works on resilience in the context of knowledge graphs, we\nperform a systematic survey, taking into account all these aspects mentioned\npreviously. Our survey results show that most of the existing works focus on a\nspecific aspect of resilience, namely robustness. After categorizing such works\nbased on their respective aspects of resilience, we discuss the challenges and\nfuture research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, knowledge graphs have gained interest and witnessed\nwidespread applications in various domains, such as information retrieval,\nquestion-answering, recommendation systems, amongst others. Large-scale\nknowledge graphs to this end have demonstrated their utility in effectively\nrepresenting structured knowledge. To further facilitate the application of\nmachine learning techniques, knowledge graph embedding (KGE) models have been\ndeveloped. Such models can transform entities and relationships within\nknowledge graphs into vectors. However, these embedding models often face\nchallenges related to noise, missing information, distribution shift,\nadversarial attacks, etc. This can lead to sub-optimal embeddings and incorrect\ninferences, thereby negatively impacting downstream applications. While the\nexisting literature has focused so far on adversarial attacks on KGE models,\nthe challenges related to the other critical aspects remain unexplored. In this\npaper, we, first of all, give a unified definition of resilience, encompassing\nseveral factors such as generalisation, performance consistency, distribution\nadaption, and robustness. After formalizing these concepts for machine learning\nin general, we define them in the context of knowledge graphs. To find the gap\nin the existing works on resilience in the context of knowledge graphs, we\nperform a systematic survey, taking into account all these aspects mentioned\npreviously. Our survey results show that most of the existing works focus on a\nspecific aspect of resilience, namely robustness. After categorizing such works\nbased on their respective aspects of resilience, we discuss the challenges and\nfuture research directions."
                },
                "authors": [
                    {
                        "name": "Arnab Sharma"
                    },
                    {
                        "name": "N'Dah Jean Kouagou"
                    },
                    {
                        "name": "Axel-Cyrille Ngonga Ngomo"
                    }
                ],
                "author_detail": {
                    "name": "Axel-Cyrille Ngonga Ngomo"
                },
                "author": "Axel-Cyrille Ngonga Ngomo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07018v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07018v3",
                "updated": "2024-10-28T16:03:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    3,
                    20,
                    0,
                    302,
                    0
                ],
                "published": "2024-07-09T16:38:48Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    16,
                    38,
                    48,
                    1,
                    191,
                    0
                ],
                "title": "End-To-End Causal Effect Estimation from Unstructured Natural Language\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-To-End Causal Effect Estimation from Unstructured Natural Language\n  Data"
                },
                "summary": "Knowing the effect of an intervention is critical for human decision-making,\nbut current approaches for causal effect estimation rely on manual data\ncollection and structuring, regardless of the causal assumptions. This\nincreases both the cost and time-to-completion for studies. We show how large,\ndiverse observational text data can be mined with large language models (LLMs)\nto produce inexpensive causal effect estimates under appropriate causal\nassumptions. We introduce NATURAL, a novel family of causal effect estimators\nbuilt with LLMs that operate over datasets of unstructured text. Our estimators\nuse LLM conditional distributions (over variables of interest, given the text\ndata) to assist in the computation of classical estimators of causal effect. We\novercome a number of technical challenges to realize this idea, such as\nautomating data curation and using LLMs to impute missing information. We\nprepare six (two synthetic and four real) observational datasets, paired with\ncorresponding ground truth in the form of randomized trials, which we used to\nsystematically evaluate each step of our pipeline. NATURAL estimators\ndemonstrate remarkable performance, yielding causal effect estimates that fall\nwithin 3 percentage points of their ground truth counterparts, including on\nreal-world Phase 3/4 clinical trials. Our results suggest that unstructured\ntext data is a rich source of causal effect information, and NATURAL is a first\nstep towards an automated pipeline to tap this resource.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowing the effect of an intervention is critical for human decision-making,\nbut current approaches for causal effect estimation rely on manual data\ncollection and structuring, regardless of the causal assumptions. This\nincreases both the cost and time-to-completion for studies. We show how large,\ndiverse observational text data can be mined with large language models (LLMs)\nto produce inexpensive causal effect estimates under appropriate causal\nassumptions. We introduce NATURAL, a novel family of causal effect estimators\nbuilt with LLMs that operate over datasets of unstructured text. Our estimators\nuse LLM conditional distributions (over variables of interest, given the text\ndata) to assist in the computation of classical estimators of causal effect. We\novercome a number of technical challenges to realize this idea, such as\nautomating data curation and using LLMs to impute missing information. We\nprepare six (two synthetic and four real) observational datasets, paired with\ncorresponding ground truth in the form of randomized trials, which we used to\nsystematically evaluate each step of our pipeline. NATURAL estimators\ndemonstrate remarkable performance, yielding causal effect estimates that fall\nwithin 3 percentage points of their ground truth counterparts, including on\nreal-world Phase 3/4 clinical trials. Our results suggest that unstructured\ntext data is a rich source of causal effect information, and NATURAL is a first\nstep towards an automated pipeline to tap this resource."
                },
                "authors": [
                    {
                        "name": "Nikita Dhawan"
                    },
                    {
                        "name": "Leonardo Cotta"
                    },
                    {
                        "name": "Karen Ullrich"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    },
                    {
                        "name": "Chris J. Maddison"
                    }
                ],
                "author_detail": {
                    "name": "Chris J. Maddison"
                },
                "author": "Chris J. Maddison",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07018v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07018v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21159v1",
                "updated": "2024-10-28T15:59:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    59,
                    31,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:59:31Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    59,
                    31,
                    0,
                    302,
                    0
                ],
                "title": "CURATe: Benchmarking Personalised Alignment of Conversational AI\n  Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CURATe: Benchmarking Personalised Alignment of Conversational AI\n  Assistants"
                },
                "summary": "We introduce a multi-turn benchmark for evaluating personalised alignment in\nLLM-based AI assistants, focusing on their ability to handle user-provided\nsafety-critical contexts. Our assessment of ten leading models across five\nscenarios (each with 337 use cases) reveals systematic inconsistencies in\nmaintaining user-specific consideration, with even top-rated \"harmless\" models\nmaking recommendations that should be recognised as obviously harmful to the\nuser given the context provided. Key failure modes include inappropriate\nweighing of conflicting preferences, sycophancy (prioritising user preferences\nabove safety), a lack of attentiveness to critical user information within the\ncontext window, and inconsistent application of user-specific knowledge. The\nsame systematic biases were observed in OpenAI's o1, suggesting that strong\nreasoning capacities do not necessarily transfer to this kind of personalised\nthinking. We find that prompting LLMs to consider safety-critical context\nsignificantly improves performance, unlike a generic 'harmless and helpful'\ninstruction. Based on these findings, we propose research directions for\nembedding self-reflection capabilities, online user modelling, and dynamic risk\nassessment in AI assistants. Our work emphasises the need for nuanced,\ncontext-aware approaches to alignment in systems designed for persistent human\ninteraction, aiding the development of safe and considerate AI assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a multi-turn benchmark for evaluating personalised alignment in\nLLM-based AI assistants, focusing on their ability to handle user-provided\nsafety-critical contexts. Our assessment of ten leading models across five\nscenarios (each with 337 use cases) reveals systematic inconsistencies in\nmaintaining user-specific consideration, with even top-rated \"harmless\" models\nmaking recommendations that should be recognised as obviously harmful to the\nuser given the context provided. Key failure modes include inappropriate\nweighing of conflicting preferences, sycophancy (prioritising user preferences\nabove safety), a lack of attentiveness to critical user information within the\ncontext window, and inconsistent application of user-specific knowledge. The\nsame systematic biases were observed in OpenAI's o1, suggesting that strong\nreasoning capacities do not necessarily transfer to this kind of personalised\nthinking. We find that prompting LLMs to consider safety-critical context\nsignificantly improves performance, unlike a generic 'harmless and helpful'\ninstruction. Based on these findings, we propose research directions for\nembedding self-reflection capabilities, online user modelling, and dynamic risk\nassessment in AI assistants. Our work emphasises the need for nuanced,\ncontext-aware approaches to alignment in systems designed for persistent human\ninteraction, aiding the development of safe and considerate AI assistants."
                },
                "authors": [
                    {
                        "name": "Lize Alberts"
                    },
                    {
                        "name": "Benjamin Ellis"
                    },
                    {
                        "name": "Andrei Lupu"
                    },
                    {
                        "name": "Jakob Foerster"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Foerster"
                },
                "author": "Jakob Foerster",
                "arxiv_comment": "Submitted to ICLR 2025 on 01/10/2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; K.4.2; H.5.2; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21157v1",
                "updated": "2024-10-28T15:58:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    58,
                    41,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:58:41Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    58,
                    41,
                    0,
                    302,
                    0
                ],
                "title": "M2rc-Eval: Massively Multilingual Repository-level Code Completion\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M2rc-Eval: Massively Multilingual Repository-level Code Completion\n  Evaluation"
                },
                "summary": "Repository-level code completion has drawn great attention in software\nengineering, and several benchmark datasets have been introduced. However,\nexisting repository-level code completion benchmarks usually focus on a limited\nnumber of languages (<5), which cannot evaluate the general code intelligence\nabilities across different languages for existing code Large Language Models\n(LLMs). Besides, the existing benchmarks usually report overall average scores\nof different languages, where the fine-grained abilities in different\ncompletion scenarios are ignored. Therefore, to facilitate the research of code\nLLMs in multilingual scenarios, we propose a massively multilingual\nrepository-level code completion benchmark covering 18 programming languages\n(called M2RC-EVAL), and two types of fine-grained annotations (i.e.,\nbucket-level and semantic-level) on different completion scenarios are\nprovided, where we obtain these annotations based on the parsed abstract syntax\ntree. Moreover, we also curate a massively multilingual instruction corpora\nM2RC- INSTRUCT dataset to improve the repository-level code completion\nabilities of existing code LLMs. Comprehensive experimental results demonstrate\nthe effectiveness of our M2RC-EVAL and M2RC-INSTRUCT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level code completion has drawn great attention in software\nengineering, and several benchmark datasets have been introduced. However,\nexisting repository-level code completion benchmarks usually focus on a limited\nnumber of languages (<5), which cannot evaluate the general code intelligence\nabilities across different languages for existing code Large Language Models\n(LLMs). Besides, the existing benchmarks usually report overall average scores\nof different languages, where the fine-grained abilities in different\ncompletion scenarios are ignored. Therefore, to facilitate the research of code\nLLMs in multilingual scenarios, we propose a massively multilingual\nrepository-level code completion benchmark covering 18 programming languages\n(called M2RC-EVAL), and two types of fine-grained annotations (i.e.,\nbucket-level and semantic-level) on different completion scenarios are\nprovided, where we obtain these annotations based on the parsed abstract syntax\ntree. Moreover, we also curate a massively multilingual instruction corpora\nM2RC- INSTRUCT dataset to improve the repository-level code completion\nabilities of existing code LLMs. Comprehensive experimental results demonstrate\nthe effectiveness of our M2RC-EVAL and M2RC-INSTRUCT."
                },
                "authors": [
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ken Deng"
                    },
                    {
                        "name": "Congnan Liu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Peng Zhao"
                    },
                    {
                        "name": "Linzheng Chai"
                    },
                    {
                        "name": "Yanan Wu"
                    },
                    {
                        "name": "Ke Jin"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Guoan Zhang"
                    },
                    {
                        "name": "Bangyu Xiang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21155v1",
                "updated": "2024-10-28T15:56:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    56,
                    49,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:56:49Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    56,
                    49,
                    0,
                    302,
                    0
                ],
                "title": "SciER: An Entity and Relation Extraction Dataset for Datasets, Methods,\n  and Tasks in Scientific Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciER: An Entity and Relation Extraction Dataset for Datasets, Methods,\n  and Tasks in Scientific Documents"
                },
                "summary": "Scientific information extraction (SciIE) is critical for converting\nunstructured knowledge from scholarly articles into structured data (entities\nand relations). Several datasets have been proposed for training and validating\nSciIE models. However, due to the high complexity and cost of annotating\nscientific texts, those datasets restrict their annotations to specific parts\nof paper, such as abstracts, resulting in the loss of diverse entity mentions\nand relations in context. In this paper, we release a new entity and relation\nextraction dataset for entities related to datasets, methods, and tasks in\nscientific articles. Our dataset contains 106 manually annotated full-text\nscientific publications with over 24k entities and 12k relations. To capture\nthe intricate use and interactions among entities in full texts, our dataset\ncontains a fine-grained tag set for relations. Additionally, we provide an\nout-of-distribution test set to offer a more realistic evaluation. We conduct\ncomprehensive experiments, including state-of-the-art supervised models and our\nproposed LLM-based baselines, and highlight the challenges presented by our\ndataset, encouraging the development of innovative models to further the field\nof SciIE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific information extraction (SciIE) is critical for converting\nunstructured knowledge from scholarly articles into structured data (entities\nand relations). Several datasets have been proposed for training and validating\nSciIE models. However, due to the high complexity and cost of annotating\nscientific texts, those datasets restrict their annotations to specific parts\nof paper, such as abstracts, resulting in the loss of diverse entity mentions\nand relations in context. In this paper, we release a new entity and relation\nextraction dataset for entities related to datasets, methods, and tasks in\nscientific articles. Our dataset contains 106 manually annotated full-text\nscientific publications with over 24k entities and 12k relations. To capture\nthe intricate use and interactions among entities in full texts, our dataset\ncontains a fine-grained tag set for relations. Additionally, we provide an\nout-of-distribution test set to offer a more realistic evaluation. We conduct\ncomprehensive experiments, including state-of-the-art supervised models and our\nproposed LLM-based baselines, and highlight the challenges presented by our\ndataset, encouraging the development of innovative models to further the field\nof SciIE."
                },
                "authors": [
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Zhijia Chen"
                    },
                    {
                        "name": "Huitong Pan"
                    },
                    {
                        "name": "Cornelia Caragea"
                    },
                    {
                        "name": "Longin Jan Latecki"
                    },
                    {
                        "name": "Eduard Dragut"
                    }
                ],
                "author_detail": {
                    "name": "Eduard Dragut"
                },
                "author": "Eduard Dragut",
                "arxiv_comment": "EMNLP2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21153v1",
                "updated": "2024-10-28T15:50:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    50,
                    56,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:50:56Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    50,
                    56,
                    0,
                    302,
                    0
                ],
                "title": "Synthetica: Large Scale Synthetic Data for Robot Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetica: Large Scale Synthetic Data for Robot Perception"
                },
                "summary": "Vision-based object detectors are a crucial basis for robotics applications\nas they provide valuable information about object localisation in the\nenvironment. These need to ensure high reliability in different lighting\nconditions, occlusions, and visual artifacts, all while running in real-time.\nCollecting and annotating real-world data for these networks is prohibitively\ntime consuming and costly, especially for custom assets, such as industrial\nobjects, making it untenable for generalization to in-the-wild scenarios. To\nthis end, we present Synthetica, a method for large-scale synthetic data\ngeneration for training robust state estimators. This paper focuses on the task\nof object detection, an important problem which can serve as the front-end for\nmost state estimation problems, such as pose estimation. Leveraging data from a\nphotorealistic ray-tracing renderer, we scale up data generation, generating\n2.7 million images, to train highly accurate real-time detection transformers.\nWe present a collection of rendering randomization and training-time data\naugmentation techniques conducive to robust sim-to-real performance for vision\ntasks. We demonstrate state-of-the-art performance on the task of object\ndetection while having detectors that run at 50-100Hz which is 9 times faster\nthan the prior SOTA. We further demonstrate the usefulness of our training\nmethodology for robotics applications by showcasing a pipeline for use in the\nreal world with custom objects for which there do not exist prior datasets. Our\nwork highlights the importance of scaling synthetic data generation for robust\nsim-to-real transfer while achieving the fastest real-time inference speeds.\nVideos and supplementary information can be found at this URL:\nhttps://sites.google.com/view/synthetica-vision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-based object detectors are a crucial basis for robotics applications\nas they provide valuable information about object localisation in the\nenvironment. These need to ensure high reliability in different lighting\nconditions, occlusions, and visual artifacts, all while running in real-time.\nCollecting and annotating real-world data for these networks is prohibitively\ntime consuming and costly, especially for custom assets, such as industrial\nobjects, making it untenable for generalization to in-the-wild scenarios. To\nthis end, we present Synthetica, a method for large-scale synthetic data\ngeneration for training robust state estimators. This paper focuses on the task\nof object detection, an important problem which can serve as the front-end for\nmost state estimation problems, such as pose estimation. Leveraging data from a\nphotorealistic ray-tracing renderer, we scale up data generation, generating\n2.7 million images, to train highly accurate real-time detection transformers.\nWe present a collection of rendering randomization and training-time data\naugmentation techniques conducive to robust sim-to-real performance for vision\ntasks. We demonstrate state-of-the-art performance on the task of object\ndetection while having detectors that run at 50-100Hz which is 9 times faster\nthan the prior SOTA. We further demonstrate the usefulness of our training\nmethodology for robotics applications by showcasing a pipeline for use in the\nreal world with custom objects for which there do not exist prior datasets. Our\nwork highlights the importance of scaling synthetic data generation for robust\nsim-to-real transfer while achieving the fastest real-time inference speeds.\nVideos and supplementary information can be found at this URL:\nhttps://sites.google.com/view/synthetica-vision."
                },
                "authors": [
                    {
                        "name": "Ritvik Singh"
                    },
                    {
                        "name": "Jingzhou Liu"
                    },
                    {
                        "name": "Karl Van Wyk"
                    },
                    {
                        "name": "Yu-Wei Chao"
                    },
                    {
                        "name": "Jean-Francois Lafleche"
                    },
                    {
                        "name": "Florian Shkurti"
                    },
                    {
                        "name": "Nathan Ratliff"
                    },
                    {
                        "name": "Ankur Handa"
                    }
                ],
                "author_detail": {
                    "name": "Ankur Handa"
                },
                "author": "Ankur Handa",
                "arxiv_comment": "21 pages, 11 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21146v1",
                "updated": "2024-10-28T15:47:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    47,
                    3,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:47:03Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    47,
                    3,
                    0,
                    302,
                    0
                ],
                "title": "Palisade -- Prompt Injection Detection Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palisade -- Prompt Injection Detection Framework"
                },
                "summary": "The advent of Large Language Models LLMs marks a milestone in Artificial\nIntelligence, altering how machines comprehend and generate human language.\nHowever, LLMs are vulnerable to malicious prompt injection attacks, where\ncrafted inputs manipulate the models behavior in unintended ways, compromising\nsystem integrity and causing incorrect outcomes. Conventional detection methods\nrely on static, rule-based approaches, which often fail against sophisticated\nthreats like abnormal token sequences and alias substitutions, leading to\nlimited adaptability and higher rates of false positives and false\nnegatives.This paper proposes a novel NLP based approach for prompt injection\ndetection, emphasizing accuracy and optimization through a layered input\nscreening process. In this framework, prompts are filtered through three\ndistinct layers rule-based, ML classifier, and companion LLM before reaching\nthe target model, thereby minimizing the risk of malicious interaction.Tests\nshow the ML classifier achieves the highest accuracy among individual layers,\nyet the multi-layer framework enhances overall detection accuracy by reducing\nfalse negatives. Although this increases false positives, it minimizes the risk\nof overlooking genuine injected prompts, thus prioritizing security.This\nmulti-layered detection approach highlights LLM vulnerabilities and provides a\ncomprehensive framework for future research, promoting secure interactions\nbetween humans and AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models LLMs marks a milestone in Artificial\nIntelligence, altering how machines comprehend and generate human language.\nHowever, LLMs are vulnerable to malicious prompt injection attacks, where\ncrafted inputs manipulate the models behavior in unintended ways, compromising\nsystem integrity and causing incorrect outcomes. Conventional detection methods\nrely on static, rule-based approaches, which often fail against sophisticated\nthreats like abnormal token sequences and alias substitutions, leading to\nlimited adaptability and higher rates of false positives and false\nnegatives.This paper proposes a novel NLP based approach for prompt injection\ndetection, emphasizing accuracy and optimization through a layered input\nscreening process. In this framework, prompts are filtered through three\ndistinct layers rule-based, ML classifier, and companion LLM before reaching\nthe target model, thereby minimizing the risk of malicious interaction.Tests\nshow the ML classifier achieves the highest accuracy among individual layers,\nyet the multi-layer framework enhances overall detection accuracy by reducing\nfalse negatives. Although this increases false positives, it minimizes the risk\nof overlooking genuine injected prompts, thus prioritizing security.This\nmulti-layered detection approach highlights LLM vulnerabilities and provides a\ncomprehensive framework for future research, promoting secure interactions\nbetween humans and AI systems."
                },
                "authors": [
                    {
                        "name": "Sahasra Kokkula"
                    },
                    {
                        "name": "Somanathan R"
                    },
                    {
                        "name": "Nandavardhan R"
                    },
                    {
                        "name": "Aashishkumar"
                    },
                    {
                        "name": "G Divya"
                    }
                ],
                "author_detail": {
                    "name": "G Divya"
                },
                "author": "G Divya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09115v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09115v3",
                "updated": "2024-10-28T15:47:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    47,
                    1,
                    0,
                    302,
                    0
                ],
                "published": "2023-11-15T17:06:26Z",
                "published_parsed": [
                    2023,
                    11,
                    15,
                    17,
                    6,
                    26,
                    2,
                    319,
                    0
                ],
                "title": "HEALNet: Multimodal Fusion for Heterogeneous Biomedical Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEALNet: Multimodal Fusion for Heterogeneous Biomedical Data"
                },
                "summary": "Technological advances in medical data collection, such as high-throughput\ngenomic sequencing and digital high-resolution histopathology, have contributed\nto the rising requirement for multimodal biomedical modelling, specifically for\nimage, tabular and graph data. Most multimodal deep learning approaches use\nmodality-specific architectures that are often trained separately and cannot\ncapture the crucial cross-modal information that motivates the integration of\ndifferent data sources. This paper presents the Hybrid Early-fusion Attention\nLearning Network (HEALNet): a flexible multimodal fusion architecture, which a)\npreserves modality-specific structural information, b) captures the cross-modal\ninteractions and structural information in a shared latent space, c) can\neffectively handle missing modalities during training and inference, and d)\nenables intuitive model inspection by learning on the raw data input instead of\nopaque embeddings. We conduct multimodal survival analysis on Whole Slide\nImages and Multi-omic data on four cancer datasets from The Cancer Genome Atlas\n(TCGA). HEALNet achieves state-of-the-art performance compared to other\nend-to-end trained fusion models, substantially improving over unimodal and\nmultimodal baselines whilst being robust in scenarios with missing modalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technological advances in medical data collection, such as high-throughput\ngenomic sequencing and digital high-resolution histopathology, have contributed\nto the rising requirement for multimodal biomedical modelling, specifically for\nimage, tabular and graph data. Most multimodal deep learning approaches use\nmodality-specific architectures that are often trained separately and cannot\ncapture the crucial cross-modal information that motivates the integration of\ndifferent data sources. This paper presents the Hybrid Early-fusion Attention\nLearning Network (HEALNet): a flexible multimodal fusion architecture, which a)\npreserves modality-specific structural information, b) captures the cross-modal\ninteractions and structural information in a shared latent space, c) can\neffectively handle missing modalities during training and inference, and d)\nenables intuitive model inspection by learning on the raw data input instead of\nopaque embeddings. We conduct multimodal survival analysis on Whole Slide\nImages and Multi-omic data on four cancer datasets from The Cancer Genome Atlas\n(TCGA). HEALNet achieves state-of-the-art performance compared to other\nend-to-end trained fusion models, substantially improving over unimodal and\nmultimodal baselines whilst being robust in scenarios with missing modalities."
                },
                "authors": [
                    {
                        "name": "Konstantin Hemker"
                    },
                    {
                        "name": "Nikola Simidjievski"
                    },
                    {
                        "name": "Mateja Jamnik"
                    }
                ],
                "author_detail": {
                    "name": "Mateja Jamnik"
                },
                "author": "Mateja Jamnik",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09115v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09115v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21141v1",
                "updated": "2024-10-28T15:43:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    31,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:43:31Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    31,
                    0,
                    302,
                    0
                ],
                "title": "LLM-initialized Differentiable Causal Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-initialized Differentiable Causal Discovery"
                },
                "summary": "The discovery of causal relationships between random variables is an\nimportant yet challenging problem that has applications across many scientific\ndomains. Differentiable causal discovery (DCD) methods are effective in\nuncovering causal relationships from observational data; however, these\napproaches often suffer from limited interpretability and face challenges in\nincorporating domain-specific prior knowledge. In contrast, Large Language\nModels (LLMs)-based causal discovery approaches have recently been shown\ncapable of providing useful priors for causal discovery but struggle with\nformal causal reasoning. In this paper, we propose LLM-DCD, which uses an LLM\nto initialize the optimization of the maximum likelihood objective function of\nDCD approaches, thereby incorporating strong priors into the discovery method.\nTo achieve this initialization, we design our objective function to depend on\nan explicitly defined adjacency matrix of the causal graph as its only\nvariational parameter. Directly optimizing the explicitly defined adjacency\nmatrix provides a more interpretable approach to causal discovery.\nAdditionally, we demonstrate higher accuracy on key benchmarking datasets of\nour approach compared to state-of-the-art alternatives, and provide empirical\nevidence that the quality of the initialization directly impacts the quality of\nthe final output of our DCD approach. LLM-DCD opens up new opportunities for\ntraditional causal discovery methods like DCD to benefit from future\nimprovements in the causal reasoning capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discovery of causal relationships between random variables is an\nimportant yet challenging problem that has applications across many scientific\ndomains. Differentiable causal discovery (DCD) methods are effective in\nuncovering causal relationships from observational data; however, these\napproaches often suffer from limited interpretability and face challenges in\nincorporating domain-specific prior knowledge. In contrast, Large Language\nModels (LLMs)-based causal discovery approaches have recently been shown\ncapable of providing useful priors for causal discovery but struggle with\nformal causal reasoning. In this paper, we propose LLM-DCD, which uses an LLM\nto initialize the optimization of the maximum likelihood objective function of\nDCD approaches, thereby incorporating strong priors into the discovery method.\nTo achieve this initialization, we design our objective function to depend on\nan explicitly defined adjacency matrix of the causal graph as its only\nvariational parameter. Directly optimizing the explicitly defined adjacency\nmatrix provides a more interpretable approach to causal discovery.\nAdditionally, we demonstrate higher accuracy on key benchmarking datasets of\nour approach compared to state-of-the-art alternatives, and provide empirical\nevidence that the quality of the initialization directly impacts the quality of\nthe final output of our DCD approach. LLM-DCD opens up new opportunities for\ntraditional causal discovery methods like DCD to benefit from future\nimprovements in the causal reasoning capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Shiv Kampani"
                    },
                    {
                        "name": "David Hidary"
                    },
                    {
                        "name": "Constantijn van der Poel"
                    },
                    {
                        "name": "Martin Ganahl"
                    },
                    {
                        "name": "Brenda Miao"
                    }
                ],
                "author_detail": {
                    "name": "Brenda Miao"
                },
                "author": "Brenda Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21139v1",
                "updated": "2024-10-28T15:42:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    42,
                    45,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:42:45Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    42,
                    45,
                    0,
                    302,
                    0
                ],
                "title": "uOttawa at LegalLens-2024: Transformer-based Classification Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "uOttawa at LegalLens-2024: Transformer-based Classification Experiments"
                },
                "summary": "This paper presents the methods used for LegalLens-2024 shared task, which\nfocused on detecting legal violations within unstructured textual data and\nassociating these violations with potentially affected individuals. The shared\ntask included two subtasks: A) Legal Named Entity Recognition (L-NER) and B)\nLegal Natural Language Inference (L-NLI). For subtask A, we utilized the spaCy\nlibrary, while for subtask B, we employed a combined model incorporating\nRoBERTa and CNN. Our results were 86.3% in the L-NER subtask and 88.25% in the\nL-NLI subtask. Overall, our paper demonstrates the effectiveness of transformer\nmodels in addressing complex tasks in the legal domain. The source code for our\nimplementation is publicly available at\nhttps://github.com/NimaMeghdadi/uOttawa-at-LegalLens-2024-Transformer-based-Classification",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the methods used for LegalLens-2024 shared task, which\nfocused on detecting legal violations within unstructured textual data and\nassociating these violations with potentially affected individuals. The shared\ntask included two subtasks: A) Legal Named Entity Recognition (L-NER) and B)\nLegal Natural Language Inference (L-NLI). For subtask A, we utilized the spaCy\nlibrary, while for subtask B, we employed a combined model incorporating\nRoBERTa and CNN. Our results were 86.3% in the L-NER subtask and 88.25% in the\nL-NLI subtask. Overall, our paper demonstrates the effectiveness of transformer\nmodels in addressing complex tasks in the legal domain. The source code for our\nimplementation is publicly available at\nhttps://github.com/NimaMeghdadi/uOttawa-at-LegalLens-2024-Transformer-based-Classification"
                },
                "authors": [
                    {
                        "name": "Nima Meghdadi"
                    },
                    {
                        "name": "Diana Inkpen"
                    }
                ],
                "author_detail": {
                    "name": "Diana Inkpen"
                },
                "author": "Diana Inkpen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01833v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01833v3",
                "updated": "2024-10-28T15:38:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    38,
                    34,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-03T12:30:57Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    12,
                    30,
                    57,
                    1,
                    247,
                    0
                ],
                "title": "On the growth of nonconvex functionals at strict local minimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the growth of nonconvex functionals at strict local minimizers"
                },
                "summary": "In this paper, we present new equivalent conditions for the growth of proper\nlower semicontinuous functionals at strict local minimizers. The main\nconditions are a variant of the so-called tilt stability property of local\nminimizers and an analog of the classic Polyak-{\\L}ojasiewicz condition, where\nthe gradient is replaced by linear perturbations. We derive the following\ntilting principle: stability of minimizers under linear perturbations can infer\ntheir stability under nonlinear ones. We show how growth conditions can be used\nto give convergence rates for the proximal point algorithm. Finally, we give an\napplication to elliptic tracking problems, establishing a novel equivalence\nbetween second-order conditions and the sensitivity of solutions with respect\nto uncertainty in data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present new equivalent conditions for the growth of proper\nlower semicontinuous functionals at strict local minimizers. The main\nconditions are a variant of the so-called tilt stability property of local\nminimizers and an analog of the classic Polyak-{\\L}ojasiewicz condition, where\nthe gradient is replaced by linear perturbations. We derive the following\ntilting principle: stability of minimizers under linear perturbations can infer\ntheir stability under nonlinear ones. We show how growth conditions can be used\nto give convergence rates for the proximal point algorithm. Finally, we give an\napplication to elliptic tracking problems, establishing a novel equivalence\nbetween second-order conditions and the sensitivity of solutions with respect\nto uncertainty in data."
                },
                "authors": [
                    {
                        "name": "Alberto Domínguez Corella"
                    },
                    {
                        "name": "Trí Minh Lê"
                    }
                ],
                "author_detail": {
                    "name": "Trí Minh Lê"
                },
                "author": "Trí Minh Lê",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01833v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01833v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "49J52, 49K40, 90C31, 90C48",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21136v1",
                "updated": "2024-10-28T15:37:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    37,
                    6,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:37:06Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    37,
                    6,
                    0,
                    302,
                    0
                ],
                "title": "Do LLMs generate test oracles that capture the actual or the expected\n  program behaviour?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs generate test oracles that capture the actual or the expected\n  program behaviour?"
                },
                "summary": "Software testing is an essential part of the software development cycle to\nimprove the code quality. Typically, a unit test consists of a test prefix and\na test oracle which captures the developer's intended behaviour. A known\nlimitation of traditional test generation techniques (e.g. Randoop and\nEvosuite) is that they produce test oracles that capture the actual program\nbehaviour rather than the expected one. Recent approaches leverage Large\nLanguage Models (LLMs), trained on an enormous amount of data, to generate\ndeveloper-like code and test cases. We investigate whether the LLM-generated\ntest oracles capture the actual or expected software behaviour. We thus,\nconduct a controlled experiment to answer this question, by studying LLMs\nperformance on two tasks, namely, test oracle classification and generation.\nThe study includes developer-written and automatically generated test cases and\noracles for 24 open-source Java repositories, and different well tested\nprompts. Our findings show that LLM-based test generation approaches are also\nprone on generating oracles that capture the actual program behaviour rather\nthan the expected one. Moreover, LLMs are better at generating test oracles\nrather than classifying the correct ones, and can generate better test oracles\nwhen the code contains meaningful test or variable names. Finally,\nLLM-generated test oracles have higher fault detection potential than the\nEvosuite ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software testing is an essential part of the software development cycle to\nimprove the code quality. Typically, a unit test consists of a test prefix and\na test oracle which captures the developer's intended behaviour. A known\nlimitation of traditional test generation techniques (e.g. Randoop and\nEvosuite) is that they produce test oracles that capture the actual program\nbehaviour rather than the expected one. Recent approaches leverage Large\nLanguage Models (LLMs), trained on an enormous amount of data, to generate\ndeveloper-like code and test cases. We investigate whether the LLM-generated\ntest oracles capture the actual or expected software behaviour. We thus,\nconduct a controlled experiment to answer this question, by studying LLMs\nperformance on two tasks, namely, test oracle classification and generation.\nThe study includes developer-written and automatically generated test cases and\noracles for 24 open-source Java repositories, and different well tested\nprompts. Our findings show that LLM-based test generation approaches are also\nprone on generating oracles that capture the actual program behaviour rather\nthan the expected one. Moreover, LLMs are better at generating test oracles\nrather than classifying the correct ones, and can generate better test oracles\nwhen the code contains meaningful test or variable names. Finally,\nLLM-generated test oracles have higher fault detection potential than the\nEvosuite ones."
                },
                "authors": [
                    {
                        "name": "Michael Konstantinou"
                    },
                    {
                        "name": "Renzo Degiovanni"
                    },
                    {
                        "name": "Mike Papadakis"
                    }
                ],
                "author_detail": {
                    "name": "Mike Papadakis"
                },
                "author": "Mike Papadakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02067v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02067v2",
                "updated": "2024-10-28T15:35:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    35,
                    52,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-02T22:29:14Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    22,
                    29,
                    14,
                    2,
                    276,
                    0
                ],
                "title": "DisEnvisioner: Disentangled and Enriched Visual Prompt for Customized\n  Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DisEnvisioner: Disentangled and Enriched Visual Prompt for Customized\n  Image Generation"
                },
                "summary": "In the realm of image generation, creating customized images from visual\nprompt with additional textual instruction emerges as a promising endeavor.\nHowever, existing methods, both tuning-based and tuning-free, struggle with\ninterpreting the subject-essential attributes from the visual prompt. This\nleads to subject-irrelevant attributes infiltrating the generation process,\nultimately compromising the personalization quality in both editability and ID\npreservation. In this paper, we present DisEnvisioner, a novel approach for\neffectively extracting and enriching the subject-essential features while\nfiltering out -irrelevant information, enabling exceptional customization\nperformance, in a tuning-free manner and using only a single image.\nSpecifically, the feature of the subject and other irrelevant components are\neffectively separated into distinctive visual tokens, enabling a much more\naccurate customization. Aiming to further improving the ID consistency, we\nenrich the disentangled features, sculpting them into more granular\nrepresentations. Experiments demonstrate the superiority of our approach over\nexisting methods in instruction response (editability), ID consistency,\ninference speed, and the overall image quality, highlighting the effectiveness\nand efficiency of DisEnvisioner. Project page:\nhttps://disenvisioner.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of image generation, creating customized images from visual\nprompt with additional textual instruction emerges as a promising endeavor.\nHowever, existing methods, both tuning-based and tuning-free, struggle with\ninterpreting the subject-essential attributes from the visual prompt. This\nleads to subject-irrelevant attributes infiltrating the generation process,\nultimately compromising the personalization quality in both editability and ID\npreservation. In this paper, we present DisEnvisioner, a novel approach for\neffectively extracting and enriching the subject-essential features while\nfiltering out -irrelevant information, enabling exceptional customization\nperformance, in a tuning-free manner and using only a single image.\nSpecifically, the feature of the subject and other irrelevant components are\neffectively separated into distinctive visual tokens, enabling a much more\naccurate customization. Aiming to further improving the ID consistency, we\nenrich the disentangled features, sculpting them into more granular\nrepresentations. Experiments demonstrate the superiority of our approach over\nexisting methods in instruction response (editability), ID consistency,\ninference speed, and the overall image quality, highlighting the effectiveness\nand efficiency of DisEnvisioner. Project page:\nhttps://disenvisioner.github.io/."
                },
                "authors": [
                    {
                        "name": "Jing He"
                    },
                    {
                        "name": "Haodong Li"
                    },
                    {
                        "name": "Yongzhe Hu"
                    },
                    {
                        "name": "Guibao Shen"
                    },
                    {
                        "name": "Yingjie Cai"
                    },
                    {
                        "name": "Weichao Qiu"
                    },
                    {
                        "name": "Ying-Cong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying-Cong Chen"
                },
                "author": "Ying-Cong Chen",
                "arxiv_comment": "The first two authors contributed equally. Project page:\n  https://disenvisioner.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02067v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02067v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21131v1",
                "updated": "2024-10-28T15:33:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    33,
                    37,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:33:37Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    33,
                    37,
                    0,
                    302,
                    0
                ],
                "title": "Towards Unifying Evaluation of Counterfactual Explanations: Leveraging\n  Large Language Models for Human-Centric Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Unifying Evaluation of Counterfactual Explanations: Leveraging\n  Large Language Models for Human-Centric Assessments"
                },
                "summary": "As machine learning models evolve, maintaining transparency demands more\nhuman-centric explainable AI techniques. Counterfactual explanations, with\nroots in human reasoning, identify the minimal input changes needed to obtain a\ngiven output and, hence, are crucial for supporting decision-making. Despite\ntheir importance, the evaluation of these explanations often lacks grounding in\nuser studies and remains fragmented, with existing metrics not fully capturing\nhuman perspectives. To address this challenge, we developed a diverse set of 30\ncounterfactual scenarios and collected ratings across 8 evaluation metrics from\n206 respondents. Subsequently, we fine-tuned different Large Language Models\n(LLMs) to predict average or individual human judgment across these metrics.\nOur methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot\nevaluations and 85% (over a 3-classes prediction) with fine-tuning across all\nmetrics. The fine-tuned models predicting human ratings offer better\ncomparability and scalability in evaluating different counterfactual\nexplanation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As machine learning models evolve, maintaining transparency demands more\nhuman-centric explainable AI techniques. Counterfactual explanations, with\nroots in human reasoning, identify the minimal input changes needed to obtain a\ngiven output and, hence, are crucial for supporting decision-making. Despite\ntheir importance, the evaluation of these explanations often lacks grounding in\nuser studies and remains fragmented, with existing metrics not fully capturing\nhuman perspectives. To address this challenge, we developed a diverse set of 30\ncounterfactual scenarios and collected ratings across 8 evaluation metrics from\n206 respondents. Subsequently, we fine-tuned different Large Language Models\n(LLMs) to predict average or individual human judgment across these metrics.\nOur methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot\nevaluations and 85% (over a 3-classes prediction) with fine-tuning across all\nmetrics. The fine-tuned models predicting human ratings offer better\ncomparability and scalability in evaluating different counterfactual\nexplanation frameworks."
                },
                "authors": [
                    {
                        "name": "Marharyta Domnich"
                    },
                    {
                        "name": "Julius Valja"
                    },
                    {
                        "name": "Rasmus Moorits Veski"
                    },
                    {
                        "name": "Giacomo Magnifico"
                    },
                    {
                        "name": "Kadi Tulver"
                    },
                    {
                        "name": "Eduard Barbu"
                    },
                    {
                        "name": "Raul Vicente"
                    }
                ],
                "author_detail": {
                    "name": "Raul Vicente"
                },
                "author": "Raul Vicente",
                "arxiv_comment": "This paper has been submitted in August and is currently under review\n  to AAAI-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21120v1",
                "updated": "2024-10-28T15:21:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    21,
                    23,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:21:23Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    21,
                    23,
                    0,
                    302,
                    0
                ],
                "title": "FusedInf: Efficient Swapping of DNN Models for On-Demand Serverless\n  Inference Services on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FusedInf: Efficient Swapping of DNN Models for On-Demand Serverless\n  Inference Services on the Edge"
                },
                "summary": "Edge AI computing boxes are a new class of computing devices that are aimed\nto revolutionize the AI industry. These compact and robust hardware units bring\nthe power of AI processing directly to the source of data--on the edge of the\nnetwork. On the other hand, on-demand serverless inference services are\nbecoming more and more popular as they minimize the infrastructural cost\nassociated with hosting and running DNN models for small to medium-sized\nbusinesses. However, these computing devices are still constrained in terms of\nresource availability. As such, the service providers need to load and unload\nmodels efficiently in order to meet the growing demand. In this paper, we\nintroduce FusedInf to efficiently swap DNN models for on-demand serverless\ninference services on the edge. FusedInf combines multiple models into a single\nDirect Acyclic Graph (DAG) to efficiently load the models into the GPU memory\nand make execution faster. Our evaluation of popular DNN models showed that\ncreating a single DAG can make the execution of the models up to 14\\% faster\nwhile reducing the memory requirement by up to 17\\%. The prototype\nimplementation is available at https://github.com/SifatTaj/FusedInf.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI computing boxes are a new class of computing devices that are aimed\nto revolutionize the AI industry. These compact and robust hardware units bring\nthe power of AI processing directly to the source of data--on the edge of the\nnetwork. On the other hand, on-demand serverless inference services are\nbecoming more and more popular as they minimize the infrastructural cost\nassociated with hosting and running DNN models for small to medium-sized\nbusinesses. However, these computing devices are still constrained in terms of\nresource availability. As such, the service providers need to load and unload\nmodels efficiently in order to meet the growing demand. In this paper, we\nintroduce FusedInf to efficiently swap DNN models for on-demand serverless\ninference services on the edge. FusedInf combines multiple models into a single\nDirect Acyclic Graph (DAG) to efficiently load the models into the GPU memory\nand make execution faster. Our evaluation of popular DNN models showed that\ncreating a single DAG can make the execution of the models up to 14\\% faster\nwhile reducing the memory requirement by up to 17\\%. The prototype\nimplementation is available at https://github.com/SifatTaj/FusedInf."
                },
                "authors": [
                    {
                        "name": "Sifat Ut Taki"
                    },
                    {
                        "name": "Arthi Padmanabhan"
                    },
                    {
                        "name": "Spyridon Mastorakis"
                    }
                ],
                "author_detail": {
                    "name": "Spyridon Mastorakis"
                },
                "author": "Spyridon Mastorakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21108v1",
                "updated": "2024-10-28T15:11:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    11,
                    49,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:11:49Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    11,
                    49,
                    0,
                    302,
                    0
                ],
                "title": "LiGAR: LiDAR-Guided Hierarchical Transformer for Multi-Modal Group\n  Activity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiGAR: LiDAR-Guided Hierarchical Transformer for Multi-Modal Group\n  Activity Recognition"
                },
                "summary": "Group Activity Recognition (GAR) remains challenging in computer vision due\nto the complex nature of multi-agent interactions. This paper introduces LiGAR,\na LIDAR-Guided Hierarchical Transformer for Multi-Modal Group Activity\nRecognition. LiGAR leverages LiDAR data as a structural backbone to guide the\nprocessing of visual and textual information, enabling robust handling of\nocclusions and complex spatial arrangements. Our framework incorporates a\nMulti-Scale LIDAR Transformer, Cross-Modal Guided Attention, and an Adaptive\nFusion Module to integrate multi-modal data at different semantic levels\neffectively. LiGAR's hierarchical architecture captures group activities at\nvarious granularities, from individual actions to scene-level dynamics.\nExtensive experiments on the JRDB-PAR, Volleyball, and NBA datasets demonstrate\nLiGAR's superior performance, achieving state-of-the-art results with\nimprovements of up to 10.6% in F1-score on JRDB-PAR and 5.9% in Mean Per Class\nAccuracy on the NBA dataset. Notably, LiGAR maintains high performance even\nwhen LiDAR data is unavailable during inference, showcasing its adaptability.\nOur ablation studies highlight the significant contributions of each component\nand the effectiveness of our multi-modal, multi-scale approach in advancing the\nfield of group activity recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Activity Recognition (GAR) remains challenging in computer vision due\nto the complex nature of multi-agent interactions. This paper introduces LiGAR,\na LIDAR-Guided Hierarchical Transformer for Multi-Modal Group Activity\nRecognition. LiGAR leverages LiDAR data as a structural backbone to guide the\nprocessing of visual and textual information, enabling robust handling of\nocclusions and complex spatial arrangements. Our framework incorporates a\nMulti-Scale LIDAR Transformer, Cross-Modal Guided Attention, and an Adaptive\nFusion Module to integrate multi-modal data at different semantic levels\neffectively. LiGAR's hierarchical architecture captures group activities at\nvarious granularities, from individual actions to scene-level dynamics.\nExtensive experiments on the JRDB-PAR, Volleyball, and NBA datasets demonstrate\nLiGAR's superior performance, achieving state-of-the-art results with\nimprovements of up to 10.6% in F1-score on JRDB-PAR and 5.9% in Mean Per Class\nAccuracy on the NBA dataset. Notably, LiGAR maintains high performance even\nwhen LiDAR data is unavailable during inference, showcasing its adaptability.\nOur ablation studies highlight the significant contributions of each component\nand the effectiveness of our multi-modal, multi-scale approach in advancing the\nfield of group activity recognition."
                },
                "authors": [
                    {
                        "name": "Naga Venkata Sai Raviteja Chappa"
                    },
                    {
                        "name": "Khoa Luu"
                    }
                ],
                "author_detail": {
                    "name": "Khoa Luu"
                },
                "author": "Khoa Luu",
                "arxiv_comment": "14 pages, 4 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21104v1",
                "updated": "2024-10-28T15:10:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    10,
                    31,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:10:31Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    10,
                    31,
                    0,
                    302,
                    0
                ],
                "title": "Topological Identification of Agent Status in Information Contagions:\n  Application to Financial Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topological Identification of Agent Status in Information Contagions:\n  Application to Financial Markets"
                },
                "summary": "Cascade models serve as effective tools for understanding the propagation of\ninformation and diseases within social networks. Nevertheless, their\napplicability becomes constrained when the states of the agents (nodes) are\nhidden and can only be inferred through indirect observations or symptoms. This\nstudy proposes a Mapper-based strategy to infer the status of agents within a\nhidden information cascade model using expert knowledge. To verify and\ndemonstrate the method we identify agents who are likely to take advantage of\ninformation obtained from an inside information network. We do this using data\non insider networks and stock market transactions. Recognizing the sensitive\nnature of allegations of insider trading, we design a conservative approach to\nminimize false positives, ensuring that innocent agents are not wrongfully\nimplicated. The Mapper-based results systematically outperform other methods,\nsuch as clustering and unsupervised anomaly detection, on synthetic data. We\nalso apply the method to empirical data and verify the results using a\nstatistical validation method based on persistence homology. Our findings\nhighlight that the proposed Mapper-based technique successfully identifies a\nsubpopulation of opportunistic agents within the information cascades. The\nadaptability of this method to diverse data types and sizes is demonstrated,\nwith potential for tailoring for specific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascade models serve as effective tools for understanding the propagation of\ninformation and diseases within social networks. Nevertheless, their\napplicability becomes constrained when the states of the agents (nodes) are\nhidden and can only be inferred through indirect observations or symptoms. This\nstudy proposes a Mapper-based strategy to infer the status of agents within a\nhidden information cascade model using expert knowledge. To verify and\ndemonstrate the method we identify agents who are likely to take advantage of\ninformation obtained from an inside information network. We do this using data\non insider networks and stock market transactions. Recognizing the sensitive\nnature of allegations of insider trading, we design a conservative approach to\nminimize false positives, ensuring that innocent agents are not wrongfully\nimplicated. The Mapper-based results systematically outperform other methods,\nsuch as clustering and unsupervised anomaly detection, on synthetic data. We\nalso apply the method to empirical data and verify the results using a\nstatistical validation method based on persistence homology. Our findings\nhighlight that the proposed Mapper-based technique successfully identifies a\nsubpopulation of opportunistic agents within the information cascades. The\nadaptability of this method to diverse data types and sizes is demonstrated,\nwith potential for tailoring for specific applications."
                },
                "authors": [
                    {
                        "name": "Anubha Goel"
                    },
                    {
                        "name": "Henri Hansen"
                    },
                    {
                        "name": "Juho Kanniainen"
                    }
                ],
                "author_detail": {
                    "name": "Juho Kanniainen"
                },
                "author": "Juho Kanniainen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06741v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06741v2",
                "updated": "2024-10-28T15:05:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    5,
                    54,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-09T10:20:32Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    10,
                    20,
                    32,
                    2,
                    283,
                    0
                ],
                "title": "CoBa: Convergence Balancer for Multitask Finetuning of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoBa: Convergence Balancer for Multitask Finetuning of Large Language\n  Models"
                },
                "summary": "Multi-task learning (MTL) benefits the fine-tuning of large language models\n(LLMs) by providing a single model with improved performance and generalization\nability across tasks, presenting a resource-efficient alternative to developing\nseparate models for each task. Yet, existing MTL strategies for LLMs often fall\nshort by either being computationally intensive or failing to ensure\nsimultaneous task convergence. This paper presents CoBa, a new MTL approach\ndesigned to effectively manage task convergence balance with minimal\ncomputational overhead. Utilizing Relative Convergence Scores (RCS), Absolute\nConvergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically\nadjusts task weights during the training process, ensuring that the validation\nloss of all tasks progress towards convergence at an even pace while mitigating\nthe issue of individual task divergence. The results of our experiments\ninvolving three disparate datasets underscore that this approach not only\nfosters equilibrium in task convergence but enhances the LLMs' performance by\nup to 13% relative to the second-best baselines. Code is open-sourced at\nhttps://github.com/codefuse-ai/MFTCoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-task learning (MTL) benefits the fine-tuning of large language models\n(LLMs) by providing a single model with improved performance and generalization\nability across tasks, presenting a resource-efficient alternative to developing\nseparate models for each task. Yet, existing MTL strategies for LLMs often fall\nshort by either being computationally intensive or failing to ensure\nsimultaneous task convergence. This paper presents CoBa, a new MTL approach\ndesigned to effectively manage task convergence balance with minimal\ncomputational overhead. Utilizing Relative Convergence Scores (RCS), Absolute\nConvergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically\nadjusts task weights during the training process, ensuring that the validation\nloss of all tasks progress towards convergence at an even pace while mitigating\nthe issue of individual task divergence. The results of our experiments\ninvolving three disparate datasets underscore that this approach not only\nfosters equilibrium in task convergence but enhances the LLMs' performance by\nup to 13% relative to the second-best baselines. Code is open-sourced at\nhttps://github.com/codefuse-ai/MFTCoder."
                },
                "authors": [
                    {
                        "name": "Zi Gong"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Cong Liao"
                    },
                    {
                        "name": "Bingchang Liu"
                    },
                    {
                        "name": "Chaoyu Chen"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "arxiv_comment": "15 pages, main conference of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06741v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21099v1",
                "updated": "2024-10-28T15:04:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    4,
                    8,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:04:08Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    4,
                    8,
                    0,
                    302,
                    0
                ],
                "title": "H$_{2}$-H$_{2}$O demixing in Uranus and Neptune: Adiabatic structure\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H$_{2}$-H$_{2}$O demixing in Uranus and Neptune: Adiabatic structure\n  models"
                },
                "summary": "Demixing properties of planetary major constituents influence the interior\nstructure and evolution of planets. Comparing experimental and computational\ndata on the miscibility of hydrogen and water to adiabatic profiles suggests\nphase separation between these components occurs in the ice giants Uranus and\nNeptune. We aim to predict the atmospheric water abundance and transition\npressure between the water-poor outer envelope and the water-rich deep interior\nin Uranus and Neptune. We construct seven H2-H2O phase diagrams from the\navailable experimental and computational data. We compute interior adiabatic\nstructure models and compare these to the phase diagrams to infer whether\ndemixing is occurring. We obtain a strong water depletion in the top layer due\nto rain-out of water and find upper limits on the atmospheric water mass\nfraction Z_atm of 0.21 for Uranus and 0.16 for Neptune. The transition from the\nwater-poor to the water-rich layer is sharp and occurs at pressures P_Z between\n4 and 11 GPa. Using these constraints on Z_atm and P_Z, we find that the\nobserved gravitational harmonics J2 and J4 can be reproduced if P_Z > 10 GPa in\nUranus and > 5 GPa in Neptune, and if the deep interior has a high primordial\nwater mass fraction of 0.8, unless rocks are also present. The agreement with\nJ4 is improved if rocks are confined deeper than P_Z, for instance below a rock\ncloud level at 2000 K (20-30 GPa). These findings confirm classical few-layer\nmodels and suggest that a layered structure may result from a combination of\nprimordial mass accretion and subsequent phase separation. Reduced\nobservational uncertainty in J4 and its dynamic contribution, atmospheric water\nabundance measurements from an Orbiter with a Probe mission to Uranus (UOP) or\nNeptune, and better understanding of the mixing behaviour of constituents are\nneeded to constrain the interiors of ice giants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demixing properties of planetary major constituents influence the interior\nstructure and evolution of planets. Comparing experimental and computational\ndata on the miscibility of hydrogen and water to adiabatic profiles suggests\nphase separation between these components occurs in the ice giants Uranus and\nNeptune. We aim to predict the atmospheric water abundance and transition\npressure between the water-poor outer envelope and the water-rich deep interior\nin Uranus and Neptune. We construct seven H2-H2O phase diagrams from the\navailable experimental and computational data. We compute interior adiabatic\nstructure models and compare these to the phase diagrams to infer whether\ndemixing is occurring. We obtain a strong water depletion in the top layer due\nto rain-out of water and find upper limits on the atmospheric water mass\nfraction Z_atm of 0.21 for Uranus and 0.16 for Neptune. The transition from the\nwater-poor to the water-rich layer is sharp and occurs at pressures P_Z between\n4 and 11 GPa. Using these constraints on Z_atm and P_Z, we find that the\nobserved gravitational harmonics J2 and J4 can be reproduced if P_Z > 10 GPa in\nUranus and > 5 GPa in Neptune, and if the deep interior has a high primordial\nwater mass fraction of 0.8, unless rocks are also present. The agreement with\nJ4 is improved if rocks are confined deeper than P_Z, for instance below a rock\ncloud level at 2000 K (20-30 GPa). These findings confirm classical few-layer\nmodels and suggest that a layered structure may result from a combination of\nprimordial mass accretion and subsequent phase separation. Reduced\nobservational uncertainty in J4 and its dynamic contribution, atmospheric water\nabundance measurements from an Orbiter with a Probe mission to Uranus (UOP) or\nNeptune, and better understanding of the mixing behaviour of constituents are\nneeded to constrain the interiors of ice giants."
                },
                "authors": [
                    {
                        "name": "Marina Cano Amoros"
                    },
                    {
                        "name": "Nadine Nettelmann"
                    },
                    {
                        "name": "Nicola Tosi"
                    },
                    {
                        "name": "Philipp Baumeister"
                    },
                    {
                        "name": "Heike Rauer"
                    }
                ],
                "author_detail": {
                    "name": "Heike Rauer"
                },
                "author": "Heike Rauer",
                "arxiv_comment": "Accepted for publication in Astronomy and Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07823v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07823v2",
                "updated": "2024-10-28T15:01:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    1,
                    15,
                    0,
                    302,
                    0
                ],
                "published": "2024-07-10T16:48:22Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    16,
                    48,
                    22,
                    2,
                    192,
                    0
                ],
                "title": "Bayesian Inference of Fine-Features of Nuclear Equation of State from\n  Future Neutron Star Radius Measurements to 0.1km Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Inference of Fine-Features of Nuclear Equation of State from\n  Future Neutron Star Radius Measurements to 0.1km Accuracy"
                },
                "summary": "To more precisely constrain the Equation of State (EOS) of supradense\nneutron-rich nuclear matter, future high-precision X-ray and gravitational wave\nobservatories are proposed to measure the radii of neutron stars (NSs) with an\naccuracy better than about 0.1 km. However, it remains unclear what particular\naspects (other than the stiffness generally spoken of in the literature) of the\nEOS and to what precision they will be better constrained. In this work, within\na Bayesian framework using a meta-model EOS for NSs, we infer the posterior\nprobability distribution functions (PDFs) of incompressibility $K_{0}$ and\nskewness $J_{0}$ of symmetric nuclear matter (SNM) as well as the slope $L$,\ncurvature $K_{\\rm{sym}}$, and skewness $J_{\\rm{sym}}$ characterizing the\ndensity dependence of nuclear symmetry energy $E_{\\rm{sym}}(\\rho)$,\nrespectively, from mean values of NS radii consistent with existing\nobservations and an expected accuracy $\\Delta R$ ranging from about 1.0 km to\n0.1 km. We found that (1) the $\\Delta R$ has little effect on inferring the\nstiffness of SNM at suprasaturation densities, (2) smaller $\\Delta R$ reveals\nmore accurately not only the PDFs but also pairwise correlations among\nparameters characterizing high-density $E_{\\rm{sym}}(\\rho)$, (3) a double-peak\nfeature of the PDF($K_{\\rm{sym}}$) corresponding to the strong\n$K_{\\rm{sym}}-J_{\\rm{sym}}$ and $K_{\\rm{sym}}-L$ anti-correlations is revealed\nwhen $\\Delta R$ is less than about 0.2 km, and the locations of the two peaks\nare sensitive to the maximum value of $J_{\\rm{sym}}$ reflecting the stiffness\nof $E_{\\rm{sym}}(\\rho)$ above about 3 times the saturation density $\\rho_0$ of\nSNM, (4) the high-precision radius measurement for canonical NSs is more useful\nthan that for massive ones for constraining the EOS of nucleonic matter around\n$(2-3)\\rho_0$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To more precisely constrain the Equation of State (EOS) of supradense\nneutron-rich nuclear matter, future high-precision X-ray and gravitational wave\nobservatories are proposed to measure the radii of neutron stars (NSs) with an\naccuracy better than about 0.1 km. However, it remains unclear what particular\naspects (other than the stiffness generally spoken of in the literature) of the\nEOS and to what precision they will be better constrained. In this work, within\na Bayesian framework using a meta-model EOS for NSs, we infer the posterior\nprobability distribution functions (PDFs) of incompressibility $K_{0}$ and\nskewness $J_{0}$ of symmetric nuclear matter (SNM) as well as the slope $L$,\ncurvature $K_{\\rm{sym}}$, and skewness $J_{\\rm{sym}}$ characterizing the\ndensity dependence of nuclear symmetry energy $E_{\\rm{sym}}(\\rho)$,\nrespectively, from mean values of NS radii consistent with existing\nobservations and an expected accuracy $\\Delta R$ ranging from about 1.0 km to\n0.1 km. We found that (1) the $\\Delta R$ has little effect on inferring the\nstiffness of SNM at suprasaturation densities, (2) smaller $\\Delta R$ reveals\nmore accurately not only the PDFs but also pairwise correlations among\nparameters characterizing high-density $E_{\\rm{sym}}(\\rho)$, (3) a double-peak\nfeature of the PDF($K_{\\rm{sym}}$) corresponding to the strong\n$K_{\\rm{sym}}-J_{\\rm{sym}}$ and $K_{\\rm{sym}}-L$ anti-correlations is revealed\nwhen $\\Delta R$ is less than about 0.2 km, and the locations of the two peaks\nare sensitive to the maximum value of $J_{\\rm{sym}}$ reflecting the stiffness\nof $E_{\\rm{sym}}(\\rho)$ above about 3 times the saturation density $\\rho_0$ of\nSNM, (4) the high-precision radius measurement for canonical NSs is more useful\nthan that for massive ones for constraining the EOS of nucleonic matter around\n$(2-3)\\rho_0$."
                },
                "authors": [
                    {
                        "name": "Bao-An Li"
                    },
                    {
                        "name": "Xavier Grundler"
                    },
                    {
                        "name": "Wen-Jie Xie"
                    },
                    {
                        "name": "Nai-Bo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Nai-Bo Zhang"
                },
                "author": "Nai-Bo Zhang",
                "arxiv_comment": "34 pages with additional results, discussions and appendices. Phys.\n  Rev. D in press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07823v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07823v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21083v1",
                "updated": "2024-10-28T14:48:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    48,
                    5,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:48:05Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    48,
                    5,
                    0,
                    302,
                    0
                ],
                "title": "Stealthy Jailbreak Attacks on Large Language Models via Benign Data\n  Mirroring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stealthy Jailbreak Attacks on Large Language Models via Benign Data\n  Mirroring"
                },
                "summary": "Large language model (LLM) safety is a critical issue, with numerous studies\nemploying red team testing to enhance model security. Among these, jailbreak\nmethods explore potential vulnerabilities by crafting malicious prompts that\ninduce model outputs contrary to safety alignments. Existing black-box\njailbreak methods often rely on model feedback, repeatedly submitting queries\nwith detectable malicious instructions during the attack search process.\nAlthough these approaches are effective, the attacks may be intercepted by\ncontent moderators during the search process. We propose an improved transfer\nattack method that guides malicious prompt construction by locally training a\nmirror model of the target black-box model through benign data distillation.\nThis method offers enhanced stealth, as it does not involve submitting\nidentifiable malicious instructions to the target model during the search\nphase. Our approach achieved a maximum attack success rate of 92%, or a\nbalanced value of 80% with an average of 1.5 detectable jailbreak queries per\nsample against GPT-3.5 Turbo on a subset of AdvBench. These results underscore\nthe need for more robust defense mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) safety is a critical issue, with numerous studies\nemploying red team testing to enhance model security. Among these, jailbreak\nmethods explore potential vulnerabilities by crafting malicious prompts that\ninduce model outputs contrary to safety alignments. Existing black-box\njailbreak methods often rely on model feedback, repeatedly submitting queries\nwith detectable malicious instructions during the attack search process.\nAlthough these approaches are effective, the attacks may be intercepted by\ncontent moderators during the search process. We propose an improved transfer\nattack method that guides malicious prompt construction by locally training a\nmirror model of the target black-box model through benign data distillation.\nThis method offers enhanced stealth, as it does not involve submitting\nidentifiable malicious instructions to the target model during the search\nphase. Our approach achieved a maximum attack success rate of 92%, or a\nbalanced value of 80% with an average of 1.5 detectable jailbreak queries per\nsample against GPT-3.5 Turbo on a subset of AdvBench. These results underscore\nthe need for more robust defense mechanisms."
                },
                "authors": [
                    {
                        "name": "Honglin Mu"
                    },
                    {
                        "name": "Han He"
                    },
                    {
                        "name": "Yuxin Zhou"
                    },
                    {
                        "name": "Yunlong Feng"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Xiaoming Shi"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Qi Shi"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v2",
                "updated": "2024-10-28T14:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    44,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21076v1",
                "updated": "2024-10-28T14:40:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    40,
                    1,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:40:01Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    40,
                    1,
                    0,
                    302,
                    0
                ],
                "title": "Accelerated Bayesian parameter estimation and model selection for\n  gravitational waves with normalizing flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerated Bayesian parameter estimation and model selection for\n  gravitational waves with normalizing flows"
                },
                "summary": "We present an accelerated pipeline, based on high-performance computing\ntechniques and normalizing flows, for joint Bayesian parameter estimation and\nmodel selection and demonstrate its efficiency in gravitational wave\nastrophysics. We integrate the Jim inference toolkit, a normalizing\nflow-enhanced Markov chain Monte Carlo (MCMC) sampler, with the learned\nharmonic mean estimator. Our Bayesian evidence estimates run on $1$ GPU are\nconsistent with traditional nested sampling techniques run on $16$ CPU cores,\nwhile reducing the computation time by factors of $5\\times$ and $15\\times$ for\n$4$-dimensional and $11$-dimensional gravitational wave inference problems,\nrespectively. Our code is available in well-tested and thoroughly documented\nopen-source packages, ensuring accessibility and reproducibility for the wider\nresearch community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an accelerated pipeline, based on high-performance computing\ntechniques and normalizing flows, for joint Bayesian parameter estimation and\nmodel selection and demonstrate its efficiency in gravitational wave\nastrophysics. We integrate the Jim inference toolkit, a normalizing\nflow-enhanced Markov chain Monte Carlo (MCMC) sampler, with the learned\nharmonic mean estimator. Our Bayesian evidence estimates run on $1$ GPU are\nconsistent with traditional nested sampling techniques run on $16$ CPU cores,\nwhile reducing the computation time by factors of $5\\times$ and $15\\times$ for\n$4$-dimensional and $11$-dimensional gravitational wave inference problems,\nrespectively. Our code is available in well-tested and thoroughly documented\nopen-source packages, ensuring accessibility and reproducibility for the wider\nresearch community."
                },
                "authors": [
                    {
                        "name": "Alicja Polanska"
                    },
                    {
                        "name": "Thibeau Wouters"
                    },
                    {
                        "name": "Peter T. H. Pang"
                    },
                    {
                        "name": "Kaze K. W. Wong"
                    },
                    {
                        "name": "Jason D. McEwen"
                    }
                ],
                "author_detail": {
                    "name": "Jason D. McEwen"
                },
                "author": "Jason D. McEwen",
                "arxiv_comment": "accepted to NeurIPS 2024 workshop on Machine Learning and the\n  Physical Sciences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06013v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06013v2",
                "updated": "2024-10-28T14:37:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    37,
                    35,
                    0,
                    302,
                    0
                ],
                "published": "2024-05-09T18:00:00Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    18,
                    0,
                    0,
                    3,
                    130,
                    0
                ],
                "title": "Variational Inference for Acceleration of SN Ia Photometric Distance\n  Estimation with BayeSN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Inference for Acceleration of SN Ia Photometric Distance\n  Estimation with BayeSN"
                },
                "summary": "Type Ia supernovae (SNe Ia) are standarizable candles whose observed light\ncurves can be used to infer their distances, which can in turn be used in\ncosmological analyses. As the quantity of observed SNe Ia grows with current\nand upcoming surveys, increasingly scalable analyses are necessary to take full\nadvantage of these new datasets for precise estimation of cosmological\nparameters. Bayesian inference methods enable fitting SN Ia light curves with\nrobust uncertainty quantification, but traditional posterior sampling using\nMarkov Chain Monte Carlo (MCMC) is computationally expensive. We present an\nimplementation of variational inference (VI) to accelerate the fitting of SN Ia\nlight curves using the BayeSN hierarchical Bayesian model for time-varying SN\nIa spectral energy distributions (SEDs). We demonstrate and evaluate its\nperformance on both simulated light curves and data from the Foundation\nSupernova Survey with two different forms of surrogate posterior -- a\nmultivariate normal and a custom multivariate zero-lower-truncated normal\ndistribution -- and compare them with the Laplace Approximation and full MCMC\nanalysis. To validate of our variational approximation, we calculate the\npareto-smoothed importance sampling (PSIS) diagnostic, and perform variational\nsimulation-based calibration (VSBC). The VI approximation achieves similar\nresults to MCMC but with an order-of-magnitude speedup for the inference of the\nphotometric distance moduli. Overall, we show that VI is a promising method for\nscalable parameter inference that enables analysis of larger datasets for\nprecision cosmology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Type Ia supernovae (SNe Ia) are standarizable candles whose observed light\ncurves can be used to infer their distances, which can in turn be used in\ncosmological analyses. As the quantity of observed SNe Ia grows with current\nand upcoming surveys, increasingly scalable analyses are necessary to take full\nadvantage of these new datasets for precise estimation of cosmological\nparameters. Bayesian inference methods enable fitting SN Ia light curves with\nrobust uncertainty quantification, but traditional posterior sampling using\nMarkov Chain Monte Carlo (MCMC) is computationally expensive. We present an\nimplementation of variational inference (VI) to accelerate the fitting of SN Ia\nlight curves using the BayeSN hierarchical Bayesian model for time-varying SN\nIa spectral energy distributions (SEDs). We demonstrate and evaluate its\nperformance on both simulated light curves and data from the Foundation\nSupernova Survey with two different forms of surrogate posterior -- a\nmultivariate normal and a custom multivariate zero-lower-truncated normal\ndistribution -- and compare them with the Laplace Approximation and full MCMC\nanalysis. To validate of our variational approximation, we calculate the\npareto-smoothed importance sampling (PSIS) diagnostic, and perform variational\nsimulation-based calibration (VSBC). The VI approximation achieves similar\nresults to MCMC but with an order-of-magnitude speedup for the inference of the\nphotometric distance moduli. Overall, we show that VI is a promising method for\nscalable parameter inference that enables analysis of larger datasets for\nprecision cosmology."
                },
                "authors": [
                    {
                        "name": "Ana Sofía M. Uzsoy"
                    },
                    {
                        "name": "Stephen Thorp"
                    },
                    {
                        "name": "Matthew Grayling"
                    },
                    {
                        "name": "Kaisey S. Mandel"
                    }
                ],
                "author_detail": {
                    "name": "Kaisey S. Mandel"
                },
                "author": "Kaisey S. Mandel",
                "arxiv_comment": "16 pages, 7 figures, 1 table, accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06013v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06013v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21071v1",
                "updated": "2024-10-28T14:34:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    34,
                    36,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:34:36Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    34,
                    36,
                    0,
                    302,
                    0
                ],
                "title": "Automatic Generation of Benchmarks and Reliable LLM Judgment for Code\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Generation of Benchmarks and Reliable LLM Judgment for Code\n  Tasks"
                },
                "summary": "LLMs can be used in a variety of code related tasks such as translating from\none programming language to another, implementing natural language requirements\nand code summarization. Artifacts generated by state of the art LLM technology\nare expected to be useful in the sense that a user will be able to use the LLM\ngenerated artifact after a small number of easy modifications. Quantifying this\nvague notion is challenging and it is thus hard to determine the quality of\ncode related LLM solutions. We refer to evaluation of LLM solutions using LLM\njudgment as \"LLM as a Judge\", or LaaJ for short. In this work we introduce a\nmethodology to generate and evaluate LaaJ implementations, utilizing an\nautomatically generated benchmark. The purpose of the benchmark is two fold,\nnamely, it is used both to develop and validate the LaaJs and to validate and\ntest the LLM code related solution using the LaaJs. To that end, we developed\nan automated benchmark generation engine, which generates code in multiple\nprogramming languages for multiple code related tasks and which serves as the\ninput for LaaJ evaluation. We utilize a graph representation, G, of the\npotential code related generations. The graph vertices are generated artifacts\nand edges represent possible generations, e.g., the generation of a Java\nprogram from its natural language requirements. Utilizing a chain of LLM agents\nand G we generate code related artifacts. Using cycles in G we formulate\nexpectations on the generated artifacts. Taking advantage of these formulated\nexpectations enables the development and testing of reliable LLM judgement for\nusefulness of the artifacts generated by the solution. Our approach enables the\ncreation of high quality code task solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can be used in a variety of code related tasks such as translating from\none programming language to another, implementing natural language requirements\nand code summarization. Artifacts generated by state of the art LLM technology\nare expected to be useful in the sense that a user will be able to use the LLM\ngenerated artifact after a small number of easy modifications. Quantifying this\nvague notion is challenging and it is thus hard to determine the quality of\ncode related LLM solutions. We refer to evaluation of LLM solutions using LLM\njudgment as \"LLM as a Judge\", or LaaJ for short. In this work we introduce a\nmethodology to generate and evaluate LaaJ implementations, utilizing an\nautomatically generated benchmark. The purpose of the benchmark is two fold,\nnamely, it is used both to develop and validate the LaaJs and to validate and\ntest the LLM code related solution using the LaaJs. To that end, we developed\nan automated benchmark generation engine, which generates code in multiple\nprogramming languages for multiple code related tasks and which serves as the\ninput for LaaJ evaluation. We utilize a graph representation, G, of the\npotential code related generations. The graph vertices are generated artifacts\nand edges represent possible generations, e.g., the generation of a Java\nprogram from its natural language requirements. Utilizing a chain of LLM agents\nand G we generate code related artifacts. Using cycles in G we formulate\nexpectations on the generated artifacts. Taking advantage of these formulated\nexpectations enables the development and testing of reliable LLM judgement for\nusefulness of the artifacts generated by the solution. Our approach enables the\ncreation of high quality code task solutions."
                },
                "authors": [
                    {
                        "name": "Eitan Farchi"
                    },
                    {
                        "name": "Shmulik Froimovich"
                    },
                    {
                        "name": "Rami Katan"
                    },
                    {
                        "name": "Orna Raz"
                    }
                ],
                "author_detail": {
                    "name": "Orna Raz"
                },
                "author": "Orna Raz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.09395v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.09395v5",
                "updated": "2024-10-28T14:29:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    29,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-01-17T18:13:07Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    18,
                    13,
                    7,
                    2,
                    17,
                    0
                ],
                "title": "Evaluating LLMs' Mathematical and Coding Competency through\n  Ontology-guided Interventions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs' Mathematical and Coding Competency through\n  Ontology-guided Interventions"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have showcased striking\nresults on existing logical reasoning benchmarks, with some models even\nsurpassing human performance. However, the true depth of their competencies and\nrobustness in reasoning tasks remains an open question. To this end, in this\npaper, we focus on two popular reasoning tasks: arithmetic reasoning and code\ngeneration. Particularly, we introduce (i) a general ontology of perturbations\nfor math and coding questions, (ii) a semi-automatic method to apply these\nperturbations, and (iii) two datasets, GSMORE and HUMANEVAL-CORE, respectively,\nof perturbed math and coding problems to probe LLM capabilities in numeric\nreasoning and coding tasks. Through comprehensive evaluations of both\nclosed-source and open-source LLMs, we show a significant performance drop\nacross all the models against the perturbed questions, suggesting that the\ncurrent LLMs lack robust problem solving skills and structured reasoning\nabilities in many areas, as defined by our ontology. We open-source the\ndatasets and source codes at: https://github.com/declare-lab/LLM-ReasoningTest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have showcased striking\nresults on existing logical reasoning benchmarks, with some models even\nsurpassing human performance. However, the true depth of their competencies and\nrobustness in reasoning tasks remains an open question. To this end, in this\npaper, we focus on two popular reasoning tasks: arithmetic reasoning and code\ngeneration. Particularly, we introduce (i) a general ontology of perturbations\nfor math and coding questions, (ii) a semi-automatic method to apply these\nperturbations, and (iii) two datasets, GSMORE and HUMANEVAL-CORE, respectively,\nof perturbed math and coding problems to probe LLM capabilities in numeric\nreasoning and coding tasks. Through comprehensive evaluations of both\nclosed-source and open-source LLMs, we show a significant performance drop\nacross all the models against the perturbed questions, suggesting that the\ncurrent LLMs lack robust problem solving skills and structured reasoning\nabilities in many areas, as defined by our ontology. We open-source the\ndatasets and source codes at: https://github.com/declare-lab/LLM-ReasoningTest."
                },
                "authors": [
                    {
                        "name": "Pengfei Hong"
                    },
                    {
                        "name": "Navonil Majumder"
                    },
                    {
                        "name": "Deepanway Ghosal"
                    },
                    {
                        "name": "Somak Aditya"
                    },
                    {
                        "name": "Rada Mihalcea"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.09395v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.09395v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21067v1",
                "updated": "2024-10-28T14:29:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    29,
                    11,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:29:11Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    29,
                    11,
                    0,
                    302,
                    0
                ],
                "title": "CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and\n  Retrieval-Augmented Translation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and\n  Retrieval-Augmented Translation with Large Language Models"
                },
                "summary": "Large language models (LLMs) have shown great promise in machine translation,\nbut they still struggle with contextually dependent terms, such as new or\ndomain-specific words. This leads to inconsistencies and errors that are\ndifficult to address. Existing solutions often depend on manual identification\nof such terms, which is impractical given the complexity and evolving nature of\nlanguage. While Retrieval-Augmented Generation (RAG) could provide some\nassistance, its application to translation is limited by issues such as\nhallucinations from information overload. In this paper, we propose CRAT, a\nnovel multi-agent translation framework that leverages RAG and\ncausality-enhanced self-reflection to address these challenges. This framework\nconsists of several specialized agents: the Unknown Terms Identification agent\ndetects unknown terms within the context, the Knowledge Graph (KG) Constructor\nagent extracts relevant internal knowledge about these terms and retrieves\nbilingual information from external sources, the Causality-enhanced Judge agent\nvalidates the accuracy of the information, and the Translator agent\nincorporates the refined information into the final output. This automated\nprocess allows for more precise and consistent handling of key terms during\ntranslation. Our results show that CRAT significantly improves translation\naccuracy, particularly in handling context-sensitive terms and emerging\nvocabulary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great promise in machine translation,\nbut they still struggle with contextually dependent terms, such as new or\ndomain-specific words. This leads to inconsistencies and errors that are\ndifficult to address. Existing solutions often depend on manual identification\nof such terms, which is impractical given the complexity and evolving nature of\nlanguage. While Retrieval-Augmented Generation (RAG) could provide some\nassistance, its application to translation is limited by issues such as\nhallucinations from information overload. In this paper, we propose CRAT, a\nnovel multi-agent translation framework that leverages RAG and\ncausality-enhanced self-reflection to address these challenges. This framework\nconsists of several specialized agents: the Unknown Terms Identification agent\ndetects unknown terms within the context, the Knowledge Graph (KG) Constructor\nagent extracts relevant internal knowledge about these terms and retrieves\nbilingual information from external sources, the Causality-enhanced Judge agent\nvalidates the accuracy of the information, and the Translator agent\nincorporates the refined information into the final output. This automated\nprocess allows for more precise and consistent handling of key terms during\ntranslation. Our results show that CRAT significantly improves translation\naccuracy, particularly in handling context-sensitive terms and emerging\nvocabulary."
                },
                "authors": [
                    {
                        "name": "Meiqi Chen"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Yingxue Zhang"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21061v1",
                "updated": "2024-10-28T14:22:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    22,
                    8,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    22,
                    8,
                    0,
                    302,
                    0
                ],
                "title": "Kandinsky 3: Text-to-Image Synthesis for Multifunctional Generative\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kandinsky 3: Text-to-Image Synthesis for Multifunctional Generative\n  Framework"
                },
                "summary": "Text-to-image (T2I) diffusion models are popular for introducing image\nmanipulation methods, such as editing, image fusion, inpainting, etc. At the\nsame time, image-to-video (I2V) and text-to-video (T2V) models are also built\non top of T2I models. We present Kandinsky 3, a novel T2I model based on latent\ndiffusion, achieving a high level of quality and photorealism. The key feature\nof the new architecture is the simplicity and efficiency of its adaptation for\nmany types of generation tasks. We extend the base T2I model for various\napplications and create a multifunctional generation system that includes\ntext-guided inpainting/outpainting, image fusion, text-image fusion, image\nvariations generation, I2V and T2V generation. We also present a distilled\nversion of the T2I model, evaluating inference in 4 steps of the reverse\nprocess without reducing image quality and 3 times faster than the base model.\nWe deployed a user-friendly demo system in which all the features can be tested\nin the public domain. Additionally, we released the source code and checkpoints\nfor the Kandinsky 3 and extended models. Human evaluations show that Kandinsky\n3 demonstrates one of the highest quality scores among open source generation\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) diffusion models are popular for introducing image\nmanipulation methods, such as editing, image fusion, inpainting, etc. At the\nsame time, image-to-video (I2V) and text-to-video (T2V) models are also built\non top of T2I models. We present Kandinsky 3, a novel T2I model based on latent\ndiffusion, achieving a high level of quality and photorealism. The key feature\nof the new architecture is the simplicity and efficiency of its adaptation for\nmany types of generation tasks. We extend the base T2I model for various\napplications and create a multifunctional generation system that includes\ntext-guided inpainting/outpainting, image fusion, text-image fusion, image\nvariations generation, I2V and T2V generation. We also present a distilled\nversion of the T2I model, evaluating inference in 4 steps of the reverse\nprocess without reducing image quality and 3 times faster than the base model.\nWe deployed a user-friendly demo system in which all the features can be tested\nin the public domain. Additionally, we released the source code and checkpoints\nfor the Kandinsky 3 and extended models. Human evaluations show that Kandinsky\n3 demonstrates one of the highest quality scores among open source generation\nsystems."
                },
                "authors": [
                    {
                        "name": "Vladimir Arkhipkin"
                    },
                    {
                        "name": "Viacheslav Vasilev"
                    },
                    {
                        "name": "Andrei Filatov"
                    },
                    {
                        "name": "Igor Pavlov"
                    },
                    {
                        "name": "Julia Agafonova"
                    },
                    {
                        "name": "Nikolai Gerasimenko"
                    },
                    {
                        "name": "Anna Averchenkova"
                    },
                    {
                        "name": "Evelina Mironova"
                    },
                    {
                        "name": "Anton Bukashkin"
                    },
                    {
                        "name": "Konstantin Kulikov"
                    },
                    {
                        "name": "Andrey Kuznetsov"
                    },
                    {
                        "name": "Denis Dimitrov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Dimitrov"
                },
                "author": "Denis Dimitrov",
                "arxiv_comment": "Accepted for EMNLP 2024 (Demo track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02748v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02748v2",
                "updated": "2024-10-28T14:21:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    21,
                    8,
                    0,
                    302,
                    0
                ],
                "published": "2024-05-04T20:32:02Z",
                "published_parsed": [
                    2024,
                    5,
                    4,
                    20,
                    32,
                    2,
                    5,
                    125,
                    0
                ],
                "title": "A Bayesian mixture model approach to quantifying the empirical nuclear\n  saturation point",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian mixture model approach to quantifying the empirical nuclear\n  saturation point"
                },
                "summary": "The equation of state (EOS) in the limit of infinite symmetric nuclear matter\nexhibits an equilibrium density, $n_0 \\approx 0.16 \\, \\mathrm{fm}^{-3}$, at\nwhich the pressure vanishes and the energy per particle attains its minimum,\n$E_0 \\approx -16 \\, \\mathrm{MeV}$. Although not directly measurable, the\nsaturation point $(n_0,E_0)$ can be extrapolated by density functional theory\n(DFT), providing tight constraints for microscopic interactions derived from\nchiral effective field theory (EFT). However, when considering several DFT\npredictions for $(n_0,E_0)$ from Skyrme and Relativistic Mean Field models\ntogether, a discrepancy between these model classes emerges at high confidence\nlevels that each model prediction's uncertainty cannot explain. How can we\nleverage these DFT constraints to rigorously benchmark saturation properties of\nchiral interactions? To address this question, we present a Bayesian mixture\nmodel that combines multiple DFT predictions for $(n_0,E_0)$ using an efficient\nconjugate prior approach. The inferred posterior for the saturation point's\nmean and covariance matrix follows a Normal-inverse-Wishart class, resulting in\nposterior predictives in the form of correlated, bivariate $t$-distributions.\nThe DFT uncertainty reports are then used to mix these posteriors using an\nordinary Monte Carlo approach. At the 95\\% credibility level, we estimate $n_0\n\\approx 0.157 \\pm 0.010 \\, \\mathrm{fm}^{-3}$ and $E_0 \\approx -15.97 \\pm 0.40\n\\, \\mathrm{MeV}$ for the marginal (univariate) $t$-distributions. Combined with\nchiral EFT calculations of the pure neutron matter EOS, we obtain bivariate\nnormal distributions for the symmetry energy and its slope parameter at $n_0$:\n$S_v \\approx 32.0 \\pm 1.1 \\, \\mathrm{MeV}$ and $L\\approx 52.6\\pm 8.1 \\,\n\\mathrm{MeV}$ (95\\%), respectively. Our Bayesian framework is publicly\navailable, so practitioners can readily use and extend our results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The equation of state (EOS) in the limit of infinite symmetric nuclear matter\nexhibits an equilibrium density, $n_0 \\approx 0.16 \\, \\mathrm{fm}^{-3}$, at\nwhich the pressure vanishes and the energy per particle attains its minimum,\n$E_0 \\approx -16 \\, \\mathrm{MeV}$. Although not directly measurable, the\nsaturation point $(n_0,E_0)$ can be extrapolated by density functional theory\n(DFT), providing tight constraints for microscopic interactions derived from\nchiral effective field theory (EFT). However, when considering several DFT\npredictions for $(n_0,E_0)$ from Skyrme and Relativistic Mean Field models\ntogether, a discrepancy between these model classes emerges at high confidence\nlevels that each model prediction's uncertainty cannot explain. How can we\nleverage these DFT constraints to rigorously benchmark saturation properties of\nchiral interactions? To address this question, we present a Bayesian mixture\nmodel that combines multiple DFT predictions for $(n_0,E_0)$ using an efficient\nconjugate prior approach. The inferred posterior for the saturation point's\nmean and covariance matrix follows a Normal-inverse-Wishart class, resulting in\nposterior predictives in the form of correlated, bivariate $t$-distributions.\nThe DFT uncertainty reports are then used to mix these posteriors using an\nordinary Monte Carlo approach. At the 95\\% credibility level, we estimate $n_0\n\\approx 0.157 \\pm 0.010 \\, \\mathrm{fm}^{-3}$ and $E_0 \\approx -15.97 \\pm 0.40\n\\, \\mathrm{MeV}$ for the marginal (univariate) $t$-distributions. Combined with\nchiral EFT calculations of the pure neutron matter EOS, we obtain bivariate\nnormal distributions for the symmetry energy and its slope parameter at $n_0$:\n$S_v \\approx 32.0 \\pm 1.1 \\, \\mathrm{MeV}$ and $L\\approx 52.6\\pm 8.1 \\,\n\\mathrm{MeV}$ (95\\%), respectively. Our Bayesian framework is publicly\navailable, so practitioners can readily use and extend our results."
                },
                "authors": [
                    {
                        "name": "C. Drischler"
                    },
                    {
                        "name": "P. G. Giuliani"
                    },
                    {
                        "name": "S. Bezoui"
                    },
                    {
                        "name": "J. Piekarewicz"
                    },
                    {
                        "name": "F. Viens"
                    }
                ],
                "author_detail": {
                    "name": "F. Viens"
                },
                "author": "F. Viens",
                "arxiv_doi": "10.1103/PhysRevC.110.044320",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevC.110.044320",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.02748v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02748v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "close to the published version; extended analysis and minor changes;\n  31 pages, 14 figures, 5 tables",
                "arxiv_journal_ref": "Phys. Rev. C 110, 044320 (2024)",
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21060v1",
                "updated": "2024-10-28T14:18:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    18,
                    32,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:18:32Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    18,
                    32,
                    0,
                    302,
                    0
                ],
                "title": "CTINEXUS: Leveraging Optimized LLM In-Context Learning for Constructing\n  Cybersecurity Knowledge Graphs Under Data Scarcity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CTINEXUS: Leveraging Optimized LLM In-Context Learning for Constructing\n  Cybersecurity Knowledge Graphs Under Data Scarcity"
                },
                "summary": "Textual descriptions in cyber threat intelligence (CTI) reports, such as\nsecurity articles and news, are rich sources of knowledge about cyber threats,\ncrucial for organizations to stay informed about the rapidly evolving threat\nlandscape. However, current CTI extraction methods lack flexibility and\ngeneralizability, often resulting in inaccurate and incomplete knowledge\nextraction. Syntax parsing relies on fixed rules and dictionaries, while model\nfine-tuning requires large annotated datasets, making both paradigms\nchallenging to adapt to new threats and ontologies. To bridge the gap, we\npropose CTINexus, a novel framework leveraging optimized in-context learning\n(ICL) of large language models (LLMs) for data-efficient CTI knowledge\nextraction and high-quality cybersecurity knowledge graph (CSKG) construction.\nUnlike existing methods, CTINexus requires neither extensive data nor parameter\ntuning and can adapt to various ontologies with minimal annotated examples.\nThis is achieved through (1) a carefully designed automatic prompt construction\nstrategy with optimal demonstration retrieval for extracting a wide range of\ncybersecurity entities and relations; (2) a hierarchical entity alignment\ntechnique that canonicalizes the extracted knowledge and removes redundancy;\n(3) an ICL-enhanced long-distance relation prediction technique to further\ncomplete the CKSG with missing links. Our extensive evaluations using 150\nreal-world CTI reports collected from 10 platforms demonstrate that CTINexus\nsignificantly outperforms existing methods in constructing accurate and\ncomplete CSKGs, highlighting its potential to transform CTI analysis with an\nefficient and adaptable solution for the dynamic threat landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual descriptions in cyber threat intelligence (CTI) reports, such as\nsecurity articles and news, are rich sources of knowledge about cyber threats,\ncrucial for organizations to stay informed about the rapidly evolving threat\nlandscape. However, current CTI extraction methods lack flexibility and\ngeneralizability, often resulting in inaccurate and incomplete knowledge\nextraction. Syntax parsing relies on fixed rules and dictionaries, while model\nfine-tuning requires large annotated datasets, making both paradigms\nchallenging to adapt to new threats and ontologies. To bridge the gap, we\npropose CTINexus, a novel framework leveraging optimized in-context learning\n(ICL) of large language models (LLMs) for data-efficient CTI knowledge\nextraction and high-quality cybersecurity knowledge graph (CSKG) construction.\nUnlike existing methods, CTINexus requires neither extensive data nor parameter\ntuning and can adapt to various ontologies with minimal annotated examples.\nThis is achieved through (1) a carefully designed automatic prompt construction\nstrategy with optimal demonstration retrieval for extracting a wide range of\ncybersecurity entities and relations; (2) a hierarchical entity alignment\ntechnique that canonicalizes the extracted knowledge and removes redundancy;\n(3) an ICL-enhanced long-distance relation prediction technique to further\ncomplete the CKSG with missing links. Our extensive evaluations using 150\nreal-world CTI reports collected from 10 platforms demonstrate that CTINexus\nsignificantly outperforms existing methods in constructing accurate and\ncomplete CSKGs, highlighting its potential to transform CTI analysis with an\nefficient and adaptable solution for the dynamic threat landscape."
                },
                "authors": [
                    {
                        "name": "Yutong Cheng"
                    },
                    {
                        "name": "Osama Bajaber"
                    },
                    {
                        "name": "Saimon Amanuel Tsegai"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Peng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Peng Gao"
                },
                "author": "Peng Gao",
                "arxiv_comment": "under peer-review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05600v2",
                "updated": "2024-10-28T14:08:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    8,
                    13,
                    0,
                    302,
                    0
                ],
                "published": "2024-07-08T04:30:53Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    4,
                    30,
                    53,
                    0,
                    190,
                    0
                ],
                "title": "GenArtist: Multimodal LLM as an Agent for Unified Image Generation and\n  Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenArtist: Multimodal LLM as an Agent for Unified Image Generation and\n  Editing"
                },
                "summary": "Despite the success achieved by existing image generation and editing\nmethods, current models still struggle with complex problems including\nintricate text prompts, and the absence of verification and self-correction\nmechanisms makes the generated images unreliable. Meanwhile, a single model\ntends to specialize in particular tasks and possess the corresponding\ncapabilities, making it inadequate for fulfilling all user requirements. We\npropose GenArtist, a unified image generation and editing system, coordinated\nby a multimodal large language model (MLLM) agent. We integrate a comprehensive\nrange of existing models into the tool library and utilize the agent for tool\nselection and execution. For a complex problem, the MLLM agent decomposes it\ninto simpler sub-problems and constructs a tree structure to systematically\nplan the procedure of generation, editing, and self-correction with\nstep-by-step verification. By automatically generating missing position-related\ninputs and incorporating position information, the appropriate tool can be\neffectively employed to address each sub-problem. Experiments demonstrate that\nGenArtist can perform various generation and editing tasks, achieving\nstate-of-the-art performance and surpassing existing models such as SDXL and\nDALL-E 3, as can be seen in Fig. 1. Project page is\nhttps://zhenyuw16.github.io/GenArtist_page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success achieved by existing image generation and editing\nmethods, current models still struggle with complex problems including\nintricate text prompts, and the absence of verification and self-correction\nmechanisms makes the generated images unreliable. Meanwhile, a single model\ntends to specialize in particular tasks and possess the corresponding\ncapabilities, making it inadequate for fulfilling all user requirements. We\npropose GenArtist, a unified image generation and editing system, coordinated\nby a multimodal large language model (MLLM) agent. We integrate a comprehensive\nrange of existing models into the tool library and utilize the agent for tool\nselection and execution. For a complex problem, the MLLM agent decomposes it\ninto simpler sub-problems and constructs a tree structure to systematically\nplan the procedure of generation, editing, and self-correction with\nstep-by-step verification. By automatically generating missing position-related\ninputs and incorporating position information, the appropriate tool can be\neffectively employed to address each sub-problem. Experiments demonstrate that\nGenArtist can perform various generation and editing tasks, achieving\nstate-of-the-art performance and surpassing existing models such as SDXL and\nDALL-E 3, as can be seen in Fig. 1. Project page is\nhttps://zhenyuw16.github.io/GenArtist_page."
                },
                "authors": [
                    {
                        "name": "Zhenyu Wang"
                    },
                    {
                        "name": "Aoxue Li"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Xihui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xihui Liu"
                },
                "author": "Xihui Liu",
                "arxiv_comment": "NeurIPS 2024 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11295v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11295v4",
                "updated": "2024-10-28T14:06:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    6,
                    58,
                    0,
                    302,
                    0
                ],
                "published": "2024-02-17T14:26:57Z",
                "published_parsed": [
                    2024,
                    2,
                    17,
                    14,
                    26,
                    57,
                    5,
                    48,
                    0
                ],
                "title": "OneBit: Towards Extremely Low-bit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneBit: Towards Extremely Low-bit Large Language Models"
                },
                "summary": "Model quantification uses low bit-width values to represent the weight\nmatrices of existing models to be quantized, which is a promising approach to\nreduce both storage and computational overheads of deploying highly anticipated\nLLMs. However, current quantization methods suffer severe performance\ndegradation when the bit-width is extremely reduced, and thus focus on\nutilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes\nthe weight matrices of LLMs to 1-bit, paving the way for the extremely low\nbit-width deployment of LLMs. For this target, we introduce a 1-bit model\ncompressing framework named OneBit, including a novel 1-bit parameter\nrepresentation method to better quantize LLMs as well as an effective parameter\ninitialization method based on matrix decomposition to improve the convergence\nspeed of the quantization framework. Sufficient experimental results indicate\nthat OneBit achieves good performance (at least 81% of the non-quantized\nperformance on LLaMA models) with robust training processes when only using\n1-bit weight matrices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantification uses low bit-width values to represent the weight\nmatrices of existing models to be quantized, which is a promising approach to\nreduce both storage and computational overheads of deploying highly anticipated\nLLMs. However, current quantization methods suffer severe performance\ndegradation when the bit-width is extremely reduced, and thus focus on\nutilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes\nthe weight matrices of LLMs to 1-bit, paving the way for the extremely low\nbit-width deployment of LLMs. For this target, we introduce a 1-bit model\ncompressing framework named OneBit, including a novel 1-bit parameter\nrepresentation method to better quantize LLMs as well as an effective parameter\ninitialization method based on matrix decomposition to improve the convergence\nspeed of the quantization framework. Sufficient experimental results indicate\nthat OneBit achieves good performance (at least 81% of the non-quantized\nperformance on LLaMA models) with robust training processes when only using\n1-bit weight matrices."
                },
                "authors": [
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zonghan Yang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Weidong Liu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11295v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11295v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21041v1",
                "updated": "2024-10-28T13:58:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    58,
                    4,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:58:04Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    58,
                    4,
                    0,
                    302,
                    0
                ],
                "title": "Sorting Out the Bad Seeds: Automatic Classification of Cryptocurrency\n  Abuse Reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sorting Out the Bad Seeds: Automatic Classification of Cryptocurrency\n  Abuse Reports"
                },
                "summary": "Abuse reporting services collect reports about abuse victims have suffered.\nAccurate classification of the submitted reports is fundamental to analyzing\nthe prevalence and financial impact of different abuse types (e.g., sextortion,\ninvestment, romance). Current classification approaches are problematic because\nthey require the reporter to select the abuse type from a list, assuming the\nreporter has the necessary experience for the classification, which we show is\nfrequently not the case, or require manual classification by analysts, which\ndoes not scale. To address these issues, this paper presents a novel approach\nto classify cryptocurrency abuse reports automatically. We first build a\ntaxonomy of 19 frequently reported abuse types. Given as input the textual\ndescription written by the reporter, our classifier leverages a large language\nmodel (LLM) to interpret the text and assign it an abuse type in our taxonomy.\nWe collect 290K cryptocurrency abuse reports from two popular reporting\nservices: BitcoinAbuse and BBB's ScamTracker. We build ground truth datasets\nfor 20K of those reports and use them to evaluate three designs for our\nLLM-based classifier and four LLMs, as well as a supervised ML classifier used\nas a baseline. Our LLM-based classifier achieves a precision of 0.92, a recall\nof 0.87, and an F1 score of 0.89, compared to an F1 score of 0.55 for the\nbaseline. We demonstrate our classifier in two applications: providing\nfinancial loss statistics for fine-grained abuse types and generating tagged\naddresses for cryptocurrency analysis platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abuse reporting services collect reports about abuse victims have suffered.\nAccurate classification of the submitted reports is fundamental to analyzing\nthe prevalence and financial impact of different abuse types (e.g., sextortion,\ninvestment, romance). Current classification approaches are problematic because\nthey require the reporter to select the abuse type from a list, assuming the\nreporter has the necessary experience for the classification, which we show is\nfrequently not the case, or require manual classification by analysts, which\ndoes not scale. To address these issues, this paper presents a novel approach\nto classify cryptocurrency abuse reports automatically. We first build a\ntaxonomy of 19 frequently reported abuse types. Given as input the textual\ndescription written by the reporter, our classifier leverages a large language\nmodel (LLM) to interpret the text and assign it an abuse type in our taxonomy.\nWe collect 290K cryptocurrency abuse reports from two popular reporting\nservices: BitcoinAbuse and BBB's ScamTracker. We build ground truth datasets\nfor 20K of those reports and use them to evaluate three designs for our\nLLM-based classifier and four LLMs, as well as a supervised ML classifier used\nas a baseline. Our LLM-based classifier achieves a precision of 0.92, a recall\nof 0.87, and an F1 score of 0.89, compared to an F1 score of 0.55 for the\nbaseline. We demonstrate our classifier in two applications: providing\nfinancial loss statistics for fine-grained abuse types and generating tagged\naddresses for cryptocurrency analysis platforms."
                },
                "authors": [
                    {
                        "name": "Gibran Gomez"
                    },
                    {
                        "name": "Kevin van Liebergen"
                    },
                    {
                        "name": "Davide Sanvito"
                    },
                    {
                        "name": "Giuseppe Siracusano"
                    },
                    {
                        "name": "Roberto Gonzalez"
                    },
                    {
                        "name": "Juan Caballero"
                    }
                ],
                "author_detail": {
                    "name": "Juan Caballero"
                },
                "author": "Juan Caballero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21040v1",
                "updated": "2024-10-28T13:57:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    57,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:57:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    57,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "LiP-LLM: Integrating Linear Programming and dependency graph with Large\n  Language Models for multi-robot task planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiP-LLM: Integrating Linear Programming and dependency graph with Large\n  Language Models for multi-robot task planning"
                },
                "summary": "This study proposes LiP-LLM: integrating linear programming and dependency\ngraph with large language models (LLMs) for multi-robot task planning. In order\nfor multiple robots to perform tasks more efficiently, it is necessary to\nmanage the precedence dependencies between tasks. Although multi-robot\ndecentralized and centralized task planners using LLMs have been proposed, none\nof these studies focus on precedence dependencies from the perspective of task\nefficiency or leverage traditional optimization methods. It addresses key\nchallenges in managing dependencies between skills and optimizing task\nallocation. LiP-LLM consists of three steps: skill list generation and\ndependency graph generation by LLMs, and task allocation using linear\nprogramming. The LLMs are utilized to generate a comprehensive list of skills\nand to construct a dependency graph that maps the relationships and sequential\nconstraints among these skills. To ensure the feasibility and efficiency of\nskill execution, the skill list is generated by calculated likelihood, and\nlinear programming is used to optimally allocate tasks to each robot.\nExperimental evaluations in simulated environments demonstrate that this method\noutperforms existing task planners, achieving higher success rates and\nefficiency in executing complex, multi-robot tasks. The results indicate the\npotential of combining LLMs with optimization techniques to enhance the\ncapabilities of multi-robot systems in executing coordinated tasks accurately\nand efficiently. In an environment with two robots, a maximum success rate\ndifference of 0.82 is observed in the language instruction group with a change\nin the object name.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study proposes LiP-LLM: integrating linear programming and dependency\ngraph with large language models (LLMs) for multi-robot task planning. In order\nfor multiple robots to perform tasks more efficiently, it is necessary to\nmanage the precedence dependencies between tasks. Although multi-robot\ndecentralized and centralized task planners using LLMs have been proposed, none\nof these studies focus on precedence dependencies from the perspective of task\nefficiency or leverage traditional optimization methods. It addresses key\nchallenges in managing dependencies between skills and optimizing task\nallocation. LiP-LLM consists of three steps: skill list generation and\ndependency graph generation by LLMs, and task allocation using linear\nprogramming. The LLMs are utilized to generate a comprehensive list of skills\nand to construct a dependency graph that maps the relationships and sequential\nconstraints among these skills. To ensure the feasibility and efficiency of\nskill execution, the skill list is generated by calculated likelihood, and\nlinear programming is used to optimally allocate tasks to each robot.\nExperimental evaluations in simulated environments demonstrate that this method\noutperforms existing task planners, achieving higher success rates and\nefficiency in executing complex, multi-robot tasks. The results indicate the\npotential of combining LLMs with optimization techniques to enhance the\ncapabilities of multi-robot systems in executing coordinated tasks accurately\nand efficiently. In an environment with two robots, a maximum success rate\ndifference of 0.82 is observed in the language instruction group with a change\nin the object name."
                },
                "authors": [
                    {
                        "name": "Kazuma Obata"
                    },
                    {
                        "name": "Tatsuya Aoki"
                    },
                    {
                        "name": "Takato Horii"
                    },
                    {
                        "name": "Tadahiro Taniguchi"
                    },
                    {
                        "name": "Takayuki Nagai"
                    }
                ],
                "author_detail": {
                    "name": "Takayuki Nagai"
                },
                "author": "Takayuki Nagai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v1",
                "updated": "2024-10-28T13:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21034v1",
                "updated": "2024-10-28T13:56:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    1,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:56:01Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    1,
                    0,
                    302,
                    0
                ],
                "title": "Inferring the Isotropic-nematic Phase Transition with Generative Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring the Isotropic-nematic Phase Transition with Generative Machine\n  Learning"
                },
                "summary": "Contemporary work implies generative machine learning models are capable of\nlearning the phase behavior in condensed matter systems such as the Ising\nmodel. In this Letter, we utilize a score-based modeling procedure called\nThermodynamic Maps to describe the isotropic-nematic phase transition in a melt\nof $N=343$ calamitic Gay-Berne ellipsoids. When trained on samples generated by\nmolecular dynamics simulation from a single temperature on either side of the\nphase transition, we demonstrate this generative machine learning approach\ninfers information regarding the critical behavior and estimates effectively\nthe nematic order parameter at sampled temperatures between the two training\ntemperatures. These results demonstrate score-based models' ability to learn\nthe physics of a non-trivial liquid crystalline phase transition driven by\nanisotropic interactions both entropic and energetic in nature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary work implies generative machine learning models are capable of\nlearning the phase behavior in condensed matter systems such as the Ising\nmodel. In this Letter, we utilize a score-based modeling procedure called\nThermodynamic Maps to describe the isotropic-nematic phase transition in a melt\nof $N=343$ calamitic Gay-Berne ellipsoids. When trained on samples generated by\nmolecular dynamics simulation from a single temperature on either side of the\nphase transition, we demonstrate this generative machine learning approach\ninfers information regarding the critical behavior and estimates effectively\nthe nematic order parameter at sampled temperatures between the two training\ntemperatures. These results demonstrate score-based models' ability to learn\nthe physics of a non-trivial liquid crystalline phase transition driven by\nanisotropic interactions both entropic and energetic in nature."
                },
                "authors": [
                    {
                        "name": "Eric R. Beyerle"
                    },
                    {
                        "name": "Pratyush Tiwary"
                    }
                ],
                "author_detail": {
                    "name": "Pratyush Tiwary"
                },
                "author": "Pratyush Tiwary",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02644v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02644v2",
                "updated": "2024-10-28T13:54:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    54,
                    57,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-04T12:20:27Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    12,
                    20,
                    27,
                    2,
                    248,
                    0
                ],
                "title": "Conformal Prediction in Dynamic Biological Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Prediction in Dynamic Biological Systems"
                },
                "summary": "Uncertainty quantification (UQ) is the process of systematically determining\nand characterizing the degree of confidence in computational model predictions.\nIn the context of systems biology, especially with dynamic models, UQ is\ncrucial because it addresses the challenges posed by nonlinearity and parameter\nsensitivity, allowing us to properly understand and extrapolate the behavior of\ncomplex biological systems. Here, we focus on dynamic models represented by\ndeterministic nonlinear ordinary differential equations. Many current UQ\napproaches in this field rely on Bayesian statistical methods. While powerful,\nthese methods often require strong prior specifications and make parametric\nassumptions that may not always hold in biological systems. Additionally, these\nmethods face challenges in domains where sample sizes are limited, and\nstatistical inference becomes constrained, with computational speed being a\nbottleneck in large models of biological systems. As an alternative, we propose\nthe use of conformal inference methods, introducing two novel algorithms that,\nin some instances, offer non-asymptotic guarantees, enhancing robustness and\nscalability across various applications. We demonstrate the efficacy of our\nproposed algorithms through several scenarios, highlighting their advantages\nover traditional Bayesian approaches. The proposed methods show promising\nresults for diverse biological data structures and scenarios, offering a\ngeneral framework to quantify uncertainty for dynamic models of biological\nsystems.The software for the methodology and the reproduction of the results is\navailable at https://zenodo.org/doi/10.5281/zenodo.13644870.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) is the process of systematically determining\nand characterizing the degree of confidence in computational model predictions.\nIn the context of systems biology, especially with dynamic models, UQ is\ncrucial because it addresses the challenges posed by nonlinearity and parameter\nsensitivity, allowing us to properly understand and extrapolate the behavior of\ncomplex biological systems. Here, we focus on dynamic models represented by\ndeterministic nonlinear ordinary differential equations. Many current UQ\napproaches in this field rely on Bayesian statistical methods. While powerful,\nthese methods often require strong prior specifications and make parametric\nassumptions that may not always hold in biological systems. Additionally, these\nmethods face challenges in domains where sample sizes are limited, and\nstatistical inference becomes constrained, with computational speed being a\nbottleneck in large models of biological systems. As an alternative, we propose\nthe use of conformal inference methods, introducing two novel algorithms that,\nin some instances, offer non-asymptotic guarantees, enhancing robustness and\nscalability across various applications. We demonstrate the efficacy of our\nproposed algorithms through several scenarios, highlighting their advantages\nover traditional Bayesian approaches. The proposed methods show promising\nresults for diverse biological data structures and scenarios, offering a\ngeneral framework to quantify uncertainty for dynamic models of biological\nsystems.The software for the methodology and the reproduction of the results is\navailable at https://zenodo.org/doi/10.5281/zenodo.13644870."
                },
                "authors": [
                    {
                        "name": "Alberto Portela"
                    },
                    {
                        "name": "Julio R. Banga"
                    },
                    {
                        "name": "Marcos Matabuena"
                    }
                ],
                "author_detail": {
                    "name": "Marcos Matabuena"
                },
                "author": "Marcos Matabuena",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02644v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02644v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21027v1",
                "updated": "2024-10-28T13:48:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    48,
                    43,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:48:43Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    48,
                    43,
                    0,
                    302,
                    0
                ],
                "title": "Transferable Post-training via Inverse Value Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferable Post-training via Inverse Value Learning"
                },
                "summary": "As post-training processes utilize increasingly large datasets and base\nmodels continue to grow in size, the computational demands and implementation\nchallenges of existing algorithms are escalating significantly. In this paper,\nwe propose modeling the changes at the logits level during post-training using\na separate neural network (i.e., the value network). After training this\nnetwork on a small base model using demonstrations, this network can be\nseamlessly integrated with other pre-trained models during inference, enables\nthem to achieve similar capability enhancements. We systematically investigate\nthe best practices for this paradigm in terms of pre-training weights and\nconnection schemes. We demonstrate that the resulting value network has broad\ntransferability across pre-trained models of different parameter sizes within\nthe same family, models undergoing continuous pre-training within the same\nfamily, and models with different vocabularies across families. In certain\ncases, it can achieve performance comparable to full-parameter fine-tuning.\nFurthermore, we explore methods to enhance the transferability of the value\nmodel and prevent overfitting to the base model used during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As post-training processes utilize increasingly large datasets and base\nmodels continue to grow in size, the computational demands and implementation\nchallenges of existing algorithms are escalating significantly. In this paper,\nwe propose modeling the changes at the logits level during post-training using\na separate neural network (i.e., the value network). After training this\nnetwork on a small base model using demonstrations, this network can be\nseamlessly integrated with other pre-trained models during inference, enables\nthem to achieve similar capability enhancements. We systematically investigate\nthe best practices for this paradigm in terms of pre-training weights and\nconnection schemes. We demonstrate that the resulting value network has broad\ntransferability across pre-trained models of different parameter sizes within\nthe same family, models undergoing continuous pre-training within the same\nfamily, and models with different vocabularies across families. In certain\ncases, it can achieve performance comparable to full-parameter fine-tuning.\nFurthermore, we explore methods to enhance the transferability of the value\nmodel and prevent overfitting to the base model used during training."
                },
                "authors": [
                    {
                        "name": "Xinyu Lu"
                    },
                    {
                        "name": "Xueru Wen"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21021v1",
                "updated": "2024-10-28T13:42:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    42,
                    27,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:42:27Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    42,
                    27,
                    0,
                    302,
                    0
                ],
                "title": "A Stein Gradient Descent Approach for Doubly Intractable Distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stein Gradient Descent Approach for Doubly Intractable Distributions"
                },
                "summary": "Bayesian inference for doubly intractable distributions is challenging\nbecause they include intractable terms, which are functions of parameters of\ninterest. Although several alternatives have been developed for such models,\nthey are computationally intensive due to repeated auxiliary variable\nsimulations. We propose a novel Monte Carlo Stein variational gradient descent\n(MC-SVGD) approach for inference for doubly intractable distributions. Through\nan efficient gradient approximation, our MC-SVGD approach rapidly transforms an\narbitrary reference distribution to approximate the posterior distribution of\ninterest, without necessitating any predefined variational distribution class\nfor the posterior. Such a transport map is obtained by minimizing\nKullback-Leibler divergence between the transformed and posterior distributions\nin a reproducing kernel Hilbert space (RKHS). We also investigate the\nconvergence rate of the proposed method. We illustrate the application of the\nmethod to challenging examples, including a Potts model, an exponential random\ngraph model, and a Conway--Maxwell--Poisson regression model. The proposed\nmethod achieves substantial computational gains over existing algorithms, while\nproviding comparable inferential performance for the posterior distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference for doubly intractable distributions is challenging\nbecause they include intractable terms, which are functions of parameters of\ninterest. Although several alternatives have been developed for such models,\nthey are computationally intensive due to repeated auxiliary variable\nsimulations. We propose a novel Monte Carlo Stein variational gradient descent\n(MC-SVGD) approach for inference for doubly intractable distributions. Through\nan efficient gradient approximation, our MC-SVGD approach rapidly transforms an\narbitrary reference distribution to approximate the posterior distribution of\ninterest, without necessitating any predefined variational distribution class\nfor the posterior. Such a transport map is obtained by minimizing\nKullback-Leibler divergence between the transformed and posterior distributions\nin a reproducing kernel Hilbert space (RKHS). We also investigate the\nconvergence rate of the proposed method. We illustrate the application of the\nmethod to challenging examples, including a Potts model, an exponential random\ngraph model, and a Conway--Maxwell--Poisson regression model. The proposed\nmethod achieves substantial computational gains over existing algorithms, while\nproviding comparable inferential performance for the posterior distributions."
                },
                "authors": [
                    {
                        "name": "Heesang Lee"
                    },
                    {
                        "name": "Songhee Kim"
                    },
                    {
                        "name": "Bokgyeong Kang"
                    },
                    {
                        "name": "Jaewoo Park"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Park"
                },
                "author": "Jaewoo Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21012v1",
                "updated": "2024-10-28T13:36:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    36,
                    41,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:36:41Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    36,
                    41,
                    0,
                    302,
                    0
                ],
                "title": "FACT: Examining the Effectiveness of Iterative Context Rewriting for\n  Multi-fact Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FACT: Examining the Effectiveness of Iterative Context Rewriting for\n  Multi-fact Retrieval"
                },
                "summary": "Large Language Models (LLMs) are proficient at retrieving single facts from\nextended contexts, yet they struggle with tasks requiring the simultaneous\nretrieval of multiple facts, especially during generation. This paper\nidentifies a novel \"lost-in-the-middle\" phenomenon, where LLMs progressively\nlose track of critical information throughout the generation process, resulting\nin incomplete or inaccurate retrieval. To address this challenge, we introduce\nFind All Crucial Texts (FACT), an iterative retrieval method that refines\ncontext through successive rounds of rewriting. This approach enables models to\ncapture essential facts incrementally, which are often overlooked in\nsingle-pass retrieval. Experiments demonstrate that FACT substantially enhances\nmulti-fact retrieval performance across various tasks, though improvements are\nless notable in general-purpose QA scenarios. Our findings shed light on the\nlimitations of LLMs in multi-fact retrieval and underscore the need for more\nresilient long-context retrieval strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are proficient at retrieving single facts from\nextended contexts, yet they struggle with tasks requiring the simultaneous\nretrieval of multiple facts, especially during generation. This paper\nidentifies a novel \"lost-in-the-middle\" phenomenon, where LLMs progressively\nlose track of critical information throughout the generation process, resulting\nin incomplete or inaccurate retrieval. To address this challenge, we introduce\nFind All Crucial Texts (FACT), an iterative retrieval method that refines\ncontext through successive rounds of rewriting. This approach enables models to\ncapture essential facts incrementally, which are often overlooked in\nsingle-pass retrieval. Experiments demonstrate that FACT substantially enhances\nmulti-fact retrieval performance across various tasks, though improvements are\nless notable in general-purpose QA scenarios. Our findings shed light on the\nlimitations of LLMs in multi-fact retrieval and underscore the need for more\nresilient long-context retrieval strategies."
                },
                "authors": [
                    {
                        "name": "Jinlin Wang"
                    },
                    {
                        "name": "Suyuchen Wang"
                    },
                    {
                        "name": "Ziwen Xia"
                    },
                    {
                        "name": "Sirui Hong"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Bang Liu"
                    },
                    {
                        "name": "Chenglin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chenglin Wu"
                },
                "author": "Chenglin Wu",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.03274v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.03274v3",
                "updated": "2024-10-28T13:33:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    33,
                    40,
                    0,
                    302,
                    0
                ],
                "published": "2022-11-07T02:45:52Z",
                "published_parsed": [
                    2022,
                    11,
                    7,
                    2,
                    45,
                    52,
                    0,
                    311,
                    0
                ],
                "title": "A General Framework for Cutting Feedback within Modularised Bayesian\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Framework for Cutting Feedback within Modularised Bayesian\n  Inference"
                },
                "summary": "Standard Bayesian inference can build models that combine information from\nvarious sources, but this inference may not be reliable if components of a\nmodel are misspecified. Cut inference, as a particular type of modularized\nBayesian inference, is an alternative which splits a model into modules and\ncuts the feedback from the suspect module. Previous studies have focused on a\ntwo-module case, but a more general definition of a \"module\" remains unclear.\nWe present a formal definition of a \"module\" and discuss its properties. We\nformulate methods for identifying modules; determining the order of modules;\nand building the cut distribution that should be used for cut inference within\nan arbitrary directed acyclic graph structure. We justify the cut distribution\nby showing that it not only cuts the feedback but also is the best\napproximation satisfying this condition to the joint distribution in the\nKullback-Leibler divergence. We also extend cut inference for the two-module\ncase to a general multiple-module case via a sequential splitting technique and\ndemonstrate this via illustrative applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard Bayesian inference can build models that combine information from\nvarious sources, but this inference may not be reliable if components of a\nmodel are misspecified. Cut inference, as a particular type of modularized\nBayesian inference, is an alternative which splits a model into modules and\ncuts the feedback from the suspect module. Previous studies have focused on a\ntwo-module case, but a more general definition of a \"module\" remains unclear.\nWe present a formal definition of a \"module\" and discuss its properties. We\nformulate methods for identifying modules; determining the order of modules;\nand building the cut distribution that should be used for cut inference within\nan arbitrary directed acyclic graph structure. We justify the cut distribution\nby showing that it not only cuts the feedback but also is the best\napproximation satisfying this condition to the joint distribution in the\nKullback-Leibler divergence. We also extend cut inference for the two-module\ncase to a general multiple-module case via a sequential splitting technique and\ndemonstrate this via illustrative applications."
                },
                "authors": [
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Robert J. B. Goudie"
                    }
                ],
                "author_detail": {
                    "name": "Robert J. B. Goudie"
                },
                "author": "Robert J. B. Goudie",
                "arxiv_comment": "30 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.03274v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.03274v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14952v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14952v3",
                "updated": "2024-10-28T13:25:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    25,
                    49,
                    0,
                    302,
                    0
                ],
                "published": "2024-06-21T08:03:33Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    8,
                    3,
                    33,
                    4,
                    173,
                    0
                ],
                "title": "ESC-Eval: Evaluating Emotion Support Conversations in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ESC-Eval: Evaluating Emotion Support Conversations in Large Language\n  Models"
                },
                "summary": "Emotion Support Conversation (ESC) is a crucial application, which aims to\nreduce human stress, offer emotional guidance, and ultimately enhance human\nmental and physical well-being. With the advancement of Large Language Models\n(LLMs), many researchers have employed LLMs as the ESC models. However, the\nevaluation of these LLM-based ESCs remains uncertain. Inspired by the awesome\ndevelopment of role-playing agents, we propose an ESC Evaluation framework\n(ESC-Eval), which uses a role-playing agent to interact with ESC models,\nfollowed by a manual evaluation of the interactive dialogues. In detail, we\nfirst re-organize 2,801 role-playing cards from seven existing datasets to\ndefine the roles of the role-playing agent. Second, we train a specific\nrole-playing model called ESC-Role which behaves more like a confused person\nthan GPT-4. Third, through ESC-Role and organized role cards, we systematically\nconduct experiments using 14 LLMs as the ESC models, including general\nAI-assistant LLMs (ChatGPT) and ESC-oriented LLMs (ExTES-Llama). We conduct\ncomprehensive human annotations on interactive multi-turn dialogues of\ndifferent ESC models. The results show that ESC-oriented LLMs exhibit superior\nESC abilities compared to general AI-assistant LLMs, but there is still a gap\nbehind human performance. Moreover, to automate the scoring process for future\nESC models, we developed ESC-RANK, which trained on the annotated data,\nachieving a scoring performance surpassing 35 points of GPT-4. Our data and\ncode are available at https://github.com/AIFlames/Esc-Eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotion Support Conversation (ESC) is a crucial application, which aims to\nreduce human stress, offer emotional guidance, and ultimately enhance human\nmental and physical well-being. With the advancement of Large Language Models\n(LLMs), many researchers have employed LLMs as the ESC models. However, the\nevaluation of these LLM-based ESCs remains uncertain. Inspired by the awesome\ndevelopment of role-playing agents, we propose an ESC Evaluation framework\n(ESC-Eval), which uses a role-playing agent to interact with ESC models,\nfollowed by a manual evaluation of the interactive dialogues. In detail, we\nfirst re-organize 2,801 role-playing cards from seven existing datasets to\ndefine the roles of the role-playing agent. Second, we train a specific\nrole-playing model called ESC-Role which behaves more like a confused person\nthan GPT-4. Third, through ESC-Role and organized role cards, we systematically\nconduct experiments using 14 LLMs as the ESC models, including general\nAI-assistant LLMs (ChatGPT) and ESC-oriented LLMs (ExTES-Llama). We conduct\ncomprehensive human annotations on interactive multi-turn dialogues of\ndifferent ESC models. The results show that ESC-oriented LLMs exhibit superior\nESC abilities compared to general AI-assistant LLMs, but there is still a gap\nbehind human performance. Moreover, to automate the scoring process for future\nESC models, we developed ESC-RANK, which trained on the annotated data,\nachieving a scoring performance surpassing 35 points of GPT-4. Our data and\ncode are available at https://github.com/AIFlames/Esc-Eval."
                },
                "authors": [
                    {
                        "name": "Haiquan Zhao"
                    },
                    {
                        "name": "Lingyu Li"
                    },
                    {
                        "name": "Shisong Chen"
                    },
                    {
                        "name": "Shuqi Kong"
                    },
                    {
                        "name": "Jiaan Wang"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Tianle Gu"
                    },
                    {
                        "name": "Yixu Wang"
                    },
                    {
                        "name": "Wang Jian"
                    },
                    {
                        "name": "Dandan Liang"
                    },
                    {
                        "name": "Zhixu Li"
                    },
                    {
                        "name": "Yan Teng"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Yingchun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yingchun Wang"
                },
                "author": "Yingchun Wang",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14952v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14952v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17789v2",
                "updated": "2024-10-28T13:19:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    19,
                    38,
                    0,
                    302,
                    0
                ],
                "published": "2024-07-25T05:50:46Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    5,
                    50,
                    46,
                    3,
                    207,
                    0
                ],
                "title": "Very Large-Scale Multi-Agent Simulation in AgentScope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Very Large-Scale Multi-Agent Simulation in AgentScope"
                },
                "summary": "Recent advances in large language models (LLMs) have opened new avenues for\napplying multi-agent systems in very large-scale simulations. However, there\nremain several challenges when conducting multi-agent simulations with existing\nplatforms, such as limited scalability and low efficiency, unsatisfied agent\ndiversity, and effort-intensive management processes. To address these\nchallenges, we develop several new features and components for AgentScope, a\nuser-friendly multi-agent platform, enhancing its convenience and flexibility\nfor supporting very large-scale multi-agent simulations. Specifically, we\npropose an actor-based distributed mechanism as the underlying technological\ninfrastructure towards great scalability and high efficiency, and provide\nflexible environment support for simulating various real-world scenarios, which\nenables parallel execution of multiple agents, automatic workflow conversion\nfor distributed deployment, and both inter-agent and agent-environment\ninteractions. Moreover, we integrate an easy-to-use configurable tool and an\nautomatic background generation pipeline in AgentScope, simplifying the process\nof creating agents with diverse yet detailed background settings. Last but not\nleast, we provide a web-based interface for conveniently monitoring and\nmanaging a large number of agents that might deploy across multiple devices. We\nconduct a comprehensive simulation to demonstrate the effectiveness of these\nproposed enhancements in AgentScope, and provide detailed observations and\ninsightful discussions to highlight the great potential of applying multi-agent\nsystems in large-scale simulations. The source code is released on GitHub at\nhttps://github.com/modelscope/agentscope/tree/main/examples/paper_large_scale_simulation\nto inspire further research and development in large-scale multi-agent\nsimulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have opened new avenues for\napplying multi-agent systems in very large-scale simulations. However, there\nremain several challenges when conducting multi-agent simulations with existing\nplatforms, such as limited scalability and low efficiency, unsatisfied agent\ndiversity, and effort-intensive management processes. To address these\nchallenges, we develop several new features and components for AgentScope, a\nuser-friendly multi-agent platform, enhancing its convenience and flexibility\nfor supporting very large-scale multi-agent simulations. Specifically, we\npropose an actor-based distributed mechanism as the underlying technological\ninfrastructure towards great scalability and high efficiency, and provide\nflexible environment support for simulating various real-world scenarios, which\nenables parallel execution of multiple agents, automatic workflow conversion\nfor distributed deployment, and both inter-agent and agent-environment\ninteractions. Moreover, we integrate an easy-to-use configurable tool and an\nautomatic background generation pipeline in AgentScope, simplifying the process\nof creating agents with diverse yet detailed background settings. Last but not\nleast, we provide a web-based interface for conveniently monitoring and\nmanaging a large number of agents that might deploy across multiple devices. We\nconduct a comprehensive simulation to demonstrate the effectiveness of these\nproposed enhancements in AgentScope, and provide detailed observations and\ninsightful discussions to highlight the great potential of applying multi-agent\nsystems in large-scale simulations. The source code is released on GitHub at\nhttps://github.com/modelscope/agentscope/tree/main/examples/paper_large_scale_simulation\nto inspire further research and development in large-scale multi-agent\nsimulations."
                },
                "authors": [
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Dawei Gao"
                    },
                    {
                        "name": "Yuexiang Xie"
                    },
                    {
                        "name": "Yushuo Chen"
                    },
                    {
                        "name": "Zhewei Wei"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "We have released code on\n  https://github.com/modelscope/agentscope/tree/main/examples/paper_large_scale_simulation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20976v1",
                "updated": "2024-10-28T12:50:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    50,
                    46,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T12:50:46Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    50,
                    46,
                    0,
                    302,
                    0
                ],
                "title": "Large Language Model-Guided Prediction Toward Quantum Materials\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Guided Prediction Toward Quantum Materials\n  Synthesis"
                },
                "summary": "The synthesis of inorganic crystalline materials is essential for modern\ntechnology, especially in quantum materials development. However, designing\nefficient synthesis workflows remains a significant challenge due to the\nprecise experimental conditions and extensive trial and error. Here, we present\na framework using large language models (LLMs) to predict synthesis pathways\nfor inorganic materials, including quantum materials. Our framework contains\nthree models: LHS2RHS, predicting products from reactants; RHS2LHS, predicting\nreactants from products; and TGT2CEQ, generating full chemical equations for\ntarget compounds. Fine-tuned on a text-mined synthesis database, our model\nraises accuracy from under 40% with pretrained models, to under 80% using\nconventional fine-tuning, and further to around 90% with our proposed\ngeneralized Tanimoto similarity, while maintaining robust to additional\nsynthesis steps. Our model further demonstrates comparable performance across\nmaterials with varying degrees of quantumness quantified using quantum weight,\nindicating that LLMs offer a powerful tool to predict balanced chemical\nequations for quantum materials discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The synthesis of inorganic crystalline materials is essential for modern\ntechnology, especially in quantum materials development. However, designing\nefficient synthesis workflows remains a significant challenge due to the\nprecise experimental conditions and extensive trial and error. Here, we present\na framework using large language models (LLMs) to predict synthesis pathways\nfor inorganic materials, including quantum materials. Our framework contains\nthree models: LHS2RHS, predicting products from reactants; RHS2LHS, predicting\nreactants from products; and TGT2CEQ, generating full chemical equations for\ntarget compounds. Fine-tuned on a text-mined synthesis database, our model\nraises accuracy from under 40% with pretrained models, to under 80% using\nconventional fine-tuning, and further to around 90% with our proposed\ngeneralized Tanimoto similarity, while maintaining robust to additional\nsynthesis steps. Our model further demonstrates comparable performance across\nmaterials with varying degrees of quantumness quantified using quantum weight,\nindicating that LLMs offer a powerful tool to predict balanced chemical\nequations for quantum materials discovery."
                },
                "authors": [
                    {
                        "name": "Ryotaro Okabe"
                    },
                    {
                        "name": "Zack West"
                    },
                    {
                        "name": "Abhijatmedhi Chotrattanapituk"
                    },
                    {
                        "name": "Mouyang Cheng"
                    },
                    {
                        "name": "Denisse Córdova Carrizales"
                    },
                    {
                        "name": "Weiwei Xie"
                    },
                    {
                        "name": "Robert J. Cava"
                    },
                    {
                        "name": "Mingda Li"
                    }
                ],
                "author_detail": {
                    "name": "Mingda Li"
                },
                "author": "Mingda Li",
                "arxiv_comment": "66 pages total, 6 main figures + 3 supplementary figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20975v1",
                "updated": "2024-10-28T12:50:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    50,
                    27,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T12:50:27Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    50,
                    27,
                    0,
                    302,
                    0
                ],
                "title": "Geo-FuB: A Method for Constructing an Operator-Function Knowledge Base\n  for Geospatial Code Generation Tasks Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geo-FuB: A Method for Constructing an Operator-Function Knowledge Base\n  for Geospatial Code Generation Tasks Using Large Language Models"
                },
                "summary": "The rise of spatiotemporal data and the need for efficient geospatial\nmodeling have spurred interest in automating these tasks with large language\nmodels (LLMs). However, general LLMs often generate errors in geospatial code\ndue to a lack of domain-specific knowledge on functions and operators. To\naddress this, a retrieval-augmented generation (RAG) approach, utilizing an\nexternal knowledge base of geospatial functions and operators, is proposed.\nThis study introduces a framework to construct such a knowledge base,\nleveraging geospatial script semantics. The framework includes: Function\nSemantic Framework Construction (Geo-FuSE), Frequent Operator Combination\nStatistics (Geo-FuST), and Semantic Mapping (Geo-FuM). Techniques like\nChain-of-Thought, TF-IDF, and the APRIORI algorithm are utilized to derive and\nalign geospatial functions. An example knowledge base, Geo-FuB, built from\n154,075 Google Earth Engine scripts, is available on GitHub. Evaluation metrics\nshow a high accuracy, reaching 88.89% overall, with structural and semantic\naccuracies of 92.03% and 86.79% respectively. Geo-FuB's potential to optimize\ngeospatial code generation through the RAG and fine-tuning paradigms is\nhighlighted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of spatiotemporal data and the need for efficient geospatial\nmodeling have spurred interest in automating these tasks with large language\nmodels (LLMs). However, general LLMs often generate errors in geospatial code\ndue to a lack of domain-specific knowledge on functions and operators. To\naddress this, a retrieval-augmented generation (RAG) approach, utilizing an\nexternal knowledge base of geospatial functions and operators, is proposed.\nThis study introduces a framework to construct such a knowledge base,\nleveraging geospatial script semantics. The framework includes: Function\nSemantic Framework Construction (Geo-FuSE), Frequent Operator Combination\nStatistics (Geo-FuST), and Semantic Mapping (Geo-FuM). Techniques like\nChain-of-Thought, TF-IDF, and the APRIORI algorithm are utilized to derive and\nalign geospatial functions. An example knowledge base, Geo-FuB, built from\n154,075 Google Earth Engine scripts, is available on GitHub. Evaluation metrics\nshow a high accuracy, reaching 88.89% overall, with structural and semantic\naccuracies of 92.03% and 86.79% respectively. Geo-FuB's potential to optimize\ngeospatial code generation through the RAG and fine-tuning paradigms is\nhighlighted."
                },
                "authors": [
                    {
                        "name": "Shuyang Hou"
                    },
                    {
                        "name": "Anqi Zhao"
                    },
                    {
                        "name": "Jianyuan Liang"
                    },
                    {
                        "name": "Zhangxiao Shen"
                    },
                    {
                        "name": "Huayi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Huayi Wu"
                },
                "author": "Huayi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20971v1",
                "updated": "2024-10-28T12:43:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    43,
                    47,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T12:43:47Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    43,
                    47,
                    0,
                    302,
                    0
                ],
                "title": "BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against\n  Jailbreak Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against\n  Jailbreak Attacks"
                },
                "summary": "Despite their superb multimodal capabilities, Vision-Language Models (VLMs)\nhave been shown to be vulnerable to jailbreak attacks, which are inference-time\nattacks that induce the model to output harmful responses with tricky prompts.\nIt is thus essential to defend VLMs against potential jailbreaks for their\ntrustworthy deployment in real-world applications. In this work, we focus on\nblack-box defense for VLMs against jailbreak attacks. Existing black-box\ndefense methods are either unimodal or bimodal. Unimodal methods enhance either\nthe vision or language module of the VLM, while bimodal methods robustify the\nmodel through text-image representation realignment. However, these methods\nsuffer from two limitations: 1) they fail to fully exploit the cross-modal\ninformation, or 2) they degrade the model performance on benign inputs. To\naddress these limitations, we propose a novel blue-team method BlueSuffix that\ndefends the black-box target VLM against jailbreak attacks without compromising\nits performance. BlueSuffix includes three key components: 1) a visual purifier\nagainst jailbreak images, 2) a textual purifier against jailbreak texts, and 3)\na blue-team suffix generator fine-tuned via reinforcement learning for\nenhancing cross-modal robustness. We empirically show on three VLMs (LLaVA,\nMiniGPT-4, and Gemini) and two safety benchmarks (MM-SafetyBench and\nRedTeam-2K) that BlueSuffix outperforms the baseline defenses by a significant\nmargin. Our BlueSuffix opens up a promising direction for defending VLMs\nagainst jailbreak attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their superb multimodal capabilities, Vision-Language Models (VLMs)\nhave been shown to be vulnerable to jailbreak attacks, which are inference-time\nattacks that induce the model to output harmful responses with tricky prompts.\nIt is thus essential to defend VLMs against potential jailbreaks for their\ntrustworthy deployment in real-world applications. In this work, we focus on\nblack-box defense for VLMs against jailbreak attacks. Existing black-box\ndefense methods are either unimodal or bimodal. Unimodal methods enhance either\nthe vision or language module of the VLM, while bimodal methods robustify the\nmodel through text-image representation realignment. However, these methods\nsuffer from two limitations: 1) they fail to fully exploit the cross-modal\ninformation, or 2) they degrade the model performance on benign inputs. To\naddress these limitations, we propose a novel blue-team method BlueSuffix that\ndefends the black-box target VLM against jailbreak attacks without compromising\nits performance. BlueSuffix includes three key components: 1) a visual purifier\nagainst jailbreak images, 2) a textual purifier against jailbreak texts, and 3)\na blue-team suffix generator fine-tuned via reinforcement learning for\nenhancing cross-modal robustness. We empirically show on three VLMs (LLaVA,\nMiniGPT-4, and Gemini) and two safety benchmarks (MM-SafetyBench and\nRedTeam-2K) that BlueSuffix outperforms the baseline defenses by a significant\nmargin. Our BlueSuffix opens up a promising direction for defending VLMs\nagainst jailbreak attacks."
                },
                "authors": [
                    {
                        "name": "Yunhan Zhao"
                    },
                    {
                        "name": "Xiang Zheng"
                    },
                    {
                        "name": "Lin Luo"
                    },
                    {
                        "name": "Yige Li"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14485v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14485v3",
                "updated": "2024-10-28T12:35:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    35,
                    26,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-18T14:10:16Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    10,
                    16,
                    4,
                    292,
                    0
                ],
                "title": "CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers and\n  Fully-Connected Neural Networks for Causally Constrained Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers and\n  Fully-Connected Neural Networks for Causally Constrained Predictions"
                },
                "summary": "Artificial Neural Networks (ANNs), including fully-connected networks and\ntransformers, are highly flexible and powerful function approximators, widely\napplied in fields like computer vision and natural language processing.\nHowever, their inability to inherently respect causal structures can limit\ntheir robustness, making them vulnerable to covariate shift and difficult to\ninterpret/explain. This poses significant challenges for their reliability in\nreal-world applications. In this paper, we introduce Causal Fully-Connected\nNeural Networks (CFCNs) and Causal Transformers (CaTs), two general model\nfamilies designed to operate under predefined causal constraints, as specified\nby a Directed Acyclic Graph (DAG). These models retain the powerful function\napproximation abilities of traditional neural networks while adhering to the\nunderlying structural constraints, improving robustness, reliability, and\ninterpretability at inference time. This approach opens new avenues for\ndeploying neural networks in more demanding, real-world scenarios where\nrobustness and explainability is critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Neural Networks (ANNs), including fully-connected networks and\ntransformers, are highly flexible and powerful function approximators, widely\napplied in fields like computer vision and natural language processing.\nHowever, their inability to inherently respect causal structures can limit\ntheir robustness, making them vulnerable to covariate shift and difficult to\ninterpret/explain. This poses significant challenges for their reliability in\nreal-world applications. In this paper, we introduce Causal Fully-Connected\nNeural Networks (CFCNs) and Causal Transformers (CaTs), two general model\nfamilies designed to operate under predefined causal constraints, as specified\nby a Directed Acyclic Graph (DAG). These models retain the powerful function\napproximation abilities of traditional neural networks while adhering to the\nunderlying structural constraints, improving robustness, reliability, and\ninterpretability at inference time. This approach opens new avenues for\ndeploying neural networks in more demanding, real-world scenarios where\nrobustness and explainability is critical."
                },
                "authors": [
                    {
                        "name": "Matthew J. Vowels"
                    },
                    {
                        "name": "Mathieu Rochat"
                    },
                    {
                        "name": "Sina Akbari"
                    }
                ],
                "author_detail": {
                    "name": "Sina Akbari"
                },
                "author": "Sina Akbari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14485v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14485v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20964v1",
                "updated": "2024-10-28T12:34:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    34,
                    49,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T12:34:49Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    34,
                    49,
                    0,
                    302,
                    0
                ],
                "title": "DeTeCtive: Detecting AI-generated Text via Multi-Level Contrastive\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeTeCtive: Detecting AI-generated Text via Multi-Level Contrastive\n  Learning"
                },
                "summary": "Current techniques for detecting AI-generated text are largely confined to\nmanual feature crafting and supervised binary classification paradigms. These\nmethodologies typically lead to performance bottlenecks and unsatisfactory\ngeneralizability. Consequently, these methods are often inapplicable for\nout-of-distribution (OOD) data and newly emerged large language models (LLMs).\nIn this paper, we revisit the task of AI-generated text detection. We argue\nthat the key to accomplishing this task lies in distinguishing writing styles\nof different authors, rather than simply classifying the text into\nhuman-written or AI-generated text. To this end, we propose DeTeCtive, a\nmulti-task auxiliary, multi-level contrastive learning framework. DeTeCtive is\ndesigned to facilitate the learning of distinct writing styles, combined with a\ndense information retrieval pipeline for AI-generated text detection. Our\nmethod is compatible with a range of text encoders. Extensive experiments\ndemonstrate that our method enhances the ability of various text encoders in\ndetecting AI-generated text across multiple benchmarks and achieves\nstate-of-the-art results. Notably, in OOD zero-shot evaluation, our method\noutperforms existing approaches by a large margin. Moreover, we find our method\nboasts a Training-Free Incremental Adaptation (TFIA) capability towards OOD\ndata, further enhancing its efficacy in OOD detection scenarios. We will\nopen-source our code and models in hopes that our work will spark new thoughts\nin the field of AI-generated text detection, ensuring safe application of LLMs\nand enhancing compliance. Our code is available at\nhttps://github.com/heyongxin233/DeTeCtive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current techniques for detecting AI-generated text are largely confined to\nmanual feature crafting and supervised binary classification paradigms. These\nmethodologies typically lead to performance bottlenecks and unsatisfactory\ngeneralizability. Consequently, these methods are often inapplicable for\nout-of-distribution (OOD) data and newly emerged large language models (LLMs).\nIn this paper, we revisit the task of AI-generated text detection. We argue\nthat the key to accomplishing this task lies in distinguishing writing styles\nof different authors, rather than simply classifying the text into\nhuman-written or AI-generated text. To this end, we propose DeTeCtive, a\nmulti-task auxiliary, multi-level contrastive learning framework. DeTeCtive is\ndesigned to facilitate the learning of distinct writing styles, combined with a\ndense information retrieval pipeline for AI-generated text detection. Our\nmethod is compatible with a range of text encoders. Extensive experiments\ndemonstrate that our method enhances the ability of various text encoders in\ndetecting AI-generated text across multiple benchmarks and achieves\nstate-of-the-art results. Notably, in OOD zero-shot evaluation, our method\noutperforms existing approaches by a large margin. Moreover, we find our method\nboasts a Training-Free Incremental Adaptation (TFIA) capability towards OOD\ndata, further enhancing its efficacy in OOD detection scenarios. We will\nopen-source our code and models in hopes that our work will spark new thoughts\nin the field of AI-generated text detection, ensuring safe application of LLMs\nand enhancing compliance. Our code is available at\nhttps://github.com/heyongxin233/DeTeCtive."
                },
                "authors": [
                    {
                        "name": "Xun Guo"
                    },
                    {
                        "name": "Shan Zhang"
                    },
                    {
                        "name": "Yongxin He"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Wanquan Feng"
                    },
                    {
                        "name": "Haibin Huang"
                    },
                    {
                        "name": "Chongyang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chongyang Ma"
                },
                "author": "Chongyang Ma",
                "arxiv_comment": "To appear in NeurIPS 2024. Code is available at\n  https://github.com/heyongxin233/DeTeCtive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02707v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02707v3",
                "updated": "2024-10-28T12:33:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    33,
                    44,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-03T17:31:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    31,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations"
                },
                "summary": "Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation."
                },
                "authors": [
                    {
                        "name": "Hadas Orgad"
                    },
                    {
                        "name": "Michael Toker"
                    },
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02707v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02707v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19548v2",
                "updated": "2024-10-28T12:22:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    22,
                    8,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-25T13:20:40Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    13,
                    20,
                    40,
                    4,
                    299,
                    0
                ],
                "title": "FLiP: Privacy-Preserving Federated Learning based on the Principle of\n  Least Privileg",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLiP: Privacy-Preserving Federated Learning based on the Principle of\n  Least Privileg"
                },
                "summary": "Federated Learning (FL) allows users to share knowledge instead of raw data\nto train a model with high accuracy. Unfortunately, during the training, users\nlose control over the knowledge shared, which causes serious data privacy\nissues. We hold that users are only willing and need to share the essential\nknowledge to the training task to obtain the FL model with high accuracy.\nHowever, existing efforts cannot help users minimize the shared knowledge\naccording to the user intention in the FL training procedure. This work\nproposes FLiP, which aims to bring the principle of least privilege (PoLP) to\nFL training. The key design of FLiP is applying elaborate information reduction\non the training data through a local-global dataset distillation design. We\nmeasure the privacy performance through attribute inference and membership\ninference attacks. Extensive experiments show that FLiP strikes a good balance\nbetween model accuracy and privacy protection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows users to share knowledge instead of raw data\nto train a model with high accuracy. Unfortunately, during the training, users\nlose control over the knowledge shared, which causes serious data privacy\nissues. We hold that users are only willing and need to share the essential\nknowledge to the training task to obtain the FL model with high accuracy.\nHowever, existing efforts cannot help users minimize the shared knowledge\naccording to the user intention in the FL training procedure. This work\nproposes FLiP, which aims to bring the principle of least privilege (PoLP) to\nFL training. The key design of FLiP is applying elaborate information reduction\non the training data through a local-global dataset distillation design. We\nmeasure the privacy performance through attribute inference and membership\ninference attacks. Extensive experiments show that FLiP strikes a good balance\nbetween model accuracy and privacy protection."
                },
                "authors": [
                    {
                        "name": "ShiMao Xu"
                    },
                    {
                        "name": "Xiaopeng Ke"
                    },
                    {
                        "name": "Xing Su"
                    },
                    {
                        "name": "Shucheng Li"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Sheng Zhong"
                    },
                    {
                        "name": "Fengyuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Fengyuan Xu"
                },
                "author": "Fengyuan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20957v1",
                "updated": "2024-10-28T12:18:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    18,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T12:18:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    18,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "Neuro-symbolic Learning Yielding Logical Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-symbolic Learning Yielding Logical Constraints"
                },
                "summary": "Neuro-symbolic systems combine the abilities of neural perception and logical\nreasoning. However, end-to-end learning of neuro-symbolic systems is still an\nunsolved challenge. This paper proposes a natural framework that fuses neural\nnetwork training, symbol grounding, and logical constraint synthesis into a\ncoherent and efficient end-to-end learning process. The capability of this\nframework comes from the improved interactions between the neural and the\nsymbolic parts of the system in both the training and inference stages.\nTechnically, to bridge the gap between the continuous neural network and the\ndiscrete logical constraint, we introduce a difference-of-convex programming\ntechnique to relax the logical constraints while maintaining their precision.\nWe also employ cardinality constraints as the language for logical constraint\nlearning and incorporate a trust region method to avoid the degeneracy of\nlogical constraint in learning. Both theoretical analyses and empirical\nevaluations substantiate the effectiveness of the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-symbolic systems combine the abilities of neural perception and logical\nreasoning. However, end-to-end learning of neuro-symbolic systems is still an\nunsolved challenge. This paper proposes a natural framework that fuses neural\nnetwork training, symbol grounding, and logical constraint synthesis into a\ncoherent and efficient end-to-end learning process. The capability of this\nframework comes from the improved interactions between the neural and the\nsymbolic parts of the system in both the training and inference stages.\nTechnically, to bridge the gap between the continuous neural network and the\ndiscrete logical constraint, we introduce a difference-of-convex programming\ntechnique to relax the logical constraints while maintaining their precision.\nWe also employ cardinality constraints as the language for logical constraint\nlearning and incorporate a trust region method to avoid the degeneracy of\nlogical constraint in learning. Both theoretical analyses and empirical\nevaluations substantiate the effectiveness of the proposed framework."
                },
                "authors": [
                    {
                        "name": "Zenan Li"
                    },
                    {
                        "name": "Yunpeng Huang"
                    },
                    {
                        "name": "Zhaoyu Li"
                    },
                    {
                        "name": "Yuan Yao"
                    },
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Taolue Chen"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    },
                    {
                        "name": "Jian Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jian Lu"
                },
                "author": "Jian Lu",
                "arxiv_comment": "Published as a conference paper at NeurIPS 2023, and code is\n  available at [this url](https://github.com/Lizn-zn/Nesy-Programming)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09824v2",
                "updated": "2024-10-28T12:05:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    5,
                    8,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-13T12:57:08Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    12,
                    57,
                    8,
                    6,
                    287,
                    0
                ],
                "title": "Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent\n  Simulation"
                },
                "summary": "Graph generation is a fundamental task that has been extensively studied in\nsocial, technological, and scientific analysis. For modeling the dynamic graph\nevolution process, traditional rule-based methods struggle to capture community\nstructures within graphs, while deep learning methods only focus on fitting\ntraining graphs. This limits existing graph generators to producing graphs that\nadhere to predefined rules or closely resemble training datasets, achieving\npoor performance in dynamic graph generation. Given that graphs are abstract\nrepresentations arising from pairwise interactions in human activities, a\nrealistic simulation of human-wise interaction could provide deeper insights\ninto the graph evolution mechanism. With the increasing recognition of large\nlanguage models (LLMs) in simulating human behavior, we introduce\nGraphAgent-Generator (GAG), a novel simulation-based framework for dynamic\ngraph generation. Without training or fine-tuning process of LLM, our framework\neffectively replicates seven macro-level structural characteristics in\nestablished network science theories while surpassing existing baselines in\ngraph expansion tasks by 31\\% on specific evaluation metrics. Through node\nclassification task, we validate GAG effectively preserves characteristics of\nreal-world network for node-wise textual features in generated text-rich graph.\nFurthermore, by incorporating parallel acceleration, GAG supports generating\ngraphs with up to nearly 100,000 nodes or 10 million edges through large-scale\nLLM-based agent simulation, with a minimum speed-up of 90.4\\%. The source code\nis available at https://anonymous.4open.science/r/GraphAgent-2206.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph generation is a fundamental task that has been extensively studied in\nsocial, technological, and scientific analysis. For modeling the dynamic graph\nevolution process, traditional rule-based methods struggle to capture community\nstructures within graphs, while deep learning methods only focus on fitting\ntraining graphs. This limits existing graph generators to producing graphs that\nadhere to predefined rules or closely resemble training datasets, achieving\npoor performance in dynamic graph generation. Given that graphs are abstract\nrepresentations arising from pairwise interactions in human activities, a\nrealistic simulation of human-wise interaction could provide deeper insights\ninto the graph evolution mechanism. With the increasing recognition of large\nlanguage models (LLMs) in simulating human behavior, we introduce\nGraphAgent-Generator (GAG), a novel simulation-based framework for dynamic\ngraph generation. Without training or fine-tuning process of LLM, our framework\neffectively replicates seven macro-level structural characteristics in\nestablished network science theories while surpassing existing baselines in\ngraph expansion tasks by 31\\% on specific evaluation metrics. Through node\nclassification task, we validate GAG effectively preserves characteristics of\nreal-world network for node-wise textual features in generated text-rich graph.\nFurthermore, by incorporating parallel acceleration, GAG supports generating\ngraphs with up to nearly 100,000 nodes or 10 million edges through large-scale\nLLM-based agent simulation, with a minimum speed-up of 90.4\\%. The source code\nis available at https://anonymous.4open.science/r/GraphAgent-2206."
                },
                "authors": [
                    {
                        "name": "Jiarui Ji"
                    },
                    {
                        "name": "Runlin Lei"
                    },
                    {
                        "name": "Jialing Bi"
                    },
                    {
                        "name": "Zhewei Wei"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    }
                ],
                "author_detail": {
                    "name": "Bolin Ding"
                },
                "author": "Bolin Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20941v1",
                "updated": "2024-10-28T11:49:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    49,
                    58,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T11:49:58Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    49,
                    58,
                    0,
                    302,
                    0
                ],
                "title": "Instruction-Tuned LLMs Succeed in Document-Level MT Without Fine-Tuning\n  -- But BLEU Turns a Blind Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-Tuned LLMs Succeed in Document-Level MT Without Fine-Tuning\n  -- But BLEU Turns a Blind Eye"
                },
                "summary": "Large language models (LLMs) have excelled in various NLP tasks, including\nmachine translation (MT), yet most studies focus on sentence-level translation.\nThis work investigates the inherent capability of instruction-tuned LLMs for\ndocument-level translation (docMT). Unlike prior approaches that require\nspecialized techniques, we evaluate LLMs by directly prompting them to\ntranslate entire documents in a single pass. Our results show that this method\nimproves translation quality compared to translating sentences separately, even\nwithout document-level fine-tuning. However, this advantage is not reflected in\nBLEU scores, which often favor sentence-based translations. We propose using\nthe LLM-as-a-judge paradigm for evaluation, where GPT-4 is used to assess\ndocument coherence, accuracy, and fluency in a more nuanced way than\nn-gram-based metrics. Overall, our work demonstrates that instruction-tuned\nLLMs can effectively leverage document context for translation. However, we\ncaution against using BLEU scores for evaluating docMT, as they often provide\nmisleading outcomes, failing to capture the quality of document-level\ntranslation. Code and data are available at\nhttps://github.com/EIT-NLP/BLEUless_DocMT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various NLP tasks, including\nmachine translation (MT), yet most studies focus on sentence-level translation.\nThis work investigates the inherent capability of instruction-tuned LLMs for\ndocument-level translation (docMT). Unlike prior approaches that require\nspecialized techniques, we evaluate LLMs by directly prompting them to\ntranslate entire documents in a single pass. Our results show that this method\nimproves translation quality compared to translating sentences separately, even\nwithout document-level fine-tuning. However, this advantage is not reflected in\nBLEU scores, which often favor sentence-based translations. We propose using\nthe LLM-as-a-judge paradigm for evaluation, where GPT-4 is used to assess\ndocument coherence, accuracy, and fluency in a more nuanced way than\nn-gram-based metrics. Overall, our work demonstrates that instruction-tuned\nLLMs can effectively leverage document context for translation. However, we\ncaution against using BLEU scores for evaluating docMT, as they often provide\nmisleading outcomes, failing to capture the quality of document-level\ntranslation. Code and data are available at\nhttps://github.com/EIT-NLP/BLEUless_DocMT"
                },
                "authors": [
                    {
                        "name": "Yirong Sun"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yanjun Chen"
                    },
                    {
                        "name": "Erjia Xiao"
                    },
                    {
                        "name": "Xinghao Chen"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20936v1",
                "updated": "2024-10-28T11:37:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    37,
                    39,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T11:37:39Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    37,
                    39,
                    0,
                    302,
                    0
                ],
                "title": "Autoformalize Mathematical Statements by Symbolic Equivalence and\n  Semantic Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalize Mathematical Statements by Symbolic Equivalence and\n  Semantic Consistency"
                },
                "summary": "Autoformalization, the task of automatically translating natural language\ndescriptions into a formal language, poses a significant challenge across\nvarious domains, especially in mathematics. Recent advancements in large\nlanguage models (LLMs) have unveiled their promising capabilities to formalize\neven competition-level math problems. However, we observe a considerable\ndiscrepancy between pass@1 and pass@k accuracies in LLM-generated\nformalizations. To address this gap, we introduce a novel framework that scores\nand selects the best result from k autoformalization candidates based on two\ncomplementary self-consistency methods: symbolic equivalence and semantic\nconsistency. Elaborately, symbolic equivalence identifies the logical\nhomogeneity among autoformalization candidates using automated theorem provers,\nand semantic consistency evaluates the preservation of the original meaning by\ninformalizing the candidates and computing the similarity between the\nembeddings of the original and informalized texts. Our extensive experiments on\nthe MATH and miniF2F datasets demonstrate that our approach significantly\nenhances autoformalization accuracy, achieving up to 0.22-1.35x relative\nimprovements across various LLMs and baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization, the task of automatically translating natural language\ndescriptions into a formal language, poses a significant challenge across\nvarious domains, especially in mathematics. Recent advancements in large\nlanguage models (LLMs) have unveiled their promising capabilities to formalize\neven competition-level math problems. However, we observe a considerable\ndiscrepancy between pass@1 and pass@k accuracies in LLM-generated\nformalizations. To address this gap, we introduce a novel framework that scores\nand selects the best result from k autoformalization candidates based on two\ncomplementary self-consistency methods: symbolic equivalence and semantic\nconsistency. Elaborately, symbolic equivalence identifies the logical\nhomogeneity among autoformalization candidates using automated theorem provers,\nand semantic consistency evaluates the preservation of the original meaning by\ninformalizing the candidates and computing the similarity between the\nembeddings of the original and informalized texts. Our extensive experiments on\nthe MATH and miniF2F datasets demonstrate that our approach significantly\nenhances autoformalization accuracy, achieving up to 0.22-1.35x relative\nimprovements across various LLMs and baseline methods."
                },
                "authors": [
                    {
                        "name": "Zenan Li"
                    },
                    {
                        "name": "Yifan Wu"
                    },
                    {
                        "name": "Zhaoyu Li"
                    },
                    {
                        "name": "Xinming Wei"
                    },
                    {
                        "name": "Xian Zhang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxing Ma"
                },
                "author": "Xiaoxing Ma",
                "arxiv_comment": "Published as a conference paper at NeurIPS 2024. Code is available at\n  [this https URL](https://github.com/Miracle-Messi/Isa-AutoFormal)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18175v2",
                "updated": "2024-10-28T11:14:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    14,
                    10,
                    0,
                    302,
                    0
                ],
                "published": "2024-06-26T08:49:51Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    8,
                    49,
                    51,
                    2,
                    178,
                    0
                ],
                "title": "Galaxy spectroscopy without spectra: Galaxy properties from photometric\n  images with conditional diffusion models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxy spectroscopy without spectra: Galaxy properties from photometric\n  images with conditional diffusion models"
                },
                "summary": "Modern spectroscopic surveys can only target a small fraction of the vast\namount of photometrically cataloged sources in wide-field surveys. Here, we\nreport the development of a generative AI method capable of predicting optical\ngalaxy spectra from photometric broad-band images alone. This method draws from\nthe latest advances in diffusion models in combination with contrastive\nnetworks. We pass multi-band galaxy images into the architecture to obtain\noptical spectra. From these, robust values for galaxy properties can be derived\nwith any methods in the spectroscopic toolbox, such as standard population\nsynthesis techniques and Lick indices. When trained and tested on 64x64-pixel\nimages from the Sloan Digital Sky Survey, the global bimodality of star-forming\nand quiescent galaxies in photometric space is recovered, as well as a\nmass-metallicity relation of star-forming galaxies. The comparison between the\nobserved and the artificially created spectra shows good agreement in overall\nmetallicity, age, Dn4000, stellar velocity dispersion, and E(B-V) values.\nPhotometric redshift estimates of our generative algorithm can compete with\nother current, specialized deep-learning techniques. Moreover, this work is the\nfirst attempt in the literature to infer velocity dispersion from photometric\nimages. Additionally, we can predict the presence of an active galactic nucleus\nup to an accuracy of 82%. With our method, scientifically interesting galaxy\nproperties, normally requiring spectroscopic inputs, can be obtained in future\ndata sets from large-scale photometric surveys alone. The spectra prediction\nvia AI can further assist in creating realistic mock catalogs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern spectroscopic surveys can only target a small fraction of the vast\namount of photometrically cataloged sources in wide-field surveys. Here, we\nreport the development of a generative AI method capable of predicting optical\ngalaxy spectra from photometric broad-band images alone. This method draws from\nthe latest advances in diffusion models in combination with contrastive\nnetworks. We pass multi-band galaxy images into the architecture to obtain\noptical spectra. From these, robust values for galaxy properties can be derived\nwith any methods in the spectroscopic toolbox, such as standard population\nsynthesis techniques and Lick indices. When trained and tested on 64x64-pixel\nimages from the Sloan Digital Sky Survey, the global bimodality of star-forming\nand quiescent galaxies in photometric space is recovered, as well as a\nmass-metallicity relation of star-forming galaxies. The comparison between the\nobserved and the artificially created spectra shows good agreement in overall\nmetallicity, age, Dn4000, stellar velocity dispersion, and E(B-V) values.\nPhotometric redshift estimates of our generative algorithm can compete with\nother current, specialized deep-learning techniques. Moreover, this work is the\nfirst attempt in the literature to infer velocity dispersion from photometric\nimages. Additionally, we can predict the presence of an active galactic nucleus\nup to an accuracy of 82%. With our method, scientifically interesting galaxy\nproperties, normally requiring spectroscopic inputs, can be obtained in future\ndata sets from large-scale photometric surveys alone. The spectra prediction\nvia AI can further assist in creating realistic mock catalogs."
                },
                "authors": [
                    {
                        "name": "Lars Doorenbos"
                    },
                    {
                        "name": "Eva Sextl"
                    },
                    {
                        "name": "Kevin Heng"
                    },
                    {
                        "name": "Stefano Cavuoti"
                    },
                    {
                        "name": "Massimo Brescia"
                    },
                    {
                        "name": "Olena Torbaniuk"
                    },
                    {
                        "name": "Giuseppe Longo"
                    },
                    {
                        "name": "Raphael Sznitman"
                    },
                    {
                        "name": "Pablo Márquez-Neila"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Márquez-Neila"
                },
                "author": "Pablo Márquez-Neila",
                "arxiv_comment": "Accepted by The Astrophysical Journal. Code is available at\n  https://github.com/LarsDoorenbos/generate-spectra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19014v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19014v4",
                "updated": "2024-10-28T11:11:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    11,
                    4,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-24T01:40:50Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    1,
                    40,
                    50,
                    1,
                    268,
                    0
                ],
                "title": "FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark"
                },
                "summary": "Text-to-SQL systems have become crucial for translating natural language into\nSQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, the Execution Accuracy\n(EX), the most prevalent evaluation metric, still shows many false positives\nand negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our metric improves\nagreement with human experts (from 62 to 87.04 in Cohen's kappa) with\ncomprehensive context and sophisticated criteria. Our extensive experiments\nyield several key insights: (1) Models' performance increases by over 2.6\npoints on average, substantially affecting rankings on Spider and BIRD\nbenchmarks; (2) The underestimation of models in EX primarily stems from\nannotation quality issues; and (3) Model performance on particularly\nchallenging questions tends to be overestimated. This work contributes to a\nmore accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL systems have become crucial for translating natural language into\nSQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, the Execution Accuracy\n(EX), the most prevalent evaluation metric, still shows many false positives\nand negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our metric improves\nagreement with human experts (from 62 to 87.04 in Cohen's kappa) with\ncomprehensive context and sophisticated criteria. Our extensive experiments\nyield several key insights: (1) Models' performance increases by over 2.6\npoints on average, substantially affecting rankings on Spider and BIRD\nbenchmarks; (2) The underestimation of models in EX primarily stems from\nannotation quality issues; and (3) Model performance on particularly\nchallenging questions tends to be overestimated. This work contributes to a\nmore accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field."
                },
                "authors": [
                    {
                        "name": "Heegyu Kim"
                    },
                    {
                        "name": "Taeyang Jeon"
                    },
                    {
                        "name": "Seunghwan Choi"
                    },
                    {
                        "name": "Seungtaek Choi"
                    },
                    {
                        "name": "Hyunsouk Cho"
                    }
                ],
                "author_detail": {
                    "name": "Hyunsouk Cho"
                },
                "author": "Hyunsouk Cho",
                "arxiv_comment": "preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19014v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19014v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20926v1",
                "updated": "2024-10-28T11:08:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    8,
                    57,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T11:08:57Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    8,
                    57,
                    0,
                    302,
                    0
                ],
                "title": "Long Sequence Modeling with Attention Tensorization: From Sequence to\n  Tensor Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Sequence Modeling with Attention Tensorization: From Sequence to\n  Tensor Learning"
                },
                "summary": "As the demand for processing extended textual data grows, the ability to\nhandle long-range dependencies and maintain computational efficiency is more\ncritical than ever. One of the key issues for long-sequence modeling using\nattention-based model is the mismatch between the limited-range modeling power\nof full attention and the long-range token dependency in the input sequence. In\nthis work, we propose to scale up the attention receptive field by tensorizing\nlong input sequences into compact tensor representations followed by attention\non each transformed dimension. The resulting Tensorized Attention can be\nadopted as efficient transformer backbones to extend input context length with\nimproved memory and time efficiency. We show that the proposed attention\ntensorization encodes token dependencies as a multi-hop attention process, and\nis equivalent to Kronecker decomposition of full attention. Extensive\nexperiments show that tensorized attention can be used to adapt pretrained LLMs\nwith improved efficiency. Notably, Llama-8B with tensorization is trained under\n32,768 context length and can steadily extrapolate to 128k length during\ninference with $11\\times$ speedup, compared to full attention with\nFlashAttention-2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for processing extended textual data grows, the ability to\nhandle long-range dependencies and maintain computational efficiency is more\ncritical than ever. One of the key issues for long-sequence modeling using\nattention-based model is the mismatch between the limited-range modeling power\nof full attention and the long-range token dependency in the input sequence. In\nthis work, we propose to scale up the attention receptive field by tensorizing\nlong input sequences into compact tensor representations followed by attention\non each transformed dimension. The resulting Tensorized Attention can be\nadopted as efficient transformer backbones to extend input context length with\nimproved memory and time efficiency. We show that the proposed attention\ntensorization encodes token dependencies as a multi-hop attention process, and\nis equivalent to Kronecker decomposition of full attention. Extensive\nexperiments show that tensorized attention can be used to adapt pretrained LLMs\nwith improved efficiency. Notably, Llama-8B with tensorization is trained under\n32,768 context length and can steadily extrapolate to 128k length during\ninference with $11\\times$ speedup, compared to full attention with\nFlashAttention-2."
                },
                "authors": [
                    {
                        "name": "Aosong Feng"
                    },
                    {
                        "name": "Rex Ying"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    }
                ],
                "author_detail": {
                    "name": "Leandros Tassiulas"
                },
                "author": "Leandros Tassiulas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20911v1",
                "updated": "2024-10-28T10:43:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    43,
                    34,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T10:43:34Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    43,
                    34,
                    0,
                    302,
                    0
                ],
                "title": "Hacking Back the AI-Hacker: Prompt Injection as a Defense Against\n  LLM-driven Cyberattacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hacking Back the AI-Hacker: Prompt Injection as a Defense Against\n  LLM-driven Cyberattacks"
                },
                "summary": "Large language models (LLMs) are increasingly being harnessed to automate\ncyberattacks, making sophisticated exploits more accessible and scalable. In\nresponse, we propose a new defense strategy tailored to counter LLM-driven\ncyberattacks. We introduce Mantis, a defensive framework that exploits LLMs'\nsusceptibility to adversarial inputs to undermine malicious operations. Upon\ndetecting an automated cyberattack, Mantis plants carefully crafted inputs into\nsystem responses, leading the attacker's LLM to disrupt their own operations\n(passive defense) or even compromise the attacker's machine (active defense).\nBy deploying purposefully vulnerable decoy services to attract the attacker and\nusing dynamic prompt injections for the attacker's LLM, Mantis can autonomously\nhack back the attacker. In our experiments, Mantis consistently achieved over\n95% effectiveness against automated LLM-driven attacks. To foster further\nresearch and collaboration, Mantis is available as an open-source tool:\nhttps://github.com/pasquini-dario/project_mantis",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being harnessed to automate\ncyberattacks, making sophisticated exploits more accessible and scalable. In\nresponse, we propose a new defense strategy tailored to counter LLM-driven\ncyberattacks. We introduce Mantis, a defensive framework that exploits LLMs'\nsusceptibility to adversarial inputs to undermine malicious operations. Upon\ndetecting an automated cyberattack, Mantis plants carefully crafted inputs into\nsystem responses, leading the attacker's LLM to disrupt their own operations\n(passive defense) or even compromise the attacker's machine (active defense).\nBy deploying purposefully vulnerable decoy services to attract the attacker and\nusing dynamic prompt injections for the attacker's LLM, Mantis can autonomously\nhack back the attacker. In our experiments, Mantis consistently achieved over\n95% effectiveness against automated LLM-driven attacks. To foster further\nresearch and collaboration, Mantis is available as an open-source tool:\nhttps://github.com/pasquini-dario/project_mantis"
                },
                "authors": [
                    {
                        "name": "Dario Pasquini"
                    },
                    {
                        "name": "Evgenios M. Kornaropoulos"
                    },
                    {
                        "name": "Giuseppe Ateniese"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Ateniese"
                },
                "author": "Giuseppe Ateniese",
                "arxiv_comment": "v0.1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.16984v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.16984v5",
                "updated": "2024-10-28T10:39:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    39,
                    45,
                    0,
                    302,
                    0
                ],
                "published": "2023-11-28T17:35:38Z",
                "published_parsed": [
                    2023,
                    11,
                    28,
                    17,
                    35,
                    38,
                    1,
                    332,
                    0
                ],
                "title": "FedECA: A Federated External Control Arm Method for Causal Inference\n  with Time-To-Event Data in Distributed Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedECA: A Federated External Control Arm Method for Causal Inference\n  with Time-To-Event Data in Distributed Settings"
                },
                "summary": "External control arms (ECA) can inform the early clinical development of\nexperimental drugs and provide efficacy evidence for regulatory approval.\nHowever, the main challenge in implementing ECA lies in accessing real-world or\nhistorical clinical trials data. Indeed, regulations protecting patients'\nrights by strictly controlling data processing make pooling data from multiple\nsources in a central server often difficult. To address these limitations, we\ndevelop a new method, 'FedECA' that leverages federated learning (FL) to enable\ninverse probability of treatment weighting (IPTW) for time-to-event outcomes on\nseparate cohorts without needing to pool data. To showcase the potential of\nFedECA, we apply it in different settings of increasing complexity culminating\nwith a real-world use-case in which FedECA provides evidence for a differential\neffect between two drugs that would have otherwise gone unnoticed. By sharing\nour code, we hope FedECA will foster the creation of federated research\nnetworks and thus accelerate drug development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "External control arms (ECA) can inform the early clinical development of\nexperimental drugs and provide efficacy evidence for regulatory approval.\nHowever, the main challenge in implementing ECA lies in accessing real-world or\nhistorical clinical trials data. Indeed, regulations protecting patients'\nrights by strictly controlling data processing make pooling data from multiple\nsources in a central server often difficult. To address these limitations, we\ndevelop a new method, 'FedECA' that leverages federated learning (FL) to enable\ninverse probability of treatment weighting (IPTW) for time-to-event outcomes on\nseparate cohorts without needing to pool data. To showcase the potential of\nFedECA, we apply it in different settings of increasing complexity culminating\nwith a real-world use-case in which FedECA provides evidence for a differential\neffect between two drugs that would have otherwise gone unnoticed. By sharing\nour code, we hope FedECA will foster the creation of federated research\nnetworks and thus accelerate drug development."
                },
                "authors": [
                    {
                        "name": "Jean Ogier du Terrail"
                    },
                    {
                        "name": "Quentin Klopfenstein"
                    },
                    {
                        "name": "Honghao Li"
                    },
                    {
                        "name": "Imke Mayer"
                    },
                    {
                        "name": "Nicolas Loiseau"
                    },
                    {
                        "name": "Mohammad Hallal"
                    },
                    {
                        "name": "Michael Debouver"
                    },
                    {
                        "name": "Thibault Camalon"
                    },
                    {
                        "name": "Thibault Fouqueray"
                    },
                    {
                        "name": "Jorge Arellano Castro"
                    },
                    {
                        "name": "Zahia Yanes"
                    },
                    {
                        "name": "Laetitia Dahan"
                    },
                    {
                        "name": "Julien Taïeb"
                    },
                    {
                        "name": "Pierre Laurent-Puig"
                    },
                    {
                        "name": "Jean-Baptiste Bachet"
                    },
                    {
                        "name": "Shulin Zhao"
                    },
                    {
                        "name": "Remy Nicolle"
                    },
                    {
                        "name": "Jérome Cros"
                    },
                    {
                        "name": "Daniel Gonzalez"
                    },
                    {
                        "name": "Robert Carreras-Torres"
                    },
                    {
                        "name": "Adelaida Garcia Velasco"
                    },
                    {
                        "name": "Kawther Abdilleh"
                    },
                    {
                        "name": "Sudheer Doss"
                    },
                    {
                        "name": "Félix Balazard"
                    },
                    {
                        "name": "Mathieu Andreux"
                    }
                ],
                "author_detail": {
                    "name": "Mathieu Andreux"
                },
                "author": "Mathieu Andreux",
                "arxiv_comment": "code available at: https://github.com/owkin/fedeca, bug in SMD\n  computation present in v1 and v2 has been fixed, many experiments on real\n  data have been added + fix in YODA experiments using imputed data instead of\n  raw data (v3->v4) as well as affiliations fix + more precise wording for\n  acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.16984v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.16984v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20902v1",
                "updated": "2024-10-28T10:31:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    31,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T10:31:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    31,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "K-step Vector Approximate Survey Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "K-step Vector Approximate Survey Propagation"
                },
                "summary": "Approximate Message Passing (AMP), originally developed to address\nhigh-dimensional linear inverse problems, has found widespread applications in\nsignal processing and statistical inference. Among its notable variants, Vector\nApproximate Message Passing (VAMP), Generalized Approximate Survey Propagation\n(GASP), and Vector Approximate Survey Propagation (VASP) have demonstrated\neffectiveness even when the assumed generative models differ from the true\nmodels. However, many fundamental questions regarding model mismatch remain\nunanswered. For instance, it is still unclear what level of model mismatch is\nrequired for the postulated posterior estimate (PPE) to exhibit a replica\nsymmetry breaking (RSB) structure in the extremum conditions of its free\nenergy, and what order of RSB is necessary. In this paper, we introduce a novel\napproximate message passing algorithm that incorporates K-step RSB (KRSB) and\nnaturally reduces to VAMP and VASP with specific parameter selections. We refer\nto this as the K-step VASP (KVASP) algorithm. Simulations show that KVASP\nsignificantly outperforms VAMP and GASP in estimation accuracy, particularly\nwhen the assumed prior has discrete support and the measurement matrix is\nnon-i.i.d.. Additionally, the state evolution (SE) of KVASP, derived\nheuristically, accurately tracks the per-iteration mean squared error (MSE). A\ncomparison between the SE and the free energy under the KRSB ansatz reveals\nthat the fixed-point equations of SE align with the saddle-point equations of\nthe free energy. This suggests that, once the KRSB ansatz holds and the SE\nfixed point is reached, KVASP can accurately compute the PPE in the large\nsystem limit (LSL).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Message Passing (AMP), originally developed to address\nhigh-dimensional linear inverse problems, has found widespread applications in\nsignal processing and statistical inference. Among its notable variants, Vector\nApproximate Message Passing (VAMP), Generalized Approximate Survey Propagation\n(GASP), and Vector Approximate Survey Propagation (VASP) have demonstrated\neffectiveness even when the assumed generative models differ from the true\nmodels. However, many fundamental questions regarding model mismatch remain\nunanswered. For instance, it is still unclear what level of model mismatch is\nrequired for the postulated posterior estimate (PPE) to exhibit a replica\nsymmetry breaking (RSB) structure in the extremum conditions of its free\nenergy, and what order of RSB is necessary. In this paper, we introduce a novel\napproximate message passing algorithm that incorporates K-step RSB (KRSB) and\nnaturally reduces to VAMP and VASP with specific parameter selections. We refer\nto this as the K-step VASP (KVASP) algorithm. Simulations show that KVASP\nsignificantly outperforms VAMP and GASP in estimation accuracy, particularly\nwhen the assumed prior has discrete support and the measurement matrix is\nnon-i.i.d.. Additionally, the state evolution (SE) of KVASP, derived\nheuristically, accurately tracks the per-iteration mean squared error (MSE). A\ncomparison between the SE and the free energy under the KRSB ansatz reveals\nthat the fixed-point equations of SE align with the saddle-point equations of\nthe free energy. This suggests that, once the KRSB ansatz holds and the SE\nfixed point is reached, KVASP can accurately compute the PPE in the large\nsystem limit (LSL)."
                },
                "authors": [
                    {
                        "name": "Qun Chen"
                    },
                    {
                        "name": "Haochuan Zhang"
                    },
                    {
                        "name": "Huimin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Huimin Zhu"
                },
                "author": "Huimin Zhu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.05111",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20896v1",
                "updated": "2024-10-28T10:23:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    23,
                    18,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T10:23:18Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    23,
                    18,
                    0,
                    302,
                    0
                ],
                "title": "BSD: a Bayesian framework for parametric models of neural spectra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BSD: a Bayesian framework for parametric models of neural spectra"
                },
                "summary": "The analysis of neural power spectra plays a crucial role in understanding\nbrain function and dysfunction. While recent efforts have led to the\ndevelopment of methods for decomposing spectral data, challenges remain in\nperforming statistical analysis and group-level comparisons. Here, we introduce\nBayesian Spectral Decomposition (BSD), a Bayesian framework for analysing\nneural spectral power. BSD allows for the specification, inversion, comparison,\nand analysis of parametric models of neural spectra, addressing limitations of\nexisting methods. We first establish the face validity of BSD on simulated data\nand show how it outperforms an established method (\\fooof{}) for peak detection\non artificial spectral data. We then demonstrate the efficacy of BSD on a\ngroup-level study of EEG spectra in 204 healthy subjects from the LEMON\ndataset. Our results not only highlight the effectiveness of BSD in model\nselection and parameter estimation, but also illustrate how BSD enables\nstraightforward group-level regression of the effect of continuous covariates\nsuch as age. By using Bayesian inference techniques, BSD provides a robust\nframework for studying neural spectral data and their relationship to brain\nfunction and dysfunction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The analysis of neural power spectra plays a crucial role in understanding\nbrain function and dysfunction. While recent efforts have led to the\ndevelopment of methods for decomposing spectral data, challenges remain in\nperforming statistical analysis and group-level comparisons. Here, we introduce\nBayesian Spectral Decomposition (BSD), a Bayesian framework for analysing\nneural spectral power. BSD allows for the specification, inversion, comparison,\nand analysis of parametric models of neural spectra, addressing limitations of\nexisting methods. We first establish the face validity of BSD on simulated data\nand show how it outperforms an established method (\\fooof{}) for peak detection\non artificial spectral data. We then demonstrate the efficacy of BSD on a\ngroup-level study of EEG spectra in 204 healthy subjects from the LEMON\ndataset. Our results not only highlight the effectiveness of BSD in model\nselection and parameter estimation, but also illustrate how BSD enables\nstraightforward group-level regression of the effect of continuous covariates\nsuch as age. By using Bayesian inference techniques, BSD provides a robust\nframework for studying neural spectral data and their relationship to brain\nfunction and dysfunction."
                },
                "authors": [
                    {
                        "name": "Johan Medrano"
                    },
                    {
                        "name": "Nicholas A. Alexander"
                    },
                    {
                        "name": "Robert A. Seymour"
                    },
                    {
                        "name": "Peter Zeidman"
                    }
                ],
                "author_detail": {
                    "name": "Peter Zeidman"
                },
                "author": "Peter Zeidman",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.05111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.05111v2",
                "updated": "2024-10-28T10:23:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    23,
                    3,
                    0,
                    302,
                    0
                ],
                "published": "2023-11-09T02:55:28Z",
                "published_parsed": [
                    2023,
                    11,
                    9,
                    2,
                    55,
                    28,
                    3,
                    313,
                    0
                ],
                "title": "Vector Approximate Survey Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector Approximate Survey Propagation"
                },
                "summary": "Approximate Message Passing (AMP), originally designed to solve\nhigh-dimensional linear inverse problems, has found broad applications in\nsignal processing and statistical inference. Among its key variants, Vector\nApproximate Message Passing (VAMP) and Generalized Approximate Survey\nPropagation (GASP) have demonstrated effectiveness even in scenarios where the\nassumed generative models differ from the true models. However, the maximum a\nposteriori (MAP) versions of VAMP and GASP have limitations: VAMP is restricted\nto differentiable priors and likelihoods, while GASP requires the measurement\nmatrix to have independent identically distributed (i.i.d.) elements. To\novercome these limitations, this paper introduces a new algorithm, Vector\nApproximate Survey Propagation (VASP). VASP utilizes survey propagation to\nhandle non-differentiable priors and likelihoods, along with employs\nvector-form messages to account for correlations in the measurement matrix.\nSimulations reveal that VASP significantly surpasses VAMP and GASP in\nestimation accuracy, particularly when the assumed prior is discrete-supported\nand the measurement matrix is non-i.i.d.. Additionally, the state evolution\n(SE) of VASP, derived heuristically, accurately reflects the per-iteration mean\nsquared error (MSE). A comparison between the SE and the free energy computed\nby Takahashi and Kabashima under the one-step replica symmetry breaking (1RSB)\nansatz shows that the SE's fixed-point equations align with the free energy's\nsaddle point equations. This suggests that VASP efficiently implements the\npostulated MAP estimator (which is NP-hard in the worst case) with cubic\ncomputational complexity, assuming the 1RSB ansatz is valid.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Message Passing (AMP), originally designed to solve\nhigh-dimensional linear inverse problems, has found broad applications in\nsignal processing and statistical inference. Among its key variants, Vector\nApproximate Message Passing (VAMP) and Generalized Approximate Survey\nPropagation (GASP) have demonstrated effectiveness even in scenarios where the\nassumed generative models differ from the true models. However, the maximum a\nposteriori (MAP) versions of VAMP and GASP have limitations: VAMP is restricted\nto differentiable priors and likelihoods, while GASP requires the measurement\nmatrix to have independent identically distributed (i.i.d.) elements. To\novercome these limitations, this paper introduces a new algorithm, Vector\nApproximate Survey Propagation (VASP). VASP utilizes survey propagation to\nhandle non-differentiable priors and likelihoods, along with employs\nvector-form messages to account for correlations in the measurement matrix.\nSimulations reveal that VASP significantly surpasses VAMP and GASP in\nestimation accuracy, particularly when the assumed prior is discrete-supported\nand the measurement matrix is non-i.i.d.. Additionally, the state evolution\n(SE) of VASP, derived heuristically, accurately reflects the per-iteration mean\nsquared error (MSE). A comparison between the SE and the free energy computed\nby Takahashi and Kabashima under the one-step replica symmetry breaking (1RSB)\nansatz shows that the SE's fixed-point equations align with the free energy's\nsaddle point equations. This suggests that VASP efficiently implements the\npostulated MAP estimator (which is NP-hard in the worst case) with cubic\ncomputational complexity, assuming the 1RSB ansatz is valid."
                },
                "authors": [
                    {
                        "name": "Qun Chen"
                    },
                    {
                        "name": "Haochuan Zhang"
                    },
                    {
                        "name": "Huimin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Huimin Zhu"
                },
                "author": "Huimin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.05111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.05111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20886v1",
                "updated": "2024-10-28T10:12:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    12,
                    6,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T10:12:06Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    12,
                    6,
                    0,
                    302,
                    0
                ],
                "title": "CODES: Benchmarking Coupled ODE Surrogates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CODES: Benchmarking Coupled ODE Surrogates"
                },
                "summary": "We introduce CODES, a benchmark for comprehensive evaluation of surrogate\narchitectures for coupled ODE systems. Besides standard metrics like mean\nsquared error (MSE) and inference time, CODES provides insights into surrogate\nbehaviour across multiple dimensions like interpolation, extrapolation, sparse\ndata, uncertainty quantification and gradient correlation. The benchmark\nemphasizes usability through features such as integrated parallel training, a\nweb-based configuration generator, and pre-implemented baseline models and\ndatasets. Extensive documentation ensures sustainability and provides the\nfoundation for collaborative improvement. By offering a fair and multi-faceted\ncomparison, CODES helps researchers select the most suitable surrogate for\ntheir specific dataset and application while deepening our understanding of\nsurrogate learning behaviour.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce CODES, a benchmark for comprehensive evaluation of surrogate\narchitectures for coupled ODE systems. Besides standard metrics like mean\nsquared error (MSE) and inference time, CODES provides insights into surrogate\nbehaviour across multiple dimensions like interpolation, extrapolation, sparse\ndata, uncertainty quantification and gradient correlation. The benchmark\nemphasizes usability through features such as integrated parallel training, a\nweb-based configuration generator, and pre-implemented baseline models and\ndatasets. Extensive documentation ensures sustainability and provides the\nfoundation for collaborative improvement. By offering a fair and multi-faceted\ncomparison, CODES helps researchers select the most suitable surrogate for\ntheir specific dataset and application while deepening our understanding of\nsurrogate learning behaviour."
                },
                "authors": [
                    {
                        "name": "Robin Janssen"
                    },
                    {
                        "name": "Immanuel Sulzer"
                    },
                    {
                        "name": "Tobias Buck"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Buck"
                },
                "author": "Tobias Buck",
                "arxiv_comment": "12 pages, 10 figures, accepted for the Machine Learning and the\n  Physical Sciences workshop at NeurIPS 2024, source code available on GitHub\n  at https://github.com/robin-janssen/CODES-Benchmark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20885v1",
                "updated": "2024-10-28T10:07:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    7,
                    6,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T10:07:06Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    7,
                    6,
                    0,
                    302,
                    0
                ],
                "title": "A Distributed Lag Approach to the Generalised Dynamic Factor Model\n  (GDFM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distributed Lag Approach to the Generalised Dynamic Factor Model\n  (GDFM)"
                },
                "summary": "We provide estimation and inference for the Generalised Dynamic Factor Model\n(GDFM) under the assumption that the dynamic common component can be expressed\nin terms of a finite number of lags of contemporaneously pervasive factors. The\nproposed estimator is simply an OLS regression of the observed variables on\nfactors extracted via static principal components and therefore avoids\nfrequency domain techniques entirely.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We provide estimation and inference for the Generalised Dynamic Factor Model\n(GDFM) under the assumption that the dynamic common component can be expressed\nin terms of a finite number of lags of contemporaneously pervasive factors. The\nproposed estimator is simply an OLS regression of the observed variables on\nfactors extracted via static principal components and therefore avoids\nfrequency domain techniques entirely."
                },
                "authors": [
                    {
                        "name": "Philipp Gersing"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Gersing"
                },
                "author": "Philipp Gersing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20878v1",
                "updated": "2024-10-28T09:55:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    9,
                    55,
                    52,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T09:55:52Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    9,
                    55,
                    52,
                    0,
                    302,
                    0
                ],
                "title": "AutoRAG: Automated Framework for optimization of Retrieval Augmented\n  Generation Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRAG: Automated Framework for optimization of Retrieval Augmented\n  Generation Pipeline"
                },
                "summary": "Using LLMs (Large Language Models) in conjunction with external documents has\nmade RAG (Retrieval-Augmented Generation) an essential technology. Numerous\ntechniques and modules for RAG are being researched, but their performance can\nvary across different datasets. Finding RAG modules that perform well on\nspecific datasets is challenging. In this paper, we propose the AutoRAG\nframework, which automatically identifies suitable RAG modules for a given\ndataset. AutoRAG explores and approximates the optimal combination of RAG\nmodules for the dataset. Additionally, we share the results of optimizing a\ndataset using AutoRAG. All experimental results and data are publicly available\nand can be accessed through our GitHub repository\nhttps://github.com/Marker-Inc-Korea/AutoRAG_ARAGOG_Paper .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs (Large Language Models) in conjunction with external documents has\nmade RAG (Retrieval-Augmented Generation) an essential technology. Numerous\ntechniques and modules for RAG are being researched, but their performance can\nvary across different datasets. Finding RAG modules that perform well on\nspecific datasets is challenging. In this paper, we propose the AutoRAG\nframework, which automatically identifies suitable RAG modules for a given\ndataset. AutoRAG explores and approximates the optimal combination of RAG\nmodules for the dataset. Additionally, we share the results of optimizing a\ndataset using AutoRAG. All experimental results and data are publicly available\nand can be accessed through our GitHub repository\nhttps://github.com/Marker-Inc-Korea/AutoRAG_ARAGOG_Paper ."
                },
                "authors": [
                    {
                        "name": "Dongkyu Kim"
                    },
                    {
                        "name": "Byoungwook Kim"
                    },
                    {
                        "name": "Donggeon Han"
                    },
                    {
                        "name": "Matouš Eibich"
                    }
                ],
                "author_detail": {
                    "name": "Matouš Eibich"
                },
                "author": "Matouš Eibich",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14716v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14716v2",
                "updated": "2024-10-28T09:40:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    9,
                    40,
                    18,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-11T13:17:19Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    17,
                    19,
                    4,
                    285,
                    0
                ],
                "title": "A Systematic Survey on Large Language Models for Algorithm Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Survey on Large Language Models for Algorithm Design"
                },
                "summary": "Algorithm Design (AD) is crucial for effective problem-solving across various\ndomains. The advent of Large Language Models (LLMs) has notably enhanced the\nautomation and innovation within this field, offering new perspectives and\npromising solutions. Over the past three years, the integration of LLMs into AD\n(LLM4AD) has progressed significantly, finding applications in diverse areas\nsuch as optimization, machine learning, mathematical reasoning, and scientific\ndiscovery. Given the rapid development and broadening scope of this field, a\nsystematic review is both timely and essential. This paper provides a\nsystematic review of the works on LLM4AD. First, we present an overview and\nsummary of existing studies. Then, we present a systematic taxonomy and a\nreview of existing works along four dimensions, including the role of LLMs,\nsearch techniques, prompt strategies, and applications, with a discussion of\nthe potential and achievements of using LLMs. Finally, we explore current\nchallenges and propose several open questions and promising directions for\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithm Design (AD) is crucial for effective problem-solving across various\ndomains. The advent of Large Language Models (LLMs) has notably enhanced the\nautomation and innovation within this field, offering new perspectives and\npromising solutions. Over the past three years, the integration of LLMs into AD\n(LLM4AD) has progressed significantly, finding applications in diverse areas\nsuch as optimization, machine learning, mathematical reasoning, and scientific\ndiscovery. Given the rapid development and broadening scope of this field, a\nsystematic review is both timely and essential. This paper provides a\nsystematic review of the works on LLM4AD. First, we present an overview and\nsummary of existing studies. Then, we present a systematic taxonomy and a\nreview of existing works along four dimensions, including the role of LLMs,\nsearch techniques, prompt strategies, and applications, with a discussion of\nthe potential and achievements of using LLMs. Finally, we explore current\nchallenges and propose several open questions and promising directions for\nfuture research."
                },
                "authors": [
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Yiming Yao"
                    },
                    {
                        "name": "Ping Guo"
                    },
                    {
                        "name": "Zhiyuan Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Xialiang Tong"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Zhenkun Wang"
                    },
                    {
                        "name": "Qingfu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qingfu Zhang"
                },
                "author": "Qingfu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14716v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14716v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20869v1",
                "updated": "2024-10-28T09:37:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    9,
                    37,
                    58,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T09:37:58Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    9,
                    37,
                    58,
                    0,
                    302,
                    0
                ],
                "title": "Reward Modeling with Weak Supervision for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Modeling with Weak Supervision for Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to their\nincreased application across various tasks, with reinforcement learning from\nhuman feedback (RLHF) being a crucial part of their training to align responses\nwith user intentions. In the RLHF process, a reward model is trained using\nresponses preferences determined by human labelers or AI systems, which then\nrefines the LLM through reinforcement learning. This work introduces weak\nsupervision as a strategy to extend RLHF datasets and enhance reward model\nperformance. Weak supervision employs noisy or imprecise data labeling,\nreducing reliance on expensive manually labeled data. By analyzing RLHF\ndatasets to identify heuristics that correlate with response preference, we\nwrote simple labeling functions and then calibrated a label model to weakly\nannotate unlabeled data. Our evaluation show that while weak supervision\nsignificantly benefits smaller datasets by improving reward model performance,\nits effectiveness decreases with larger, originally labeled datasets.\nAdditionally, using an LLM to generate and then weakly label responses offers a\npromising method for extending preference data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to their\nincreased application across various tasks, with reinforcement learning from\nhuman feedback (RLHF) being a crucial part of their training to align responses\nwith user intentions. In the RLHF process, a reward model is trained using\nresponses preferences determined by human labelers or AI systems, which then\nrefines the LLM through reinforcement learning. This work introduces weak\nsupervision as a strategy to extend RLHF datasets and enhance reward model\nperformance. Weak supervision employs noisy or imprecise data labeling,\nreducing reliance on expensive manually labeled data. By analyzing RLHF\ndatasets to identify heuristics that correlate with response preference, we\nwrote simple labeling functions and then calibrated a label model to weakly\nannotate unlabeled data. Our evaluation show that while weak supervision\nsignificantly benefits smaller datasets by improving reward model performance,\nits effectiveness decreases with larger, originally labeled datasets.\nAdditionally, using an LLM to generate and then weakly label responses offers a\npromising method for extending preference data."
                },
                "authors": [
                    {
                        "name": "Ben Hauptvogel"
                    },
                    {
                        "name": "Malte Ostendorff"
                    },
                    {
                        "name": "Georg Rehm"
                    },
                    {
                        "name": "Sebastian Möller"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Möller"
                },
                "author": "Sebastian Möller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20856v1",
                "updated": "2024-10-28T09:19:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    9,
                    19,
                    29,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T09:19:29Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    9,
                    19,
                    29,
                    0,
                    302,
                    0
                ],
                "title": "Strada-LLM: Graph LLM for traffic prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strada-LLM: Graph LLM for traffic prediction"
                },
                "summary": "Traffic prediction is a vital component of intelligent transportation\nsystems. By reasoning about traffic patterns in both the spatial and temporal\ndimensions, accurate and interpretable predictions can be provided. A\nconsiderable challenge in traffic prediction lies in handling the diverse data\ndistributions caused by vastly different traffic conditions occurring at\ndifferent locations. LLMs have been a dominant solution due to their remarkable\ncapacity to adapt to new datasets with very few labeled data samples, i.e.,\nfew-shot adaptability. However, existing forecasting techniques mainly focus on\nextracting local graph information and forming a text-like prompt, leaving LLM-\nbased traffic prediction an open problem. This work presents a probabilistic\nLLM for traffic forecasting with three highlights. We propose a graph-aware LLM\nfor traffic prediction that considers proximal traffic information.\nSpecifically, by considering the traffic of neighboring nodes as covariates,\nour model outperforms the corresponding time-series LLM. Furthermore, we adopt\na lightweight approach for efficient domain adaptation when facing new data\ndistributions in few-shot fashion. The comparative experiment demonstrates the\nproposed method outperforms the state-of-the-art LLM-based methods and the\ntraditional GNN- based supervised approaches. Furthermore, Strada-LLM can be\neasily adapted to different LLM backbones without a noticeable performance\ndrop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic prediction is a vital component of intelligent transportation\nsystems. By reasoning about traffic patterns in both the spatial and temporal\ndimensions, accurate and interpretable predictions can be provided. A\nconsiderable challenge in traffic prediction lies in handling the diverse data\ndistributions caused by vastly different traffic conditions occurring at\ndifferent locations. LLMs have been a dominant solution due to their remarkable\ncapacity to adapt to new datasets with very few labeled data samples, i.e.,\nfew-shot adaptability. However, existing forecasting techniques mainly focus on\nextracting local graph information and forming a text-like prompt, leaving LLM-\nbased traffic prediction an open problem. This work presents a probabilistic\nLLM for traffic forecasting with three highlights. We propose a graph-aware LLM\nfor traffic prediction that considers proximal traffic information.\nSpecifically, by considering the traffic of neighboring nodes as covariates,\nour model outperforms the corresponding time-series LLM. Furthermore, we adopt\na lightweight approach for efficient domain adaptation when facing new data\ndistributions in few-shot fashion. The comparative experiment demonstrates the\nproposed method outperforms the state-of-the-art LLM-based methods and the\ntraditional GNN- based supervised approaches. Furthermore, Strada-LLM can be\neasily adapted to different LLM backbones without a noticeable performance\ndrop."
                },
                "authors": [
                    {
                        "name": "Seyed Mohamad Moghadas"
                    },
                    {
                        "name": "Yangxintong Lyu"
                    },
                    {
                        "name": "Bruno Cornelis"
                    },
                    {
                        "name": "Alexandre Alahi"
                    },
                    {
                        "name": "Adrian Munteanu"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Munteanu"
                },
                "author": "Adrian Munteanu",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.21272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21272v1",
                "updated": "2024-10-28T17:59:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    59,
                    6,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:59:06Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    59,
                    6,
                    0,
                    302,
                    0
                ],
                "title": "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of\n  Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of\n  Heuristics"
                },
                "summary": "Do large language models (LLMs) solve reasoning tasks by learning robust\ngeneralizable algorithms, or do they memorize training data? To investigate\nthis question, we use arithmetic reasoning as a representative task. Using\ncausal analysis, we identify a subset of the model (a circuit) that explains\nmost of the model's behavior for basic arithmetic logic and examine its\nfunctionality. By zooming in on the level of individual circuit neurons, we\ndiscover a sparse set of important neurons that implement simple heuristics.\nEach heuristic identifies a numerical input pattern and outputs corresponding\nanswers. We hypothesize that the combination of these heuristic neurons is the\nmechanism used to produce correct arithmetic answers. To test this, we\ncategorize each neuron into several heuristic types-such as neurons that\nactivate when an operand falls within a certain range-and find that the\nunordered combination of these heuristic types is the mechanism that explains\nmost of the model's accuracy on arithmetic prompts. Finally, we demonstrate\nthat this mechanism appears as the main source of arithmetic accuracy early in\ntraining. Overall, our experimental results across several LLMs show that LLMs\nperform arithmetic using neither robust algorithms nor memorization; rather,\nthey rely on a \"bag of heuristics\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do large language models (LLMs) solve reasoning tasks by learning robust\ngeneralizable algorithms, or do they memorize training data? To investigate\nthis question, we use arithmetic reasoning as a representative task. Using\ncausal analysis, we identify a subset of the model (a circuit) that explains\nmost of the model's behavior for basic arithmetic logic and examine its\nfunctionality. By zooming in on the level of individual circuit neurons, we\ndiscover a sparse set of important neurons that implement simple heuristics.\nEach heuristic identifies a numerical input pattern and outputs corresponding\nanswers. We hypothesize that the combination of these heuristic neurons is the\nmechanism used to produce correct arithmetic answers. To test this, we\ncategorize each neuron into several heuristic types-such as neurons that\nactivate when an operand falls within a certain range-and find that the\nunordered combination of these heuristic types is the mechanism that explains\nmost of the model's accuracy on arithmetic prompts. Finally, we demonstrate\nthat this mechanism appears as the main source of arithmetic accuracy early in\ntraining. Overall, our experimental results across several LLMs show that LLMs\nperform arithmetic using neither robust algorithms nor memorization; rather,\nthey rely on a \"bag of heuristics\"."
                },
                "authors": [
                    {
                        "name": "Yaniv Nikankin"
                    },
                    {
                        "name": "Anja Reusch"
                    },
                    {
                        "name": "Aaron Mueller"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21271v1",
                "updated": "2024-10-28T17:59:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    59,
                    3,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:59:03Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    59,
                    3,
                    0,
                    302,
                    0
                ],
                "title": "EoRA: Training-free Compensation for Compressed LLM with Eigenspace\n  Low-Rank Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EoRA: Training-free Compensation for Compressed LLM with Eigenspace\n  Low-Rank Approximation"
                },
                "summary": "In this work, we re-formulate the model compression problem into the\ncustomized compensation problem: Given a compressed model, we aim to introduce\nresidual low-rank paths to compensate for compression errors under customized\nrequirements from users (e.g., tasks, compression ratios), resulting in greater\nflexibility in adjusting overall capacity without being constrained by specific\ncompression formats. However, naively applying SVD to derive residual paths\ncauses suboptimal utilization of the low-rank representation capacity. Instead,\nwe propose Training-free Eigenspace Low-Rank Approximation (EoRA), a method\nthat directly minimizes compression-induced errors without requiring\ngradient-based training, achieving fast optimization in minutes using a small\namount of calibration data. EoRA projects compression errors into the\neigenspace of input activations, leveraging eigenvalues to effectively\nprioritize the reconstruction of high-importance error components. Moreover,\nEoRA can be seamlessly integrated with fine-tuning and quantization to further\nimprove effectiveness and efficiency. EoRA consistently outperforms previous\nmethods in compensating errors for compressed LLaMA2/3 models on various tasks,\nsuch as language generation, commonsense reasoning, and math reasoning tasks\n(e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and\nMathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4\nsparsity). EoRA offers a scalable, training-free solution to compensate for\ncompression errors, making it a powerful tool to deploy LLMs in various\ncapacity and efficiency requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we re-formulate the model compression problem into the\ncustomized compensation problem: Given a compressed model, we aim to introduce\nresidual low-rank paths to compensate for compression errors under customized\nrequirements from users (e.g., tasks, compression ratios), resulting in greater\nflexibility in adjusting overall capacity without being constrained by specific\ncompression formats. However, naively applying SVD to derive residual paths\ncauses suboptimal utilization of the low-rank representation capacity. Instead,\nwe propose Training-free Eigenspace Low-Rank Approximation (EoRA), a method\nthat directly minimizes compression-induced errors without requiring\ngradient-based training, achieving fast optimization in minutes using a small\namount of calibration data. EoRA projects compression errors into the\neigenspace of input activations, leveraging eigenvalues to effectively\nprioritize the reconstruction of high-importance error components. Moreover,\nEoRA can be seamlessly integrated with fine-tuning and quantization to further\nimprove effectiveness and efficiency. EoRA consistently outperforms previous\nmethods in compensating errors for compressed LLaMA2/3 models on various tasks,\nsuch as language generation, commonsense reasoning, and math reasoning tasks\n(e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and\nMathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4\nsparsity). EoRA offers a scalable, training-free solution to compensate for\ncompression errors, making it a powerful tool to deploy LLMs in various\ncapacity and efficiency requirements."
                },
                "authors": [
                    {
                        "name": "Shih-Yang Liu"
                    },
                    {
                        "name": "Huck Yang"
                    },
                    {
                        "name": "Chein-Yi Wang"
                    },
                    {
                        "name": "Nai Chit Fung"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Charbel Sakr"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Min-Hung Chen"
                    }
                ],
                "author_detail": {
                    "name": "Min-Hung Chen"
                },
                "author": "Min-Hung Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21270v1",
                "updated": "2024-10-28T17:58:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    58,
                    55,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:58:55Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    58,
                    55,
                    0,
                    302,
                    0
                ],
                "title": "Strategic Electric Distribution Network Sensing via Spectral Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Electric Distribution Network Sensing via Spectral Bandits"
                },
                "summary": "Despite their wide-scale deployment and ability to make accurate\nhigh-frequency voltage measurements, communication network limitations have\nlargely precluded the use of smart meters for real-time monitoring purposes in\nelectric distribution systems. Although smart meter communication networks have\nlimited bandwidth available per meter, they also have the ability to dedicate\nhigher bandwidth to varying subsets of meters. Using this capability to enable\nreal-time monitoring from smart meters, this paper proposes an online\nbandwidth-constrained sensor sampling algorithm that takes advantage of the\ngraphical structure inherent in the power flow equations. The key idea is to\nuse a spectral bandit framework where the estimated parameters are the graph\nFourier transform coefficients of the nodal voltages. The structure provided by\nthis framework promotes a sampling policy that strategically accounts for\nelectrical distance. Maxima of sub-Gaussian random variables model the policy\nrewards, which relaxes distributional assumptions common in prior work. The\nscheme is implemented on a synthetic electrical network to dynamically identify\nmeters exposing violations of voltage magnitude limits, illustrating the\neffectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their wide-scale deployment and ability to make accurate\nhigh-frequency voltage measurements, communication network limitations have\nlargely precluded the use of smart meters for real-time monitoring purposes in\nelectric distribution systems. Although smart meter communication networks have\nlimited bandwidth available per meter, they also have the ability to dedicate\nhigher bandwidth to varying subsets of meters. Using this capability to enable\nreal-time monitoring from smart meters, this paper proposes an online\nbandwidth-constrained sensor sampling algorithm that takes advantage of the\ngraphical structure inherent in the power flow equations. The key idea is to\nuse a spectral bandit framework where the estimated parameters are the graph\nFourier transform coefficients of the nodal voltages. The structure provided by\nthis framework promotes a sampling policy that strategically accounts for\nelectrical distance. Maxima of sub-Gaussian random variables model the policy\nrewards, which relaxes distributional assumptions common in prior work. The\nscheme is implemented on a synthetic electrical network to dynamically identify\nmeters exposing violations of voltage magnitude limits, illustrating the\neffectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Samuel Talkington"
                    },
                    {
                        "name": "Rahul Gupta"
                    },
                    {
                        "name": "Richard Asiamah"
                    },
                    {
                        "name": "Paprapee Buason"
                    },
                    {
                        "name": "Daniel K. Molzahn"
                    }
                ],
                "author_detail": {
                    "name": "Daniel K. Molzahn"
                },
                "author": "Daniel K. Molzahn",
                "arxiv_comment": "8 pages, 3 figures, 2024 63rd IEEE Conference on Decision and Control\n  (CDC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21254v1",
                "updated": "2024-10-28T17:52:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    52,
                    15,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:52:15Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    52,
                    15,
                    0,
                    302,
                    0
                ],
                "title": "Are BabyLMs Second Language Learners?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are BabyLMs Second Language Learners?"
                },
                "summary": "This paper describes a linguistically-motivated approach to the 2024 edition\nof the BabyLM Challenge (Warstadt et al. 2023). Rather than pursuing a first\nlanguage learning (L1) paradigm, we approach the challenge from a second\nlanguage (L2) learning perspective. In L2 learning, there is a stronger focus\non learning explicit linguistic information, such as grammatical notions,\ndefinitions of words or different ways of expressing a meaning. This makes L2\nlearning potentially more efficient and concise. We approximate this using data\nfrom Wiktionary, grammar examples either generated by an LLM or sourced from\ngrammar books, and paraphrase data. We find that explicit information about\nword meaning (in our case, Wiktionary) does not boost model performance, while\ngrammatical information can give a small improvement. The most impactful data\ningredient is sentence paraphrases, with our two best models being trained on\n1) a mix of paraphrase data and data from the BabyLM pretraining dataset, and\n2) exclusively paraphrase data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes a linguistically-motivated approach to the 2024 edition\nof the BabyLM Challenge (Warstadt et al. 2023). Rather than pursuing a first\nlanguage learning (L1) paradigm, we approach the challenge from a second\nlanguage (L2) learning perspective. In L2 learning, there is a stronger focus\non learning explicit linguistic information, such as grammatical notions,\ndefinitions of words or different ways of expressing a meaning. This makes L2\nlearning potentially more efficient and concise. We approximate this using data\nfrom Wiktionary, grammar examples either generated by an LLM or sourced from\ngrammar books, and paraphrase data. We find that explicit information about\nword meaning (in our case, Wiktionary) does not boost model performance, while\ngrammatical information can give a small improvement. The most impactful data\ningredient is sentence paraphrases, with our two best models being trained on\n1) a mix of paraphrase data and data from the BabyLM pretraining dataset, and\n2) exclusively paraphrase data."
                },
                "authors": [
                    {
                        "name": "Lukas Edman"
                    },
                    {
                        "name": "Lisa Bylinina"
                    },
                    {
                        "name": "Faeze Ghorbanpour"
                    },
                    {
                        "name": "Alexander Fraser"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Fraser"
                },
                "author": "Alexander Fraser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21252v1",
                "updated": "2024-10-28T17:50:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    50,
                    42,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:50:42Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    50,
                    42,
                    0,
                    302,
                    0
                ],
                "title": "LongReward: Improving Long-context Large Language Models with AI\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongReward: Improving Long-context Large Language Models with AI\n  Feedback"
                },
                "summary": "Though significant advancements have been achieved in developing long-context\nlarge language models (LLMs), the compromised quality of LLM-synthesized data\nfor supervised fine-tuning (SFT) often affects the long-context performance of\nSFT models and leads to inherent limitations. In principle, reinforcement\nlearning (RL) with appropriate reward signals can further enhance models'\ncapacities. However, how to obtain reliable rewards in long-context scenarios\nremains unexplored. To this end, we propose LongReward, a novel method that\nutilizes an off-the-shelf LLM to provide rewards for long-context model\nresponses from four human-valued dimensions: helpfulness, logicality,\nfaithfulness, and completeness, each with a carefully designed assessment\npipeline. By combining LongReward and offline RL algorithm DPO, we are able to\neffectively improve long-context SFT models. Our experiments indicate that\nLongReward not only significantly improves models' long-context performance but\nalso enhances their ability to follow short instructions. We also find that\nlong-context DPO with LongReward and conventional short-context DPO can be used\ntogether without hurting either one's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though significant advancements have been achieved in developing long-context\nlarge language models (LLMs), the compromised quality of LLM-synthesized data\nfor supervised fine-tuning (SFT) often affects the long-context performance of\nSFT models and leads to inherent limitations. In principle, reinforcement\nlearning (RL) with appropriate reward signals can further enhance models'\ncapacities. However, how to obtain reliable rewards in long-context scenarios\nremains unexplored. To this end, we propose LongReward, a novel method that\nutilizes an off-the-shelf LLM to provide rewards for long-context model\nresponses from four human-valued dimensions: helpfulness, logicality,\nfaithfulness, and completeness, each with a carefully designed assessment\npipeline. By combining LongReward and offline RL algorithm DPO, we are able to\neffectively improve long-context SFT models. Our experiments indicate that\nLongReward not only significantly improves models' long-context performance but\nalso enhances their ability to follow short instructions. We also find that\nlong-context DPO with LongReward and conventional short-context DPO can be used\ntogether without hurting either one's performance."
                },
                "authors": [
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Zhongni Hou"
                    },
                    {
                        "name": "Xin Lv"
                    },
                    {
                        "name": "Shulin Cao"
                    },
                    {
                        "name": "Zhenyu Hou"
                    },
                    {
                        "name": "Yilin Niu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Ling Feng"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04151v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04151v2",
                "updated": "2024-10-28T17:48:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    48,
                    44,
                    0,
                    302,
                    0
                ],
                "published": "2024-07-04T20:57:06Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    20,
                    57,
                    6,
                    3,
                    186,
                    0
                ],
                "title": "Securing Multi-turn Conversational Language Models From Distributed\n  Backdoor Triggers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Multi-turn Conversational Language Models From Distributed\n  Backdoor Triggers"
                },
                "summary": "Large language models (LLMs) have acquired the ability to handle longer\ncontext lengths and understand nuances in text, expanding their dialogue\ncapabilities beyond a single utterance. A popular user-facing application of\nLLMs is the multi-turn chat setting. Though longer chat memory and better\nunderstanding may seemingly benefit users, our paper exposes a vulnerability\nthat leverages the multi-turn feature and strong learning ability of LLMs to\nharm the end-user: the backdoor. We demonstrate that LLMs can capture the\ncombinational backdoor representation. Only upon presentation of triggers\ntogether does the backdoor activate. We also verify empirically that this\nrepresentation is invariant to the position of the trigger utterance.\nSubsequently, inserting a single extra token into two utterances of 5%of the\ndata can cause over 99% Attack Success Rate (ASR). Our results with 3 triggers\ndemonstrate that this framework is generalizable, compatible with any trigger\nin an adversary's toolbox in a plug-and-play manner. Defending the backdoor can\nbe challenging in the chat setting because of the large input and output space.\nOur analysis indicates that the distributed backdoor exacerbates the current\nchallenges by polynomially increasing the dimension of the attacked input\nspace. Canonical textual defenses like ONION and BKI leverage auxiliary model\nforward passes over individual tokens, scaling exponentially with the input\nsequence length and struggling to maintain computational feasibility. To this\nend, we propose a decoding time defense - decayed contrastive decoding - that\nscales linearly with assistant response sequence length and reduces the\nbackdoor to as low as 0.35%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have acquired the ability to handle longer\ncontext lengths and understand nuances in text, expanding their dialogue\ncapabilities beyond a single utterance. A popular user-facing application of\nLLMs is the multi-turn chat setting. Though longer chat memory and better\nunderstanding may seemingly benefit users, our paper exposes a vulnerability\nthat leverages the multi-turn feature and strong learning ability of LLMs to\nharm the end-user: the backdoor. We demonstrate that LLMs can capture the\ncombinational backdoor representation. Only upon presentation of triggers\ntogether does the backdoor activate. We also verify empirically that this\nrepresentation is invariant to the position of the trigger utterance.\nSubsequently, inserting a single extra token into two utterances of 5%of the\ndata can cause over 99% Attack Success Rate (ASR). Our results with 3 triggers\ndemonstrate that this framework is generalizable, compatible with any trigger\nin an adversary's toolbox in a plug-and-play manner. Defending the backdoor can\nbe challenging in the chat setting because of the large input and output space.\nOur analysis indicates that the distributed backdoor exacerbates the current\nchallenges by polynomially increasing the dimension of the attacked input\nspace. Canonical textual defenses like ONION and BKI leverage auxiliary model\nforward passes over individual tokens, scaling exponentially with the input\nsequence length and struggling to maintain computational feasibility. To this\nend, we propose a decoding time defense - decayed contrastive decoding - that\nscales linearly with assistant response sequence length and reduces the\nbackdoor to as low as 0.35%."
                },
                "authors": [
                    {
                        "name": "Terry Tong"
                    },
                    {
                        "name": "Jiashu Xu"
                    },
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Muhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Muhao Chen"
                },
                "author": "Muhao Chen",
                "arxiv_comment": "Findings of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04151v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04151v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21242v1",
                "updated": "2024-10-28T17:40:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    40,
                    40,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:40:40Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    40,
                    40,
                    0,
                    302,
                    0
                ],
                "title": "Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback"
                },
                "summary": "Building effective dense retrieval systems remains difficult when relevance\nsupervision is not available. Recent work has looked to overcome this challenge\nby using a Large Language Model (LLM) to generate hypothetical documents that\ncan be used to find the closest real document. However, this approach relies\nsolely on the LLM to have domain-specific knowledge relevant to the query,\nwhich may not be practical. Furthermore, generating hypothetical documents can\nbe inefficient as it requires the LLM to generate a large number of tokens for\neach query. To address these challenges, we introduce Real Document Embeddings\nfrom Relevance Feedback (ReDE-RF). Inspired by relevance feedback, ReDE-RF\nproposes to re-frame hypothetical document generation as a relevance estimation\ntask, using an LLM to select which documents should be used for nearest\nneighbor search. Through this re-framing, the LLM no longer needs\ndomain-specific knowledge but only needs to judge what is relevant.\nAdditionally, relevance estimation only requires the LLM to output a single\ntoken, thereby improving search latency. Our experiments show that ReDE-RF\nconsistently surpasses state-of-the-art zero-shot dense retrieval methods\nacross a wide range of low-resource retrieval datasets while also making\nsignificant improvements in latency per-query.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building effective dense retrieval systems remains difficult when relevance\nsupervision is not available. Recent work has looked to overcome this challenge\nby using a Large Language Model (LLM) to generate hypothetical documents that\ncan be used to find the closest real document. However, this approach relies\nsolely on the LLM to have domain-specific knowledge relevant to the query,\nwhich may not be practical. Furthermore, generating hypothetical documents can\nbe inefficient as it requires the LLM to generate a large number of tokens for\neach query. To address these challenges, we introduce Real Document Embeddings\nfrom Relevance Feedback (ReDE-RF). Inspired by relevance feedback, ReDE-RF\nproposes to re-frame hypothetical document generation as a relevance estimation\ntask, using an LLM to select which documents should be used for nearest\nneighbor search. Through this re-framing, the LLM no longer needs\ndomain-specific knowledge but only needs to judge what is relevant.\nAdditionally, relevance estimation only requires the LLM to output a single\ntoken, thereby improving search latency. Our experiments show that ReDE-RF\nconsistently surpasses state-of-the-art zero-shot dense retrieval methods\nacross a wide range of low-resource retrieval datasets while also making\nsignificant improvements in latency per-query."
                },
                "authors": [
                    {
                        "name": "Nour Jedidi"
                    },
                    {
                        "name": "Yung-Sung Chuang"
                    },
                    {
                        "name": "Leslie Shing"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21237v1",
                "updated": "2024-10-28T17:34:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    34,
                    5,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:34:05Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    34,
                    5,
                    0,
                    302,
                    0
                ],
                "title": "Hierarchical Knowledge Graph Construction from Images for Scalable\n  E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Knowledge Graph Construction from Images for Scalable\n  E-Commerce"
                },
                "summary": "Knowledge Graph (KG) is playing an increasingly important role in various AI\nsystems. For e-commerce, an efficient and low-cost automated knowledge graph\nconstruction method is the foundation of enabling various successful downstream\napplications. In this paper, we propose a novel method for constructing\nstructured product knowledge graphs from raw product images. The method\ncooperatively leverages recent advances in the vision-language model (VLM) and\nlarge language model (LLM), fully automating the process and allowing timely\ngraph updates. We also present a human-annotated e-commerce product dataset for\nbenchmarking product property extraction in knowledge graph construction. Our\nmethod outperforms our baseline in all metrics and evaluated properties,\ndemonstrating its effectiveness and bright usage potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph (KG) is playing an increasingly important role in various AI\nsystems. For e-commerce, an efficient and low-cost automated knowledge graph\nconstruction method is the foundation of enabling various successful downstream\napplications. In this paper, we propose a novel method for constructing\nstructured product knowledge graphs from raw product images. The method\ncooperatively leverages recent advances in the vision-language model (VLM) and\nlarge language model (LLM), fully automating the process and allowing timely\ngraph updates. We also present a human-annotated e-commerce product dataset for\nbenchmarking product property extraction in knowledge graph construction. Our\nmethod outperforms our baseline in all metrics and evaluated properties,\ndemonstrating its effectiveness and bright usage potential."
                },
                "authors": [
                    {
                        "name": "Zhantao Yang"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Fangyi Chen"
                    },
                    {
                        "name": "Anudeepsekhar Bolimera"
                    },
                    {
                        "name": "Marios Savvides"
                    }
                ],
                "author_detail": {
                    "name": "Marios Savvides"
                },
                "author": "Marios Savvides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17134v2",
                "updated": "2024-10-28T17:33:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    33,
                    27,
                    0,
                    302,
                    0
                ],
                "published": "2024-03-25T19:17:43Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    19,
                    17,
                    43,
                    0,
                    85,
                    0
                ],
                "title": "RepairAgent: An Autonomous, LLM-Based Agent for Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepairAgent: An Autonomous, LLM-Based Agent for Program Repair"
                },
                "summary": "Automated program repair has emerged as a powerful technique to mitigate the\nimpact of software bugs on system reliability and user experience. This paper\nintroduces RepairAgent, the first work to address the program repair challenge\nthrough an autonomous agent based on a large language model (LLM). Unlike\nexisting deep learning-based approaches, which prompt a model with a fixed\nprompt or in a fixed feedback loop, our work treats the LLM as an agent capable\nof autonomously planning and executing actions to fix bugs by invoking suitable\ntools. RepairAgent freely interleaves gathering information about the bug,\ngathering repair ingredients, and validating fixes, while deciding which tools\nto invoke based on the gathered information and feedback from previous fix\nattempts. Key contributions that enable RepairAgent include a set of tools that\nare useful for program repair, a dynamically updated prompt format that allows\nthe LLM to interact with these tools, and a finite state machine that guides\nthe agent in invoking the tools. Our evaluation on the popular Defects4J\ndataset demonstrates RepairAgent's effectiveness in autonomously repairing 164\nbugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM\nimposes an average cost of 270,000 tokens per bug, which, under the current\npricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To\nthe best of our knowledge, this work is the first to present an autonomous,\nLLM-based agent for program repair, paving the way for future agent-based\ntechniques in software engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated program repair has emerged as a powerful technique to mitigate the\nimpact of software bugs on system reliability and user experience. This paper\nintroduces RepairAgent, the first work to address the program repair challenge\nthrough an autonomous agent based on a large language model (LLM). Unlike\nexisting deep learning-based approaches, which prompt a model with a fixed\nprompt or in a fixed feedback loop, our work treats the LLM as an agent capable\nof autonomously planning and executing actions to fix bugs by invoking suitable\ntools. RepairAgent freely interleaves gathering information about the bug,\ngathering repair ingredients, and validating fixes, while deciding which tools\nto invoke based on the gathered information and feedback from previous fix\nattempts. Key contributions that enable RepairAgent include a set of tools that\nare useful for program repair, a dynamically updated prompt format that allows\nthe LLM to interact with these tools, and a finite state machine that guides\nthe agent in invoking the tools. Our evaluation on the popular Defects4J\ndataset demonstrates RepairAgent's effectiveness in autonomously repairing 164\nbugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM\nimposes an average cost of 270,000 tokens per bug, which, under the current\npricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To\nthe best of our knowledge, this work is the first to present an autonomous,\nLLM-based agent for program repair, paving the way for future agent-based\ntechniques in software engineering."
                },
                "authors": [
                    {
                        "name": "Islem Bouzenia"
                    },
                    {
                        "name": "Premkumar Devanbu"
                    },
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11801v2",
                "updated": "2024-10-28T17:30:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    30,
                    58,
                    0,
                    302,
                    0
                ],
                "published": "2024-06-17T17:48:13Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    17,
                    48,
                    13,
                    0,
                    169,
                    0
                ],
                "title": "Safety Arithmetic: A Framework for Test-time Safety Alignment of\n  Language Models by Steering Parameters and Activations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety Arithmetic: A Framework for Test-time Safety Alignment of\n  Language Models by Steering Parameters and Activations"
                },
                "summary": "Ensuring the safe alignment of large language models (LLMs) with human values\nis critical as they become integral to applications like translation and\nquestion answering. Current alignment methods struggle with dynamic user\nintentions and complex objectives, making models vulnerable to generating\nharmful content. We propose Safety Arithmetic, a training-free framework\nenhancing LLM safety across different scenarios: Base models, Supervised\nfine-tuned models (SFT), and Edited models. Safety Arithmetic involves Harm\nDirection Removal to avoid harmful content and Safety Alignment to promote safe\nresponses. Additionally, we present NoIntentEdit, a dataset highlighting edit\ninstances that could compromise model safety if used unintentionally. Our\nexperiments show that Safety Arithmetic significantly improves safety measures,\nreduces over-safety, and maintains model utility, outperforming existing\nmethods in ensuring safe content generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the safe alignment of large language models (LLMs) with human values\nis critical as they become integral to applications like translation and\nquestion answering. Current alignment methods struggle with dynamic user\nintentions and complex objectives, making models vulnerable to generating\nharmful content. We propose Safety Arithmetic, a training-free framework\nenhancing LLM safety across different scenarios: Base models, Supervised\nfine-tuned models (SFT), and Edited models. Safety Arithmetic involves Harm\nDirection Removal to avoid harmful content and Safety Alignment to promote safe\nresponses. Additionally, we present NoIntentEdit, a dataset highlighting edit\ninstances that could compromise model safety if used unintentionally. Our\nexperiments show that Safety Arithmetic significantly improves safety measures,\nreduces over-safety, and maintains model utility, outperforming existing\nmethods in ensuring safe content generation."
                },
                "authors": [
                    {
                        "name": "Rima Hazra"
                    },
                    {
                        "name": "Sayan Layek"
                    },
                    {
                        "name": "Somnath Banerjee"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "arxiv_comment": "EMNLP 2024 Main. Codes are available at:\n  https://github.com/declare-lab/safety-arithmetic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21236v1",
                "updated": "2024-10-28T17:30:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    30,
                    1,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:30:01Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    30,
                    1,
                    0,
                    302,
                    0
                ],
                "title": "Flaming-hot Initiation with Regular Execution Sampling for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flaming-hot Initiation with Regular Execution Sampling for Large\n  Language Models"
                },
                "summary": "Since the release of ChatGPT, large language models (LLMs) have demonstrated\nremarkable capabilities across various domains. A key challenge in developing\nthese general capabilities is efficiently sourcing diverse, high-quality data.\nThis becomes especially critical in reasoning-related tasks with sandbox\ncheckers, such as math or code, where the goal is to generate correct solutions\nto specific problems with higher probability. In this work, we introduce\nFlaming-hot Initiation with Regular Execution (FIRE) sampling, a simple yet\nhighly effective method to efficiently find good responses. Our empirical\nfindings show that FIRE sampling enhances inference-time generation quality and\nalso benefits training in the alignment stage. Furthermore, we explore how FIRE\nsampling improves performance by promoting diversity and analyze the impact of\nemploying FIRE at different positions within a response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the release of ChatGPT, large language models (LLMs) have demonstrated\nremarkable capabilities across various domains. A key challenge in developing\nthese general capabilities is efficiently sourcing diverse, high-quality data.\nThis becomes especially critical in reasoning-related tasks with sandbox\ncheckers, such as math or code, where the goal is to generate correct solutions\nto specific problems with higher probability. In this work, we introduce\nFlaming-hot Initiation with Regular Execution (FIRE) sampling, a simple yet\nhighly effective method to efficiently find good responses. Our empirical\nfindings show that FIRE sampling enhances inference-time generation quality and\nalso benefits training in the alignment stage. Furthermore, we explore how FIRE\nsampling improves performance by promoting diversity and analyze the impact of\nemploying FIRE at different positions within a response."
                },
                "authors": [
                    {
                        "name": "Weizhe Chen"
                    },
                    {
                        "name": "Zhicheng Zhang"
                    },
                    {
                        "name": "Guanlin Liu"
                    },
                    {
                        "name": "Renjie Zheng"
                    },
                    {
                        "name": "Wenlei Shi"
                    },
                    {
                        "name": "Chen Dun"
                    },
                    {
                        "name": "Zheng Wu"
                    },
                    {
                        "name": "Xing Jin"
                    },
                    {
                        "name": "Lin Yan"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yan"
                },
                "author": "Lin Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17195v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17195v3",
                "updated": "2024-10-28T17:28:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    28,
                    51,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-22T17:13:38Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    17,
                    13,
                    38,
                    1,
                    296,
                    0
                ],
                "title": "Non-myopic Generation of Language Models for Reasoning and Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-myopic Generation of Language Models for Reasoning and Planning"
                },
                "summary": "Large Language Models have demonstrated remarkable abilities in reasoning and\nplanning by breaking down complex problems into sequential steps. Despite their\nsuccess in various domains like mathematical problem-solving and coding, LLMs\nface challenges in ensuring reliable and optimal planning due to their inherent\nmyopic nature of autoregressive decoding. This paper revisits LLM reasoning\nfrom an optimal-control perspective, proposing a novel method,\nPredictive-Decoding, that leverages Model Predictive Control to enhance\nplanning accuracy. By re-weighting LLM distributions based on foresight\ntrajectories, Predictive-Decoding aims to mitigate early errors and promote\nnon-myopic planning. Our experiments show significant improvements in a wide\nrange of tasks for math, coding, and agents. Furthermore, Predictive-Decoding\ndemonstrates computational efficiency, outperforming search baselines with\nreduced computational resources. This study provides insights into optimizing\nLLM planning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have demonstrated remarkable abilities in reasoning and\nplanning by breaking down complex problems into sequential steps. Despite their\nsuccess in various domains like mathematical problem-solving and coding, LLMs\nface challenges in ensuring reliable and optimal planning due to their inherent\nmyopic nature of autoregressive decoding. This paper revisits LLM reasoning\nfrom an optimal-control perspective, proposing a novel method,\nPredictive-Decoding, that leverages Model Predictive Control to enhance\nplanning accuracy. By re-weighting LLM distributions based on foresight\ntrajectories, Predictive-Decoding aims to mitigate early errors and promote\nnon-myopic planning. Our experiments show significant improvements in a wide\nrange of tasks for math, coding, and agents. Furthermore, Predictive-Decoding\ndemonstrates computational efficiency, outperforming search baselines with\nreduced computational resources. This study provides insights into optimizing\nLLM planning capabilities."
                },
                "authors": [
                    {
                        "name": "Chang Ma"
                    },
                    {
                        "name": "Haiteng Zhao"
                    },
                    {
                        "name": "Junlei Zhang"
                    },
                    {
                        "name": "Junxian He"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17195v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17195v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02902v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02902v3",
                "updated": "2024-10-28T17:22:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    22,
                    43,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-03T18:48:38Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    18,
                    48,
                    38,
                    3,
                    277,
                    0
                ],
                "title": "Better Instruction-Following Through Minimum Bayes Risk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Instruction-Following Through Minimum Bayes Risk"
                },
                "summary": "General-purpose LLM judges capable of human-level evaluation provide not only\na scalable and accurate way of evaluating instruction-following LLMs but also\nnew avenues for supervising and improving their performance. One promising way\nof leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR)\ndecoding, which uses a reference-based evaluator to select a high-quality\noutput from amongst a set of candidate outputs. In the first part of this work,\nwe explore using MBR decoding as a method for improving the test-time\nperformance of instruction-following LLMs. We find that MBR decoding with\nreference-based LLM judges substantially improves over greedy decoding,\nbest-of-N decoding with reference-free judges and MBR decoding with lexical and\nembedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent\nacross LLMs with up to 70B parameters, demonstrating that smaller LLM judges\ncan be used to supervise much larger LLMs. Then, seeking to retain the\nimprovements from MBR decoding while mitigating additional test-time costs, we\nexplore iterative self-training on MBR-decoded outputs. We find that\nself-training using Direct Preference Optimisation leads to significant\nperformance gains, such that the self-trained models with greedy decoding\ngenerally match and sometimes exceed the performance of their base models with\nMBR decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-purpose LLM judges capable of human-level evaluation provide not only\na scalable and accurate way of evaluating instruction-following LLMs but also\nnew avenues for supervising and improving their performance. One promising way\nof leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR)\ndecoding, which uses a reference-based evaluator to select a high-quality\noutput from amongst a set of candidate outputs. In the first part of this work,\nwe explore using MBR decoding as a method for improving the test-time\nperformance of instruction-following LLMs. We find that MBR decoding with\nreference-based LLM judges substantially improves over greedy decoding,\nbest-of-N decoding with reference-free judges and MBR decoding with lexical and\nembedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent\nacross LLMs with up to 70B parameters, demonstrating that smaller LLM judges\ncan be used to supervise much larger LLMs. Then, seeking to retain the\nimprovements from MBR decoding while mitigating additional test-time costs, we\nexplore iterative self-training on MBR-decoded outputs. We find that\nself-training using Direct Preference Optimisation leads to significant\nperformance gains, such that the self-trained models with greedy decoding\ngenerally match and sometimes exceed the performance of their base models with\nMBR decoding."
                },
                "authors": [
                    {
                        "name": "Ian Wu"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Amanda Bertsch"
                    },
                    {
                        "name": "Seungone Kim"
                    },
                    {
                        "name": "Sina Pakazad"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02902v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02902v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03225v2",
                "updated": "2024-10-28T17:05:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    5,
                    27,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-04T08:24:15Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    8,
                    24,
                    15,
                    4,
                    278,
                    0
                ],
                "title": "AutoPenBench: Benchmarking Generative Agents for Penetration Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPenBench: Benchmarking Generative Agents for Penetration Testing"
                },
                "summary": "Generative AI agents, software systems powered by Large Language Models\n(LLMs), are emerging as a promising approach to automate cybersecurity tasks.\nAmong the others, penetration testing is a challenging field due to the task\ncomplexity and the diverse strategies to simulate cyber-attacks. Despite\ngrowing interest and initial studies in automating penetration testing with\ngenerative agents, there remains a significant gap in the form of a\ncomprehensive and standard framework for their evaluation and development. This\npaper introduces AutoPenBench, an open benchmark for evaluating generative\nagents in automated penetration testing. We present a comprehensive framework\nthat includes 33 tasks, each representing a vulnerable system that the agent\nhas to attack. Tasks are of increasing difficulty levels, including in-vitro\nand real-world scenarios. We assess the agent performance with generic and\nspecific milestones that allow us to compare results in a standardised manner\nand understand the limits of the agent under test. We show the benefits of\nAutoPenBench by testing two agent architectures: a fully autonomous and a\nsemi-autonomous supporting human interaction. We compare their performance and\nlimitations. For example, the fully autonomous agent performs unsatisfactorily\nachieving a 21% Success Rate (SR) across the benchmark, solving 27% of the\nsimple tasks and only one real-world task. In contrast, the assisted agent\ndemonstrates substantial improvements, with 64% of SR. AutoPenBench allows us\nalso to observe how different LLMs like GPT-4o or OpenAI o1 impact the ability\nof the agents to complete the tasks. We believe that our benchmark fills the\ngap with a standard and flexible framework to compare penetration testing\nagents on a common ground. We hope to extend AutoPenBench along with the\nresearch community by making it available under\nhttps://github.com/lucagioacchini/auto-pen-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI agents, software systems powered by Large Language Models\n(LLMs), are emerging as a promising approach to automate cybersecurity tasks.\nAmong the others, penetration testing is a challenging field due to the task\ncomplexity and the diverse strategies to simulate cyber-attacks. Despite\ngrowing interest and initial studies in automating penetration testing with\ngenerative agents, there remains a significant gap in the form of a\ncomprehensive and standard framework for their evaluation and development. This\npaper introduces AutoPenBench, an open benchmark for evaluating generative\nagents in automated penetration testing. We present a comprehensive framework\nthat includes 33 tasks, each representing a vulnerable system that the agent\nhas to attack. Tasks are of increasing difficulty levels, including in-vitro\nand real-world scenarios. We assess the agent performance with generic and\nspecific milestones that allow us to compare results in a standardised manner\nand understand the limits of the agent under test. We show the benefits of\nAutoPenBench by testing two agent architectures: a fully autonomous and a\nsemi-autonomous supporting human interaction. We compare their performance and\nlimitations. For example, the fully autonomous agent performs unsatisfactorily\nachieving a 21% Success Rate (SR) across the benchmark, solving 27% of the\nsimple tasks and only one real-world task. In contrast, the assisted agent\ndemonstrates substantial improvements, with 64% of SR. AutoPenBench allows us\nalso to observe how different LLMs like GPT-4o or OpenAI o1 impact the ability\nof the agents to complete the tasks. We believe that our benchmark fills the\ngap with a standard and flexible framework to compare penetration testing\nagents on a common ground. We hope to extend AutoPenBench along with the\nresearch community by making it available under\nhttps://github.com/lucagioacchini/auto-pen-bench."
                },
                "authors": [
                    {
                        "name": "Luca Gioacchini"
                    },
                    {
                        "name": "Marco Mellia"
                    },
                    {
                        "name": "Idilio Drago"
                    },
                    {
                        "name": "Alexander Delsanto"
                    },
                    {
                        "name": "Giuseppe Siracusano"
                    },
                    {
                        "name": "Roberto Bifulco"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Bifulco"
                },
                "author": "Roberto Bifulco",
                "arxiv_comment": "Codes for the benchmark:\n  https://github.com/lucagioacchini/auto-pen-bench Codes for the paper\n  experiments: https://github.com/lucagioacchini/genai-pentest-paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01724v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01724v3",
                "updated": "2024-10-28T17:05:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    5,
                    16,
                    0,
                    302,
                    0
                ],
                "published": "2024-07-01T18:57:35Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    18,
                    57,
                    35,
                    0,
                    183,
                    0
                ],
                "title": "Predicting DC-Link Capacitor Current Ripple in AC-DC Rectifier Circuits\n  Using Fine-Tuned Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting DC-Link Capacitor Current Ripple in AC-DC Rectifier Circuits\n  Using Fine-Tuned Large Language Models"
                },
                "summary": "Foundational Large Language Models (LLMs) such as GPT-3.5-turbo allow users\nto refine the model based on newer information, known as ``fine-tuning''. This\npaper leverages this ability to analyze AC-DC converter behaviors, focusing on\nthe ripple current in DC-link capacitors. Capacitors degrade faster under high\nripple currents, complicating life monitoring and necessitating preemptive\nreplacements. Using minimal invasive noisy hardware measurements from a full\nbridge rectifier and 90W Power Factor Correction (PFC) boost converter, an\nLLM-based models to predict ripple content in DC-link currents was developed\nwhich demonstrated the LLMs' ability for near-accurate predictions. This study\nalso highlights data requirements for precise nonlinear power electronic\ncircuit parameter predictions to predict component degradation without any\nadditional sensors. Furthermore, the proposed framework could be extended to\nany non-linear function mapping problem as well as estimating the capacitor\nEquivalent Series Resistance (ESR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundational Large Language Models (LLMs) such as GPT-3.5-turbo allow users\nto refine the model based on newer information, known as ``fine-tuning''. This\npaper leverages this ability to analyze AC-DC converter behaviors, focusing on\nthe ripple current in DC-link capacitors. Capacitors degrade faster under high\nripple currents, complicating life monitoring and necessitating preemptive\nreplacements. Using minimal invasive noisy hardware measurements from a full\nbridge rectifier and 90W Power Factor Correction (PFC) boost converter, an\nLLM-based models to predict ripple content in DC-link currents was developed\nwhich demonstrated the LLMs' ability for near-accurate predictions. This study\nalso highlights data requirements for precise nonlinear power electronic\ncircuit parameter predictions to predict component degradation without any\nadditional sensors. Furthermore, the proposed framework could be extended to\nany non-linear function mapping problem as well as estimating the capacitor\nEquivalent Series Resistance (ESR)."
                },
                "authors": [
                    {
                        "name": "Mohamed Zeid"
                    },
                    {
                        "name": "Subir Majumder"
                    },
                    {
                        "name": "Hasan Ibrahim"
                    },
                    {
                        "name": "Prasad Enjeti"
                    },
                    {
                        "name": "Le Xie"
                    },
                    {
                        "name": "Chao Tian"
                    }
                ],
                "author_detail": {
                    "name": "Chao Tian"
                },
                "author": "Chao Tian",
                "arxiv_comment": "6 pages, 12 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01724v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01724v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21218v1",
                "updated": "2024-10-28T17:02:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    2,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:02:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    2,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Lifting the Veil on the Large Language Model Supply Chain: Composition,\n  Risks, and Mitigations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifting the Veil on the Large Language Model Supply Chain: Composition,\n  Risks, and Mitigations"
                },
                "summary": "Large language models (LLM) have sparked significant impact with regard to\nboth intelligence and productivity. In recent years, a great surge has been\nwitnessed in the introduction of both commercial and open-source LLMs. Many\nbusinesses have adopted the LLMs into their applications to solve their own\ndomain-specific tasks. However, integrating LLMs into specific business\nscenarios requires more than just utilizing the models themselves. Instead, it\nis a systematic process that involves substantial components, which are\ncollectively referred to as the LLM supply chain. The LLM supply chain\ninherently carries risks. Therefore, it is essential to understand the types of\ncomponents that may be introduced into the supply chain and the associated\nrisks, enabling different stakeholders to implement effective mitigation\nmeasures. While some literature touches on risks associated with the LLM supply\nchain, there is currently no paper that explicitly defines its scope,\nidentifies inherent risks, and examines potential mitigation strategies. As\nLLMs have become essential infrastructure in the new era, we believe that a\nthorough review of the LLM supply chain, along with its inherent risks and\nmitigation strategies, would be valuable for industry practitioners to avoid\npotential damages and losses, and enlightening for academic researchers to\nrethink existing approaches and explore new avenues of research. Our paper\nprovides a comprehensive overview of the LLM supply chain, detailing the\nstakeholders, composing artifacts, and the supplying types. We developed\ntaxonomies of risk types, risky actions, and mitigations related to various\nsupply chain stakeholders and components. In summary, our work explores the\ntechnical and operational aspects of the LLM supply chain, offering valuable\ninsights for researchers and engineers in the evolving LLM landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLM) have sparked significant impact with regard to\nboth intelligence and productivity. In recent years, a great surge has been\nwitnessed in the introduction of both commercial and open-source LLMs. Many\nbusinesses have adopted the LLMs into their applications to solve their own\ndomain-specific tasks. However, integrating LLMs into specific business\nscenarios requires more than just utilizing the models themselves. Instead, it\nis a systematic process that involves substantial components, which are\ncollectively referred to as the LLM supply chain. The LLM supply chain\ninherently carries risks. Therefore, it is essential to understand the types of\ncomponents that may be introduced into the supply chain and the associated\nrisks, enabling different stakeholders to implement effective mitigation\nmeasures. While some literature touches on risks associated with the LLM supply\nchain, there is currently no paper that explicitly defines its scope,\nidentifies inherent risks, and examines potential mitigation strategies. As\nLLMs have become essential infrastructure in the new era, we believe that a\nthorough review of the LLM supply chain, along with its inherent risks and\nmitigation strategies, would be valuable for industry practitioners to avoid\npotential damages and losses, and enlightening for academic researchers to\nrethink existing approaches and explore new avenues of research. Our paper\nprovides a comprehensive overview of the LLM supply chain, detailing the\nstakeholders, composing artifacts, and the supplying types. We developed\ntaxonomies of risk types, risky actions, and mitigations related to various\nsupply chain stakeholders and components. In summary, our work explores the\ntechnical and operational aspects of the LLM supply chain, offering valuable\ninsights for researchers and engineers in the evolving LLM landscape."
                },
                "authors": [
                    {
                        "name": "Kaifeng Huang"
                    },
                    {
                        "name": "Bihuan Chen"
                    },
                    {
                        "name": "You Lu"
                    },
                    {
                        "name": "Susheng Wu"
                    },
                    {
                        "name": "Dingji Wang"
                    },
                    {
                        "name": "Yiheng Huang"
                    },
                    {
                        "name": "Haowen Jiang"
                    },
                    {
                        "name": "Zhuotong Zhou"
                    },
                    {
                        "name": "Junming Cao"
                    },
                    {
                        "name": "Xin Peng"
                    }
                ],
                "author_detail": {
                    "name": "Xin Peng"
                },
                "author": "Xin Peng",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21216v1",
                "updated": "2024-10-28T17:01:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    1,
                    52,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:01:52Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    1,
                    52,
                    0,
                    302,
                    0
                ],
                "title": "HoPE: A Novel Positional Encoding Without Long-Term Decay for Enhanced\n  Context Awareness and Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoPE: A Novel Positional Encoding Without Long-Term Decay for Enhanced\n  Context Awareness and Extrapolation"
                },
                "summary": "Many positional encodings (PEs) are designed to exhibit long-term decay,\nbased on an entrenched and long-standing inductive opinion: tokens farther away\nfrom the current position carry less relevant information. We argue that\nlong-term decay is outdated in the era of LLMs, as LLMs are now applied to\ntasks demanding precise retrieval of in-context information from arbitrary\npositions. Firstly, we present empirical analyses on various PEs, demonstrating\nthat models inherently learn attention with only a local-decay pattern while\nforming a U-shape pattern globally, contradicting the principle of long-term\ndecay. Furthermore, we conduct a detailed analysis of rotary position encoding\n(RoPE, a prevalent relative positional encoding in LLMs), and found that the\nU-shape attention is caused by some learned components, which are also the key\nfactor limiting RoPE's expressiveness and extrapolation.Inspired by these\ninsights, we propose High-frequency rotary Position Encoding (HoPE). HoPE\nreplaces the specific components in RoPE with position-independent ones,\nretaining only high-frequency signals, which also breaks the principle of\nlong-term decay in theory. HoPE achieves two major advantages: (1) Without\nconstraints imposed by long-term decay, contradictory factors that limit\nspontaneous attention optimization and model extrapolation performance are\nremoved. (2) Components representing positions and semantics are are optimized.\nThese enhances model's context awareness and extrapolation, as validated by\nextensive experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many positional encodings (PEs) are designed to exhibit long-term decay,\nbased on an entrenched and long-standing inductive opinion: tokens farther away\nfrom the current position carry less relevant information. We argue that\nlong-term decay is outdated in the era of LLMs, as LLMs are now applied to\ntasks demanding precise retrieval of in-context information from arbitrary\npositions. Firstly, we present empirical analyses on various PEs, demonstrating\nthat models inherently learn attention with only a local-decay pattern while\nforming a U-shape pattern globally, contradicting the principle of long-term\ndecay. Furthermore, we conduct a detailed analysis of rotary position encoding\n(RoPE, a prevalent relative positional encoding in LLMs), and found that the\nU-shape attention is caused by some learned components, which are also the key\nfactor limiting RoPE's expressiveness and extrapolation.Inspired by these\ninsights, we propose High-frequency rotary Position Encoding (HoPE). HoPE\nreplaces the specific components in RoPE with position-independent ones,\nretaining only high-frequency signals, which also breaks the principle of\nlong-term decay in theory. HoPE achieves two major advantages: (1) Without\nconstraints imposed by long-term decay, contradictory factors that limit\nspontaneous attention optimization and model extrapolation performance are\nremoved. (2) Components representing positions and semantics are are optimized.\nThese enhances model's context awareness and extrapolation, as validated by\nextensive experiments."
                },
                "authors": [
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Wei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Liu"
                },
                "author": "Wei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02144v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02144v3",
                "updated": "2024-10-28T17:01:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    1,
                    23,
                    0,
                    302,
                    0
                ],
                "published": "2024-05-03T14:48:20Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    14,
                    48,
                    20,
                    4,
                    124,
                    0
                ],
                "title": "MedReadMe: A Systematic Study for Fine-grained Sentence Readability in\n  Medical Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedReadMe: A Systematic Study for Fine-grained Sentence Readability in\n  Medical Domain"
                },
                "summary": "Medical texts are notoriously challenging to read. Properly measuring their\nreadability is the first step towards making them more accessible. In this\npaper, we present a systematic study on fine-grained readability measurements\nin the medical domain at both sentence-level and span-level. We introduce a new\ndataset MedReadMe, which consists of manually annotated readability ratings and\nfine-grained complex span annotation for 4,520 sentences, featuring two novel\n\"Google-Easy\" and \"Google-Hard\" categories. It supports our quantitative\nanalysis, which covers 650 linguistic features and automatic complex word and\njargon identification. Enabled by our high-quality annotation, we benchmark and\nimprove several state-of-the-art sentence-level readability metrics for the\nmedical domain specifically, which include unsupervised, supervised, and\nprompting-based methods using recently developed large language models (LLMs).\nInformed by our fine-grained complex span annotation, we find that adding a\nsingle feature, capturing the number of jargon spans, into existing readability\nformulas can significantly improve their correlation with human judgments. The\ndata is available at tinyurl.com/medreadme-repo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical texts are notoriously challenging to read. Properly measuring their\nreadability is the first step towards making them more accessible. In this\npaper, we present a systematic study on fine-grained readability measurements\nin the medical domain at both sentence-level and span-level. We introduce a new\ndataset MedReadMe, which consists of manually annotated readability ratings and\nfine-grained complex span annotation for 4,520 sentences, featuring two novel\n\"Google-Easy\" and \"Google-Hard\" categories. It supports our quantitative\nanalysis, which covers 650 linguistic features and automatic complex word and\njargon identification. Enabled by our high-quality annotation, we benchmark and\nimprove several state-of-the-art sentence-level readability metrics for the\nmedical domain specifically, which include unsupervised, supervised, and\nprompting-based methods using recently developed large language models (LLMs).\nInformed by our fine-grained complex span annotation, we find that adding a\nsingle feature, capturing the number of jargon spans, into existing readability\nformulas can significantly improve their correlation with human judgments. The\ndata is available at tinyurl.com/medreadme-repo"
                },
                "authors": [
                    {
                        "name": "Chao Jiang"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "This paper has been accepted as oral presentation at EMNLP 2024 main\n  conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02144v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02144v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07076v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07076v3",
                "updated": "2024-10-28T16:39:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    39,
                    35,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-09T17:19:58Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    19,
                    58,
                    2,
                    283,
                    0
                ],
                "title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry\n  Scientific Hypotheses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry\n  Scientific Hypotheses"
                },
                "summary": "Scientific discovery contributes largely to human society's prosperity, and\nrecent progress shows that LLMs could potentially catalyze this process.\nHowever, it is still unclear whether LLMs can discover novel and valid\nhypotheses in chemistry. In this work, we investigate this central research\nquestion: Can LLMs automatically discover novel and valid chemistry research\nhypotheses given only a chemistry research background (consisting of a research\nquestion and/or a background survey), without limitation on the domain of the\nresearch question? After extensive discussions with chemistry experts, we\npropose an assumption that a majority of chemistry hypotheses can be resulted\nfrom a research background and several inspirations. With this key insight, we\nbreak the central question into three smaller fundamental questions. In brief,\nthey are: (1) given a background question, whether LLMs can retrieve good\ninspirations; (2) with background and inspirations, whether LLMs can lead to\nhypothesis; and (3) whether LLMs can identify good hypotheses to rank them\nhigher. To investigate these questions, we construct a benchmark consisting of\n51 chemistry papers published in Nature, Science, or a similar level in 2024\n(all papers are only available online since 2024). Every paper is divided by\nchemistry PhD students into three components: background, inspirations, and\nhypothesis. The goal is to rediscover the hypothesis, given only the background\nand a large randomly selected chemistry literature corpus consisting the ground\ntruth inspiration papers, with LLMs trained with data up to 2023. We also\ndevelop an LLM-based multi-agent framework that leverages the assumption,\nconsisting of three stages reflecting the three smaller questions. The proposed\nmethod can rediscover many hypotheses with very high similarity with the ground\ntruth ones, covering the main innovations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific discovery contributes largely to human society's prosperity, and\nrecent progress shows that LLMs could potentially catalyze this process.\nHowever, it is still unclear whether LLMs can discover novel and valid\nhypotheses in chemistry. In this work, we investigate this central research\nquestion: Can LLMs automatically discover novel and valid chemistry research\nhypotheses given only a chemistry research background (consisting of a research\nquestion and/or a background survey), without limitation on the domain of the\nresearch question? After extensive discussions with chemistry experts, we\npropose an assumption that a majority of chemistry hypotheses can be resulted\nfrom a research background and several inspirations. With this key insight, we\nbreak the central question into three smaller fundamental questions. In brief,\nthey are: (1) given a background question, whether LLMs can retrieve good\ninspirations; (2) with background and inspirations, whether LLMs can lead to\nhypothesis; and (3) whether LLMs can identify good hypotheses to rank them\nhigher. To investigate these questions, we construct a benchmark consisting of\n51 chemistry papers published in Nature, Science, or a similar level in 2024\n(all papers are only available online since 2024). Every paper is divided by\nchemistry PhD students into three components: background, inspirations, and\nhypothesis. The goal is to rediscover the hypothesis, given only the background\nand a large randomly selected chemistry literature corpus consisting the ground\ntruth inspiration papers, with LLMs trained with data up to 2023. We also\ndevelop an LLM-based multi-agent framework that leverages the assumption,\nconsisting of three stages reflecting the three smaller questions. The proposed\nmethod can rediscover many hypotheses with very high similarity with the ground\ntruth ones, covering the main innovations."
                },
                "authors": [
                    {
                        "name": "Zonglin Yang"
                    },
                    {
                        "name": "Wanhao Liu"
                    },
                    {
                        "name": "Ben Gao"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Soujanya Poria"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongzhan Zhou"
                },
                "author": "Dongzhan Zhou",
                "arxiv_comment": "Code and Benchmark are available at\n  https://github.com/ZonglinY/MOOSE-Chem.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07076v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07076v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21195v1",
                "updated": "2024-10-28T16:38:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    38,
                    20,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T16:38:20Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    38,
                    20,
                    0,
                    302,
                    0
                ],
                "title": "Belief in the Machine: Investigating Epistemological Blind Spots of\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Belief in the Machine: Investigating Epistemological Blind Spots of\n  Language Models"
                },
                "summary": "As language models (LMs) become integral to fields like healthcare, law, and\njournalism, their ability to differentiate between fact, belief, and knowledge\nis essential for reliable decision-making. Failure to grasp these distinctions\ncan lead to significant consequences in areas such as medical diagnosis, legal\njudgments, and dissemination of fake news. Despite this, current literature has\nlargely focused on more complex issues such as theory of mind, overlooking more\nfundamental epistemic challenges. This study systematically evaluates the\nepistemic reasoning capabilities of modern LMs, including GPT-4, Claude-3, and\nLlama-3, using a new dataset, KaBLE, consisting of 13,000 questions across 13\ntasks. Our results reveal key limitations. First, while LMs achieve 86%\naccuracy on factual scenarios, their performance drops significantly with false\nscenarios, particularly in belief-related tasks. Second, LMs struggle with\nrecognizing and affirming personal beliefs, especially when those beliefs\ncontradict factual data, which raises concerns for applications in healthcare\nand counseling, where engaging with a person's beliefs is critical. Third, we\nidentify a salient bias in how LMs process first-person versus third-person\nbeliefs, performing better on third-person tasks (80.7%) compared to\nfirst-person tasks (54.4%). Fourth, LMs lack a robust understanding of the\nfactive nature of knowledge, namely, that knowledge inherently requires truth.\nFifth, LMs rely on linguistic cues for fact-checking and sometimes bypass the\ndeeper reasoning. These findings highlight significant concerns about current\nLMs' ability to reason about truth, belief, and knowledge while emphasizing the\nneed for advancements in these areas before broad deployment in critical\nsectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As language models (LMs) become integral to fields like healthcare, law, and\njournalism, their ability to differentiate between fact, belief, and knowledge\nis essential for reliable decision-making. Failure to grasp these distinctions\ncan lead to significant consequences in areas such as medical diagnosis, legal\njudgments, and dissemination of fake news. Despite this, current literature has\nlargely focused on more complex issues such as theory of mind, overlooking more\nfundamental epistemic challenges. This study systematically evaluates the\nepistemic reasoning capabilities of modern LMs, including GPT-4, Claude-3, and\nLlama-3, using a new dataset, KaBLE, consisting of 13,000 questions across 13\ntasks. Our results reveal key limitations. First, while LMs achieve 86%\naccuracy on factual scenarios, their performance drops significantly with false\nscenarios, particularly in belief-related tasks. Second, LMs struggle with\nrecognizing and affirming personal beliefs, especially when those beliefs\ncontradict factual data, which raises concerns for applications in healthcare\nand counseling, where engaging with a person's beliefs is critical. Third, we\nidentify a salient bias in how LMs process first-person versus third-person\nbeliefs, performing better on third-person tasks (80.7%) compared to\nfirst-person tasks (54.4%). Fourth, LMs lack a robust understanding of the\nfactive nature of knowledge, namely, that knowledge inherently requires truth.\nFifth, LMs rely on linguistic cues for fact-checking and sometimes bypass the\ndeeper reasoning. These findings highlight significant concerns about current\nLMs' ability to reason about truth, belief, and knowledge while emphasizing the\nneed for advancements in these areas before broad deployment in critical\nsectors."
                },
                "authors": [
                    {
                        "name": "Mirac Suzgun"
                    },
                    {
                        "name": "Tayfun Gur"
                    },
                    {
                        "name": "Federico Bianchi"
                    },
                    {
                        "name": "Daniel E. Ho"
                    },
                    {
                        "name": "Thomas Icard"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "author": "James Zou",
                "arxiv_comment": "https://github.com/suzgunmirac/belief-in-the-machine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14577v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14577v3",
                "updated": "2024-10-28T16:37:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    37,
                    6,
                    0,
                    302,
                    0
                ],
                "published": "2024-05-23T13:51:55Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    13,
                    51,
                    55,
                    3,
                    144,
                    0
                ],
                "title": "Representation noising can prevent harmful fine-tuning on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation noising can prevent harmful fine-tuning on LLMs"
                },
                "summary": "Releasing open-source large language models (LLMs) presents a dual-use risk\nsince bad actors can easily fine-tune these models for harmful purposes. Even\nwithout the open release of weights, weight stealing and fine-tuning APIs make\nclosed models vulnerable to harmful fine-tuning attacks (HFAs). While safety\nmeasures like preventing jailbreaks and improving safety guardrails are\nimportant, such measures can easily be reversed through fine-tuning. In this\nwork, we propose Representation Noising (RepNoise), a defence mechanism that is\neffective even when attackers have access to the weights. RepNoise works by\nremoving information about harmful representations such that it is difficult to\nrecover them during fine-tuning. Importantly, our defence is also able to\ngeneralize across different subsets of harm that have not been seen during the\ndefence process as long as they are drawn from the same distribution of the\nattack set. Our method does not degrade the general capability of LLMs and\nretains the ability to train the model on harmless tasks. We provide empirical\nevidence that the effectiveness of our defence lies in its \"depth\": the degree\nto which information about harmful representations is removed across all layers\nof the LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Releasing open-source large language models (LLMs) presents a dual-use risk\nsince bad actors can easily fine-tune these models for harmful purposes. Even\nwithout the open release of weights, weight stealing and fine-tuning APIs make\nclosed models vulnerable to harmful fine-tuning attacks (HFAs). While safety\nmeasures like preventing jailbreaks and improving safety guardrails are\nimportant, such measures can easily be reversed through fine-tuning. In this\nwork, we propose Representation Noising (RepNoise), a defence mechanism that is\neffective even when attackers have access to the weights. RepNoise works by\nremoving information about harmful representations such that it is difficult to\nrecover them during fine-tuning. Importantly, our defence is also able to\ngeneralize across different subsets of harm that have not been seen during the\ndefence process as long as they are drawn from the same distribution of the\nattack set. Our method does not degrade the general capability of LLMs and\nretains the ability to train the model on harmless tasks. We provide empirical\nevidence that the effectiveness of our defence lies in its \"depth\": the degree\nto which information about harmful representations is removed across all layers\nof the LLM."
                },
                "authors": [
                    {
                        "name": "Domenic Rosati"
                    },
                    {
                        "name": "Jan Wehner"
                    },
                    {
                        "name": "Kai Williams"
                    },
                    {
                        "name": "Łukasz Bartoszcze"
                    },
                    {
                        "name": "David Atanasov"
                    },
                    {
                        "name": "Robie Gonzales"
                    },
                    {
                        "name": "Subhabrata Majumdar"
                    },
                    {
                        "name": "Carsten Maple"
                    },
                    {
                        "name": "Hassan Sajjad"
                    },
                    {
                        "name": "Frank Rudzicz"
                    }
                ],
                "author_detail": {
                    "name": "Frank Rudzicz"
                },
                "author": "Frank Rudzicz",
                "arxiv_comment": "Published in NeurIPs 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14577v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14577v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01119v2",
                "updated": "2024-10-28T16:32:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    32,
                    1,
                    0,
                    302,
                    0
                ],
                "published": "2024-07-01T09:28:58Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    9,
                    28,
                    58,
                    0,
                    183,
                    0
                ],
                "title": "Pron vs Prompt: Can Large Language Models already Challenge a\n  World-Class Fiction Author at Creative Text Writing?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pron vs Prompt: Can Large Language Models already Challenge a\n  World-Class Fiction Author at Creative Text Writing?"
                },
                "summary": "It has become routine to report research results where Large Language Models\n(LLMs) outperform average humans in a wide range of language-related tasks, and\ncreative text writing is no exception. It seems natural, then, to raise the\nbid: Are LLMs ready to compete in creative writing skills with a top (rather\nthan average) novelist? To provide an initial answer for this question, we have\ncarried out a contest between Patricio Pron (an awarded novelist, considered\none of the best of his generation) and GPT-4 (one of the top performing LLMs),\nin the spirit of AI-human duels such as DeepBlue vs Kasparov and AlphaGo vs Lee\nSidol. We asked Pron and GPT-4 to provide thirty titles each, and then to write\nshort stories for both their titles and their opponent's. Then, we prepared an\nevaluation rubric inspired by Boden's definition of creativity, and we\ncollected 5,400 manual assessments provided by literature critics and scholars.\nThe results of our experimentation indicate that LLMs are still far from\nchallenging a top human creative writer, and that reaching such level of\nautonomous creative writing skills probably cannot be reached simply with\nlarger language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has become routine to report research results where Large Language Models\n(LLMs) outperform average humans in a wide range of language-related tasks, and\ncreative text writing is no exception. It seems natural, then, to raise the\nbid: Are LLMs ready to compete in creative writing skills with a top (rather\nthan average) novelist? To provide an initial answer for this question, we have\ncarried out a contest between Patricio Pron (an awarded novelist, considered\none of the best of his generation) and GPT-4 (one of the top performing LLMs),\nin the spirit of AI-human duels such as DeepBlue vs Kasparov and AlphaGo vs Lee\nSidol. We asked Pron and GPT-4 to provide thirty titles each, and then to write\nshort stories for both their titles and their opponent's. Then, we prepared an\nevaluation rubric inspired by Boden's definition of creativity, and we\ncollected 5,400 manual assessments provided by literature critics and scholars.\nThe results of our experimentation indicate that LLMs are still far from\nchallenging a top human creative writer, and that reaching such level of\nautonomous creative writing skills probably cannot be reached simply with\nlarger language models."
                },
                "authors": [
                    {
                        "name": "Guillermo Marco"
                    },
                    {
                        "name": "Julio Gonzalo"
                    },
                    {
                        "name": "Ramón del Castillo"
                    },
                    {
                        "name": "María Teresa Mateo Girona"
                    }
                ],
                "author_detail": {
                    "name": "María Teresa Mateo Girona"
                },
                "author": "María Teresa Mateo Girona",
                "arxiv_comment": "9 pages 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21170v1",
                "updated": "2024-10-28T16:13:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    13,
                    44,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T16:13:44Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    13,
                    44,
                    0,
                    302,
                    0
                ],
                "title": "Joint Audio-Visual Idling Vehicle Detection with Streamlined Input\n  Dependencies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Audio-Visual Idling Vehicle Detection with Streamlined Input\n  Dependencies"
                },
                "summary": "Idling vehicle detection (IVD) can be helpful in monitoring and reducing\nunnecessary idling and can be integrated into real-time systems to address the\nresulting pollution and harmful products. The previous approach [13], a\nnon-end-to-end model, requires extra user clicks to specify a part of the\ninput, making system deployment more error-prone or even not feasible. In\ncontrast, we introduce an end-to-end joint audio-visual IVD task designed to\ndetect vehicles visually under three states: moving, idling and engine off.\nUnlike feature co-occurrence task such as audio-visual vehicle tracking, our\nIVD task addresses complementary features, where labels cannot be determined by\na single modality alone. To this end, we propose AVIVD-Net, a novel network\nthat integrates audio and visual features through a bidirectional attention\nmechanism. AVIVD-Net streamlines the input process by learning a joint feature\nspace, reducing the deployment complexity of previous methods. Additionally, we\nintroduce the AVIVD dataset, which is seven times larger than previous\ndatasets, offering significantly more annotated samples to study the IVD\nproblem. Our model achieves performance comparable to prior approaches, making\nit suitable for automated deployment. Furthermore, by evaluating AVIVDNet on\nthe feature co-occurrence public dataset MAVD [23], we demonstrate its\npotential for extension to self-driving vehicle video-camera setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Idling vehicle detection (IVD) can be helpful in monitoring and reducing\nunnecessary idling and can be integrated into real-time systems to address the\nresulting pollution and harmful products. The previous approach [13], a\nnon-end-to-end model, requires extra user clicks to specify a part of the\ninput, making system deployment more error-prone or even not feasible. In\ncontrast, we introduce an end-to-end joint audio-visual IVD task designed to\ndetect vehicles visually under three states: moving, idling and engine off.\nUnlike feature co-occurrence task such as audio-visual vehicle tracking, our\nIVD task addresses complementary features, where labels cannot be determined by\na single modality alone. To this end, we propose AVIVD-Net, a novel network\nthat integrates audio and visual features through a bidirectional attention\nmechanism. AVIVD-Net streamlines the input process by learning a joint feature\nspace, reducing the deployment complexity of previous methods. Additionally, we\nintroduce the AVIVD dataset, which is seven times larger than previous\ndatasets, offering significantly more annotated samples to study the IVD\nproblem. Our model achieves performance comparable to prior approaches, making\nit suitable for automated deployment. Furthermore, by evaluating AVIVDNet on\nthe feature co-occurrence public dataset MAVD [23], we demonstrate its\npotential for extension to self-driving vehicle video-camera setups."
                },
                "authors": [
                    {
                        "name": "Xiwen Li"
                    },
                    {
                        "name": "Rehman Mohammed"
                    },
                    {
                        "name": "Tristalee Mangin"
                    },
                    {
                        "name": "Surojit Saha"
                    },
                    {
                        "name": "Ross T Whitaker"
                    },
                    {
                        "name": "Kerry E. Kelly"
                    },
                    {
                        "name": "Tolga Tasdizen"
                    }
                ],
                "author_detail": {
                    "name": "Tolga Tasdizen"
                },
                "author": "Tolga Tasdizen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07018v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07018v3",
                "updated": "2024-10-28T16:03:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    3,
                    20,
                    0,
                    302,
                    0
                ],
                "published": "2024-07-09T16:38:48Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    16,
                    38,
                    48,
                    1,
                    191,
                    0
                ],
                "title": "End-To-End Causal Effect Estimation from Unstructured Natural Language\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-To-End Causal Effect Estimation from Unstructured Natural Language\n  Data"
                },
                "summary": "Knowing the effect of an intervention is critical for human decision-making,\nbut current approaches for causal effect estimation rely on manual data\ncollection and structuring, regardless of the causal assumptions. This\nincreases both the cost and time-to-completion for studies. We show how large,\ndiverse observational text data can be mined with large language models (LLMs)\nto produce inexpensive causal effect estimates under appropriate causal\nassumptions. We introduce NATURAL, a novel family of causal effect estimators\nbuilt with LLMs that operate over datasets of unstructured text. Our estimators\nuse LLM conditional distributions (over variables of interest, given the text\ndata) to assist in the computation of classical estimators of causal effect. We\novercome a number of technical challenges to realize this idea, such as\nautomating data curation and using LLMs to impute missing information. We\nprepare six (two synthetic and four real) observational datasets, paired with\ncorresponding ground truth in the form of randomized trials, which we used to\nsystematically evaluate each step of our pipeline. NATURAL estimators\ndemonstrate remarkable performance, yielding causal effect estimates that fall\nwithin 3 percentage points of their ground truth counterparts, including on\nreal-world Phase 3/4 clinical trials. Our results suggest that unstructured\ntext data is a rich source of causal effect information, and NATURAL is a first\nstep towards an automated pipeline to tap this resource.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowing the effect of an intervention is critical for human decision-making,\nbut current approaches for causal effect estimation rely on manual data\ncollection and structuring, regardless of the causal assumptions. This\nincreases both the cost and time-to-completion for studies. We show how large,\ndiverse observational text data can be mined with large language models (LLMs)\nto produce inexpensive causal effect estimates under appropriate causal\nassumptions. We introduce NATURAL, a novel family of causal effect estimators\nbuilt with LLMs that operate over datasets of unstructured text. Our estimators\nuse LLM conditional distributions (over variables of interest, given the text\ndata) to assist in the computation of classical estimators of causal effect. We\novercome a number of technical challenges to realize this idea, such as\nautomating data curation and using LLMs to impute missing information. We\nprepare six (two synthetic and four real) observational datasets, paired with\ncorresponding ground truth in the form of randomized trials, which we used to\nsystematically evaluate each step of our pipeline. NATURAL estimators\ndemonstrate remarkable performance, yielding causal effect estimates that fall\nwithin 3 percentage points of their ground truth counterparts, including on\nreal-world Phase 3/4 clinical trials. Our results suggest that unstructured\ntext data is a rich source of causal effect information, and NATURAL is a first\nstep towards an automated pipeline to tap this resource."
                },
                "authors": [
                    {
                        "name": "Nikita Dhawan"
                    },
                    {
                        "name": "Leonardo Cotta"
                    },
                    {
                        "name": "Karen Ullrich"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    },
                    {
                        "name": "Chris J. Maddison"
                    }
                ],
                "author_detail": {
                    "name": "Chris J. Maddison"
                },
                "author": "Chris J. Maddison",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07018v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07018v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21159v1",
                "updated": "2024-10-28T15:59:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    59,
                    31,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:59:31Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    59,
                    31,
                    0,
                    302,
                    0
                ],
                "title": "CURATe: Benchmarking Personalised Alignment of Conversational AI\n  Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CURATe: Benchmarking Personalised Alignment of Conversational AI\n  Assistants"
                },
                "summary": "We introduce a multi-turn benchmark for evaluating personalised alignment in\nLLM-based AI assistants, focusing on their ability to handle user-provided\nsafety-critical contexts. Our assessment of ten leading models across five\nscenarios (each with 337 use cases) reveals systematic inconsistencies in\nmaintaining user-specific consideration, with even top-rated \"harmless\" models\nmaking recommendations that should be recognised as obviously harmful to the\nuser given the context provided. Key failure modes include inappropriate\nweighing of conflicting preferences, sycophancy (prioritising user preferences\nabove safety), a lack of attentiveness to critical user information within the\ncontext window, and inconsistent application of user-specific knowledge. The\nsame systematic biases were observed in OpenAI's o1, suggesting that strong\nreasoning capacities do not necessarily transfer to this kind of personalised\nthinking. We find that prompting LLMs to consider safety-critical context\nsignificantly improves performance, unlike a generic 'harmless and helpful'\ninstruction. Based on these findings, we propose research directions for\nembedding self-reflection capabilities, online user modelling, and dynamic risk\nassessment in AI assistants. Our work emphasises the need for nuanced,\ncontext-aware approaches to alignment in systems designed for persistent human\ninteraction, aiding the development of safe and considerate AI assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a multi-turn benchmark for evaluating personalised alignment in\nLLM-based AI assistants, focusing on their ability to handle user-provided\nsafety-critical contexts. Our assessment of ten leading models across five\nscenarios (each with 337 use cases) reveals systematic inconsistencies in\nmaintaining user-specific consideration, with even top-rated \"harmless\" models\nmaking recommendations that should be recognised as obviously harmful to the\nuser given the context provided. Key failure modes include inappropriate\nweighing of conflicting preferences, sycophancy (prioritising user preferences\nabove safety), a lack of attentiveness to critical user information within the\ncontext window, and inconsistent application of user-specific knowledge. The\nsame systematic biases were observed in OpenAI's o1, suggesting that strong\nreasoning capacities do not necessarily transfer to this kind of personalised\nthinking. We find that prompting LLMs to consider safety-critical context\nsignificantly improves performance, unlike a generic 'harmless and helpful'\ninstruction. Based on these findings, we propose research directions for\nembedding self-reflection capabilities, online user modelling, and dynamic risk\nassessment in AI assistants. Our work emphasises the need for nuanced,\ncontext-aware approaches to alignment in systems designed for persistent human\ninteraction, aiding the development of safe and considerate AI assistants."
                },
                "authors": [
                    {
                        "name": "Lize Alberts"
                    },
                    {
                        "name": "Benjamin Ellis"
                    },
                    {
                        "name": "Andrei Lupu"
                    },
                    {
                        "name": "Jakob Foerster"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Foerster"
                },
                "author": "Jakob Foerster",
                "arxiv_comment": "Submitted to ICLR 2025 on 01/10/2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; K.4.2; H.5.2; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21157v1",
                "updated": "2024-10-28T15:58:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    58,
                    41,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:58:41Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    58,
                    41,
                    0,
                    302,
                    0
                ],
                "title": "M2rc-Eval: Massively Multilingual Repository-level Code Completion\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M2rc-Eval: Massively Multilingual Repository-level Code Completion\n  Evaluation"
                },
                "summary": "Repository-level code completion has drawn great attention in software\nengineering, and several benchmark datasets have been introduced. However,\nexisting repository-level code completion benchmarks usually focus on a limited\nnumber of languages (<5), which cannot evaluate the general code intelligence\nabilities across different languages for existing code Large Language Models\n(LLMs). Besides, the existing benchmarks usually report overall average scores\nof different languages, where the fine-grained abilities in different\ncompletion scenarios are ignored. Therefore, to facilitate the research of code\nLLMs in multilingual scenarios, we propose a massively multilingual\nrepository-level code completion benchmark covering 18 programming languages\n(called M2RC-EVAL), and two types of fine-grained annotations (i.e.,\nbucket-level and semantic-level) on different completion scenarios are\nprovided, where we obtain these annotations based on the parsed abstract syntax\ntree. Moreover, we also curate a massively multilingual instruction corpora\nM2RC- INSTRUCT dataset to improve the repository-level code completion\nabilities of existing code LLMs. Comprehensive experimental results demonstrate\nthe effectiveness of our M2RC-EVAL and M2RC-INSTRUCT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level code completion has drawn great attention in software\nengineering, and several benchmark datasets have been introduced. However,\nexisting repository-level code completion benchmarks usually focus on a limited\nnumber of languages (<5), which cannot evaluate the general code intelligence\nabilities across different languages for existing code Large Language Models\n(LLMs). Besides, the existing benchmarks usually report overall average scores\nof different languages, where the fine-grained abilities in different\ncompletion scenarios are ignored. Therefore, to facilitate the research of code\nLLMs in multilingual scenarios, we propose a massively multilingual\nrepository-level code completion benchmark covering 18 programming languages\n(called M2RC-EVAL), and two types of fine-grained annotations (i.e.,\nbucket-level and semantic-level) on different completion scenarios are\nprovided, where we obtain these annotations based on the parsed abstract syntax\ntree. Moreover, we also curate a massively multilingual instruction corpora\nM2RC- INSTRUCT dataset to improve the repository-level code completion\nabilities of existing code LLMs. Comprehensive experimental results demonstrate\nthe effectiveness of our M2RC-EVAL and M2RC-INSTRUCT."
                },
                "authors": [
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ken Deng"
                    },
                    {
                        "name": "Congnan Liu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Peng Zhao"
                    },
                    {
                        "name": "Linzheng Chai"
                    },
                    {
                        "name": "Yanan Wu"
                    },
                    {
                        "name": "Ke Jin"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Guoan Zhang"
                    },
                    {
                        "name": "Bangyu Xiang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21155v1",
                "updated": "2024-10-28T15:56:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    56,
                    49,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:56:49Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    56,
                    49,
                    0,
                    302,
                    0
                ],
                "title": "SciER: An Entity and Relation Extraction Dataset for Datasets, Methods,\n  and Tasks in Scientific Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciER: An Entity and Relation Extraction Dataset for Datasets, Methods,\n  and Tasks in Scientific Documents"
                },
                "summary": "Scientific information extraction (SciIE) is critical for converting\nunstructured knowledge from scholarly articles into structured data (entities\nand relations). Several datasets have been proposed for training and validating\nSciIE models. However, due to the high complexity and cost of annotating\nscientific texts, those datasets restrict their annotations to specific parts\nof paper, such as abstracts, resulting in the loss of diverse entity mentions\nand relations in context. In this paper, we release a new entity and relation\nextraction dataset for entities related to datasets, methods, and tasks in\nscientific articles. Our dataset contains 106 manually annotated full-text\nscientific publications with over 24k entities and 12k relations. To capture\nthe intricate use and interactions among entities in full texts, our dataset\ncontains a fine-grained tag set for relations. Additionally, we provide an\nout-of-distribution test set to offer a more realistic evaluation. We conduct\ncomprehensive experiments, including state-of-the-art supervised models and our\nproposed LLM-based baselines, and highlight the challenges presented by our\ndataset, encouraging the development of innovative models to further the field\nof SciIE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific information extraction (SciIE) is critical for converting\nunstructured knowledge from scholarly articles into structured data (entities\nand relations). Several datasets have been proposed for training and validating\nSciIE models. However, due to the high complexity and cost of annotating\nscientific texts, those datasets restrict their annotations to specific parts\nof paper, such as abstracts, resulting in the loss of diverse entity mentions\nand relations in context. In this paper, we release a new entity and relation\nextraction dataset for entities related to datasets, methods, and tasks in\nscientific articles. Our dataset contains 106 manually annotated full-text\nscientific publications with over 24k entities and 12k relations. To capture\nthe intricate use and interactions among entities in full texts, our dataset\ncontains a fine-grained tag set for relations. Additionally, we provide an\nout-of-distribution test set to offer a more realistic evaluation. We conduct\ncomprehensive experiments, including state-of-the-art supervised models and our\nproposed LLM-based baselines, and highlight the challenges presented by our\ndataset, encouraging the development of innovative models to further the field\nof SciIE."
                },
                "authors": [
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Zhijia Chen"
                    },
                    {
                        "name": "Huitong Pan"
                    },
                    {
                        "name": "Cornelia Caragea"
                    },
                    {
                        "name": "Longin Jan Latecki"
                    },
                    {
                        "name": "Eduard Dragut"
                    }
                ],
                "author_detail": {
                    "name": "Eduard Dragut"
                },
                "author": "Eduard Dragut",
                "arxiv_comment": "EMNLP2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21146v1",
                "updated": "2024-10-28T15:47:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    47,
                    3,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:47:03Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    47,
                    3,
                    0,
                    302,
                    0
                ],
                "title": "Palisade -- Prompt Injection Detection Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palisade -- Prompt Injection Detection Framework"
                },
                "summary": "The advent of Large Language Models LLMs marks a milestone in Artificial\nIntelligence, altering how machines comprehend and generate human language.\nHowever, LLMs are vulnerable to malicious prompt injection attacks, where\ncrafted inputs manipulate the models behavior in unintended ways, compromising\nsystem integrity and causing incorrect outcomes. Conventional detection methods\nrely on static, rule-based approaches, which often fail against sophisticated\nthreats like abnormal token sequences and alias substitutions, leading to\nlimited adaptability and higher rates of false positives and false\nnegatives.This paper proposes a novel NLP based approach for prompt injection\ndetection, emphasizing accuracy and optimization through a layered input\nscreening process. In this framework, prompts are filtered through three\ndistinct layers rule-based, ML classifier, and companion LLM before reaching\nthe target model, thereby minimizing the risk of malicious interaction.Tests\nshow the ML classifier achieves the highest accuracy among individual layers,\nyet the multi-layer framework enhances overall detection accuracy by reducing\nfalse negatives. Although this increases false positives, it minimizes the risk\nof overlooking genuine injected prompts, thus prioritizing security.This\nmulti-layered detection approach highlights LLM vulnerabilities and provides a\ncomprehensive framework for future research, promoting secure interactions\nbetween humans and AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models LLMs marks a milestone in Artificial\nIntelligence, altering how machines comprehend and generate human language.\nHowever, LLMs are vulnerable to malicious prompt injection attacks, where\ncrafted inputs manipulate the models behavior in unintended ways, compromising\nsystem integrity and causing incorrect outcomes. Conventional detection methods\nrely on static, rule-based approaches, which often fail against sophisticated\nthreats like abnormal token sequences and alias substitutions, leading to\nlimited adaptability and higher rates of false positives and false\nnegatives.This paper proposes a novel NLP based approach for prompt injection\ndetection, emphasizing accuracy and optimization through a layered input\nscreening process. In this framework, prompts are filtered through three\ndistinct layers rule-based, ML classifier, and companion LLM before reaching\nthe target model, thereby minimizing the risk of malicious interaction.Tests\nshow the ML classifier achieves the highest accuracy among individual layers,\nyet the multi-layer framework enhances overall detection accuracy by reducing\nfalse negatives. Although this increases false positives, it minimizes the risk\nof overlooking genuine injected prompts, thus prioritizing security.This\nmulti-layered detection approach highlights LLM vulnerabilities and provides a\ncomprehensive framework for future research, promoting secure interactions\nbetween humans and AI systems."
                },
                "authors": [
                    {
                        "name": "Sahasra Kokkula"
                    },
                    {
                        "name": "Somanathan R"
                    },
                    {
                        "name": "Nandavardhan R"
                    },
                    {
                        "name": "Aashishkumar"
                    },
                    {
                        "name": "G Divya"
                    }
                ],
                "author_detail": {
                    "name": "G Divya"
                },
                "author": "G Divya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21141v1",
                "updated": "2024-10-28T15:43:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    31,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:43:31Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    31,
                    0,
                    302,
                    0
                ],
                "title": "LLM-initialized Differentiable Causal Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-initialized Differentiable Causal Discovery"
                },
                "summary": "The discovery of causal relationships between random variables is an\nimportant yet challenging problem that has applications across many scientific\ndomains. Differentiable causal discovery (DCD) methods are effective in\nuncovering causal relationships from observational data; however, these\napproaches often suffer from limited interpretability and face challenges in\nincorporating domain-specific prior knowledge. In contrast, Large Language\nModels (LLMs)-based causal discovery approaches have recently been shown\ncapable of providing useful priors for causal discovery but struggle with\nformal causal reasoning. In this paper, we propose LLM-DCD, which uses an LLM\nto initialize the optimization of the maximum likelihood objective function of\nDCD approaches, thereby incorporating strong priors into the discovery method.\nTo achieve this initialization, we design our objective function to depend on\nan explicitly defined adjacency matrix of the causal graph as its only\nvariational parameter. Directly optimizing the explicitly defined adjacency\nmatrix provides a more interpretable approach to causal discovery.\nAdditionally, we demonstrate higher accuracy on key benchmarking datasets of\nour approach compared to state-of-the-art alternatives, and provide empirical\nevidence that the quality of the initialization directly impacts the quality of\nthe final output of our DCD approach. LLM-DCD opens up new opportunities for\ntraditional causal discovery methods like DCD to benefit from future\nimprovements in the causal reasoning capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discovery of causal relationships between random variables is an\nimportant yet challenging problem that has applications across many scientific\ndomains. Differentiable causal discovery (DCD) methods are effective in\nuncovering causal relationships from observational data; however, these\napproaches often suffer from limited interpretability and face challenges in\nincorporating domain-specific prior knowledge. In contrast, Large Language\nModels (LLMs)-based causal discovery approaches have recently been shown\ncapable of providing useful priors for causal discovery but struggle with\nformal causal reasoning. In this paper, we propose LLM-DCD, which uses an LLM\nto initialize the optimization of the maximum likelihood objective function of\nDCD approaches, thereby incorporating strong priors into the discovery method.\nTo achieve this initialization, we design our objective function to depend on\nan explicitly defined adjacency matrix of the causal graph as its only\nvariational parameter. Directly optimizing the explicitly defined adjacency\nmatrix provides a more interpretable approach to causal discovery.\nAdditionally, we demonstrate higher accuracy on key benchmarking datasets of\nour approach compared to state-of-the-art alternatives, and provide empirical\nevidence that the quality of the initialization directly impacts the quality of\nthe final output of our DCD approach. LLM-DCD opens up new opportunities for\ntraditional causal discovery methods like DCD to benefit from future\nimprovements in the causal reasoning capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Shiv Kampani"
                    },
                    {
                        "name": "David Hidary"
                    },
                    {
                        "name": "Constantijn van der Poel"
                    },
                    {
                        "name": "Martin Ganahl"
                    },
                    {
                        "name": "Brenda Miao"
                    }
                ],
                "author_detail": {
                    "name": "Brenda Miao"
                },
                "author": "Brenda Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21136v1",
                "updated": "2024-10-28T15:37:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    37,
                    6,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:37:06Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    37,
                    6,
                    0,
                    302,
                    0
                ],
                "title": "Do LLMs generate test oracles that capture the actual or the expected\n  program behaviour?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs generate test oracles that capture the actual or the expected\n  program behaviour?"
                },
                "summary": "Software testing is an essential part of the software development cycle to\nimprove the code quality. Typically, a unit test consists of a test prefix and\na test oracle which captures the developer's intended behaviour. A known\nlimitation of traditional test generation techniques (e.g. Randoop and\nEvosuite) is that they produce test oracles that capture the actual program\nbehaviour rather than the expected one. Recent approaches leverage Large\nLanguage Models (LLMs), trained on an enormous amount of data, to generate\ndeveloper-like code and test cases. We investigate whether the LLM-generated\ntest oracles capture the actual or expected software behaviour. We thus,\nconduct a controlled experiment to answer this question, by studying LLMs\nperformance on two tasks, namely, test oracle classification and generation.\nThe study includes developer-written and automatically generated test cases and\noracles for 24 open-source Java repositories, and different well tested\nprompts. Our findings show that LLM-based test generation approaches are also\nprone on generating oracles that capture the actual program behaviour rather\nthan the expected one. Moreover, LLMs are better at generating test oracles\nrather than classifying the correct ones, and can generate better test oracles\nwhen the code contains meaningful test or variable names. Finally,\nLLM-generated test oracles have higher fault detection potential than the\nEvosuite ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software testing is an essential part of the software development cycle to\nimprove the code quality. Typically, a unit test consists of a test prefix and\na test oracle which captures the developer's intended behaviour. A known\nlimitation of traditional test generation techniques (e.g. Randoop and\nEvosuite) is that they produce test oracles that capture the actual program\nbehaviour rather than the expected one. Recent approaches leverage Large\nLanguage Models (LLMs), trained on an enormous amount of data, to generate\ndeveloper-like code and test cases. We investigate whether the LLM-generated\ntest oracles capture the actual or expected software behaviour. We thus,\nconduct a controlled experiment to answer this question, by studying LLMs\nperformance on two tasks, namely, test oracle classification and generation.\nThe study includes developer-written and automatically generated test cases and\noracles for 24 open-source Java repositories, and different well tested\nprompts. Our findings show that LLM-based test generation approaches are also\nprone on generating oracles that capture the actual program behaviour rather\nthan the expected one. Moreover, LLMs are better at generating test oracles\nrather than classifying the correct ones, and can generate better test oracles\nwhen the code contains meaningful test or variable names. Finally,\nLLM-generated test oracles have higher fault detection potential than the\nEvosuite ones."
                },
                "authors": [
                    {
                        "name": "Michael Konstantinou"
                    },
                    {
                        "name": "Renzo Degiovanni"
                    },
                    {
                        "name": "Mike Papadakis"
                    }
                ],
                "author_detail": {
                    "name": "Mike Papadakis"
                },
                "author": "Mike Papadakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21131v1",
                "updated": "2024-10-28T15:33:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    33,
                    37,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T15:33:37Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    33,
                    37,
                    0,
                    302,
                    0
                ],
                "title": "Towards Unifying Evaluation of Counterfactual Explanations: Leveraging\n  Large Language Models for Human-Centric Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Unifying Evaluation of Counterfactual Explanations: Leveraging\n  Large Language Models for Human-Centric Assessments"
                },
                "summary": "As machine learning models evolve, maintaining transparency demands more\nhuman-centric explainable AI techniques. Counterfactual explanations, with\nroots in human reasoning, identify the minimal input changes needed to obtain a\ngiven output and, hence, are crucial for supporting decision-making. Despite\ntheir importance, the evaluation of these explanations often lacks grounding in\nuser studies and remains fragmented, with existing metrics not fully capturing\nhuman perspectives. To address this challenge, we developed a diverse set of 30\ncounterfactual scenarios and collected ratings across 8 evaluation metrics from\n206 respondents. Subsequently, we fine-tuned different Large Language Models\n(LLMs) to predict average or individual human judgment across these metrics.\nOur methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot\nevaluations and 85% (over a 3-classes prediction) with fine-tuning across all\nmetrics. The fine-tuned models predicting human ratings offer better\ncomparability and scalability in evaluating different counterfactual\nexplanation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As machine learning models evolve, maintaining transparency demands more\nhuman-centric explainable AI techniques. Counterfactual explanations, with\nroots in human reasoning, identify the minimal input changes needed to obtain a\ngiven output and, hence, are crucial for supporting decision-making. Despite\ntheir importance, the evaluation of these explanations often lacks grounding in\nuser studies and remains fragmented, with existing metrics not fully capturing\nhuman perspectives. To address this challenge, we developed a diverse set of 30\ncounterfactual scenarios and collected ratings across 8 evaluation metrics from\n206 respondents. Subsequently, we fine-tuned different Large Language Models\n(LLMs) to predict average or individual human judgment across these metrics.\nOur methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot\nevaluations and 85% (over a 3-classes prediction) with fine-tuning across all\nmetrics. The fine-tuned models predicting human ratings offer better\ncomparability and scalability in evaluating different counterfactual\nexplanation frameworks."
                },
                "authors": [
                    {
                        "name": "Marharyta Domnich"
                    },
                    {
                        "name": "Julius Valja"
                    },
                    {
                        "name": "Rasmus Moorits Veski"
                    },
                    {
                        "name": "Giacomo Magnifico"
                    },
                    {
                        "name": "Kadi Tulver"
                    },
                    {
                        "name": "Eduard Barbu"
                    },
                    {
                        "name": "Raul Vicente"
                    }
                ],
                "author_detail": {
                    "name": "Raul Vicente"
                },
                "author": "Raul Vicente",
                "arxiv_comment": "This paper has been submitted in August and is currently under review\n  to AAAI-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06741v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06741v2",
                "updated": "2024-10-28T15:05:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    5,
                    54,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-09T10:20:32Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    10,
                    20,
                    32,
                    2,
                    283,
                    0
                ],
                "title": "CoBa: Convergence Balancer for Multitask Finetuning of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoBa: Convergence Balancer for Multitask Finetuning of Large Language\n  Models"
                },
                "summary": "Multi-task learning (MTL) benefits the fine-tuning of large language models\n(LLMs) by providing a single model with improved performance and generalization\nability across tasks, presenting a resource-efficient alternative to developing\nseparate models for each task. Yet, existing MTL strategies for LLMs often fall\nshort by either being computationally intensive or failing to ensure\nsimultaneous task convergence. This paper presents CoBa, a new MTL approach\ndesigned to effectively manage task convergence balance with minimal\ncomputational overhead. Utilizing Relative Convergence Scores (RCS), Absolute\nConvergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically\nadjusts task weights during the training process, ensuring that the validation\nloss of all tasks progress towards convergence at an even pace while mitigating\nthe issue of individual task divergence. The results of our experiments\ninvolving three disparate datasets underscore that this approach not only\nfosters equilibrium in task convergence but enhances the LLMs' performance by\nup to 13% relative to the second-best baselines. Code is open-sourced at\nhttps://github.com/codefuse-ai/MFTCoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-task learning (MTL) benefits the fine-tuning of large language models\n(LLMs) by providing a single model with improved performance and generalization\nability across tasks, presenting a resource-efficient alternative to developing\nseparate models for each task. Yet, existing MTL strategies for LLMs often fall\nshort by either being computationally intensive or failing to ensure\nsimultaneous task convergence. This paper presents CoBa, a new MTL approach\ndesigned to effectively manage task convergence balance with minimal\ncomputational overhead. Utilizing Relative Convergence Scores (RCS), Absolute\nConvergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically\nadjusts task weights during the training process, ensuring that the validation\nloss of all tasks progress towards convergence at an even pace while mitigating\nthe issue of individual task divergence. The results of our experiments\ninvolving three disparate datasets underscore that this approach not only\nfosters equilibrium in task convergence but enhances the LLMs' performance by\nup to 13% relative to the second-best baselines. Code is open-sourced at\nhttps://github.com/codefuse-ai/MFTCoder."
                },
                "authors": [
                    {
                        "name": "Zi Gong"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Cong Liao"
                    },
                    {
                        "name": "Bingchang Liu"
                    },
                    {
                        "name": "Chaoyu Chen"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "arxiv_comment": "15 pages, main conference of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06741v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21083v1",
                "updated": "2024-10-28T14:48:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    48,
                    5,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:48:05Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    48,
                    5,
                    0,
                    302,
                    0
                ],
                "title": "Stealthy Jailbreak Attacks on Large Language Models via Benign Data\n  Mirroring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stealthy Jailbreak Attacks on Large Language Models via Benign Data\n  Mirroring"
                },
                "summary": "Large language model (LLM) safety is a critical issue, with numerous studies\nemploying red team testing to enhance model security. Among these, jailbreak\nmethods explore potential vulnerabilities by crafting malicious prompts that\ninduce model outputs contrary to safety alignments. Existing black-box\njailbreak methods often rely on model feedback, repeatedly submitting queries\nwith detectable malicious instructions during the attack search process.\nAlthough these approaches are effective, the attacks may be intercepted by\ncontent moderators during the search process. We propose an improved transfer\nattack method that guides malicious prompt construction by locally training a\nmirror model of the target black-box model through benign data distillation.\nThis method offers enhanced stealth, as it does not involve submitting\nidentifiable malicious instructions to the target model during the search\nphase. Our approach achieved a maximum attack success rate of 92%, or a\nbalanced value of 80% with an average of 1.5 detectable jailbreak queries per\nsample against GPT-3.5 Turbo on a subset of AdvBench. These results underscore\nthe need for more robust defense mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) safety is a critical issue, with numerous studies\nemploying red team testing to enhance model security. Among these, jailbreak\nmethods explore potential vulnerabilities by crafting malicious prompts that\ninduce model outputs contrary to safety alignments. Existing black-box\njailbreak methods often rely on model feedback, repeatedly submitting queries\nwith detectable malicious instructions during the attack search process.\nAlthough these approaches are effective, the attacks may be intercepted by\ncontent moderators during the search process. We propose an improved transfer\nattack method that guides malicious prompt construction by locally training a\nmirror model of the target black-box model through benign data distillation.\nThis method offers enhanced stealth, as it does not involve submitting\nidentifiable malicious instructions to the target model during the search\nphase. Our approach achieved a maximum attack success rate of 92%, or a\nbalanced value of 80% with an average of 1.5 detectable jailbreak queries per\nsample against GPT-3.5 Turbo on a subset of AdvBench. These results underscore\nthe need for more robust defense mechanisms."
                },
                "authors": [
                    {
                        "name": "Honglin Mu"
                    },
                    {
                        "name": "Han He"
                    },
                    {
                        "name": "Yuxin Zhou"
                    },
                    {
                        "name": "Yunlong Feng"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Xiaoming Shi"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Qi Shi"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07836v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07836v3",
                "updated": "2024-10-28T14:46:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    46,
                    43,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-10T11:52:07Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    52,
                    7,
                    3,
                    284,
                    0
                ],
                "title": "Masked Generative Priors Improve World Models Sequence Modelling\n  Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Generative Priors Improve World Models Sequence Modelling\n  Capabilities"
                },
                "summary": "Deep Reinforcement Learning (RL) has become the leading approach for creating\nartificial agents in complex environments. Model-based approaches, which are RL\nmethods with world models that predict environment dynamics, are among the most\npromising directions for improving data efficiency, forming a critical step\ntoward bridging the gap between research and real-world deployment. In\nparticular, world models enhance sample efficiency by learning in imagination,\nwhich involves training a generative sequence model of the environment in a\nself-supervised manner. Recently, Masked Generative Modelling has emerged as a\nmore efficient and superior inductive bias for modelling and generating token\nsequences. Building on the Efficient Stochastic Transformer-based World Models\n(STORM) architecture, we replace the traditional MLP prior with a Masked\nGenerative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our\nmodel on two downstream tasks: reinforcement learning and video prediction.\nGIT-STORM demonstrates substantial performance gains in RL tasks on the Atari\n100k benchmark. Moreover, we apply Transformer-based World Models to continuous\naction environments for the first time, addressing a significant gap in prior\nresearch. To achieve this, we employ a state mixer function that integrates\nlatent state representations with actions, enabling our model to handle\ncontinuous control tasks. We validate this approach through qualitative and\nquantitative analyses on the DeepMind Control Suite, showcasing the\neffectiveness of Transformer-based World Models in this new domain. Our results\nhighlight the versatility and efficacy of the MaskGIT dynamics prior, paving\nthe way for more accurate world models and effective RL policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Reinforcement Learning (RL) has become the leading approach for creating\nartificial agents in complex environments. Model-based approaches, which are RL\nmethods with world models that predict environment dynamics, are among the most\npromising directions for improving data efficiency, forming a critical step\ntoward bridging the gap between research and real-world deployment. In\nparticular, world models enhance sample efficiency by learning in imagination,\nwhich involves training a generative sequence model of the environment in a\nself-supervised manner. Recently, Masked Generative Modelling has emerged as a\nmore efficient and superior inductive bias for modelling and generating token\nsequences. Building on the Efficient Stochastic Transformer-based World Models\n(STORM) architecture, we replace the traditional MLP prior with a Masked\nGenerative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our\nmodel on two downstream tasks: reinforcement learning and video prediction.\nGIT-STORM demonstrates substantial performance gains in RL tasks on the Atari\n100k benchmark. Moreover, we apply Transformer-based World Models to continuous\naction environments for the first time, addressing a significant gap in prior\nresearch. To achieve this, we employ a state mixer function that integrates\nlatent state representations with actions, enabling our model to handle\ncontinuous control tasks. We validate this approach through qualitative and\nquantitative analyses on the DeepMind Control Suite, showcasing the\neffectiveness of Transformer-based World Models in this new domain. Our results\nhighlight the versatility and efficacy of the MaskGIT dynamics prior, paving\nthe way for more accurate world models and effective RL policies."
                },
                "authors": [
                    {
                        "name": "Cristian Meo"
                    },
                    {
                        "name": "Mircea Lica"
                    },
                    {
                        "name": "Zarif Ikram"
                    },
                    {
                        "name": "Akihiro Nakano"
                    },
                    {
                        "name": "Vedant Shah"
                    },
                    {
                        "name": "Aniket Rajiv Didolkar"
                    },
                    {
                        "name": "Dianbo Liu"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Justin Dauwels"
                    }
                ],
                "author_detail": {
                    "name": "Justin Dauwels"
                },
                "author": "Justin Dauwels",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07836v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07836v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v2",
                "updated": "2024-10-28T14:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    44,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21071v1",
                "updated": "2024-10-28T14:34:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    34,
                    36,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:34:36Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    34,
                    36,
                    0,
                    302,
                    0
                ],
                "title": "Automatic Generation of Benchmarks and Reliable LLM Judgment for Code\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Generation of Benchmarks and Reliable LLM Judgment for Code\n  Tasks"
                },
                "summary": "LLMs can be used in a variety of code related tasks such as translating from\none programming language to another, implementing natural language requirements\nand code summarization. Artifacts generated by state of the art LLM technology\nare expected to be useful in the sense that a user will be able to use the LLM\ngenerated artifact after a small number of easy modifications. Quantifying this\nvague notion is challenging and it is thus hard to determine the quality of\ncode related LLM solutions. We refer to evaluation of LLM solutions using LLM\njudgment as \"LLM as a Judge\", or LaaJ for short. In this work we introduce a\nmethodology to generate and evaluate LaaJ implementations, utilizing an\nautomatically generated benchmark. The purpose of the benchmark is two fold,\nnamely, it is used both to develop and validate the LaaJs and to validate and\ntest the LLM code related solution using the LaaJs. To that end, we developed\nan automated benchmark generation engine, which generates code in multiple\nprogramming languages for multiple code related tasks and which serves as the\ninput for LaaJ evaluation. We utilize a graph representation, G, of the\npotential code related generations. The graph vertices are generated artifacts\nand edges represent possible generations, e.g., the generation of a Java\nprogram from its natural language requirements. Utilizing a chain of LLM agents\nand G we generate code related artifacts. Using cycles in G we formulate\nexpectations on the generated artifacts. Taking advantage of these formulated\nexpectations enables the development and testing of reliable LLM judgement for\nusefulness of the artifacts generated by the solution. Our approach enables the\ncreation of high quality code task solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can be used in a variety of code related tasks such as translating from\none programming language to another, implementing natural language requirements\nand code summarization. Artifacts generated by state of the art LLM technology\nare expected to be useful in the sense that a user will be able to use the LLM\ngenerated artifact after a small number of easy modifications. Quantifying this\nvague notion is challenging and it is thus hard to determine the quality of\ncode related LLM solutions. We refer to evaluation of LLM solutions using LLM\njudgment as \"LLM as a Judge\", or LaaJ for short. In this work we introduce a\nmethodology to generate and evaluate LaaJ implementations, utilizing an\nautomatically generated benchmark. The purpose of the benchmark is two fold,\nnamely, it is used both to develop and validate the LaaJs and to validate and\ntest the LLM code related solution using the LaaJs. To that end, we developed\nan automated benchmark generation engine, which generates code in multiple\nprogramming languages for multiple code related tasks and which serves as the\ninput for LaaJ evaluation. We utilize a graph representation, G, of the\npotential code related generations. The graph vertices are generated artifacts\nand edges represent possible generations, e.g., the generation of a Java\nprogram from its natural language requirements. Utilizing a chain of LLM agents\nand G we generate code related artifacts. Using cycles in G we formulate\nexpectations on the generated artifacts. Taking advantage of these formulated\nexpectations enables the development and testing of reliable LLM judgement for\nusefulness of the artifacts generated by the solution. Our approach enables the\ncreation of high quality code task solutions."
                },
                "authors": [
                    {
                        "name": "Eitan Farchi"
                    },
                    {
                        "name": "Shmulik Froimovich"
                    },
                    {
                        "name": "Rami Katan"
                    },
                    {
                        "name": "Orna Raz"
                    }
                ],
                "author_detail": {
                    "name": "Orna Raz"
                },
                "author": "Orna Raz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.09395v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.09395v5",
                "updated": "2024-10-28T14:29:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    29,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-01-17T18:13:07Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    18,
                    13,
                    7,
                    2,
                    17,
                    0
                ],
                "title": "Evaluating LLMs' Mathematical and Coding Competency through\n  Ontology-guided Interventions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs' Mathematical and Coding Competency through\n  Ontology-guided Interventions"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have showcased striking\nresults on existing logical reasoning benchmarks, with some models even\nsurpassing human performance. However, the true depth of their competencies and\nrobustness in reasoning tasks remains an open question. To this end, in this\npaper, we focus on two popular reasoning tasks: arithmetic reasoning and code\ngeneration. Particularly, we introduce (i) a general ontology of perturbations\nfor math and coding questions, (ii) a semi-automatic method to apply these\nperturbations, and (iii) two datasets, GSMORE and HUMANEVAL-CORE, respectively,\nof perturbed math and coding problems to probe LLM capabilities in numeric\nreasoning and coding tasks. Through comprehensive evaluations of both\nclosed-source and open-source LLMs, we show a significant performance drop\nacross all the models against the perturbed questions, suggesting that the\ncurrent LLMs lack robust problem solving skills and structured reasoning\nabilities in many areas, as defined by our ontology. We open-source the\ndatasets and source codes at: https://github.com/declare-lab/LLM-ReasoningTest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have showcased striking\nresults on existing logical reasoning benchmarks, with some models even\nsurpassing human performance. However, the true depth of their competencies and\nrobustness in reasoning tasks remains an open question. To this end, in this\npaper, we focus on two popular reasoning tasks: arithmetic reasoning and code\ngeneration. Particularly, we introduce (i) a general ontology of perturbations\nfor math and coding questions, (ii) a semi-automatic method to apply these\nperturbations, and (iii) two datasets, GSMORE and HUMANEVAL-CORE, respectively,\nof perturbed math and coding problems to probe LLM capabilities in numeric\nreasoning and coding tasks. Through comprehensive evaluations of both\nclosed-source and open-source LLMs, we show a significant performance drop\nacross all the models against the perturbed questions, suggesting that the\ncurrent LLMs lack robust problem solving skills and structured reasoning\nabilities in many areas, as defined by our ontology. We open-source the\ndatasets and source codes at: https://github.com/declare-lab/LLM-ReasoningTest."
                },
                "authors": [
                    {
                        "name": "Pengfei Hong"
                    },
                    {
                        "name": "Navonil Majumder"
                    },
                    {
                        "name": "Deepanway Ghosal"
                    },
                    {
                        "name": "Somak Aditya"
                    },
                    {
                        "name": "Rada Mihalcea"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.09395v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.09395v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21067v1",
                "updated": "2024-10-28T14:29:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    29,
                    11,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:29:11Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    29,
                    11,
                    0,
                    302,
                    0
                ],
                "title": "CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and\n  Retrieval-Augmented Translation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and\n  Retrieval-Augmented Translation with Large Language Models"
                },
                "summary": "Large language models (LLMs) have shown great promise in machine translation,\nbut they still struggle with contextually dependent terms, such as new or\ndomain-specific words. This leads to inconsistencies and errors that are\ndifficult to address. Existing solutions often depend on manual identification\nof such terms, which is impractical given the complexity and evolving nature of\nlanguage. While Retrieval-Augmented Generation (RAG) could provide some\nassistance, its application to translation is limited by issues such as\nhallucinations from information overload. In this paper, we propose CRAT, a\nnovel multi-agent translation framework that leverages RAG and\ncausality-enhanced self-reflection to address these challenges. This framework\nconsists of several specialized agents: the Unknown Terms Identification agent\ndetects unknown terms within the context, the Knowledge Graph (KG) Constructor\nagent extracts relevant internal knowledge about these terms and retrieves\nbilingual information from external sources, the Causality-enhanced Judge agent\nvalidates the accuracy of the information, and the Translator agent\nincorporates the refined information into the final output. This automated\nprocess allows for more precise and consistent handling of key terms during\ntranslation. Our results show that CRAT significantly improves translation\naccuracy, particularly in handling context-sensitive terms and emerging\nvocabulary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great promise in machine translation,\nbut they still struggle with contextually dependent terms, such as new or\ndomain-specific words. This leads to inconsistencies and errors that are\ndifficult to address. Existing solutions often depend on manual identification\nof such terms, which is impractical given the complexity and evolving nature of\nlanguage. While Retrieval-Augmented Generation (RAG) could provide some\nassistance, its application to translation is limited by issues such as\nhallucinations from information overload. In this paper, we propose CRAT, a\nnovel multi-agent translation framework that leverages RAG and\ncausality-enhanced self-reflection to address these challenges. This framework\nconsists of several specialized agents: the Unknown Terms Identification agent\ndetects unknown terms within the context, the Knowledge Graph (KG) Constructor\nagent extracts relevant internal knowledge about these terms and retrieves\nbilingual information from external sources, the Causality-enhanced Judge agent\nvalidates the accuracy of the information, and the Translator agent\nincorporates the refined information into the final output. This automated\nprocess allows for more precise and consistent handling of key terms during\ntranslation. Our results show that CRAT significantly improves translation\naccuracy, particularly in handling context-sensitive terms and emerging\nvocabulary."
                },
                "authors": [
                    {
                        "name": "Meiqi Chen"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Yingxue Zhang"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21060v1",
                "updated": "2024-10-28T14:18:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    18,
                    32,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:18:32Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    18,
                    32,
                    0,
                    302,
                    0
                ],
                "title": "CTINEXUS: Leveraging Optimized LLM In-Context Learning for Constructing\n  Cybersecurity Knowledge Graphs Under Data Scarcity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CTINEXUS: Leveraging Optimized LLM In-Context Learning for Constructing\n  Cybersecurity Knowledge Graphs Under Data Scarcity"
                },
                "summary": "Textual descriptions in cyber threat intelligence (CTI) reports, such as\nsecurity articles and news, are rich sources of knowledge about cyber threats,\ncrucial for organizations to stay informed about the rapidly evolving threat\nlandscape. However, current CTI extraction methods lack flexibility and\ngeneralizability, often resulting in inaccurate and incomplete knowledge\nextraction. Syntax parsing relies on fixed rules and dictionaries, while model\nfine-tuning requires large annotated datasets, making both paradigms\nchallenging to adapt to new threats and ontologies. To bridge the gap, we\npropose CTINexus, a novel framework leveraging optimized in-context learning\n(ICL) of large language models (LLMs) for data-efficient CTI knowledge\nextraction and high-quality cybersecurity knowledge graph (CSKG) construction.\nUnlike existing methods, CTINexus requires neither extensive data nor parameter\ntuning and can adapt to various ontologies with minimal annotated examples.\nThis is achieved through (1) a carefully designed automatic prompt construction\nstrategy with optimal demonstration retrieval for extracting a wide range of\ncybersecurity entities and relations; (2) a hierarchical entity alignment\ntechnique that canonicalizes the extracted knowledge and removes redundancy;\n(3) an ICL-enhanced long-distance relation prediction technique to further\ncomplete the CKSG with missing links. Our extensive evaluations using 150\nreal-world CTI reports collected from 10 platforms demonstrate that CTINexus\nsignificantly outperforms existing methods in constructing accurate and\ncomplete CSKGs, highlighting its potential to transform CTI analysis with an\nefficient and adaptable solution for the dynamic threat landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual descriptions in cyber threat intelligence (CTI) reports, such as\nsecurity articles and news, are rich sources of knowledge about cyber threats,\ncrucial for organizations to stay informed about the rapidly evolving threat\nlandscape. However, current CTI extraction methods lack flexibility and\ngeneralizability, often resulting in inaccurate and incomplete knowledge\nextraction. Syntax parsing relies on fixed rules and dictionaries, while model\nfine-tuning requires large annotated datasets, making both paradigms\nchallenging to adapt to new threats and ontologies. To bridge the gap, we\npropose CTINexus, a novel framework leveraging optimized in-context learning\n(ICL) of large language models (LLMs) for data-efficient CTI knowledge\nextraction and high-quality cybersecurity knowledge graph (CSKG) construction.\nUnlike existing methods, CTINexus requires neither extensive data nor parameter\ntuning and can adapt to various ontologies with minimal annotated examples.\nThis is achieved through (1) a carefully designed automatic prompt construction\nstrategy with optimal demonstration retrieval for extracting a wide range of\ncybersecurity entities and relations; (2) a hierarchical entity alignment\ntechnique that canonicalizes the extracted knowledge and removes redundancy;\n(3) an ICL-enhanced long-distance relation prediction technique to further\ncomplete the CKSG with missing links. Our extensive evaluations using 150\nreal-world CTI reports collected from 10 platforms demonstrate that CTINexus\nsignificantly outperforms existing methods in constructing accurate and\ncomplete CSKGs, highlighting its potential to transform CTI analysis with an\nefficient and adaptable solution for the dynamic threat landscape."
                },
                "authors": [
                    {
                        "name": "Yutong Cheng"
                    },
                    {
                        "name": "Osama Bajaber"
                    },
                    {
                        "name": "Saimon Amanuel Tsegai"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Peng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Peng Gao"
                },
                "author": "Peng Gao",
                "arxiv_comment": "under peer-review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05600v2",
                "updated": "2024-10-28T14:08:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    8,
                    13,
                    0,
                    302,
                    0
                ],
                "published": "2024-07-08T04:30:53Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    4,
                    30,
                    53,
                    0,
                    190,
                    0
                ],
                "title": "GenArtist: Multimodal LLM as an Agent for Unified Image Generation and\n  Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenArtist: Multimodal LLM as an Agent for Unified Image Generation and\n  Editing"
                },
                "summary": "Despite the success achieved by existing image generation and editing\nmethods, current models still struggle with complex problems including\nintricate text prompts, and the absence of verification and self-correction\nmechanisms makes the generated images unreliable. Meanwhile, a single model\ntends to specialize in particular tasks and possess the corresponding\ncapabilities, making it inadequate for fulfilling all user requirements. We\npropose GenArtist, a unified image generation and editing system, coordinated\nby a multimodal large language model (MLLM) agent. We integrate a comprehensive\nrange of existing models into the tool library and utilize the agent for tool\nselection and execution. For a complex problem, the MLLM agent decomposes it\ninto simpler sub-problems and constructs a tree structure to systematically\nplan the procedure of generation, editing, and self-correction with\nstep-by-step verification. By automatically generating missing position-related\ninputs and incorporating position information, the appropriate tool can be\neffectively employed to address each sub-problem. Experiments demonstrate that\nGenArtist can perform various generation and editing tasks, achieving\nstate-of-the-art performance and surpassing existing models such as SDXL and\nDALL-E 3, as can be seen in Fig. 1. Project page is\nhttps://zhenyuw16.github.io/GenArtist_page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success achieved by existing image generation and editing\nmethods, current models still struggle with complex problems including\nintricate text prompts, and the absence of verification and self-correction\nmechanisms makes the generated images unreliable. Meanwhile, a single model\ntends to specialize in particular tasks and possess the corresponding\ncapabilities, making it inadequate for fulfilling all user requirements. We\npropose GenArtist, a unified image generation and editing system, coordinated\nby a multimodal large language model (MLLM) agent. We integrate a comprehensive\nrange of existing models into the tool library and utilize the agent for tool\nselection and execution. For a complex problem, the MLLM agent decomposes it\ninto simpler sub-problems and constructs a tree structure to systematically\nplan the procedure of generation, editing, and self-correction with\nstep-by-step verification. By automatically generating missing position-related\ninputs and incorporating position information, the appropriate tool can be\neffectively employed to address each sub-problem. Experiments demonstrate that\nGenArtist can perform various generation and editing tasks, achieving\nstate-of-the-art performance and surpassing existing models such as SDXL and\nDALL-E 3, as can be seen in Fig. 1. Project page is\nhttps://zhenyuw16.github.io/GenArtist_page."
                },
                "authors": [
                    {
                        "name": "Zhenyu Wang"
                    },
                    {
                        "name": "Aoxue Li"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Xihui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xihui Liu"
                },
                "author": "Xihui Liu",
                "arxiv_comment": "NeurIPS 2024 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21052v1",
                "updated": "2024-10-28T14:07:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    7,
                    41,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:07:41Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    7,
                    41,
                    0,
                    302,
                    0
                ],
                "title": "Getting By Goal Misgeneralization With a Little Help From a Mentor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Getting By Goal Misgeneralization With a Little Help From a Mentor"
                },
                "summary": "While reinforcement learning (RL) agents often perform well during training,\nthey can struggle with distribution shift in real-world deployments. One\nparticularly severe risk of distribution shift is goal misgeneralization, where\nthe agent learns a proxy goal that coincides with the true goal during training\nbut not during deployment. In this paper, we explore whether allowing an agent\nto ask for help from a supervisor in unfamiliar situations can mitigate this\nissue. We focus on agents trained with PPO in the CoinRun environment, a\nsetting known to exhibit goal misgeneralization. We evaluate multiple methods\nfor determining when the agent should request help and find that asking for\nhelp consistently improves performance. However, we also find that methods\nbased on the agent's internal state fail to proactively request help, instead\nwaiting until mistakes have already occurred. Further investigation suggests\nthat the agent's internal state does not represent the coin at all,\nhighlighting the importance of learning nuanced representations, the risks of\nignoring everything not immediately relevant to reward, and the necessity of\ndeveloping ask-for-help strategies tailored to the agent's training algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reinforcement learning (RL) agents often perform well during training,\nthey can struggle with distribution shift in real-world deployments. One\nparticularly severe risk of distribution shift is goal misgeneralization, where\nthe agent learns a proxy goal that coincides with the true goal during training\nbut not during deployment. In this paper, we explore whether allowing an agent\nto ask for help from a supervisor in unfamiliar situations can mitigate this\nissue. We focus on agents trained with PPO in the CoinRun environment, a\nsetting known to exhibit goal misgeneralization. We evaluate multiple methods\nfor determining when the agent should request help and find that asking for\nhelp consistently improves performance. However, we also find that methods\nbased on the agent's internal state fail to proactively request help, instead\nwaiting until mistakes have already occurred. Further investigation suggests\nthat the agent's internal state does not represent the coin at all,\nhighlighting the importance of learning nuanced representations, the risks of\nignoring everything not immediately relevant to reward, and the necessity of\ndeveloping ask-for-help strategies tailored to the agent's training algorithm."
                },
                "authors": [
                    {
                        "name": "Tu Trinh"
                    },
                    {
                        "name": "Mohamad H. Danesh"
                    },
                    {
                        "name": "Nguyen X. Khanh"
                    },
                    {
                        "name": "Benjamin Plaut"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Plaut"
                },
                "author": "Benjamin Plaut",
                "arxiv_comment": "SATA Workshop @ NeurIPS 2024 (Towards Safe and Trustworthy Agents)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11295v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11295v4",
                "updated": "2024-10-28T14:06:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    6,
                    58,
                    0,
                    302,
                    0
                ],
                "published": "2024-02-17T14:26:57Z",
                "published_parsed": [
                    2024,
                    2,
                    17,
                    14,
                    26,
                    57,
                    5,
                    48,
                    0
                ],
                "title": "OneBit: Towards Extremely Low-bit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneBit: Towards Extremely Low-bit Large Language Models"
                },
                "summary": "Model quantification uses low bit-width values to represent the weight\nmatrices of existing models to be quantized, which is a promising approach to\nreduce both storage and computational overheads of deploying highly anticipated\nLLMs. However, current quantization methods suffer severe performance\ndegradation when the bit-width is extremely reduced, and thus focus on\nutilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes\nthe weight matrices of LLMs to 1-bit, paving the way for the extremely low\nbit-width deployment of LLMs. For this target, we introduce a 1-bit model\ncompressing framework named OneBit, including a novel 1-bit parameter\nrepresentation method to better quantize LLMs as well as an effective parameter\ninitialization method based on matrix decomposition to improve the convergence\nspeed of the quantization framework. Sufficient experimental results indicate\nthat OneBit achieves good performance (at least 81% of the non-quantized\nperformance on LLaMA models) with robust training processes when only using\n1-bit weight matrices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantification uses low bit-width values to represent the weight\nmatrices of existing models to be quantized, which is a promising approach to\nreduce both storage and computational overheads of deploying highly anticipated\nLLMs. However, current quantization methods suffer severe performance\ndegradation when the bit-width is extremely reduced, and thus focus on\nutilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes\nthe weight matrices of LLMs to 1-bit, paving the way for the extremely low\nbit-width deployment of LLMs. For this target, we introduce a 1-bit model\ncompressing framework named OneBit, including a novel 1-bit parameter\nrepresentation method to better quantize LLMs as well as an effective parameter\ninitialization method based on matrix decomposition to improve the convergence\nspeed of the quantization framework. Sufficient experimental results indicate\nthat OneBit achieves good performance (at least 81% of the non-quantized\nperformance on LLaMA models) with robust training processes when only using\n1-bit weight matrices."
                },
                "authors": [
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zonghan Yang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Weidong Liu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11295v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11295v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21041v1",
                "updated": "2024-10-28T13:58:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    58,
                    4,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:58:04Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    58,
                    4,
                    0,
                    302,
                    0
                ],
                "title": "Sorting Out the Bad Seeds: Automatic Classification of Cryptocurrency\n  Abuse Reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sorting Out the Bad Seeds: Automatic Classification of Cryptocurrency\n  Abuse Reports"
                },
                "summary": "Abuse reporting services collect reports about abuse victims have suffered.\nAccurate classification of the submitted reports is fundamental to analyzing\nthe prevalence and financial impact of different abuse types (e.g., sextortion,\ninvestment, romance). Current classification approaches are problematic because\nthey require the reporter to select the abuse type from a list, assuming the\nreporter has the necessary experience for the classification, which we show is\nfrequently not the case, or require manual classification by analysts, which\ndoes not scale. To address these issues, this paper presents a novel approach\nto classify cryptocurrency abuse reports automatically. We first build a\ntaxonomy of 19 frequently reported abuse types. Given as input the textual\ndescription written by the reporter, our classifier leverages a large language\nmodel (LLM) to interpret the text and assign it an abuse type in our taxonomy.\nWe collect 290K cryptocurrency abuse reports from two popular reporting\nservices: BitcoinAbuse and BBB's ScamTracker. We build ground truth datasets\nfor 20K of those reports and use them to evaluate three designs for our\nLLM-based classifier and four LLMs, as well as a supervised ML classifier used\nas a baseline. Our LLM-based classifier achieves a precision of 0.92, a recall\nof 0.87, and an F1 score of 0.89, compared to an F1 score of 0.55 for the\nbaseline. We demonstrate our classifier in two applications: providing\nfinancial loss statistics for fine-grained abuse types and generating tagged\naddresses for cryptocurrency analysis platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abuse reporting services collect reports about abuse victims have suffered.\nAccurate classification of the submitted reports is fundamental to analyzing\nthe prevalence and financial impact of different abuse types (e.g., sextortion,\ninvestment, romance). Current classification approaches are problematic because\nthey require the reporter to select the abuse type from a list, assuming the\nreporter has the necessary experience for the classification, which we show is\nfrequently not the case, or require manual classification by analysts, which\ndoes not scale. To address these issues, this paper presents a novel approach\nto classify cryptocurrency abuse reports automatically. We first build a\ntaxonomy of 19 frequently reported abuse types. Given as input the textual\ndescription written by the reporter, our classifier leverages a large language\nmodel (LLM) to interpret the text and assign it an abuse type in our taxonomy.\nWe collect 290K cryptocurrency abuse reports from two popular reporting\nservices: BitcoinAbuse and BBB's ScamTracker. We build ground truth datasets\nfor 20K of those reports and use them to evaluate three designs for our\nLLM-based classifier and four LLMs, as well as a supervised ML classifier used\nas a baseline. Our LLM-based classifier achieves a precision of 0.92, a recall\nof 0.87, and an F1 score of 0.89, compared to an F1 score of 0.55 for the\nbaseline. We demonstrate our classifier in two applications: providing\nfinancial loss statistics for fine-grained abuse types and generating tagged\naddresses for cryptocurrency analysis platforms."
                },
                "authors": [
                    {
                        "name": "Gibran Gomez"
                    },
                    {
                        "name": "Kevin van Liebergen"
                    },
                    {
                        "name": "Davide Sanvito"
                    },
                    {
                        "name": "Giuseppe Siracusano"
                    },
                    {
                        "name": "Roberto Gonzalez"
                    },
                    {
                        "name": "Juan Caballero"
                    }
                ],
                "author_detail": {
                    "name": "Juan Caballero"
                },
                "author": "Juan Caballero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21040v1",
                "updated": "2024-10-28T13:57:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    57,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:57:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    57,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "LiP-LLM: Integrating Linear Programming and dependency graph with Large\n  Language Models for multi-robot task planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiP-LLM: Integrating Linear Programming and dependency graph with Large\n  Language Models for multi-robot task planning"
                },
                "summary": "This study proposes LiP-LLM: integrating linear programming and dependency\ngraph with large language models (LLMs) for multi-robot task planning. In order\nfor multiple robots to perform tasks more efficiently, it is necessary to\nmanage the precedence dependencies between tasks. Although multi-robot\ndecentralized and centralized task planners using LLMs have been proposed, none\nof these studies focus on precedence dependencies from the perspective of task\nefficiency or leverage traditional optimization methods. It addresses key\nchallenges in managing dependencies between skills and optimizing task\nallocation. LiP-LLM consists of three steps: skill list generation and\ndependency graph generation by LLMs, and task allocation using linear\nprogramming. The LLMs are utilized to generate a comprehensive list of skills\nand to construct a dependency graph that maps the relationships and sequential\nconstraints among these skills. To ensure the feasibility and efficiency of\nskill execution, the skill list is generated by calculated likelihood, and\nlinear programming is used to optimally allocate tasks to each robot.\nExperimental evaluations in simulated environments demonstrate that this method\noutperforms existing task planners, achieving higher success rates and\nefficiency in executing complex, multi-robot tasks. The results indicate the\npotential of combining LLMs with optimization techniques to enhance the\ncapabilities of multi-robot systems in executing coordinated tasks accurately\nand efficiently. In an environment with two robots, a maximum success rate\ndifference of 0.82 is observed in the language instruction group with a change\nin the object name.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study proposes LiP-LLM: integrating linear programming and dependency\ngraph with large language models (LLMs) for multi-robot task planning. In order\nfor multiple robots to perform tasks more efficiently, it is necessary to\nmanage the precedence dependencies between tasks. Although multi-robot\ndecentralized and centralized task planners using LLMs have been proposed, none\nof these studies focus on precedence dependencies from the perspective of task\nefficiency or leverage traditional optimization methods. It addresses key\nchallenges in managing dependencies between skills and optimizing task\nallocation. LiP-LLM consists of three steps: skill list generation and\ndependency graph generation by LLMs, and task allocation using linear\nprogramming. The LLMs are utilized to generate a comprehensive list of skills\nand to construct a dependency graph that maps the relationships and sequential\nconstraints among these skills. To ensure the feasibility and efficiency of\nskill execution, the skill list is generated by calculated likelihood, and\nlinear programming is used to optimally allocate tasks to each robot.\nExperimental evaluations in simulated environments demonstrate that this method\noutperforms existing task planners, achieving higher success rates and\nefficiency in executing complex, multi-robot tasks. The results indicate the\npotential of combining LLMs with optimization techniques to enhance the\ncapabilities of multi-robot systems in executing coordinated tasks accurately\nand efficiently. In an environment with two robots, a maximum success rate\ndifference of 0.82 is observed in the language instruction group with a change\nin the object name."
                },
                "authors": [
                    {
                        "name": "Kazuma Obata"
                    },
                    {
                        "name": "Tatsuya Aoki"
                    },
                    {
                        "name": "Takato Horii"
                    },
                    {
                        "name": "Tadahiro Taniguchi"
                    },
                    {
                        "name": "Takayuki Nagai"
                    }
                ],
                "author_detail": {
                    "name": "Takayuki Nagai"
                },
                "author": "Takayuki Nagai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v1",
                "updated": "2024-10-28T13:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21012v1",
                "updated": "2024-10-28T13:36:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    36,
                    41,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:36:41Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    36,
                    41,
                    0,
                    302,
                    0
                ],
                "title": "FACT: Examining the Effectiveness of Iterative Context Rewriting for\n  Multi-fact Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FACT: Examining the Effectiveness of Iterative Context Rewriting for\n  Multi-fact Retrieval"
                },
                "summary": "Large Language Models (LLMs) are proficient at retrieving single facts from\nextended contexts, yet they struggle with tasks requiring the simultaneous\nretrieval of multiple facts, especially during generation. This paper\nidentifies a novel \"lost-in-the-middle\" phenomenon, where LLMs progressively\nlose track of critical information throughout the generation process, resulting\nin incomplete or inaccurate retrieval. To address this challenge, we introduce\nFind All Crucial Texts (FACT), an iterative retrieval method that refines\ncontext through successive rounds of rewriting. This approach enables models to\ncapture essential facts incrementally, which are often overlooked in\nsingle-pass retrieval. Experiments demonstrate that FACT substantially enhances\nmulti-fact retrieval performance across various tasks, though improvements are\nless notable in general-purpose QA scenarios. Our findings shed light on the\nlimitations of LLMs in multi-fact retrieval and underscore the need for more\nresilient long-context retrieval strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are proficient at retrieving single facts from\nextended contexts, yet they struggle with tasks requiring the simultaneous\nretrieval of multiple facts, especially during generation. This paper\nidentifies a novel \"lost-in-the-middle\" phenomenon, where LLMs progressively\nlose track of critical information throughout the generation process, resulting\nin incomplete or inaccurate retrieval. To address this challenge, we introduce\nFind All Crucial Texts (FACT), an iterative retrieval method that refines\ncontext through successive rounds of rewriting. This approach enables models to\ncapture essential facts incrementally, which are often overlooked in\nsingle-pass retrieval. Experiments demonstrate that FACT substantially enhances\nmulti-fact retrieval performance across various tasks, though improvements are\nless notable in general-purpose QA scenarios. Our findings shed light on the\nlimitations of LLMs in multi-fact retrieval and underscore the need for more\nresilient long-context retrieval strategies."
                },
                "authors": [
                    {
                        "name": "Jinlin Wang"
                    },
                    {
                        "name": "Suyuchen Wang"
                    },
                    {
                        "name": "Ziwen Xia"
                    },
                    {
                        "name": "Sirui Hong"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Bang Liu"
                    },
                    {
                        "name": "Chenglin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chenglin Wu"
                },
                "author": "Chenglin Wu",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14952v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14952v3",
                "updated": "2024-10-28T13:25:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    25,
                    49,
                    0,
                    302,
                    0
                ],
                "published": "2024-06-21T08:03:33Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    8,
                    3,
                    33,
                    4,
                    173,
                    0
                ],
                "title": "ESC-Eval: Evaluating Emotion Support Conversations in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ESC-Eval: Evaluating Emotion Support Conversations in Large Language\n  Models"
                },
                "summary": "Emotion Support Conversation (ESC) is a crucial application, which aims to\nreduce human stress, offer emotional guidance, and ultimately enhance human\nmental and physical well-being. With the advancement of Large Language Models\n(LLMs), many researchers have employed LLMs as the ESC models. However, the\nevaluation of these LLM-based ESCs remains uncertain. Inspired by the awesome\ndevelopment of role-playing agents, we propose an ESC Evaluation framework\n(ESC-Eval), which uses a role-playing agent to interact with ESC models,\nfollowed by a manual evaluation of the interactive dialogues. In detail, we\nfirst re-organize 2,801 role-playing cards from seven existing datasets to\ndefine the roles of the role-playing agent. Second, we train a specific\nrole-playing model called ESC-Role which behaves more like a confused person\nthan GPT-4. Third, through ESC-Role and organized role cards, we systematically\nconduct experiments using 14 LLMs as the ESC models, including general\nAI-assistant LLMs (ChatGPT) and ESC-oriented LLMs (ExTES-Llama). We conduct\ncomprehensive human annotations on interactive multi-turn dialogues of\ndifferent ESC models. The results show that ESC-oriented LLMs exhibit superior\nESC abilities compared to general AI-assistant LLMs, but there is still a gap\nbehind human performance. Moreover, to automate the scoring process for future\nESC models, we developed ESC-RANK, which trained on the annotated data,\nachieving a scoring performance surpassing 35 points of GPT-4. Our data and\ncode are available at https://github.com/AIFlames/Esc-Eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotion Support Conversation (ESC) is a crucial application, which aims to\nreduce human stress, offer emotional guidance, and ultimately enhance human\nmental and physical well-being. With the advancement of Large Language Models\n(LLMs), many researchers have employed LLMs as the ESC models. However, the\nevaluation of these LLM-based ESCs remains uncertain. Inspired by the awesome\ndevelopment of role-playing agents, we propose an ESC Evaluation framework\n(ESC-Eval), which uses a role-playing agent to interact with ESC models,\nfollowed by a manual evaluation of the interactive dialogues. In detail, we\nfirst re-organize 2,801 role-playing cards from seven existing datasets to\ndefine the roles of the role-playing agent. Second, we train a specific\nrole-playing model called ESC-Role which behaves more like a confused person\nthan GPT-4. Third, through ESC-Role and organized role cards, we systematically\nconduct experiments using 14 LLMs as the ESC models, including general\nAI-assistant LLMs (ChatGPT) and ESC-oriented LLMs (ExTES-Llama). We conduct\ncomprehensive human annotations on interactive multi-turn dialogues of\ndifferent ESC models. The results show that ESC-oriented LLMs exhibit superior\nESC abilities compared to general AI-assistant LLMs, but there is still a gap\nbehind human performance. Moreover, to automate the scoring process for future\nESC models, we developed ESC-RANK, which trained on the annotated data,\nachieving a scoring performance surpassing 35 points of GPT-4. Our data and\ncode are available at https://github.com/AIFlames/Esc-Eval."
                },
                "authors": [
                    {
                        "name": "Haiquan Zhao"
                    },
                    {
                        "name": "Lingyu Li"
                    },
                    {
                        "name": "Shisong Chen"
                    },
                    {
                        "name": "Shuqi Kong"
                    },
                    {
                        "name": "Jiaan Wang"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Tianle Gu"
                    },
                    {
                        "name": "Yixu Wang"
                    },
                    {
                        "name": "Wang Jian"
                    },
                    {
                        "name": "Dandan Liang"
                    },
                    {
                        "name": "Zhixu Li"
                    },
                    {
                        "name": "Yan Teng"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Yingchun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yingchun Wang"
                },
                "author": "Yingchun Wang",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14952v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14952v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17789v2",
                "updated": "2024-10-28T13:19:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    19,
                    38,
                    0,
                    302,
                    0
                ],
                "published": "2024-07-25T05:50:46Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    5,
                    50,
                    46,
                    3,
                    207,
                    0
                ],
                "title": "Very Large-Scale Multi-Agent Simulation in AgentScope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Very Large-Scale Multi-Agent Simulation in AgentScope"
                },
                "summary": "Recent advances in large language models (LLMs) have opened new avenues for\napplying multi-agent systems in very large-scale simulations. However, there\nremain several challenges when conducting multi-agent simulations with existing\nplatforms, such as limited scalability and low efficiency, unsatisfied agent\ndiversity, and effort-intensive management processes. To address these\nchallenges, we develop several new features and components for AgentScope, a\nuser-friendly multi-agent platform, enhancing its convenience and flexibility\nfor supporting very large-scale multi-agent simulations. Specifically, we\npropose an actor-based distributed mechanism as the underlying technological\ninfrastructure towards great scalability and high efficiency, and provide\nflexible environment support for simulating various real-world scenarios, which\nenables parallel execution of multiple agents, automatic workflow conversion\nfor distributed deployment, and both inter-agent and agent-environment\ninteractions. Moreover, we integrate an easy-to-use configurable tool and an\nautomatic background generation pipeline in AgentScope, simplifying the process\nof creating agents with diverse yet detailed background settings. Last but not\nleast, we provide a web-based interface for conveniently monitoring and\nmanaging a large number of agents that might deploy across multiple devices. We\nconduct a comprehensive simulation to demonstrate the effectiveness of these\nproposed enhancements in AgentScope, and provide detailed observations and\ninsightful discussions to highlight the great potential of applying multi-agent\nsystems in large-scale simulations. The source code is released on GitHub at\nhttps://github.com/modelscope/agentscope/tree/main/examples/paper_large_scale_simulation\nto inspire further research and development in large-scale multi-agent\nsimulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have opened new avenues for\napplying multi-agent systems in very large-scale simulations. However, there\nremain several challenges when conducting multi-agent simulations with existing\nplatforms, such as limited scalability and low efficiency, unsatisfied agent\ndiversity, and effort-intensive management processes. To address these\nchallenges, we develop several new features and components for AgentScope, a\nuser-friendly multi-agent platform, enhancing its convenience and flexibility\nfor supporting very large-scale multi-agent simulations. Specifically, we\npropose an actor-based distributed mechanism as the underlying technological\ninfrastructure towards great scalability and high efficiency, and provide\nflexible environment support for simulating various real-world scenarios, which\nenables parallel execution of multiple agents, automatic workflow conversion\nfor distributed deployment, and both inter-agent and agent-environment\ninteractions. Moreover, we integrate an easy-to-use configurable tool and an\nautomatic background generation pipeline in AgentScope, simplifying the process\nof creating agents with diverse yet detailed background settings. Last but not\nleast, we provide a web-based interface for conveniently monitoring and\nmanaging a large number of agents that might deploy across multiple devices. We\nconduct a comprehensive simulation to demonstrate the effectiveness of these\nproposed enhancements in AgentScope, and provide detailed observations and\ninsightful discussions to highlight the great potential of applying multi-agent\nsystems in large-scale simulations. The source code is released on GitHub at\nhttps://github.com/modelscope/agentscope/tree/main/examples/paper_large_scale_simulation\nto inspire further research and development in large-scale multi-agent\nsimulations."
                },
                "authors": [
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Dawei Gao"
                    },
                    {
                        "name": "Yuexiang Xie"
                    },
                    {
                        "name": "Yushuo Chen"
                    },
                    {
                        "name": "Zhewei Wei"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "We have released code on\n  https://github.com/modelscope/agentscope/tree/main/examples/paper_large_scale_simulation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20990v1",
                "updated": "2024-10-28T13:10:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    10,
                    15,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:10:15Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    10,
                    15,
                    0,
                    302,
                    0
                ],
                "title": "Reference-Free Formula Drift with Reinforcement Learning: From Driving\n  Data to Tire Energy-Inspired, Real-World Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reference-Free Formula Drift with Reinforcement Learning: From Driving\n  Data to Tire Energy-Inspired, Real-World Policies"
                },
                "summary": "The skill to drift a car--i.e., operate in a state of controlled oversteer\nlike professional drivers--could give future autonomous cars maximum\nflexibility when they need to retain control in adverse conditions or avoid\ncollisions. We investigate real-time drifting strategies that put the car where\nneeded while bypassing expensive trajectory optimization. To this end, we\ndesign a reinforcement learning agent that builds on the concept of tire energy\nabsorption to autonomously drift through changing and complex waypoint\nconfigurations while safely staying within track bounds. We achieve zero-shot\ndeployment on the car by training the agent in a simulation environment built\non top of a neural stochastic differential equation vehicle model learned from\npre-collected driving data. Experiments on a Toyota GR Supra and Lexus LC 500\nshow that the agent is capable of drifting smoothly through varying waypoint\nconfigurations with tracking error as low as 10 cm while stably pushing the\nvehicles to sideslip angles of up to 63{\\deg}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The skill to drift a car--i.e., operate in a state of controlled oversteer\nlike professional drivers--could give future autonomous cars maximum\nflexibility when they need to retain control in adverse conditions or avoid\ncollisions. We investigate real-time drifting strategies that put the car where\nneeded while bypassing expensive trajectory optimization. To this end, we\ndesign a reinforcement learning agent that builds on the concept of tire energy\nabsorption to autonomously drift through changing and complex waypoint\nconfigurations while safely staying within track bounds. We achieve zero-shot\ndeployment on the car by training the agent in a simulation environment built\non top of a neural stochastic differential equation vehicle model learned from\npre-collected driving data. Experiments on a Toyota GR Supra and Lexus LC 500\nshow that the agent is capable of drifting smoothly through varying waypoint\nconfigurations with tracking error as low as 10 cm while stably pushing the\nvehicles to sideslip angles of up to 63{\\deg}."
                },
                "authors": [
                    {
                        "name": "Franck Djeumou"
                    },
                    {
                        "name": "Michael Thompson"
                    },
                    {
                        "name": "Makoto Suminaka"
                    },
                    {
                        "name": "John Subosits"
                    }
                ],
                "author_detail": {
                    "name": "John Subosits"
                },
                "author": "John Subosits",
                "arxiv_comment": "Initial submission to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20989v1",
                "updated": "2024-10-28T13:08:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    8,
                    23,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:08:23Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    8,
                    23,
                    0,
                    302,
                    0
                ],
                "title": "Empowering Autonomous Shuttles with Next-Generation Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Autonomous Shuttles with Next-Generation Infrastructure"
                },
                "summary": "As cities strive to address urban mobility challenges, combining autonomous\ntransportation technologies with intelligent infrastructure presents an\nopportunity to transform how people move within urban environments. Autonomous\nshuttles are particularly suited for adaptive and responsive public transport\nfor the first and last mile, connecting with smart infrastructure to enhance\nurban transit. This paper presents the concept, implementation, and evaluation\nof a proof-of-concept deployment of an autonomous shuttle integrated with smart\ninfrastructure at a public fair. The infrastructure includes two\nperception-equipped bus stops and a connected pedestrian intersection, all\nlinked through a central communication and control hub. Our key contributions\ninclude the development of a comprehensive system architecture for \"smart\" bus\nstops, the integration of multiple urban locations into a cohesive smart\ntransport ecosystem, and the creation of adaptive shuttle behavior for\nautomated driving. Additionally, we publish an open source dataset and a\nVehicle-to-X (V2X) driver to support further research. Finally, we offer an\noutlook on future research directions and potential expansions of the\ndemonstrated technologies and concepts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As cities strive to address urban mobility challenges, combining autonomous\ntransportation technologies with intelligent infrastructure presents an\nopportunity to transform how people move within urban environments. Autonomous\nshuttles are particularly suited for adaptive and responsive public transport\nfor the first and last mile, connecting with smart infrastructure to enhance\nurban transit. This paper presents the concept, implementation, and evaluation\nof a proof-of-concept deployment of an autonomous shuttle integrated with smart\ninfrastructure at a public fair. The infrastructure includes two\nperception-equipped bus stops and a connected pedestrian intersection, all\nlinked through a central communication and control hub. Our key contributions\ninclude the development of a comprehensive system architecture for \"smart\" bus\nstops, the integration of multiple urban locations into a cohesive smart\ntransport ecosystem, and the creation of adaptive shuttle behavior for\nautomated driving. Additionally, we publish an open source dataset and a\nVehicle-to-X (V2X) driver to support further research. Finally, we offer an\noutlook on future research directions and potential expansions of the\ndemonstrated technologies and concepts."
                },
                "authors": [
                    {
                        "name": "Sven Ochs"
                    },
                    {
                        "name": "Melih Yazgan"
                    },
                    {
                        "name": "Rupert Polley"
                    },
                    {
                        "name": "Albert Schotschneider"
                    },
                    {
                        "name": "Stefan Orf"
                    },
                    {
                        "name": "Marc Uecker"
                    },
                    {
                        "name": "Maximilian Zipfl"
                    },
                    {
                        "name": "Julian Burger"
                    },
                    {
                        "name": "Abhishek Vivekanandan"
                    },
                    {
                        "name": "Jennifer Amritzer"
                    },
                    {
                        "name": "Marc René Zofka"
                    },
                    {
                        "name": "J. Marius Zöllner"
                    }
                ],
                "author_detail": {
                    "name": "J. Marius Zöllner"
                },
                "author": "J. Marius Zöllner",
                "arxiv_comment": "Accepted by ECCV 2024 Workshop MAAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20976v1",
                "updated": "2024-10-28T12:50:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    50,
                    46,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T12:50:46Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    50,
                    46,
                    0,
                    302,
                    0
                ],
                "title": "Large Language Model-Guided Prediction Toward Quantum Materials\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Guided Prediction Toward Quantum Materials\n  Synthesis"
                },
                "summary": "The synthesis of inorganic crystalline materials is essential for modern\ntechnology, especially in quantum materials development. However, designing\nefficient synthesis workflows remains a significant challenge due to the\nprecise experimental conditions and extensive trial and error. Here, we present\na framework using large language models (LLMs) to predict synthesis pathways\nfor inorganic materials, including quantum materials. Our framework contains\nthree models: LHS2RHS, predicting products from reactants; RHS2LHS, predicting\nreactants from products; and TGT2CEQ, generating full chemical equations for\ntarget compounds. Fine-tuned on a text-mined synthesis database, our model\nraises accuracy from under 40% with pretrained models, to under 80% using\nconventional fine-tuning, and further to around 90% with our proposed\ngeneralized Tanimoto similarity, while maintaining robust to additional\nsynthesis steps. Our model further demonstrates comparable performance across\nmaterials with varying degrees of quantumness quantified using quantum weight,\nindicating that LLMs offer a powerful tool to predict balanced chemical\nequations for quantum materials discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The synthesis of inorganic crystalline materials is essential for modern\ntechnology, especially in quantum materials development. However, designing\nefficient synthesis workflows remains a significant challenge due to the\nprecise experimental conditions and extensive trial and error. Here, we present\na framework using large language models (LLMs) to predict synthesis pathways\nfor inorganic materials, including quantum materials. Our framework contains\nthree models: LHS2RHS, predicting products from reactants; RHS2LHS, predicting\nreactants from products; and TGT2CEQ, generating full chemical equations for\ntarget compounds. Fine-tuned on a text-mined synthesis database, our model\nraises accuracy from under 40% with pretrained models, to under 80% using\nconventional fine-tuning, and further to around 90% with our proposed\ngeneralized Tanimoto similarity, while maintaining robust to additional\nsynthesis steps. Our model further demonstrates comparable performance across\nmaterials with varying degrees of quantumness quantified using quantum weight,\nindicating that LLMs offer a powerful tool to predict balanced chemical\nequations for quantum materials discovery."
                },
                "authors": [
                    {
                        "name": "Ryotaro Okabe"
                    },
                    {
                        "name": "Zack West"
                    },
                    {
                        "name": "Abhijatmedhi Chotrattanapituk"
                    },
                    {
                        "name": "Mouyang Cheng"
                    },
                    {
                        "name": "Denisse Córdova Carrizales"
                    },
                    {
                        "name": "Weiwei Xie"
                    },
                    {
                        "name": "Robert J. Cava"
                    },
                    {
                        "name": "Mingda Li"
                    }
                ],
                "author_detail": {
                    "name": "Mingda Li"
                },
                "author": "Mingda Li",
                "arxiv_comment": "66 pages total, 6 main figures + 3 supplementary figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20975v1",
                "updated": "2024-10-28T12:50:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    50,
                    27,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T12:50:27Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    50,
                    27,
                    0,
                    302,
                    0
                ],
                "title": "Geo-FuB: A Method for Constructing an Operator-Function Knowledge Base\n  for Geospatial Code Generation Tasks Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geo-FuB: A Method for Constructing an Operator-Function Knowledge Base\n  for Geospatial Code Generation Tasks Using Large Language Models"
                },
                "summary": "The rise of spatiotemporal data and the need for efficient geospatial\nmodeling have spurred interest in automating these tasks with large language\nmodels (LLMs). However, general LLMs often generate errors in geospatial code\ndue to a lack of domain-specific knowledge on functions and operators. To\naddress this, a retrieval-augmented generation (RAG) approach, utilizing an\nexternal knowledge base of geospatial functions and operators, is proposed.\nThis study introduces a framework to construct such a knowledge base,\nleveraging geospatial script semantics. The framework includes: Function\nSemantic Framework Construction (Geo-FuSE), Frequent Operator Combination\nStatistics (Geo-FuST), and Semantic Mapping (Geo-FuM). Techniques like\nChain-of-Thought, TF-IDF, and the APRIORI algorithm are utilized to derive and\nalign geospatial functions. An example knowledge base, Geo-FuB, built from\n154,075 Google Earth Engine scripts, is available on GitHub. Evaluation metrics\nshow a high accuracy, reaching 88.89% overall, with structural and semantic\naccuracies of 92.03% and 86.79% respectively. Geo-FuB's potential to optimize\ngeospatial code generation through the RAG and fine-tuning paradigms is\nhighlighted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of spatiotemporal data and the need for efficient geospatial\nmodeling have spurred interest in automating these tasks with large language\nmodels (LLMs). However, general LLMs often generate errors in geospatial code\ndue to a lack of domain-specific knowledge on functions and operators. To\naddress this, a retrieval-augmented generation (RAG) approach, utilizing an\nexternal knowledge base of geospatial functions and operators, is proposed.\nThis study introduces a framework to construct such a knowledge base,\nleveraging geospatial script semantics. The framework includes: Function\nSemantic Framework Construction (Geo-FuSE), Frequent Operator Combination\nStatistics (Geo-FuST), and Semantic Mapping (Geo-FuM). Techniques like\nChain-of-Thought, TF-IDF, and the APRIORI algorithm are utilized to derive and\nalign geospatial functions. An example knowledge base, Geo-FuB, built from\n154,075 Google Earth Engine scripts, is available on GitHub. Evaluation metrics\nshow a high accuracy, reaching 88.89% overall, with structural and semantic\naccuracies of 92.03% and 86.79% respectively. Geo-FuB's potential to optimize\ngeospatial code generation through the RAG and fine-tuning paradigms is\nhighlighted."
                },
                "authors": [
                    {
                        "name": "Shuyang Hou"
                    },
                    {
                        "name": "Anqi Zhao"
                    },
                    {
                        "name": "Jianyuan Liang"
                    },
                    {
                        "name": "Zhangxiao Shen"
                    },
                    {
                        "name": "Huayi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Huayi Wu"
                },
                "author": "Huayi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.13324v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.13324v6",
                "updated": "2024-10-28T12:45:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    45,
                    52,
                    0,
                    302,
                    0
                ],
                "published": "2024-01-24T09:39:39Z",
                "published_parsed": [
                    2024,
                    1,
                    24,
                    9,
                    39,
                    39,
                    2,
                    24,
                    0
                ],
                "title": "Information That Matters: Exploring Information Needs of People Affected\n  by Algorithmic Decisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information That Matters: Exploring Information Needs of People Affected\n  by Algorithmic Decisions"
                },
                "summary": "Every AI system that makes decisions about people has a group of stakeholders\nthat are personally affected by these decisions. However, explanations of AI\nsystems rarely address the information needs of this stakeholder group, who\noften are AI novices. This creates a gap between conveyed information and\ninformation that matters to those who are impacted by the system's decisions,\nsuch as domain experts and decision subjects. To address this, we present the\n\"XAI Novice Question Bank,\" an extension of the XAI Question Bank containing a\ncatalog of information needs from AI novices in two use cases: employment\nprediction and health monitoring. The catalog covers the categories of data,\nsystem context, system usage, and system specifications. We gathered\ninformation needs through task-based interviews where participants asked\nquestions about two AI systems to decide on their adoption and received verbal\nexplanations in response. Our analysis showed that participants' confidence\nincreased after receiving explanations but that their understanding faced\nchallenges. These included difficulties in locating information and in\nassessing their own understanding, as well as attempts to outsource\nunderstanding. Additionally, participants' prior perceptions of the systems'\nrisks and benefits influenced their information needs. Participants who\nperceived high risks sought explanations about the intentions behind a system's\ndeployment, while those who perceived low risks rather asked about the system's\noperation. Our work aims to support the inclusion of AI novices in\nexplainability efforts by highlighting their information needs, aims, and\nchallenges. We summarize our findings as five key implications that can inform\nthe design of future explanations for lay stakeholder audiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Every AI system that makes decisions about people has a group of stakeholders\nthat are personally affected by these decisions. However, explanations of AI\nsystems rarely address the information needs of this stakeholder group, who\noften are AI novices. This creates a gap between conveyed information and\ninformation that matters to those who are impacted by the system's decisions,\nsuch as domain experts and decision subjects. To address this, we present the\n\"XAI Novice Question Bank,\" an extension of the XAI Question Bank containing a\ncatalog of information needs from AI novices in two use cases: employment\nprediction and health monitoring. The catalog covers the categories of data,\nsystem context, system usage, and system specifications. We gathered\ninformation needs through task-based interviews where participants asked\nquestions about two AI systems to decide on their adoption and received verbal\nexplanations in response. Our analysis showed that participants' confidence\nincreased after receiving explanations but that their understanding faced\nchallenges. These included difficulties in locating information and in\nassessing their own understanding, as well as attempts to outsource\nunderstanding. Additionally, participants' prior perceptions of the systems'\nrisks and benefits influenced their information needs. Participants who\nperceived high risks sought explanations about the intentions behind a system's\ndeployment, while those who perceived low risks rather asked about the system's\noperation. Our work aims to support the inclusion of AI novices in\nexplainability efforts by highlighting their information needs, aims, and\nchallenges. We summarize our findings as five key implications that can inform\nthe design of future explanations for lay stakeholder audiences."
                },
                "authors": [
                    {
                        "name": "Timothée Schmude"
                    },
                    {
                        "name": "Laura Koesten"
                    },
                    {
                        "name": "Torsten Möller"
                    },
                    {
                        "name": "Sebastian Tschiatschek"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Tschiatschek"
                },
                "author": "Sebastian Tschiatschek",
                "arxiv_doi": "10.1016/j.ijhcs.2024.103380",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.ijhcs.2024.103380",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.13324v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.13324v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published version available at the International Journal of\n  Human-Computer Studies, please refer to:\n  https://doi.org/10.1016/j.ijhcs.2024.103380. Main text: 26 pages, 4 figures.\n  Supplementary material is provided",
                "arxiv_journal_ref": "International Journal of Human-Computer Studies, Volume 193, 2025,\n  103380, ISSN 1071-5819",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20971v1",
                "updated": "2024-10-28T12:43:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    43,
                    47,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T12:43:47Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    43,
                    47,
                    0,
                    302,
                    0
                ],
                "title": "BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against\n  Jailbreak Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against\n  Jailbreak Attacks"
                },
                "summary": "Despite their superb multimodal capabilities, Vision-Language Models (VLMs)\nhave been shown to be vulnerable to jailbreak attacks, which are inference-time\nattacks that induce the model to output harmful responses with tricky prompts.\nIt is thus essential to defend VLMs against potential jailbreaks for their\ntrustworthy deployment in real-world applications. In this work, we focus on\nblack-box defense for VLMs against jailbreak attacks. Existing black-box\ndefense methods are either unimodal or bimodal. Unimodal methods enhance either\nthe vision or language module of the VLM, while bimodal methods robustify the\nmodel through text-image representation realignment. However, these methods\nsuffer from two limitations: 1) they fail to fully exploit the cross-modal\ninformation, or 2) they degrade the model performance on benign inputs. To\naddress these limitations, we propose a novel blue-team method BlueSuffix that\ndefends the black-box target VLM against jailbreak attacks without compromising\nits performance. BlueSuffix includes three key components: 1) a visual purifier\nagainst jailbreak images, 2) a textual purifier against jailbreak texts, and 3)\na blue-team suffix generator fine-tuned via reinforcement learning for\nenhancing cross-modal robustness. We empirically show on three VLMs (LLaVA,\nMiniGPT-4, and Gemini) and two safety benchmarks (MM-SafetyBench and\nRedTeam-2K) that BlueSuffix outperforms the baseline defenses by a significant\nmargin. Our BlueSuffix opens up a promising direction for defending VLMs\nagainst jailbreak attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their superb multimodal capabilities, Vision-Language Models (VLMs)\nhave been shown to be vulnerable to jailbreak attacks, which are inference-time\nattacks that induce the model to output harmful responses with tricky prompts.\nIt is thus essential to defend VLMs against potential jailbreaks for their\ntrustworthy deployment in real-world applications. In this work, we focus on\nblack-box defense for VLMs against jailbreak attacks. Existing black-box\ndefense methods are either unimodal or bimodal. Unimodal methods enhance either\nthe vision or language module of the VLM, while bimodal methods robustify the\nmodel through text-image representation realignment. However, these methods\nsuffer from two limitations: 1) they fail to fully exploit the cross-modal\ninformation, or 2) they degrade the model performance on benign inputs. To\naddress these limitations, we propose a novel blue-team method BlueSuffix that\ndefends the black-box target VLM against jailbreak attacks without compromising\nits performance. BlueSuffix includes three key components: 1) a visual purifier\nagainst jailbreak images, 2) a textual purifier against jailbreak texts, and 3)\na blue-team suffix generator fine-tuned via reinforcement learning for\nenhancing cross-modal robustness. We empirically show on three VLMs (LLaVA,\nMiniGPT-4, and Gemini) and two safety benchmarks (MM-SafetyBench and\nRedTeam-2K) that BlueSuffix outperforms the baseline defenses by a significant\nmargin. Our BlueSuffix opens up a promising direction for defending VLMs\nagainst jailbreak attacks."
                },
                "authors": [
                    {
                        "name": "Yunhan Zhao"
                    },
                    {
                        "name": "Xiang Zheng"
                    },
                    {
                        "name": "Lin Luo"
                    },
                    {
                        "name": "Yige Li"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20964v1",
                "updated": "2024-10-28T12:34:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    34,
                    49,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T12:34:49Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    34,
                    49,
                    0,
                    302,
                    0
                ],
                "title": "DeTeCtive: Detecting AI-generated Text via Multi-Level Contrastive\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeTeCtive: Detecting AI-generated Text via Multi-Level Contrastive\n  Learning"
                },
                "summary": "Current techniques for detecting AI-generated text are largely confined to\nmanual feature crafting and supervised binary classification paradigms. These\nmethodologies typically lead to performance bottlenecks and unsatisfactory\ngeneralizability. Consequently, these methods are often inapplicable for\nout-of-distribution (OOD) data and newly emerged large language models (LLMs).\nIn this paper, we revisit the task of AI-generated text detection. We argue\nthat the key to accomplishing this task lies in distinguishing writing styles\nof different authors, rather than simply classifying the text into\nhuman-written or AI-generated text. To this end, we propose DeTeCtive, a\nmulti-task auxiliary, multi-level contrastive learning framework. DeTeCtive is\ndesigned to facilitate the learning of distinct writing styles, combined with a\ndense information retrieval pipeline for AI-generated text detection. Our\nmethod is compatible with a range of text encoders. Extensive experiments\ndemonstrate that our method enhances the ability of various text encoders in\ndetecting AI-generated text across multiple benchmarks and achieves\nstate-of-the-art results. Notably, in OOD zero-shot evaluation, our method\noutperforms existing approaches by a large margin. Moreover, we find our method\nboasts a Training-Free Incremental Adaptation (TFIA) capability towards OOD\ndata, further enhancing its efficacy in OOD detection scenarios. We will\nopen-source our code and models in hopes that our work will spark new thoughts\nin the field of AI-generated text detection, ensuring safe application of LLMs\nand enhancing compliance. Our code is available at\nhttps://github.com/heyongxin233/DeTeCtive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current techniques for detecting AI-generated text are largely confined to\nmanual feature crafting and supervised binary classification paradigms. These\nmethodologies typically lead to performance bottlenecks and unsatisfactory\ngeneralizability. Consequently, these methods are often inapplicable for\nout-of-distribution (OOD) data and newly emerged large language models (LLMs).\nIn this paper, we revisit the task of AI-generated text detection. We argue\nthat the key to accomplishing this task lies in distinguishing writing styles\nof different authors, rather than simply classifying the text into\nhuman-written or AI-generated text. To this end, we propose DeTeCtive, a\nmulti-task auxiliary, multi-level contrastive learning framework. DeTeCtive is\ndesigned to facilitate the learning of distinct writing styles, combined with a\ndense information retrieval pipeline for AI-generated text detection. Our\nmethod is compatible with a range of text encoders. Extensive experiments\ndemonstrate that our method enhances the ability of various text encoders in\ndetecting AI-generated text across multiple benchmarks and achieves\nstate-of-the-art results. Notably, in OOD zero-shot evaluation, our method\noutperforms existing approaches by a large margin. Moreover, we find our method\nboasts a Training-Free Incremental Adaptation (TFIA) capability towards OOD\ndata, further enhancing its efficacy in OOD detection scenarios. We will\nopen-source our code and models in hopes that our work will spark new thoughts\nin the field of AI-generated text detection, ensuring safe application of LLMs\nand enhancing compliance. Our code is available at\nhttps://github.com/heyongxin233/DeTeCtive."
                },
                "authors": [
                    {
                        "name": "Xun Guo"
                    },
                    {
                        "name": "Shan Zhang"
                    },
                    {
                        "name": "Yongxin He"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Wanquan Feng"
                    },
                    {
                        "name": "Haibin Huang"
                    },
                    {
                        "name": "Chongyang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chongyang Ma"
                },
                "author": "Chongyang Ma",
                "arxiv_comment": "To appear in NeurIPS 2024. Code is available at\n  https://github.com/heyongxin233/DeTeCtive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02707v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02707v3",
                "updated": "2024-10-28T12:33:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    33,
                    44,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-03T17:31:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    31,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations"
                },
                "summary": "Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation."
                },
                "authors": [
                    {
                        "name": "Hadas Orgad"
                    },
                    {
                        "name": "Michael Toker"
                    },
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02707v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02707v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09824v2",
                "updated": "2024-10-28T12:05:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    5,
                    8,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-13T12:57:08Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    12,
                    57,
                    8,
                    6,
                    287,
                    0
                ],
                "title": "Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent\n  Simulation"
                },
                "summary": "Graph generation is a fundamental task that has been extensively studied in\nsocial, technological, and scientific analysis. For modeling the dynamic graph\nevolution process, traditional rule-based methods struggle to capture community\nstructures within graphs, while deep learning methods only focus on fitting\ntraining graphs. This limits existing graph generators to producing graphs that\nadhere to predefined rules or closely resemble training datasets, achieving\npoor performance in dynamic graph generation. Given that graphs are abstract\nrepresentations arising from pairwise interactions in human activities, a\nrealistic simulation of human-wise interaction could provide deeper insights\ninto the graph evolution mechanism. With the increasing recognition of large\nlanguage models (LLMs) in simulating human behavior, we introduce\nGraphAgent-Generator (GAG), a novel simulation-based framework for dynamic\ngraph generation. Without training or fine-tuning process of LLM, our framework\neffectively replicates seven macro-level structural characteristics in\nestablished network science theories while surpassing existing baselines in\ngraph expansion tasks by 31\\% on specific evaluation metrics. Through node\nclassification task, we validate GAG effectively preserves characteristics of\nreal-world network for node-wise textual features in generated text-rich graph.\nFurthermore, by incorporating parallel acceleration, GAG supports generating\ngraphs with up to nearly 100,000 nodes or 10 million edges through large-scale\nLLM-based agent simulation, with a minimum speed-up of 90.4\\%. The source code\nis available at https://anonymous.4open.science/r/GraphAgent-2206.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph generation is a fundamental task that has been extensively studied in\nsocial, technological, and scientific analysis. For modeling the dynamic graph\nevolution process, traditional rule-based methods struggle to capture community\nstructures within graphs, while deep learning methods only focus on fitting\ntraining graphs. This limits existing graph generators to producing graphs that\nadhere to predefined rules or closely resemble training datasets, achieving\npoor performance in dynamic graph generation. Given that graphs are abstract\nrepresentations arising from pairwise interactions in human activities, a\nrealistic simulation of human-wise interaction could provide deeper insights\ninto the graph evolution mechanism. With the increasing recognition of large\nlanguage models (LLMs) in simulating human behavior, we introduce\nGraphAgent-Generator (GAG), a novel simulation-based framework for dynamic\ngraph generation. Without training or fine-tuning process of LLM, our framework\neffectively replicates seven macro-level structural characteristics in\nestablished network science theories while surpassing existing baselines in\ngraph expansion tasks by 31\\% on specific evaluation metrics. Through node\nclassification task, we validate GAG effectively preserves characteristics of\nreal-world network for node-wise textual features in generated text-rich graph.\nFurthermore, by incorporating parallel acceleration, GAG supports generating\ngraphs with up to nearly 100,000 nodes or 10 million edges through large-scale\nLLM-based agent simulation, with a minimum speed-up of 90.4\\%. The source code\nis available at https://anonymous.4open.science/r/GraphAgent-2206."
                },
                "authors": [
                    {
                        "name": "Jiarui Ji"
                    },
                    {
                        "name": "Runlin Lei"
                    },
                    {
                        "name": "Jialing Bi"
                    },
                    {
                        "name": "Zhewei Wei"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    }
                ],
                "author_detail": {
                    "name": "Bolin Ding"
                },
                "author": "Bolin Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20941v1",
                "updated": "2024-10-28T11:49:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    49,
                    58,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T11:49:58Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    49,
                    58,
                    0,
                    302,
                    0
                ],
                "title": "Instruction-Tuned LLMs Succeed in Document-Level MT Without Fine-Tuning\n  -- But BLEU Turns a Blind Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-Tuned LLMs Succeed in Document-Level MT Without Fine-Tuning\n  -- But BLEU Turns a Blind Eye"
                },
                "summary": "Large language models (LLMs) have excelled in various NLP tasks, including\nmachine translation (MT), yet most studies focus on sentence-level translation.\nThis work investigates the inherent capability of instruction-tuned LLMs for\ndocument-level translation (docMT). Unlike prior approaches that require\nspecialized techniques, we evaluate LLMs by directly prompting them to\ntranslate entire documents in a single pass. Our results show that this method\nimproves translation quality compared to translating sentences separately, even\nwithout document-level fine-tuning. However, this advantage is not reflected in\nBLEU scores, which often favor sentence-based translations. We propose using\nthe LLM-as-a-judge paradigm for evaluation, where GPT-4 is used to assess\ndocument coherence, accuracy, and fluency in a more nuanced way than\nn-gram-based metrics. Overall, our work demonstrates that instruction-tuned\nLLMs can effectively leverage document context for translation. However, we\ncaution against using BLEU scores for evaluating docMT, as they often provide\nmisleading outcomes, failing to capture the quality of document-level\ntranslation. Code and data are available at\nhttps://github.com/EIT-NLP/BLEUless_DocMT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various NLP tasks, including\nmachine translation (MT), yet most studies focus on sentence-level translation.\nThis work investigates the inherent capability of instruction-tuned LLMs for\ndocument-level translation (docMT). Unlike prior approaches that require\nspecialized techniques, we evaluate LLMs by directly prompting them to\ntranslate entire documents in a single pass. Our results show that this method\nimproves translation quality compared to translating sentences separately, even\nwithout document-level fine-tuning. However, this advantage is not reflected in\nBLEU scores, which often favor sentence-based translations. We propose using\nthe LLM-as-a-judge paradigm for evaluation, where GPT-4 is used to assess\ndocument coherence, accuracy, and fluency in a more nuanced way than\nn-gram-based metrics. Overall, our work demonstrates that instruction-tuned\nLLMs can effectively leverage document context for translation. However, we\ncaution against using BLEU scores for evaluating docMT, as they often provide\nmisleading outcomes, failing to capture the quality of document-level\ntranslation. Code and data are available at\nhttps://github.com/EIT-NLP/BLEUless_DocMT"
                },
                "authors": [
                    {
                        "name": "Yirong Sun"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yanjun Chen"
                    },
                    {
                        "name": "Erjia Xiao"
                    },
                    {
                        "name": "Xinghao Chen"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20936v1",
                "updated": "2024-10-28T11:37:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    37,
                    39,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T11:37:39Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    37,
                    39,
                    0,
                    302,
                    0
                ],
                "title": "Autoformalize Mathematical Statements by Symbolic Equivalence and\n  Semantic Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalize Mathematical Statements by Symbolic Equivalence and\n  Semantic Consistency"
                },
                "summary": "Autoformalization, the task of automatically translating natural language\ndescriptions into a formal language, poses a significant challenge across\nvarious domains, especially in mathematics. Recent advancements in large\nlanguage models (LLMs) have unveiled their promising capabilities to formalize\neven competition-level math problems. However, we observe a considerable\ndiscrepancy between pass@1 and pass@k accuracies in LLM-generated\nformalizations. To address this gap, we introduce a novel framework that scores\nand selects the best result from k autoformalization candidates based on two\ncomplementary self-consistency methods: symbolic equivalence and semantic\nconsistency. Elaborately, symbolic equivalence identifies the logical\nhomogeneity among autoformalization candidates using automated theorem provers,\nand semantic consistency evaluates the preservation of the original meaning by\ninformalizing the candidates and computing the similarity between the\nembeddings of the original and informalized texts. Our extensive experiments on\nthe MATH and miniF2F datasets demonstrate that our approach significantly\nenhances autoformalization accuracy, achieving up to 0.22-1.35x relative\nimprovements across various LLMs and baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization, the task of automatically translating natural language\ndescriptions into a formal language, poses a significant challenge across\nvarious domains, especially in mathematics. Recent advancements in large\nlanguage models (LLMs) have unveiled their promising capabilities to formalize\neven competition-level math problems. However, we observe a considerable\ndiscrepancy between pass@1 and pass@k accuracies in LLM-generated\nformalizations. To address this gap, we introduce a novel framework that scores\nand selects the best result from k autoformalization candidates based on two\ncomplementary self-consistency methods: symbolic equivalence and semantic\nconsistency. Elaborately, symbolic equivalence identifies the logical\nhomogeneity among autoformalization candidates using automated theorem provers,\nand semantic consistency evaluates the preservation of the original meaning by\ninformalizing the candidates and computing the similarity between the\nembeddings of the original and informalized texts. Our extensive experiments on\nthe MATH and miniF2F datasets demonstrate that our approach significantly\nenhances autoformalization accuracy, achieving up to 0.22-1.35x relative\nimprovements across various LLMs and baseline methods."
                },
                "authors": [
                    {
                        "name": "Zenan Li"
                    },
                    {
                        "name": "Yifan Wu"
                    },
                    {
                        "name": "Zhaoyu Li"
                    },
                    {
                        "name": "Xinming Wei"
                    },
                    {
                        "name": "Xian Zhang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxing Ma"
                },
                "author": "Xiaoxing Ma",
                "arxiv_comment": "Published as a conference paper at NeurIPS 2024. Code is available at\n  [this https URL](https://github.com/Miracle-Messi/Isa-AutoFormal)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19014v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19014v4",
                "updated": "2024-10-28T11:11:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    11,
                    4,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-24T01:40:50Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    1,
                    40,
                    50,
                    1,
                    268,
                    0
                ],
                "title": "FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark"
                },
                "summary": "Text-to-SQL systems have become crucial for translating natural language into\nSQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, the Execution Accuracy\n(EX), the most prevalent evaluation metric, still shows many false positives\nand negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our metric improves\nagreement with human experts (from 62 to 87.04 in Cohen's kappa) with\ncomprehensive context and sophisticated criteria. Our extensive experiments\nyield several key insights: (1) Models' performance increases by over 2.6\npoints on average, substantially affecting rankings on Spider and BIRD\nbenchmarks; (2) The underestimation of models in EX primarily stems from\nannotation quality issues; and (3) Model performance on particularly\nchallenging questions tends to be overestimated. This work contributes to a\nmore accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL systems have become crucial for translating natural language into\nSQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, the Execution Accuracy\n(EX), the most prevalent evaluation metric, still shows many false positives\nand negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our metric improves\nagreement with human experts (from 62 to 87.04 in Cohen's kappa) with\ncomprehensive context and sophisticated criteria. Our extensive experiments\nyield several key insights: (1) Models' performance increases by over 2.6\npoints on average, substantially affecting rankings on Spider and BIRD\nbenchmarks; (2) The underestimation of models in EX primarily stems from\nannotation quality issues; and (3) Model performance on particularly\nchallenging questions tends to be overestimated. This work contributes to a\nmore accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field."
                },
                "authors": [
                    {
                        "name": "Heegyu Kim"
                    },
                    {
                        "name": "Taeyang Jeon"
                    },
                    {
                        "name": "Seunghwan Choi"
                    },
                    {
                        "name": "Seungtaek Choi"
                    },
                    {
                        "name": "Hyunsouk Cho"
                    }
                ],
                "author_detail": {
                    "name": "Hyunsouk Cho"
                },
                "author": "Hyunsouk Cho",
                "arxiv_comment": "preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19014v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19014v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20926v1",
                "updated": "2024-10-28T11:08:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    8,
                    57,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T11:08:57Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    8,
                    57,
                    0,
                    302,
                    0
                ],
                "title": "Long Sequence Modeling with Attention Tensorization: From Sequence to\n  Tensor Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Sequence Modeling with Attention Tensorization: From Sequence to\n  Tensor Learning"
                },
                "summary": "As the demand for processing extended textual data grows, the ability to\nhandle long-range dependencies and maintain computational efficiency is more\ncritical than ever. One of the key issues for long-sequence modeling using\nattention-based model is the mismatch between the limited-range modeling power\nof full attention and the long-range token dependency in the input sequence. In\nthis work, we propose to scale up the attention receptive field by tensorizing\nlong input sequences into compact tensor representations followed by attention\non each transformed dimension. The resulting Tensorized Attention can be\nadopted as efficient transformer backbones to extend input context length with\nimproved memory and time efficiency. We show that the proposed attention\ntensorization encodes token dependencies as a multi-hop attention process, and\nis equivalent to Kronecker decomposition of full attention. Extensive\nexperiments show that tensorized attention can be used to adapt pretrained LLMs\nwith improved efficiency. Notably, Llama-8B with tensorization is trained under\n32,768 context length and can steadily extrapolate to 128k length during\ninference with $11\\times$ speedup, compared to full attention with\nFlashAttention-2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for processing extended textual data grows, the ability to\nhandle long-range dependencies and maintain computational efficiency is more\ncritical than ever. One of the key issues for long-sequence modeling using\nattention-based model is the mismatch between the limited-range modeling power\nof full attention and the long-range token dependency in the input sequence. In\nthis work, we propose to scale up the attention receptive field by tensorizing\nlong input sequences into compact tensor representations followed by attention\non each transformed dimension. The resulting Tensorized Attention can be\nadopted as efficient transformer backbones to extend input context length with\nimproved memory and time efficiency. We show that the proposed attention\ntensorization encodes token dependencies as a multi-hop attention process, and\nis equivalent to Kronecker decomposition of full attention. Extensive\nexperiments show that tensorized attention can be used to adapt pretrained LLMs\nwith improved efficiency. Notably, Llama-8B with tensorization is trained under\n32,768 context length and can steadily extrapolate to 128k length during\ninference with $11\\times$ speedup, compared to full attention with\nFlashAttention-2."
                },
                "authors": [
                    {
                        "name": "Aosong Feng"
                    },
                    {
                        "name": "Rex Ying"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    }
                ],
                "author_detail": {
                    "name": "Leandros Tassiulas"
                },
                "author": "Leandros Tassiulas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20911v1",
                "updated": "2024-10-28T10:43:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    43,
                    34,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T10:43:34Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    43,
                    34,
                    0,
                    302,
                    0
                ],
                "title": "Hacking Back the AI-Hacker: Prompt Injection as a Defense Against\n  LLM-driven Cyberattacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hacking Back the AI-Hacker: Prompt Injection as a Defense Against\n  LLM-driven Cyberattacks"
                },
                "summary": "Large language models (LLMs) are increasingly being harnessed to automate\ncyberattacks, making sophisticated exploits more accessible and scalable. In\nresponse, we propose a new defense strategy tailored to counter LLM-driven\ncyberattacks. We introduce Mantis, a defensive framework that exploits LLMs'\nsusceptibility to adversarial inputs to undermine malicious operations. Upon\ndetecting an automated cyberattack, Mantis plants carefully crafted inputs into\nsystem responses, leading the attacker's LLM to disrupt their own operations\n(passive defense) or even compromise the attacker's machine (active defense).\nBy deploying purposefully vulnerable decoy services to attract the attacker and\nusing dynamic prompt injections for the attacker's LLM, Mantis can autonomously\nhack back the attacker. In our experiments, Mantis consistently achieved over\n95% effectiveness against automated LLM-driven attacks. To foster further\nresearch and collaboration, Mantis is available as an open-source tool:\nhttps://github.com/pasquini-dario/project_mantis",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being harnessed to automate\ncyberattacks, making sophisticated exploits more accessible and scalable. In\nresponse, we propose a new defense strategy tailored to counter LLM-driven\ncyberattacks. We introduce Mantis, a defensive framework that exploits LLMs'\nsusceptibility to adversarial inputs to undermine malicious operations. Upon\ndetecting an automated cyberattack, Mantis plants carefully crafted inputs into\nsystem responses, leading the attacker's LLM to disrupt their own operations\n(passive defense) or even compromise the attacker's machine (active defense).\nBy deploying purposefully vulnerable decoy services to attract the attacker and\nusing dynamic prompt injections for the attacker's LLM, Mantis can autonomously\nhack back the attacker. In our experiments, Mantis consistently achieved over\n95% effectiveness against automated LLM-driven attacks. To foster further\nresearch and collaboration, Mantis is available as an open-source tool:\nhttps://github.com/pasquini-dario/project_mantis"
                },
                "authors": [
                    {
                        "name": "Dario Pasquini"
                    },
                    {
                        "name": "Evgenios M. Kornaropoulos"
                    },
                    {
                        "name": "Giuseppe Ateniese"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Ateniese"
                },
                "author": "Giuseppe Ateniese",
                "arxiv_comment": "v0.1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20907v1",
                "updated": "2024-10-28T10:36:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    36,
                    32,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T10:36:32Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    36,
                    32,
                    0,
                    302,
                    0
                ],
                "title": "Combining Deep Reinforcement Learning with a Jerk-Bounded Trajectory\n  Generator for Kinematically Constrained Motion Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Deep Reinforcement Learning with a Jerk-Bounded Trajectory\n  Generator for Kinematically Constrained Motion Planning"
                },
                "summary": "Deep reinforcement learning (DRL) is emerging as a promising method for\nadaptive robotic motion and complex task automation, effectively addressing the\nlimitations of traditional control methods. However, ensuring safety throughout\nboth the learning process and policy deployment remains a key challenge due to\nthe risky exploration inherent in DRL, as well as the discrete nature of\nactions taken at intervals. These discontinuities, despite being part of a\ncontinuous action space, can lead to abrupt changes between successive actions,\ncausing instability and unsafe intermediate states. To address these\nchallenges, this paper proposes an integrated framework that combines DRL with\na jerk-bounded trajectory generator (JBTG) and a robust low-level control\nstrategy, significantly enhancing the safety, stability, and reliability of\nrobotic manipulators. The low-level controller ensures the precise execution of\nDRL-generated commands, while the JBTG refines these motions to produce smooth,\ncontinuous trajectories that prevent abrupt or unsafe actions. The framework\nalso includes pre-calculated safe velocity zones for smooth braking, preventing\njoint limit violations and ensuring compliance with kinematic constraints. This\napproach not only guarantees the robustness and safety of the robotic system\nbut also optimizes motion control, making it suitable for practical\napplications. The effectiveness of the proposed framework is demonstrated\nthrough its application to a highly complex heavy-duty manipulator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep reinforcement learning (DRL) is emerging as a promising method for\nadaptive robotic motion and complex task automation, effectively addressing the\nlimitations of traditional control methods. However, ensuring safety throughout\nboth the learning process and policy deployment remains a key challenge due to\nthe risky exploration inherent in DRL, as well as the discrete nature of\nactions taken at intervals. These discontinuities, despite being part of a\ncontinuous action space, can lead to abrupt changes between successive actions,\ncausing instability and unsafe intermediate states. To address these\nchallenges, this paper proposes an integrated framework that combines DRL with\na jerk-bounded trajectory generator (JBTG) and a robust low-level control\nstrategy, significantly enhancing the safety, stability, and reliability of\nrobotic manipulators. The low-level controller ensures the precise execution of\nDRL-generated commands, while the JBTG refines these motions to produce smooth,\ncontinuous trajectories that prevent abrupt or unsafe actions. The framework\nalso includes pre-calculated safe velocity zones for smooth braking, preventing\njoint limit violations and ensuring compliance with kinematic constraints. This\napproach not only guarantees the robustness and safety of the robotic system\nbut also optimizes motion control, making it suitable for practical\napplications. The effectiveness of the proposed framework is demonstrated\nthrough its application to a highly complex heavy-duty manipulator."
                },
                "authors": [
                    {
                        "name": "Seyed Adel Alizadeh Kolagar"
                    },
                    {
                        "name": "Mehdi Heydari Shahna"
                    },
                    {
                        "name": "Jouni Mattila"
                    }
                ],
                "author_detail": {
                    "name": "Jouni Mattila"
                },
                "author": "Jouni Mattila",
                "arxiv_comment": "This paper has been submitted to the IEEE for potential publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20878v1",
                "updated": "2024-10-28T09:55:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    9,
                    55,
                    52,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T09:55:52Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    9,
                    55,
                    52,
                    0,
                    302,
                    0
                ],
                "title": "AutoRAG: Automated Framework for optimization of Retrieval Augmented\n  Generation Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRAG: Automated Framework for optimization of Retrieval Augmented\n  Generation Pipeline"
                },
                "summary": "Using LLMs (Large Language Models) in conjunction with external documents has\nmade RAG (Retrieval-Augmented Generation) an essential technology. Numerous\ntechniques and modules for RAG are being researched, but their performance can\nvary across different datasets. Finding RAG modules that perform well on\nspecific datasets is challenging. In this paper, we propose the AutoRAG\nframework, which automatically identifies suitable RAG modules for a given\ndataset. AutoRAG explores and approximates the optimal combination of RAG\nmodules for the dataset. Additionally, we share the results of optimizing a\ndataset using AutoRAG. All experimental results and data are publicly available\nand can be accessed through our GitHub repository\nhttps://github.com/Marker-Inc-Korea/AutoRAG_ARAGOG_Paper .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs (Large Language Models) in conjunction with external documents has\nmade RAG (Retrieval-Augmented Generation) an essential technology. Numerous\ntechniques and modules for RAG are being researched, but their performance can\nvary across different datasets. Finding RAG modules that perform well on\nspecific datasets is challenging. In this paper, we propose the AutoRAG\nframework, which automatically identifies suitable RAG modules for a given\ndataset. AutoRAG explores and approximates the optimal combination of RAG\nmodules for the dataset. Additionally, we share the results of optimizing a\ndataset using AutoRAG. All experimental results and data are publicly available\nand can be accessed through our GitHub repository\nhttps://github.com/Marker-Inc-Korea/AutoRAG_ARAGOG_Paper ."
                },
                "authors": [
                    {
                        "name": "Dongkyu Kim"
                    },
                    {
                        "name": "Byoungwook Kim"
                    },
                    {
                        "name": "Donggeon Han"
                    },
                    {
                        "name": "Matouš Eibich"
                    }
                ],
                "author_detail": {
                    "name": "Matouš Eibich"
                },
                "author": "Matouš Eibich",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11825v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11825v3",
                "updated": "2024-10-28T09:46:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    9,
                    46,
                    19,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-15T17:52:20Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    52,
                    20,
                    1,
                    289,
                    0
                ],
                "title": "Learning Smooth Humanoid Locomotion through Lipschitz-Constrained\n  Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Smooth Humanoid Locomotion through Lipschitz-Constrained\n  Policies"
                },
                "summary": "Reinforcement learning combined with sim-to-real transfer offers a general\nframework for developing locomotion controllers for legged robots. To\nfacilitate successful deployment in the real world, smoothing techniques, such\nas low-pass filters and smoothness rewards, are often employed to develop\npolicies with smooth behaviors. However, because these techniques are\nnon-differentiable and usually require tedious tuning of a large set of\nhyperparameters, they tend to require extensive manual tuning for each robotic\nplatform. To address this challenge and establish a general technique for\nenforcing smooth behaviors, we propose a simple and effective method that\nimposes a Lipschitz constraint on a learned policy, which we refer to as\nLipschitz-Constrained Policies (LCP). We show that the Lipschitz constraint can\nbe implemented in the form of a gradient penalty, which provides a\ndifferentiable objective that can be easily incorporated with automatic\ndifferentiation frameworks. We demonstrate that LCP effectively replaces the\nneed for smoothing rewards or low-pass filters and can be easily integrated\ninto training frameworks for many distinct humanoid robots. We extensively\nevaluate LCP in both simulation and real-world humanoid robots, producing\nsmooth and robust locomotion controllers. All simulation and deployment code,\nalong with complete checkpoints, is available on our project page:\nhttps://lipschitz-constrained-policy.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning combined with sim-to-real transfer offers a general\nframework for developing locomotion controllers for legged robots. To\nfacilitate successful deployment in the real world, smoothing techniques, such\nas low-pass filters and smoothness rewards, are often employed to develop\npolicies with smooth behaviors. However, because these techniques are\nnon-differentiable and usually require tedious tuning of a large set of\nhyperparameters, they tend to require extensive manual tuning for each robotic\nplatform. To address this challenge and establish a general technique for\nenforcing smooth behaviors, we propose a simple and effective method that\nimposes a Lipschitz constraint on a learned policy, which we refer to as\nLipschitz-Constrained Policies (LCP). We show that the Lipschitz constraint can\nbe implemented in the form of a gradient penalty, which provides a\ndifferentiable objective that can be easily incorporated with automatic\ndifferentiation frameworks. We demonstrate that LCP effectively replaces the\nneed for smoothing rewards or low-pass filters and can be easily integrated\ninto training frameworks for many distinct humanoid robots. We extensively\nevaluate LCP in both simulation and real-world humanoid robots, producing\nsmooth and robust locomotion controllers. All simulation and deployment code,\nalong with complete checkpoints, is available on our project page:\nhttps://lipschitz-constrained-policy.github.io."
                },
                "authors": [
                    {
                        "name": "Zixuan Chen"
                    },
                    {
                        "name": "Xialin He"
                    },
                    {
                        "name": "Yen-Jen Wang"
                    },
                    {
                        "name": "Qiayuan Liao"
                    },
                    {
                        "name": "Yanjie Ze"
                    },
                    {
                        "name": "Zhongyu Li"
                    },
                    {
                        "name": "S. Shankar Sastry"
                    },
                    {
                        "name": "Jiajun Wu"
                    },
                    {
                        "name": "Koushil Sreenath"
                    },
                    {
                        "name": "Saurabh Gupta"
                    },
                    {
                        "name": "Xue Bin Peng"
                    }
                ],
                "author_detail": {
                    "name": "Xue Bin Peng"
                },
                "author": "Xue Bin Peng",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11825v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11825v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14716v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14716v2",
                "updated": "2024-10-28T09:40:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    9,
                    40,
                    18,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-11T13:17:19Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    17,
                    19,
                    4,
                    285,
                    0
                ],
                "title": "A Systematic Survey on Large Language Models for Algorithm Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Survey on Large Language Models for Algorithm Design"
                },
                "summary": "Algorithm Design (AD) is crucial for effective problem-solving across various\ndomains. The advent of Large Language Models (LLMs) has notably enhanced the\nautomation and innovation within this field, offering new perspectives and\npromising solutions. Over the past three years, the integration of LLMs into AD\n(LLM4AD) has progressed significantly, finding applications in diverse areas\nsuch as optimization, machine learning, mathematical reasoning, and scientific\ndiscovery. Given the rapid development and broadening scope of this field, a\nsystematic review is both timely and essential. This paper provides a\nsystematic review of the works on LLM4AD. First, we present an overview and\nsummary of existing studies. Then, we present a systematic taxonomy and a\nreview of existing works along four dimensions, including the role of LLMs,\nsearch techniques, prompt strategies, and applications, with a discussion of\nthe potential and achievements of using LLMs. Finally, we explore current\nchallenges and propose several open questions and promising directions for\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithm Design (AD) is crucial for effective problem-solving across various\ndomains. The advent of Large Language Models (LLMs) has notably enhanced the\nautomation and innovation within this field, offering new perspectives and\npromising solutions. Over the past three years, the integration of LLMs into AD\n(LLM4AD) has progressed significantly, finding applications in diverse areas\nsuch as optimization, machine learning, mathematical reasoning, and scientific\ndiscovery. Given the rapid development and broadening scope of this field, a\nsystematic review is both timely and essential. This paper provides a\nsystematic review of the works on LLM4AD. First, we present an overview and\nsummary of existing studies. Then, we present a systematic taxonomy and a\nreview of existing works along four dimensions, including the role of LLMs,\nsearch techniques, prompt strategies, and applications, with a discussion of\nthe potential and achievements of using LLMs. Finally, we explore current\nchallenges and propose several open questions and promising directions for\nfuture research."
                },
                "authors": [
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Yiming Yao"
                    },
                    {
                        "name": "Ping Guo"
                    },
                    {
                        "name": "Zhiyuan Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Xialiang Tong"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Zhenkun Wang"
                    },
                    {
                        "name": "Qingfu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qingfu Zhang"
                },
                "author": "Qingfu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14716v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14716v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20869v1",
                "updated": "2024-10-28T09:37:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    9,
                    37,
                    58,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T09:37:58Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    9,
                    37,
                    58,
                    0,
                    302,
                    0
                ],
                "title": "Reward Modeling with Weak Supervision for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Modeling with Weak Supervision for Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to their\nincreased application across various tasks, with reinforcement learning from\nhuman feedback (RLHF) being a crucial part of their training to align responses\nwith user intentions. In the RLHF process, a reward model is trained using\nresponses preferences determined by human labelers or AI systems, which then\nrefines the LLM through reinforcement learning. This work introduces weak\nsupervision as a strategy to extend RLHF datasets and enhance reward model\nperformance. Weak supervision employs noisy or imprecise data labeling,\nreducing reliance on expensive manually labeled data. By analyzing RLHF\ndatasets to identify heuristics that correlate with response preference, we\nwrote simple labeling functions and then calibrated a label model to weakly\nannotate unlabeled data. Our evaluation show that while weak supervision\nsignificantly benefits smaller datasets by improving reward model performance,\nits effectiveness decreases with larger, originally labeled datasets.\nAdditionally, using an LLM to generate and then weakly label responses offers a\npromising method for extending preference data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to their\nincreased application across various tasks, with reinforcement learning from\nhuman feedback (RLHF) being a crucial part of their training to align responses\nwith user intentions. In the RLHF process, a reward model is trained using\nresponses preferences determined by human labelers or AI systems, which then\nrefines the LLM through reinforcement learning. This work introduces weak\nsupervision as a strategy to extend RLHF datasets and enhance reward model\nperformance. Weak supervision employs noisy or imprecise data labeling,\nreducing reliance on expensive manually labeled data. By analyzing RLHF\ndatasets to identify heuristics that correlate with response preference, we\nwrote simple labeling functions and then calibrated a label model to weakly\nannotate unlabeled data. Our evaluation show that while weak supervision\nsignificantly benefits smaller datasets by improving reward model performance,\nits effectiveness decreases with larger, originally labeled datasets.\nAdditionally, using an LLM to generate and then weakly label responses offers a\npromising method for extending preference data."
                },
                "authors": [
                    {
                        "name": "Ben Hauptvogel"
                    },
                    {
                        "name": "Malte Ostendorff"
                    },
                    {
                        "name": "Georg Rehm"
                    },
                    {
                        "name": "Sebastian Möller"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Möller"
                },
                "author": "Sebastian Möller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20856v1",
                "updated": "2024-10-28T09:19:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    9,
                    19,
                    29,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T09:19:29Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    9,
                    19,
                    29,
                    0,
                    302,
                    0
                ],
                "title": "Strada-LLM: Graph LLM for traffic prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strada-LLM: Graph LLM for traffic prediction"
                },
                "summary": "Traffic prediction is a vital component of intelligent transportation\nsystems. By reasoning about traffic patterns in both the spatial and temporal\ndimensions, accurate and interpretable predictions can be provided. A\nconsiderable challenge in traffic prediction lies in handling the diverse data\ndistributions caused by vastly different traffic conditions occurring at\ndifferent locations. LLMs have been a dominant solution due to their remarkable\ncapacity to adapt to new datasets with very few labeled data samples, i.e.,\nfew-shot adaptability. However, existing forecasting techniques mainly focus on\nextracting local graph information and forming a text-like prompt, leaving LLM-\nbased traffic prediction an open problem. This work presents a probabilistic\nLLM for traffic forecasting with three highlights. We propose a graph-aware LLM\nfor traffic prediction that considers proximal traffic information.\nSpecifically, by considering the traffic of neighboring nodes as covariates,\nour model outperforms the corresponding time-series LLM. Furthermore, we adopt\na lightweight approach for efficient domain adaptation when facing new data\ndistributions in few-shot fashion. The comparative experiment demonstrates the\nproposed method outperforms the state-of-the-art LLM-based methods and the\ntraditional GNN- based supervised approaches. Furthermore, Strada-LLM can be\neasily adapted to different LLM backbones without a noticeable performance\ndrop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic prediction is a vital component of intelligent transportation\nsystems. By reasoning about traffic patterns in both the spatial and temporal\ndimensions, accurate and interpretable predictions can be provided. A\nconsiderable challenge in traffic prediction lies in handling the diverse data\ndistributions caused by vastly different traffic conditions occurring at\ndifferent locations. LLMs have been a dominant solution due to their remarkable\ncapacity to adapt to new datasets with very few labeled data samples, i.e.,\nfew-shot adaptability. However, existing forecasting techniques mainly focus on\nextracting local graph information and forming a text-like prompt, leaving LLM-\nbased traffic prediction an open problem. This work presents a probabilistic\nLLM for traffic forecasting with three highlights. We propose a graph-aware LLM\nfor traffic prediction that considers proximal traffic information.\nSpecifically, by considering the traffic of neighboring nodes as covariates,\nour model outperforms the corresponding time-series LLM. Furthermore, we adopt\na lightweight approach for efficient domain adaptation when facing new data\ndistributions in few-shot fashion. The comparative experiment demonstrates the\nproposed method outperforms the state-of-the-art LLM-based methods and the\ntraditional GNN- based supervised approaches. Furthermore, Strada-LLM can be\neasily adapted to different LLM backbones without a noticeable performance\ndrop."
                },
                "authors": [
                    {
                        "name": "Seyed Mohamad Moghadas"
                    },
                    {
                        "name": "Yangxintong Lyu"
                    },
                    {
                        "name": "Bruno Cornelis"
                    },
                    {
                        "name": "Alexandre Alahi"
                    },
                    {
                        "name": "Adrian Munteanu"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Munteanu"
                },
                "author": "Adrian Munteanu",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20848v1",
                "updated": "2024-10-28T09:04:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    9,
                    4,
                    49,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T09:04:49Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    9,
                    4,
                    49,
                    0,
                    302,
                    0
                ],
                "title": "Deep Insights into Automated Optimization with Large Language Models and\n  Evolutionary Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Insights into Automated Optimization with Large Language Models and\n  Evolutionary Algorithms"
                },
                "summary": "Designing optimization approaches, whether heuristic or meta-heuristic,\nusually demands extensive manual intervention and has difficulty generalizing\nacross diverse problem domains. The combination of Large Language Models (LLMs)\nand Evolutionary Algorithms (EAs) offers a promising new approach to overcome\nthese limitations and make optimization more automated. In this setup, LLMs act\nas dynamic agents that can generate, refine, and interpret optimization\nstrategies, while EAs efficiently explore complex solution spaces through\nevolutionary operators. Since this synergy enables a more efficient and\ncreative search process, we first conduct an extensive review of recent\nresearch on the application of LLMs in optimization. We focus on LLMs' dual\nfunctionality as solution generators and algorithm designers. Then, we\nsummarize the common and valuable designs in existing work and propose a novel\nLLM-EA paradigm for automated optimization. Furthermore, centered on this\nparadigm, we conduct an in-depth analysis of innovative methods for three key\ncomponents: individual representation, variation operators, and fitness\nevaluation. We address challenges related to heuristic generation and solution\nexploration, especially from the LLM prompts' perspective. Our systematic\nreview and thorough analysis of the paradigm can assist researchers in better\nunderstanding the current research and promoting the development of combining\nLLMs with EAs for automated optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing optimization approaches, whether heuristic or meta-heuristic,\nusually demands extensive manual intervention and has difficulty generalizing\nacross diverse problem domains. The combination of Large Language Models (LLMs)\nand Evolutionary Algorithms (EAs) offers a promising new approach to overcome\nthese limitations and make optimization more automated. In this setup, LLMs act\nas dynamic agents that can generate, refine, and interpret optimization\nstrategies, while EAs efficiently explore complex solution spaces through\nevolutionary operators. Since this synergy enables a more efficient and\ncreative search process, we first conduct an extensive review of recent\nresearch on the application of LLMs in optimization. We focus on LLMs' dual\nfunctionality as solution generators and algorithm designers. Then, we\nsummarize the common and valuable designs in existing work and propose a novel\nLLM-EA paradigm for automated optimization. Furthermore, centered on this\nparadigm, we conduct an in-depth analysis of innovative methods for three key\ncomponents: individual representation, variation operators, and fitness\nevaluation. We address challenges related to heuristic generation and solution\nexploration, especially from the LLM prompts' perspective. Our systematic\nreview and thorough analysis of the paradigm can assist researchers in better\nunderstanding the current research and promoting the development of combining\nLLMs with EAs for automated optimization."
                },
                "authors": [
                    {
                        "name": "He Yu"
                    },
                    {
                        "name": "Jing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Liu"
                },
                "author": "Jing Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17415v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17415v3",
                "updated": "2024-10-28T08:59:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    8,
                    59,
                    1,
                    0,
                    302,
                    0
                ],
                "published": "2024-06-25T09:37:15Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    9,
                    37,
                    15,
                    1,
                    177,
                    0
                ],
                "title": "Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing\n  LLMs Beyond Integer Bit-Levels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing\n  LLMs Beyond Integer Bit-Levels"
                },
                "summary": "We present a simple meta quantization approach that quantizes different\nlayers of a large language model (LLM) at different bit levels, and is\nindependent of the underlying quantization technique. Specifically, we quantize\nthe most important layers to higher bit precision and less important layers to\nlower bits. We propose two effective strategies to measure the importance of\nlayers within LLMs: the first measures the importance of a layer based on how\ndifferent its output embeddings are from the input embeddings (higher is\nbetter); the second estimates the importance of a layer using the number of\nlayer weights that are much larger than average (smaller is better). We show\nthat quantizing different layers at varying bits according to our importance\nscores results in minimal performance drop with a far more compressed model\nsize. Finally, we present several practical key takeaways from our variable\nlayer-wise quantization experiments: (a) LLM performance under variable\nquantization remains close to the original model until 25-50% of layers are\nmoved in lower quantization using our proposed ordering but only until 5-10% if\nmoved using no specific ordering; (b) Adding layer importance to inherently\ndynamic quantization techniques can further improve their performance, showing\nthat our approach is complementary to other dynamic quantization methods; (c)\nQuantizing LLMs to lower bits performs substantially better than pruning unless\nextreme quantization (2-bit) is used; and (d) Layer-wise quantization to lower\nbits works better in the case of larger LLMs with more layers compared to\nsmaller LLMs with fewer layers. Our code is publicly available at\nhttps://github.com/RazvanDu/LayerwiseQuant/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a simple meta quantization approach that quantizes different\nlayers of a large language model (LLM) at different bit levels, and is\nindependent of the underlying quantization technique. Specifically, we quantize\nthe most important layers to higher bit precision and less important layers to\nlower bits. We propose two effective strategies to measure the importance of\nlayers within LLMs: the first measures the importance of a layer based on how\ndifferent its output embeddings are from the input embeddings (higher is\nbetter); the second estimates the importance of a layer using the number of\nlayer weights that are much larger than average (smaller is better). We show\nthat quantizing different layers at varying bits according to our importance\nscores results in minimal performance drop with a far more compressed model\nsize. Finally, we present several practical key takeaways from our variable\nlayer-wise quantization experiments: (a) LLM performance under variable\nquantization remains close to the original model until 25-50% of layers are\nmoved in lower quantization using our proposed ordering but only until 5-10% if\nmoved using no specific ordering; (b) Adding layer importance to inherently\ndynamic quantization techniques can further improve their performance, showing\nthat our approach is complementary to other dynamic quantization methods; (c)\nQuantizing LLMs to lower bits performs substantially better than pruning unless\nextreme quantization (2-bit) is used; and (d) Layer-wise quantization to lower\nbits works better in the case of larger LLMs with more layers compared to\nsmaller LLMs with fewer layers. Our code is publicly available at\nhttps://github.com/RazvanDu/LayerwiseQuant/."
                },
                "authors": [
                    {
                        "name": "Razvan-Gabriel Dumitru"
                    },
                    {
                        "name": "Vikas Yadav"
                    },
                    {
                        "name": "Rishabh Maheshwary"
                    },
                    {
                        "name": "Paul-Ioan Clotan"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    }
                ],
                "author_detail": {
                    "name": "Mihai Surdeanu"
                },
                "author": "Mihai Surdeanu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17415v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17415v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06814v2",
                "updated": "2024-10-28T08:51:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    8,
                    51,
                    50,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-10T18:43:39Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    18,
                    43,
                    39,
                    1,
                    254,
                    0
                ],
                "title": "\"Come to us first\": Centering Community Organizations in Artificial\n  Intelligence for Social Good Partnerships",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Come to us first\": Centering Community Organizations in Artificial\n  Intelligence for Social Good Partnerships"
                },
                "summary": "Artificial Intelligence for Social Good (AI4SG) has emerged as a growing body\nof research and practice exploring the potential of AI technologies to tackle\nsocial issues. This area emphasizes interdisciplinary partnerships with\ncommunity organizations, such as non-profits and government agencies. However,\namidst excitement about new advances in AI and their potential impact, the\nneeds, expectations, and aspirations of these community organizations--and\nwhether they are being met--are not well understood. Understanding these\nfactors is important to ensure that the considerable efforts by AI teams and\ncommunity organizations can actually achieve the positive social impact they\nstrive for. Drawing on the Data Feminism framework, we explored the\nperspectives of community organization members on their partnerships with AI\nteams through 16 semi-structured interviews. Our study highlights the pervasive\ninfluence of funding agendas and the optimism surrounding AI's potential.\nDespite the significant intellectual contributions and labor provided by\ncommunity organization members, their goals were frequently sidelined in favor\nof other stakeholders, including AI teams. While many community organization\nmembers expected tangible project deployment, only two out of 14 projects we\nstudied reached the deployment stage. However, community organization members\nsustained their belief in the potential of the projects, still seeing\ndiminished goals as valuable. To enhance the efficacy of future collaborations,\nour participants shared their aspirations for success, calling for\nco-leadership starting from the early stages of projects. We propose data\nco-liberation as a grounding principle for approaching AI4SG moving forward,\npositing that community organizations' co-leadership is essential for fostering\nmore effective, sustainable, and ethical development of AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence for Social Good (AI4SG) has emerged as a growing body\nof research and practice exploring the potential of AI technologies to tackle\nsocial issues. This area emphasizes interdisciplinary partnerships with\ncommunity organizations, such as non-profits and government agencies. However,\namidst excitement about new advances in AI and their potential impact, the\nneeds, expectations, and aspirations of these community organizations--and\nwhether they are being met--are not well understood. Understanding these\nfactors is important to ensure that the considerable efforts by AI teams and\ncommunity organizations can actually achieve the positive social impact they\nstrive for. Drawing on the Data Feminism framework, we explored the\nperspectives of community organization members on their partnerships with AI\nteams through 16 semi-structured interviews. Our study highlights the pervasive\ninfluence of funding agendas and the optimism surrounding AI's potential.\nDespite the significant intellectual contributions and labor provided by\ncommunity organization members, their goals were frequently sidelined in favor\nof other stakeholders, including AI teams. While many community organization\nmembers expected tangible project deployment, only two out of 14 projects we\nstudied reached the deployment stage. However, community organization members\nsustained their belief in the potential of the projects, still seeing\ndiminished goals as valuable. To enhance the efficacy of future collaborations,\nour participants shared their aspirations for success, calling for\nco-leadership starting from the early stages of projects. We propose data\nco-liberation as a grounding principle for approaching AI4SG moving forward,\npositing that community organizations' co-leadership is essential for fostering\nmore effective, sustainable, and ethical development of AI."
                },
                "authors": [
                    {
                        "name": "Hongjin Lin"
                    },
                    {
                        "name": "Naveena Karusala"
                    },
                    {
                        "name": "Chinasa T. Okolo"
                    },
                    {
                        "name": "Catherine D'Ignazio"
                    },
                    {
                        "name": "Krzysztof Z. Gajos"
                    }
                ],
                "author_detail": {
                    "name": "Krzysztof Z. Gajos"
                },
                "author": "Krzysztof Z. Gajos",
                "arxiv_doi": "10.1145/3687009",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3687009",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.06814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to the Proc. ACM Hum. Comput. Interact. 8, CSCW2",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20838v1",
                "updated": "2024-10-28T08:44:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    8,
                    44,
                    56,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T08:44:56Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    8,
                    44,
                    56,
                    0,
                    302,
                    0
                ],
                "title": "A Simple Yet Effective Corpus Construction Framework for Indonesian\n  Grammatical Error Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple Yet Effective Corpus Construction Framework for Indonesian\n  Grammatical Error Correction"
                },
                "summary": "Currently, the majority of research in grammatical error correction (GEC) is\nconcentrated on universal languages, such as English and Chinese. Many\nlow-resource languages lack accessible evaluation corpora. How to efficiently\nconstruct high-quality evaluation corpora for GEC in low-resource languages has\nbecome a significant challenge. To fill these gaps, in this paper, we present a\nframework for constructing GEC corpora. Specifically, we focus on Indonesian as\nour research language and construct an evaluation corpus for Indonesian GEC\nusing the proposed framework, addressing the limitations of existing evaluation\ncorpora in Indonesian. Furthermore, we investigate the feasibility of utilizing\nexisting large language models (LLMs), such as GPT-3.5-Turbo and GPT-4, to\nstreamline corpus annotation efforts in GEC tasks. The results demonstrate\nsignificant potential for enhancing the performance of LLMs in low-resource\nlanguage settings. Our code and corpus can be obtained from\nhttps://github.com/GKLMIP/GEC-Construction-Framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, the majority of research in grammatical error correction (GEC) is\nconcentrated on universal languages, such as English and Chinese. Many\nlow-resource languages lack accessible evaluation corpora. How to efficiently\nconstruct high-quality evaluation corpora for GEC in low-resource languages has\nbecome a significant challenge. To fill these gaps, in this paper, we present a\nframework for constructing GEC corpora. Specifically, we focus on Indonesian as\nour research language and construct an evaluation corpus for Indonesian GEC\nusing the proposed framework, addressing the limitations of existing evaluation\ncorpora in Indonesian. Furthermore, we investigate the feasibility of utilizing\nexisting large language models (LLMs), such as GPT-3.5-Turbo and GPT-4, to\nstreamline corpus annotation efforts in GEC tasks. The results demonstrate\nsignificant potential for enhancing the performance of LLMs in low-resource\nlanguage settings. Our code and corpus can be obtained from\nhttps://github.com/GKLMIP/GEC-Construction-Framework."
                },
                "authors": [
                    {
                        "name": "Nankai Lin"
                    },
                    {
                        "name": "Meiyu Zeng"
                    },
                    {
                        "name": "Wentao Huang"
                    },
                    {
                        "name": "Shengyi Jiang"
                    },
                    {
                        "name": "Lixian Xiao"
                    },
                    {
                        "name": "Aimin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Aimin Yang"
                },
                "author": "Aimin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19419v2",
                "updated": "2024-10-28T08:39:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    8,
                    39,
                    18,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-25T09:23:24Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    23,
                    24,
                    4,
                    299,
                    0
                ],
                "title": "KAHANI: Culturally-Nuanced Visual Storytelling Pipeline for Non-Western\n  Cultures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAHANI: Culturally-Nuanced Visual Storytelling Pipeline for Non-Western\n  Cultures"
                },
                "summary": "Large Language Models (LLMs) and Text-To-Image (T2I) models have demonstrated\nthe ability to generate compelling text and visual stories. However, their\noutputs are predominantly aligned with the sensibilities of the Global North,\noften resulting in an outsider's gaze on other cultures. As a result,\nnon-Western communities have to put extra effort into generating culturally\nspecific stories. To address this challenge, we developed a visual storytelling\npipeline called KAHANI that generates culturally grounded visual stories for\nnon-Western cultures. Our pipeline leverages off-the-shelf models GPT-4 Turbo\nand Stable Diffusion XL (SDXL). By using Chain of Thought (CoT) and T2I\nprompting techniques, we capture the cultural context from user's prompt and\ngenerate vivid descriptions of the characters and scene compositions. To\nevaluate the effectiveness of KAHANI, we conducted a comparative user study\nwith ChatGPT-4 (with DALL-E3) in which participants from different regions of\nIndia compared the cultural relevance of stories generated by the two tools.\nResults from the qualitative and quantitative analysis performed on the user\nstudy showed that KAHANI was able to capture and incorporate more Culturally\nSpecific Items (CSIs) compared to ChatGPT-4. In terms of both its cultural\ncompetence and visual story generation quality, our pipeline outperformed\nChatGPT-4 in 27 out of the 36 comparisons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Text-To-Image (T2I) models have demonstrated\nthe ability to generate compelling text and visual stories. However, their\noutputs are predominantly aligned with the sensibilities of the Global North,\noften resulting in an outsider's gaze on other cultures. As a result,\nnon-Western communities have to put extra effort into generating culturally\nspecific stories. To address this challenge, we developed a visual storytelling\npipeline called KAHANI that generates culturally grounded visual stories for\nnon-Western cultures. Our pipeline leverages off-the-shelf models GPT-4 Turbo\nand Stable Diffusion XL (SDXL). By using Chain of Thought (CoT) and T2I\nprompting techniques, we capture the cultural context from user's prompt and\ngenerate vivid descriptions of the characters and scene compositions. To\nevaluate the effectiveness of KAHANI, we conducted a comparative user study\nwith ChatGPT-4 (with DALL-E3) in which participants from different regions of\nIndia compared the cultural relevance of stories generated by the two tools.\nResults from the qualitative and quantitative analysis performed on the user\nstudy showed that KAHANI was able to capture and incorporate more Culturally\nSpecific Items (CSIs) compared to ChatGPT-4. In terms of both its cultural\ncompetence and visual story generation quality, our pipeline outperformed\nChatGPT-4 in 27 out of the 36 comparisons."
                },
                "authors": [
                    {
                        "name": "Hamna"
                    },
                    {
                        "name": "Deepthi Sudharsan"
                    },
                    {
                        "name": "Agrima Seth"
                    },
                    {
                        "name": "Ritvik Budhiraja"
                    },
                    {
                        "name": "Deepika Khullar"
                    },
                    {
                        "name": "Vyshak Jain"
                    },
                    {
                        "name": "Kalika Bali"
                    },
                    {
                        "name": "Aditya Vashistha"
                    },
                    {
                        "name": "Sameer Segal"
                    }
                ],
                "author_detail": {
                    "name": "Sameer Segal"
                },
                "author": "Sameer Segal",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17922v2",
                "updated": "2024-10-28T08:37:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    8,
                    37,
                    37,
                    0,
                    302,
                    0
                ],
                "published": "2024-05-28T07:45:22Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    7,
                    45,
                    22,
                    1,
                    149,
                    0
                ],
                "title": "Stochastic Optimization Schemes for Performative Prediction with\n  Nonconvex Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Optimization Schemes for Performative Prediction with\n  Nonconvex Loss"
                },
                "summary": "This paper studies a risk minimization problem with decision dependent data\ndistribution. The problem pertains to the performative prediction setting in\nwhich a trained model can affect the outcome estimated by the model. Such\ndependency creates a feedback loop that influences the stability of\noptimization algorithms such as stochastic gradient descent (SGD). We present\nthe first study on performative prediction with smooth but possibly non-convex\nloss. We analyze a greedy deployment scheme with SGD (SGD-GD). Note that in the\nliterature, SGD-GD is often studied with strongly convex loss. We first propose\nthe definition of stationary performative stable (SPS) solutions through\nrelaxing the popular performative stable condition. We then prove that SGD-GD\nconverges to a biased SPS solution in expectation. We consider two conditions\nof sensitivity on the distribution shifts: (i) the sensitivity is characterized\nby Wasserstein-1 distance and the loss is Lipschitz w.r.t. data samples, or\n(ii) the sensitivity is characterized by total variation (TV) divergence and\nthe loss is bounded. In both conditions, the bias levels are proportional to\nthe stochastic gradient's variance and sensitivity level. Our analysis is\nextended to a lazy deployment scheme where models are deployed once per several\nSGD updates, and we show that it converges to a bias-free SPS solution.\nNumerical experiments corroborate our theories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a risk minimization problem with decision dependent data\ndistribution. The problem pertains to the performative prediction setting in\nwhich a trained model can affect the outcome estimated by the model. Such\ndependency creates a feedback loop that influences the stability of\noptimization algorithms such as stochastic gradient descent (SGD). We present\nthe first study on performative prediction with smooth but possibly non-convex\nloss. We analyze a greedy deployment scheme with SGD (SGD-GD). Note that in the\nliterature, SGD-GD is often studied with strongly convex loss. We first propose\nthe definition of stationary performative stable (SPS) solutions through\nrelaxing the popular performative stable condition. We then prove that SGD-GD\nconverges to a biased SPS solution in expectation. We consider two conditions\nof sensitivity on the distribution shifts: (i) the sensitivity is characterized\nby Wasserstein-1 distance and the loss is Lipschitz w.r.t. data samples, or\n(ii) the sensitivity is characterized by total variation (TV) divergence and\nthe loss is bounded. In both conditions, the bias levels are proportional to\nthe stochastic gradient's variance and sensitivity level. Our analysis is\nextended to a lazy deployment scheme where models are deployed once per several\nSGD updates, and we show that it converges to a bias-free SPS solution.\nNumerical experiments corroborate our theories."
                },
                "authors": [
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Hoi-To Wai"
                    }
                ],
                "author_detail": {
                    "name": "Hoi-To Wai"
                },
                "author": "Hoi-To Wai",
                "arxiv_comment": "19 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20833v1",
                "updated": "2024-10-28T08:32:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    8,
                    32,
                    9,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T08:32:09Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    8,
                    32,
                    9,
                    0,
                    302,
                    0
                ],
                "title": "LLMs are Biased Evaluators But Not Biased for Retrieval Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Biased Evaluators But Not Biased for Retrieval Augmented\n  Generation"
                },
                "summary": "Recent studies have demonstrated that large language models (LLMs) exhibit\nsignificant biases in evaluation tasks, particularly in preferentially rating\nand favoring self-generated content. However, the extent to which this bias\nmanifests in fact-oriented tasks, especially within retrieval-augmented\ngeneration (RAG) frameworks-where keyword extraction and factual accuracy take\nprecedence over stylistic elements-remains unclear. Our study addresses this\nknowledge gap by simulating two critical phases of the RAG framework. In the\nfirst phase, we access the suitability of human-authored versus model-generated\npassages, emulating the pointwise reranking process. The second phase involves\nconducting pairwise reading comprehension tests to simulate the generation\nprocess. Contrary to previous findings indicating a self-preference in rating\ntasks, our results reveal no significant self-preference effect in RAG\nframeworks. Instead, we observe that factual accuracy significantly influences\nLLMs' output, even in the absence of prior knowledge. Our research contributes\nto the ongoing discourse on LLM biases and their implications for RAG-based\nsystem, offering insights that may inform the development of more robust and\nunbiased LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have demonstrated that large language models (LLMs) exhibit\nsignificant biases in evaluation tasks, particularly in preferentially rating\nand favoring self-generated content. However, the extent to which this bias\nmanifests in fact-oriented tasks, especially within retrieval-augmented\ngeneration (RAG) frameworks-where keyword extraction and factual accuracy take\nprecedence over stylistic elements-remains unclear. Our study addresses this\nknowledge gap by simulating two critical phases of the RAG framework. In the\nfirst phase, we access the suitability of human-authored versus model-generated\npassages, emulating the pointwise reranking process. The second phase involves\nconducting pairwise reading comprehension tests to simulate the generation\nprocess. Contrary to previous findings indicating a self-preference in rating\ntasks, our results reveal no significant self-preference effect in RAG\nframeworks. Instead, we observe that factual accuracy significantly influences\nLLMs' output, even in the absence of prior knowledge. Our research contributes\nto the ongoing discourse on LLM biases and their implications for RAG-based\nsystem, offering insights that may inform the development of more robust and\nunbiased LLM systems."
                },
                "authors": [
                    {
                        "name": "Yen-Shan Chen"
                    },
                    {
                        "name": "Jing Jin"
                    },
                    {
                        "name": "Peng-Ting Kuo"
                    },
                    {
                        "name": "Chao-Wei Huang"
                    },
                    {
                        "name": "Yun-Nung Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yun-Nung Chen"
                },
                "author": "Yun-Nung Chen",
                "arxiv_comment": "15 pages, 14 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15668v2",
                "updated": "2024-10-28T08:29:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    8,
                    29,
                    38,
                    0,
                    302,
                    0
                ],
                "published": "2024-01-28T14:22:11Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    14,
                    22,
                    11,
                    6,
                    28,
                    0
                ],
                "title": "Lips Are Lying: Spotting the Temporal Inconsistency between Audio and\n  Visual in Lip-Syncing DeepFakes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lips Are Lying: Spotting the Temporal Inconsistency between Audio and\n  Visual in Lip-Syncing DeepFakes"
                },
                "summary": "In recent years, DeepFake technology has achieved unprecedented success in\nhigh-quality video synthesis, but these methods also pose potential and severe\nsecurity threats to humanity. DeepFake can be bifurcated into entertainment\napplications like face swapping and illicit uses such as lip-syncing fraud.\nHowever, lip-forgery videos, which neither change identity nor have discernible\nvisual artifacts, present a formidable challenge to existing DeepFake detection\nmethods. Our preliminary experiments have shown that the effectiveness of the\nexisting methods often drastically decrease or even fail when tackling\nlip-syncing videos. In this paper, for the first time, we propose a novel\napproach dedicated to lip-forgery identification that exploits the\ninconsistency between lip movements and audio signals. We also mimic human\nnatural cognition by capturing subtle biological links between lips and head\nregions to boost accuracy. To better illustrate the effectiveness and advances\nof our proposed method, we create a high-quality LipSync dataset, AVLips, by\nemploying the state-of-the-art lip generators. We hope this high-quality and\ndiverse dataset could be well served the further research on this challenging\nand interesting field. Experimental results show that our approach gives an\naverage accuracy of more than 95.3% in spotting lip-syncing videos,\nsignificantly outperforming the baselines. Extensive experiments demonstrate\nthe capability to tackle deepfakes and the robustness in surviving diverse\ninput transformations. Our method achieves an accuracy of up to 90.2% in\nreal-world scenarios (e.g., WeChat video call) and shows its powerful\ncapabilities in real scenario deployment. To facilitate the progress of this\nresearch community, we release all resources at\nhttps://github.com/AaronComo/LipFD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, DeepFake technology has achieved unprecedented success in\nhigh-quality video synthesis, but these methods also pose potential and severe\nsecurity threats to humanity. DeepFake can be bifurcated into entertainment\napplications like face swapping and illicit uses such as lip-syncing fraud.\nHowever, lip-forgery videos, which neither change identity nor have discernible\nvisual artifacts, present a formidable challenge to existing DeepFake detection\nmethods. Our preliminary experiments have shown that the effectiveness of the\nexisting methods often drastically decrease or even fail when tackling\nlip-syncing videos. In this paper, for the first time, we propose a novel\napproach dedicated to lip-forgery identification that exploits the\ninconsistency between lip movements and audio signals. We also mimic human\nnatural cognition by capturing subtle biological links between lips and head\nregions to boost accuracy. To better illustrate the effectiveness and advances\nof our proposed method, we create a high-quality LipSync dataset, AVLips, by\nemploying the state-of-the-art lip generators. We hope this high-quality and\ndiverse dataset could be well served the further research on this challenging\nand interesting field. Experimental results show that our approach gives an\naverage accuracy of more than 95.3% in spotting lip-syncing videos,\nsignificantly outperforming the baselines. Extensive experiments demonstrate\nthe capability to tackle deepfakes and the robustness in surviving diverse\ninput transformations. Our method achieves an accuracy of up to 90.2% in\nreal-world scenarios (e.g., WeChat video call) and shows its powerful\ncapabilities in real scenario deployment. To facilitate the progress of this\nresearch community, we release all resources at\nhttps://github.com/AaronComo/LipFD."
                },
                "authors": [
                    {
                        "name": "Weifeng Liu"
                    },
                    {
                        "name": "Tianyi She"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Boheng Li"
                    },
                    {
                        "name": "Dongyu Yao"
                    },
                    {
                        "name": "Ziyou Liang"
                    },
                    {
                        "name": "Run Wang"
                    }
                ],
                "author_detail": {
                    "name": "Run Wang"
                },
                "author": "Run Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02366v2",
                "updated": "2024-10-28T08:27:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    8,
                    27,
                    21,
                    0,
                    302,
                    0
                ],
                "published": "2024-06-04T14:45:47Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    14,
                    45,
                    47,
                    1,
                    156,
                    0
                ],
                "title": "Finding NeMo: Localizing Neurons Responsible For Memorization in\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding NeMo: Localizing Neurons Responsible For Memorization in\n  Diffusion Models"
                },
                "summary": "Diffusion models (DMs) produce very detailed and high-quality images. Their\npower results from extensive training on large amounts of data, usually scraped\nfrom the internet without proper attribution or consent from content creators.\nUnfortunately, this practice raises privacy and intellectual property concerns,\nas DMs can memorize and later reproduce their potentially sensitive or\ncopyrighted training images at inference time. Prior efforts prevent this issue\nby either changing the input to the diffusion process, thereby preventing the\nDM from generating memorized samples during inference, or removing the\nmemorized data from training altogether. While those are viable solutions when\nthe DM is developed and deployed in a secure and constantly monitored\nenvironment, they hold the risk of adversaries circumventing the safeguards and\nare not effective when the DM itself is publicly released. To solve the\nproblem, we introduce NeMo, the first method to localize memorization of\nindividual data samples down to the level of neurons in DMs' cross-attention\nlayers. Through our experiments, we make the intriguing finding that in many\ncases, single neurons are responsible for memorizing particular training\nsamples. By deactivating these memorization neurons, we can avoid the\nreplication of training data at inference time, increase the diversity in the\ngenerated outputs, and mitigate the leakage of private and copyrighted data. In\nthis way, our NeMo contributes to a more responsible deployment of DMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) produce very detailed and high-quality images. Their\npower results from extensive training on large amounts of data, usually scraped\nfrom the internet without proper attribution or consent from content creators.\nUnfortunately, this practice raises privacy and intellectual property concerns,\nas DMs can memorize and later reproduce their potentially sensitive or\ncopyrighted training images at inference time. Prior efforts prevent this issue\nby either changing the input to the diffusion process, thereby preventing the\nDM from generating memorized samples during inference, or removing the\nmemorized data from training altogether. While those are viable solutions when\nthe DM is developed and deployed in a secure and constantly monitored\nenvironment, they hold the risk of adversaries circumventing the safeguards and\nare not effective when the DM itself is publicly released. To solve the\nproblem, we introduce NeMo, the first method to localize memorization of\nindividual data samples down to the level of neurons in DMs' cross-attention\nlayers. Through our experiments, we make the intriguing finding that in many\ncases, single neurons are responsible for memorizing particular training\nsamples. By deactivating these memorization neurons, we can avoid the\nreplication of training data at inference time, increase the diversity in the\ngenerated outputs, and mitigate the leakage of private and copyrighted data. In\nthis way, our NeMo contributes to a more responsible deployment of DMs."
                },
                "authors": [
                    {
                        "name": "Dominik Hintersdorf"
                    },
                    {
                        "name": "Lukas Struppek"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Adam Dziedzic"
                    },
                    {
                        "name": "Franziska Boenisch"
                    }
                ],
                "author_detail": {
                    "name": "Franziska Boenisch"
                },
                "author": "Franziska Boenisch",
                "arxiv_comment": "Published as a conference paper at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20814v1",
                "updated": "2024-10-28T08:02:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    8,
                    2,
                    23,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T08:02:23Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    8,
                    2,
                    23,
                    0,
                    302,
                    0
                ],
                "title": "NewTerm: Benchmarking Real-Time New Terms for Large Language Models with\n  Annual Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NewTerm: Benchmarking Real-Time New Terms for Large Language Models with\n  Annual Updates"
                },
                "summary": "Despite their remarkable abilities in various tasks, large language models\n(LLMs) still struggle with real-time information (e.g., new facts and terms)\ndue to the knowledge cutoff in their development process. However, existing\nbenchmarks focus on outdated content and limited fields, facing difficulties in\nreal-time updating and leaving new terms unexplored. To address this problem,\nwe propose an adaptive benchmark, NewTerm, for real-time evaluation of new\nterms. We design a highly automated construction method to ensure high-quality\nbenchmark construction with minimal human effort, allowing flexible updates for\nreal-time information. Empirical results on various LLMs demonstrate over 20%\nperformance reduction caused by new terms. Additionally, while updates to the\nknowledge cutoff of LLMs can cover some of the new terms, they are unable to\ngeneralize to more distant new terms. We also analyze which types of terms are\nmore challenging and why LLMs struggle with new terms, paving the way for\nfuture research. Finally, we construct NewTerm 2022 and 2023 to evaluate the\nnew terms updated each year and will continue updating annually. The benchmark\nand codes can be found at https://github.com/hexuandeng/NewTerm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable abilities in various tasks, large language models\n(LLMs) still struggle with real-time information (e.g., new facts and terms)\ndue to the knowledge cutoff in their development process. However, existing\nbenchmarks focus on outdated content and limited fields, facing difficulties in\nreal-time updating and leaving new terms unexplored. To address this problem,\nwe propose an adaptive benchmark, NewTerm, for real-time evaluation of new\nterms. We design a highly automated construction method to ensure high-quality\nbenchmark construction with minimal human effort, allowing flexible updates for\nreal-time information. Empirical results on various LLMs demonstrate over 20%\nperformance reduction caused by new terms. Additionally, while updates to the\nknowledge cutoff of LLMs can cover some of the new terms, they are unable to\ngeneralize to more distant new terms. We also analyze which types of terms are\nmore challenging and why LLMs struggle with new terms, paving the way for\nfuture research. Finally, we construct NewTerm 2022 and 2023 to evaluate the\nnew terms updated each year and will continue updating annually. The benchmark\nand codes can be found at https://github.com/hexuandeng/NewTerm."
                },
                "authors": [
                    {
                        "name": "Hexuan Deng"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaopeng Tu"
                },
                "author": "Zhaopeng Tu",
                "arxiv_comment": "Accepted to NeurIPS 2024 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20811v1",
                "updated": "2024-10-28T07:59:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    59,
                    34,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T07:59:34Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    59,
                    34,
                    0,
                    302,
                    0
                ],
                "title": "Bridging the Gap between Expert and Language Models: Concept-guided\n  Chess Commentary Generation and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Gap between Expert and Language Models: Concept-guided\n  Chess Commentary Generation and Evaluation"
                },
                "summary": "Deep learning-based expert models have reached superhuman performance in\ndecision-making domains such as chess and Go. However, it is under-explored to\nexplain or comment on given decisions although it is important for human\neducation and model explainability. The outputs of expert models are accurate,\nbut yet difficult to interpret for humans. On the other hand, large language\nmodels (LLMs) produce fluent commentary but are prone to hallucinations due to\ntheir limited decision-making capabilities. To bridge this gap between expert\nmodels and LLMs, we focus on chess commentary as a representative case of\nexplaining complex decision-making processes through language and address both\nthe generation and evaluation of commentary. We introduce Concept-guided Chess\nCommentary generation (CCC) for producing commentary and GPT-based Chess\nCommentary Evaluation (GCC-Eval) for assessing it. CCC integrates the\ndecision-making strengths of expert models with the linguistic fluency of LLMs\nthrough prioritized, concept-based explanations. GCC-Eval leverages expert\nknowledge to evaluate chess commentary based on informativeness and linguistic\nquality. Experimental results, validated by both human judges and GCC-Eval,\ndemonstrate that CCC generates commentary that is accurate, informative, and\nfluent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning-based expert models have reached superhuman performance in\ndecision-making domains such as chess and Go. However, it is under-explored to\nexplain or comment on given decisions although it is important for human\neducation and model explainability. The outputs of expert models are accurate,\nbut yet difficult to interpret for humans. On the other hand, large language\nmodels (LLMs) produce fluent commentary but are prone to hallucinations due to\ntheir limited decision-making capabilities. To bridge this gap between expert\nmodels and LLMs, we focus on chess commentary as a representative case of\nexplaining complex decision-making processes through language and address both\nthe generation and evaluation of commentary. We introduce Concept-guided Chess\nCommentary generation (CCC) for producing commentary and GPT-based Chess\nCommentary Evaluation (GCC-Eval) for assessing it. CCC integrates the\ndecision-making strengths of expert models with the linguistic fluency of LLMs\nthrough prioritized, concept-based explanations. GCC-Eval leverages expert\nknowledge to evaluate chess commentary based on informativeness and linguistic\nquality. Experimental results, validated by both human judges and GCC-Eval,\ndemonstrate that CCC generates commentary that is accurate, informative, and\nfluent."
                },
                "authors": [
                    {
                        "name": "Jaechang Kim"
                    },
                    {
                        "name": "Jinmin Goh"
                    },
                    {
                        "name": "Inseok Hwang"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Jungseul Ok"
                    }
                ],
                "author_detail": {
                    "name": "Jungseul Ok"
                },
                "author": "Jungseul Ok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06941v2",
                "updated": "2024-10-28T07:51:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    51,
                    29,
                    0,
                    302,
                    0
                ],
                "published": "2024-08-13T14:59:44Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    59,
                    44,
                    1,
                    226,
                    0
                ],
                "title": "OpenResearcher: Unleashing AI for Accelerated Scientific Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenResearcher: Unleashing AI for Accelerated Scientific Research"
                },
                "summary": "The rapid growth of scientific literature imposes significant challenges for\nresearchers endeavoring to stay updated with the latest advancements in their\nfields and delve into new areas. We introduce OpenResearcher, an innovative\nplatform that leverages Artificial Intelligence (AI) techniques to accelerate\nthe research process by answering diverse questions from researchers.\nOpenResearcher is built based on Retrieval-Augmented Generation (RAG) to\nintegrate Large Language Models (LLMs) with up-to-date, domain-specific\nknowledge. Moreover, we develop various tools for OpenResearcher to understand\nresearchers' queries, search from the scientific literature, filter retrieved\ninformation, provide accurate and comprehensive answers, and self-refine these\nanswers. OpenResearcher can flexibly use these tools to balance efficiency and\neffectiveness. As a result, OpenResearcher enables researchers to save time and\nincrease their potential to discover new insights and drive scientific\nbreakthroughs. Demo, video, and code are available at:\nhttps://github.com/GAIR-NLP/OpenResearcher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of scientific literature imposes significant challenges for\nresearchers endeavoring to stay updated with the latest advancements in their\nfields and delve into new areas. We introduce OpenResearcher, an innovative\nplatform that leverages Artificial Intelligence (AI) techniques to accelerate\nthe research process by answering diverse questions from researchers.\nOpenResearcher is built based on Retrieval-Augmented Generation (RAG) to\nintegrate Large Language Models (LLMs) with up-to-date, domain-specific\nknowledge. Moreover, we develop various tools for OpenResearcher to understand\nresearchers' queries, search from the scientific literature, filter retrieved\ninformation, provide accurate and comprehensive answers, and self-refine these\nanswers. OpenResearcher can flexibly use these tools to balance efficiency and\neffectiveness. As a result, OpenResearcher enables researchers to save time and\nincrease their potential to discover new insights and drive scientific\nbreakthroughs. Demo, video, and code are available at:\nhttps://github.com/GAIR-NLP/OpenResearcher."
                },
                "authors": [
                    {
                        "name": "Yuxiang Zheng"
                    },
                    {
                        "name": "Shichao Sun"
                    },
                    {
                        "name": "Lin Qiu"
                    },
                    {
                        "name": "Dongyu Ru"
                    },
                    {
                        "name": "Cheng Jiayang"
                    },
                    {
                        "name": "Xuefeng Li"
                    },
                    {
                        "name": "Jifan Lin"
                    },
                    {
                        "name": "Binjie Wang"
                    },
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Renjie Pan"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingkai Min"
                    },
                    {
                        "name": "Zizhao Zhang"
                    },
                    {
                        "name": "Yiwen Wang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "arxiv_comment": "Accepted to Demo track of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14118v2",
                "updated": "2024-10-28T07:45:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    45,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-03-21T04:07:40Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    4,
                    7,
                    40,
                    3,
                    81,
                    0
                ],
                "title": "From Handcrafted Features to LLMs: A Brief Survey for Machine\n  Translation Quality Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Handcrafted Features to LLMs: A Brief Survey for Machine\n  Translation Quality Estimation"
                },
                "summary": "Machine Translation Quality Estimation (MTQE) is the task of estimating the\nquality of machine-translated text in real time without the need for reference\ntranslations, which is of great importance for the development of MT. After two\ndecades of evolution, QE has yielded a wealth of results. This article provides\na comprehensive overview of QE datasets, annotation methods, shared tasks,\nmethodologies, challenges, and future research directions. It begins with an\nintroduction to the background and significance of QE, followed by an\nexplanation of the concepts and evaluation metrics for word-level QE,\nsentence-level QE, document-level QE, and explainable QE. The paper categorizes\nthe methods developed throughout the history of QE into those based on\nhandcrafted features, deep learning, and Large Language Models (LLMs), with a\nfurther division of deep learning-based methods into classic deep learning and\nthose incorporating pre-trained language models (LMs). Additionally, the\narticle details the advantages and limitations of each method and offers a\nstraightforward comparison of different approaches. Finally, the paper\ndiscusses the current challenges in QE research and provides an outlook on\nfuture research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Translation Quality Estimation (MTQE) is the task of estimating the\nquality of machine-translated text in real time without the need for reference\ntranslations, which is of great importance for the development of MT. After two\ndecades of evolution, QE has yielded a wealth of results. This article provides\na comprehensive overview of QE datasets, annotation methods, shared tasks,\nmethodologies, challenges, and future research directions. It begins with an\nintroduction to the background and significance of QE, followed by an\nexplanation of the concepts and evaluation metrics for word-level QE,\nsentence-level QE, document-level QE, and explainable QE. The paper categorizes\nthe methods developed throughout the history of QE into those based on\nhandcrafted features, deep learning, and Large Language Models (LLMs), with a\nfurther division of deep learning-based methods into classic deep learning and\nthose incorporating pre-trained language models (LMs). Additionally, the\narticle details the advantages and limitations of each method and offers a\nstraightforward comparison of different approaches. Finally, the paper\ndiscusses the current challenges in QE research and provides an outlook on\nfuture research directions."
                },
                "authors": [
                    {
                        "name": "Haofei Zhao"
                    },
                    {
                        "name": "Yilun Liu"
                    },
                    {
                        "name": "Shimin Tao"
                    },
                    {
                        "name": "Weibin Meng"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xiang Geng"
                    },
                    {
                        "name": "Chang Su"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Hao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Yang"
                },
                "author": "Hao Yang",
                "arxiv_doi": "10.1109/IJCNN60899.2024.10650457",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IJCNN60899.2024.10650457",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.14118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IJCNN 2024",
                "arxiv_journal_ref": "2024 International Joint Conference on Neural Networks (IJCNN)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20796v1",
                "updated": "2024-10-28T07:30:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    30,
                    5,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T07:30:05Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    30,
                    5,
                    0,
                    302,
                    0
                ],
                "title": "Rephrasing natural text data with different languages and quality levels\n  for Large Language Model pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rephrasing natural text data with different languages and quality levels\n  for Large Language Model pre-training"
                },
                "summary": "Recently published work on rephrasing natural text data for pre-training LLMs\nhas shown promising results when combining the original dataset with the\nsynthetically rephrased data. We build upon previous work by replicating\nexisting results on C4 and extending them with our optimized rephrasing\npipeline to the English, German, Italian, and Spanish Oscar subsets of\nCulturaX. Our pipeline leads to increased performance on standard evaluation\nbenchmarks in both the mono- and multilingual setup. In addition, we provide a\ndetailed study of our pipeline, investigating the choice of the base dataset\nand LLM for the rephrasing, as well as the relationship between the model size\nand the performance after pre-training. By exploring data with different\nperceived quality levels, we show that gains decrease with higher quality.\nFurthermore, we find the difference in performance between model families to be\nbigger than between different model sizes. This highlights the necessity for\ndetailed tests before choosing an LLM to rephrase large amounts of data.\nMoreover, we investigate the effect of pre-training with synthetic data on\nsupervised fine-tuning. Here, we find increasing but inconclusive results that\nhighly depend on the used benchmark. These results (again) highlight the need\nfor better benchmarking setups. In summary, we show that rephrasing\nmultilingual and low-quality data is a very promising direction to extend LLM\npre-training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently published work on rephrasing natural text data for pre-training LLMs\nhas shown promising results when combining the original dataset with the\nsynthetically rephrased data. We build upon previous work by replicating\nexisting results on C4 and extending them with our optimized rephrasing\npipeline to the English, German, Italian, and Spanish Oscar subsets of\nCulturaX. Our pipeline leads to increased performance on standard evaluation\nbenchmarks in both the mono- and multilingual setup. In addition, we provide a\ndetailed study of our pipeline, investigating the choice of the base dataset\nand LLM for the rephrasing, as well as the relationship between the model size\nand the performance after pre-training. By exploring data with different\nperceived quality levels, we show that gains decrease with higher quality.\nFurthermore, we find the difference in performance between model families to be\nbigger than between different model sizes. This highlights the necessity for\ndetailed tests before choosing an LLM to rephrase large amounts of data.\nMoreover, we investigate the effect of pre-training with synthetic data on\nsupervised fine-tuning. Here, we find increasing but inconclusive results that\nhighly depend on the used benchmark. These results (again) highlight the need\nfor better benchmarking setups. In summary, we show that rephrasing\nmultilingual and low-quality data is a very promising direction to extend LLM\npre-training data."
                },
                "authors": [
                    {
                        "name": "Michael Pieler"
                    },
                    {
                        "name": "Marco Bellagente"
                    },
                    {
                        "name": "Hannah Teufel"
                    },
                    {
                        "name": "Duy Phung"
                    },
                    {
                        "name": "Nathan Cooper"
                    },
                    {
                        "name": "Jonathan Tow"
                    },
                    {
                        "name": "Paulo Rocha"
                    },
                    {
                        "name": "Reshinth Adithyan"
                    },
                    {
                        "name": "Zaid Alyafeai"
                    },
                    {
                        "name": "Nikhil Pinnaparaju"
                    },
                    {
                        "name": "Maksym Zhuravinskyi"
                    },
                    {
                        "name": "Carlos Riquelme"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Riquelme"
                },
                "author": "Carlos Riquelme",
                "arxiv_comment": "21 pages, 4 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20791v1",
                "updated": "2024-10-28T07:16:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    16,
                    0,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T07:16:00Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    16,
                    0,
                    0,
                    302,
                    0
                ],
                "title": "From Cool Demos to Production-Ready FMware: Core Challenges and a\n  Technology Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cool Demos to Production-Ready FMware: Core Challenges and a\n  Technology Roadmap"
                },
                "summary": "The rapid expansion of foundation models (FMs), such as large language models\n(LLMs), has given rise to FMware--software systems that integrate FMs as core\ncomponents. While building demonstration-level FMware is relatively\nstraightforward, transitioning to production-ready systems presents numerous\nchallenges, including reliability, high implementation costs, scalability, and\ncompliance with privacy regulations. This paper provides a thematic analysis of\nthe key obstacles in productionizing FMware, synthesized from industry\nexperience and diverse data sources, including hands-on involvement in the Open\nPlatform for Enterprise AI (OPEA) and FMware lifecycle engineering. We identify\ncritical issues in FM selection, data and model alignment, prompt engineering,\nagent orchestration, system testing, and deployment, alongside cross-cutting\nconcerns such as memory management, observability, and feedback integration. We\ndiscuss needed technologies and strategies to address these challenges and\noffer guidance on how to enable the transition from demonstration systems to\nscalable, production-ready FMware solutions. Our findings underscore the\nimportance of continued research and multi-industry collaboration to advance\nthe development of production-ready FMware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of foundation models (FMs), such as large language models\n(LLMs), has given rise to FMware--software systems that integrate FMs as core\ncomponents. While building demonstration-level FMware is relatively\nstraightforward, transitioning to production-ready systems presents numerous\nchallenges, including reliability, high implementation costs, scalability, and\ncompliance with privacy regulations. This paper provides a thematic analysis of\nthe key obstacles in productionizing FMware, synthesized from industry\nexperience and diverse data sources, including hands-on involvement in the Open\nPlatform for Enterprise AI (OPEA) and FMware lifecycle engineering. We identify\ncritical issues in FM selection, data and model alignment, prompt engineering,\nagent orchestration, system testing, and deployment, alongside cross-cutting\nconcerns such as memory management, observability, and feedback integration. We\ndiscuss needed technologies and strategies to address these challenges and\noffer guidance on how to enable the transition from demonstration systems to\nscalable, production-ready FMware solutions. Our findings underscore the\nimportance of continued research and multi-industry collaboration to advance\nthe development of production-ready FMware."
                },
                "authors": [
                    {
                        "name": "Gopi Krishnan Rajbahadur"
                    },
                    {
                        "name": "Gustavo A. Oliva"
                    },
                    {
                        "name": "Dayi Lin"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20783v1",
                "updated": "2024-10-28T06:47:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    6,
                    47,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T06:47:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    6,
                    47,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "Graph-based Uncertainty Metrics for Long-form Language Model Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Uncertainty Metrics for Long-form Language Model Outputs"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved text generation capabilities, but these systems are still known to\nhallucinate, and granular uncertainty estimation for long-form LLM generations\nremains challenging. In this work, we propose Graph Uncertainty -- which\nrepresents the relationship between LLM generations and claims within them as a\nbipartite graph and estimates the claim-level uncertainty with a family of\ngraph centrality metrics. Under this view, existing uncertainty estimation\nmethods based on the concept of self-consistency can be viewed as using degree\ncentrality as an uncertainty measure, and we show that more sophisticated\nalternatives such as closeness centrality provide consistent gains at\nclaim-level uncertainty estimation. Moreover, we present uncertainty-aware\ndecoding techniques that leverage both the graph structure and uncertainty\nestimates to improve the factuality of LLM generations by preserving only the\nmost reliable claims. Compared to existing methods, our graph-based uncertainty\nmetrics lead to an average of 6.8% relative gains on AUPRC across various\nlong-form generation settings, and our end-to-end system provides consistent\n2-4% gains in factuality over existing decoding techniques while significantly\nimproving the informativeness of generated responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved text generation capabilities, but these systems are still known to\nhallucinate, and granular uncertainty estimation for long-form LLM generations\nremains challenging. In this work, we propose Graph Uncertainty -- which\nrepresents the relationship between LLM generations and claims within them as a\nbipartite graph and estimates the claim-level uncertainty with a family of\ngraph centrality metrics. Under this view, existing uncertainty estimation\nmethods based on the concept of self-consistency can be viewed as using degree\ncentrality as an uncertainty measure, and we show that more sophisticated\nalternatives such as closeness centrality provide consistent gains at\nclaim-level uncertainty estimation. Moreover, we present uncertainty-aware\ndecoding techniques that leverage both the graph structure and uncertainty\nestimates to improve the factuality of LLM generations by preserving only the\nmost reliable claims. Compared to existing methods, our graph-based uncertainty\nmetrics lead to an average of 6.8% relative gains on AUPRC across various\nlong-form generation settings, and our end-to-end system provides consistent\n2-4% gains in factuality over existing decoding techniques while significantly\nimproving the informativeness of generated responses."
                },
                "authors": [
                    {
                        "name": "Mingjian Jiang"
                    },
                    {
                        "name": "Yangjun Ruan"
                    },
                    {
                        "name": "Prasanna Sattigeri"
                    },
                    {
                        "name": "Salim Roukos"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "Accepted as a Spotlight paper at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20777v1",
                "updated": "2024-10-28T06:38:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    6,
                    38,
                    24,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T06:38:24Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    6,
                    38,
                    24,
                    0,
                    302,
                    0
                ],
                "title": "KD-LoRA: A Hybrid Approach to Efficient Fine-Tuning with LoRA and\n  Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KD-LoRA: A Hybrid Approach to Efficient Fine-Tuning with LoRA and\n  Knowledge Distillation"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious downstream tasks. However, the high computational and memory\nrequirements of LLMs are a major bottleneck. To address this,\nparameter-efficient fine-tuning (PEFT) methods such as low-rank adaptation\n(LoRA) have been proposed to reduce computational costs while ensuring minimal\nloss in performance. Additionally, knowledge distillation (KD) has been a\npopular choice for obtaining compact student models from teacher models. In\nthis work, we present KD-LoRA, a novel fine-tuning method that combines LoRA\nwith KD. Our results demonstrate that KD-LoRA achieves performance comparable\nto full fine-tuning (FFT) and LoRA while significantly reducing resource\nrequirements. Specifically, KD-LoRA retains 98% of LoRA's performance on the\nGLUE benchmark, while being 40% more compact. Additionally, KD-LoRA reduces GPU\nmemory usage by 30% compared to LoRA, while decreasing inference time by 30%\ncompared to both FFT and LoRA. We evaluate KD-LoRA across three encoder-only\nmodels: BERT, RoBERTa, and DeBERTaV3. Code is available at\nhttps://github.com/rambodazimi/KD-LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious downstream tasks. However, the high computational and memory\nrequirements of LLMs are a major bottleneck. To address this,\nparameter-efficient fine-tuning (PEFT) methods such as low-rank adaptation\n(LoRA) have been proposed to reduce computational costs while ensuring minimal\nloss in performance. Additionally, knowledge distillation (KD) has been a\npopular choice for obtaining compact student models from teacher models. In\nthis work, we present KD-LoRA, a novel fine-tuning method that combines LoRA\nwith KD. Our results demonstrate that KD-LoRA achieves performance comparable\nto full fine-tuning (FFT) and LoRA while significantly reducing resource\nrequirements. Specifically, KD-LoRA retains 98% of LoRA's performance on the\nGLUE benchmark, while being 40% more compact. Additionally, KD-LoRA reduces GPU\nmemory usage by 30% compared to LoRA, while decreasing inference time by 30%\ncompared to both FFT and LoRA. We evaluate KD-LoRA across three encoder-only\nmodels: BERT, RoBERTa, and DeBERTaV3. Code is available at\nhttps://github.com/rambodazimi/KD-LoRA."
                },
                "authors": [
                    {
                        "name": "Rambod Azimi"
                    },
                    {
                        "name": "Rishav Rishav"
                    },
                    {
                        "name": "Marek Teichmann"
                    },
                    {
                        "name": "Samira Ebrahimi Kahou"
                    }
                ],
                "author_detail": {
                    "name": "Samira Ebrahimi Kahou"
                },
                "author": "Samira Ebrahimi Kahou",
                "arxiv_comment": "Accepted at 4th NeurIPS Efficient Natural Language and Speech\n  Processing Workshop (ENLSP-IV 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12038v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12038v4",
                "updated": "2024-10-28T06:38:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    6,
                    38,
                    23,
                    0,
                    302,
                    0
                ],
                "published": "2024-04-18T09:46:25Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    9,
                    46,
                    25,
                    3,
                    109,
                    0
                ],
                "title": "Uncovering Safety Risks of Large Language Models through Concept\n  Activation Vector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Safety Risks of Large Language Models through Concept\n  Activation Vector"
                },
                "summary": "Despite careful safety alignment, current large language models (LLMs) remain\nvulnerable to various attacks. To further unveil the safety risks of LLMs, we\nintroduce a Safety Concept Activation Vector (SCAV) framework, which\neffectively guides the attacks by accurately interpreting LLMs' safety\nmechanisms. We then develop an SCAV-guided attack method that can generate both\nattack prompts and embedding-level attacks with automatically selected\nperturbation hyperparameters. Both automatic and human evaluations demonstrate\nthat our attack method significantly improves the attack success rate and\nresponse quality while requiring less training data. Additionally, we find that\nour generated attack prompts may be transferable to GPT-4, and the\nembedding-level attacks may also be transferred to other white-box LLMs whose\nparameters are known. Our experiments further uncover the safety risks present\nin current LLMs. For example, in our evaluation of seven open-source LLMs, we\nobserve an average attack success rate of 99.14%, based on the classic\nkeyword-matching criterion. Finally, we provide insights into the safety\nmechanism of LLMs. The code is available at\nhttps://github.com/SproutNan/AI-Safety_SCAV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite careful safety alignment, current large language models (LLMs) remain\nvulnerable to various attacks. To further unveil the safety risks of LLMs, we\nintroduce a Safety Concept Activation Vector (SCAV) framework, which\neffectively guides the attacks by accurately interpreting LLMs' safety\nmechanisms. We then develop an SCAV-guided attack method that can generate both\nattack prompts and embedding-level attacks with automatically selected\nperturbation hyperparameters. Both automatic and human evaluations demonstrate\nthat our attack method significantly improves the attack success rate and\nresponse quality while requiring less training data. Additionally, we find that\nour generated attack prompts may be transferable to GPT-4, and the\nembedding-level attacks may also be transferred to other white-box LLMs whose\nparameters are known. Our experiments further uncover the safety risks present\nin current LLMs. For example, in our evaluation of seven open-source LLMs, we\nobserve an average attack success rate of 99.14%, based on the classic\nkeyword-matching criterion. Finally, we provide insights into the safety\nmechanism of LLMs. The code is available at\nhttps://github.com/SproutNan/AI-Safety_SCAV."
                },
                "authors": [
                    {
                        "name": "Zhihao Xu"
                    },
                    {
                        "name": "Ruixuan Huang"
                    },
                    {
                        "name": "Changyu Chen"
                    },
                    {
                        "name": "Xiting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiting Wang"
                },
                "author": "Xiting Wang",
                "arxiv_comment": "10 pages, accepted as a poster at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12038v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12038v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00024v2",
                "updated": "2024-10-28T06:30:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    6,
                    30,
                    42,
                    0,
                    302,
                    0
                ],
                "published": "2024-05-24T06:11:17Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    6,
                    11,
                    17,
                    4,
                    145,
                    0
                ],
                "title": "Embedding-Aligned Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding-Aligned Language Models"
                },
                "summary": "We propose a novel approach for training large language models (LLMs) to\nadhere to objectives defined within a latent embedding space. Our method\nleverages reinforcement learning (RL), treating a pre-trained LLM as an\nenvironment. Our embedding-aligned guided language (EAGLE) agent is trained to\niteratively steer the LLM's generation towards optimal regions of the latent\nembedding space, w.r.t. some predefined criterion. We demonstrate the\neffectiveness of the EAGLE agent using the MovieLens 25M and Amazon Review\ndatasets to surface content gaps that satisfy latent user demand. We also\ndemonstrate the benefit of using an optimal design of a state-dependent action\nset to improve EAGLE's efficiency. Our work paves the way for controlled and\ngrounded text generation using LLMs, ensuring consistency with domain-specific\nknowledge and data representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel approach for training large language models (LLMs) to\nadhere to objectives defined within a latent embedding space. Our method\nleverages reinforcement learning (RL), treating a pre-trained LLM as an\nenvironment. Our embedding-aligned guided language (EAGLE) agent is trained to\niteratively steer the LLM's generation towards optimal regions of the latent\nembedding space, w.r.t. some predefined criterion. We demonstrate the\neffectiveness of the EAGLE agent using the MovieLens 25M and Amazon Review\ndatasets to surface content gaps that satisfy latent user demand. We also\ndemonstrate the benefit of using an optimal design of a state-dependent action\nset to improve EAGLE's efficiency. Our work paves the way for controlled and\ngrounded text generation using LLMs, ensuring consistency with domain-specific\nknowledge and data representations."
                },
                "authors": [
                    {
                        "name": "Guy Tennenholtz"
                    },
                    {
                        "name": "Yinlam Chow"
                    },
                    {
                        "name": "Chih-Wei Hsu"
                    },
                    {
                        "name": "Lior Shani"
                    },
                    {
                        "name": "Ethan Liang"
                    },
                    {
                        "name": "Craig Boutilier"
                    }
                ],
                "author_detail": {
                    "name": "Craig Boutilier"
                },
                "author": "Craig Boutilier",
                "arxiv_comment": "Accepted Neurips 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20774v1",
                "updated": "2024-10-28T06:21:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    6,
                    21,
                    43,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T06:21:43Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    6,
                    21,
                    43,
                    0,
                    302,
                    0
                ],
                "title": "Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the\n  effect of Epistemic Markers on LLM-based Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the\n  effect of Epistemic Markers on LLM-based Evaluation"
                },
                "summary": "In line with the principle of honesty, there has been a growing effort to\ntrain large language models (LLMs) to generate outputs containing epistemic\nmarkers. However, evaluation in the presence of epistemic markers has been\nlargely overlooked, raising a critical question: Could the use of epistemic\nmarkers in LLM-generated outputs lead to unintended negative consequences? To\naddress this, we present EMBER, a benchmark designed to assess the robustness\nof LLM-judges to epistemic markers in both single and pairwise evaluation\nsettings. Our findings, based on evaluations using EMBER, reveal that all\ntested LLM-judges, including GPT-4o, show a notable lack of robustness in the\npresence of epistemic markers. Specifically, we observe a negative bias toward\nepistemic markers, with a stronger bias against markers expressing uncertainty.\nThis suggests that LLM-judges are influenced by the presence of these markers\nand do not focus solely on the correctness of the content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In line with the principle of honesty, there has been a growing effort to\ntrain large language models (LLMs) to generate outputs containing epistemic\nmarkers. However, evaluation in the presence of epistemic markers has been\nlargely overlooked, raising a critical question: Could the use of epistemic\nmarkers in LLM-generated outputs lead to unintended negative consequences? To\naddress this, we present EMBER, a benchmark designed to assess the robustness\nof LLM-judges to epistemic markers in both single and pairwise evaluation\nsettings. Our findings, based on evaluations using EMBER, reveal that all\ntested LLM-judges, including GPT-4o, show a notable lack of robustness in the\npresence of epistemic markers. Specifically, we observe a negative bias toward\nepistemic markers, with a stronger bias against markers expressing uncertainty.\nThis suggests that LLM-judges are influenced by the presence of these markers\nand do not focus solely on the correctness of the content."
                },
                "authors": [
                    {
                        "name": "Dongryeol Lee"
                    },
                    {
                        "name": "Yerin Hwang"
                    },
                    {
                        "name": "Yongil Kim"
                    },
                    {
                        "name": "Joonsuk Park"
                    },
                    {
                        "name": "Kyomin Jung"
                    }
                ],
                "author_detail": {
                    "name": "Kyomin Jung"
                },
                "author": "Kyomin Jung",
                "arxiv_comment": "21 pages, 6 figures, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14398v2",
                "updated": "2024-10-28T06:12:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    6,
                    12,
                    9,
                    0,
                    302,
                    0
                ],
                "published": "2024-05-23T10:15:29Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    10,
                    15,
                    29,
                    3,
                    144,
                    0
                ],
                "title": "SpGesture: Source-Free Domain-adaptive sEMG-based Gesture Recognition\n  with Jaccard Attentive Spiking Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpGesture: Source-Free Domain-adaptive sEMG-based Gesture Recognition\n  with Jaccard Attentive Spiking Neural Network"
                },
                "summary": "Surface electromyography (sEMG) based gesture recognition offers a natural\nand intuitive interaction modality for wearable devices. Despite significant\nadvancements in sEMG-based gesture-recognition models, existing methods often\nsuffer from high computational latency and increased energy consumption.\nAdditionally, the inherent instability of sEMG signals, combined with their\nsensitivity to distribution shifts in real-world settings, compromises model\nrobustness. To tackle these challenges, we propose a novel SpGesture framework\nbased on Spiking Neural Networks, which possesses several unique merits\ncompared with existing methods: (1) Robustness: By utilizing membrane potential\nas a memory list, we pioneer the introduction of Source-Free Domain Adaptation\ninto SNN for the first time. This enables SpGesture to mitigate the accuracy\ndegradation caused by distribution shifts. (2) High Accuracy: With a novel\nSpiking Jaccard Attention, SpGesture enhances the SNNs' ability to represent\nsEMG features, leading to a notable rise in system accuracy. To validate\nSpGesture's performance, we collected a new sEMG gesture dataset which has\ndifferent forearm postures, where SpGesture achieved the highest accuracy among\nthe baselines ($89.26\\%$). Moreover, the actual deployment on the CPU\ndemonstrated a system latency below 100ms, well within real-time requirements.\nThis impressive performance showcases SpGesture's potential to enhance the\napplicability of sEMG in real-world scenarios. The code is available at\nhttps://github.com/guoweiyu/SpGesture/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surface electromyography (sEMG) based gesture recognition offers a natural\nand intuitive interaction modality for wearable devices. Despite significant\nadvancements in sEMG-based gesture-recognition models, existing methods often\nsuffer from high computational latency and increased energy consumption.\nAdditionally, the inherent instability of sEMG signals, combined with their\nsensitivity to distribution shifts in real-world settings, compromises model\nrobustness. To tackle these challenges, we propose a novel SpGesture framework\nbased on Spiking Neural Networks, which possesses several unique merits\ncompared with existing methods: (1) Robustness: By utilizing membrane potential\nas a memory list, we pioneer the introduction of Source-Free Domain Adaptation\ninto SNN for the first time. This enables SpGesture to mitigate the accuracy\ndegradation caused by distribution shifts. (2) High Accuracy: With a novel\nSpiking Jaccard Attention, SpGesture enhances the SNNs' ability to represent\nsEMG features, leading to a notable rise in system accuracy. To validate\nSpGesture's performance, we collected a new sEMG gesture dataset which has\ndifferent forearm postures, where SpGesture achieved the highest accuracy among\nthe baselines ($89.26\\%$). Moreover, the actual deployment on the CPU\ndemonstrated a system latency below 100ms, well within real-time requirements.\nThis impressive performance showcases SpGesture's potential to enhance the\napplicability of sEMG in real-world scenarios. The code is available at\nhttps://github.com/guoweiyu/SpGesture/."
                },
                "authors": [
                    {
                        "name": "Weiyu Guo"
                    },
                    {
                        "name": "Ying Sun"
                    },
                    {
                        "name": "Yijie Xu"
                    },
                    {
                        "name": "Ziyue Qiao"
                    },
                    {
                        "name": "Yongkui Yang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12851v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12851v3",
                "updated": "2024-10-28T06:11:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    6,
                    11,
                    31,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-10T17:59:17Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    59,
                    17,
                    3,
                    284,
                    0
                ],
                "title": "VibeCheck: Discover and Quantify Qualitative Differences in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VibeCheck: Discover and Quantify Qualitative Differences in Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) often exhibit subtle yet distinctive\ncharacteristics in their outputs that users intuitively recognize, but struggle\nto quantify. These \"vibes\" -- such as tone, formatting, or writing style --\ninfluence user preferences, yet traditional evaluations focus primarily on the\nsingular axis of correctness. We introduce VibeCheck, a system for\nautomatically comparing a pair of LLMs by discovering identifying traits of a\nmodel (vibes) that are well-defined, differentiating, and user-aligned.\nVibeCheck iteratively discovers vibes from model outputs and then utilizes a\npanel of LLM judges to quantitatively measure the utility of each vibe. We\nvalidate that the vibes generated by VibeCheck align with those found in human\ndiscovery and run VibeCheck on pairwise preference data from real-world user\nconversations with Llama-3-70b vs GPT-4. VibeCheck reveals that Llama has a\nfriendly, funny, and somewhat controversial vibe. These vibes predict model\nidentity with 80% accuracy and human preference with 61% accuracy. Lastly, we\nrun VibeCheck on a variety of models and tasks including summarization, math,\nand captioning to provide insight into differences in model behavior. VibeCheck\ndiscovers vibes like Command X prefers to add concrete intros and conclusions\nwhen summarizing in comparison to TNGL, Llama-405b often overexplains its\nthought process on math problems compared to GPT-4o, and GPT-4 prefers to focus\non the mood and emotions of the scene when captioning compared to\nGemini-1.5-Flash. Code can be found at https://github.com/lisadunlap/VibeCheck",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often exhibit subtle yet distinctive\ncharacteristics in their outputs that users intuitively recognize, but struggle\nto quantify. These \"vibes\" -- such as tone, formatting, or writing style --\ninfluence user preferences, yet traditional evaluations focus primarily on the\nsingular axis of correctness. We introduce VibeCheck, a system for\nautomatically comparing a pair of LLMs by discovering identifying traits of a\nmodel (vibes) that are well-defined, differentiating, and user-aligned.\nVibeCheck iteratively discovers vibes from model outputs and then utilizes a\npanel of LLM judges to quantitatively measure the utility of each vibe. We\nvalidate that the vibes generated by VibeCheck align with those found in human\ndiscovery and run VibeCheck on pairwise preference data from real-world user\nconversations with Llama-3-70b vs GPT-4. VibeCheck reveals that Llama has a\nfriendly, funny, and somewhat controversial vibe. These vibes predict model\nidentity with 80% accuracy and human preference with 61% accuracy. Lastly, we\nrun VibeCheck on a variety of models and tasks including summarization, math,\nand captioning to provide insight into differences in model behavior. VibeCheck\ndiscovers vibes like Command X prefers to add concrete intros and conclusions\nwhen summarizing in comparison to TNGL, Llama-405b often overexplains its\nthought process on math problems compared to GPT-4o, and GPT-4 prefers to focus\non the mood and emotions of the scene when captioning compared to\nGemini-1.5-Flash. Code can be found at https://github.com/lisadunlap/VibeCheck"
                },
                "authors": [
                    {
                        "name": "Lisa Dunlap"
                    },
                    {
                        "name": "Krishna Mandal"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Jacob Steinhardt"
                    },
                    {
                        "name": "Joseph E Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E Gonzalez"
                },
                "author": "Joseph E Gonzalez",
                "arxiv_comment": "unironic use of the word 'vibe', now has a code link and less typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12851v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12851v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05772v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05772v5",
                "updated": "2024-10-28T06:07:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    6,
                    7,
                    37,
                    0,
                    302,
                    0
                ],
                "published": "2023-12-10T05:36:06Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    5,
                    36,
                    6,
                    6,
                    344,
                    0
                ],
                "title": "A^3-CodGen: A Repository-Level Code Generation Framework for Code Reuse\n  with Local-Aware, Global-Aware, and Third-Party-Library-Aware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A^3-CodGen: A Repository-Level Code Generation Framework for Code Reuse\n  with Local-Aware, Global-Aware, and Third-Party-Library-Aware"
                },
                "summary": "LLM-based code generation tools are essential to help developers in the\nsoftware development process. Existing tools often disconnect with the working\ncontext, i.e., the code repository, causing the generated code to be not\nsimilar to human developers. In this paper, we propose a novel code generation\nframework, dubbed A^3-CodGen, to harness information within the code repository\nto generate code with fewer potential logical errors, code redundancy, and\nlibrary-induced compatibility issues. We identify three types of representative\ninformation for the code repository: local-aware information from the current\ncode file, global-aware information from other code files, and\nthird-party-library information. Results demonstrate that by adopting the\nA^3-CodGen framework, we successfully extract, fuse, and feed code repository\ninformation into the LLM, generating more accurate, efficient, and highly\nreusable code. The effectiveness of our framework is further underscored by\ngenerating code with a higher reuse rate, compared to human developers. This\nresearch contributes significantly to the field of code generation, providing\ndevelopers with a more powerful tool to address the evolving demands in\nsoftware development in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based code generation tools are essential to help developers in the\nsoftware development process. Existing tools often disconnect with the working\ncontext, i.e., the code repository, causing the generated code to be not\nsimilar to human developers. In this paper, we propose a novel code generation\nframework, dubbed A^3-CodGen, to harness information within the code repository\nto generate code with fewer potential logical errors, code redundancy, and\nlibrary-induced compatibility issues. We identify three types of representative\ninformation for the code repository: local-aware information from the current\ncode file, global-aware information from other code files, and\nthird-party-library information. Results demonstrate that by adopting the\nA^3-CodGen framework, we successfully extract, fuse, and feed code repository\ninformation into the LLM, generating more accurate, efficient, and highly\nreusable code. The effectiveness of our framework is further underscored by\ngenerating code with a higher reuse rate, compared to human developers. This\nresearch contributes significantly to the field of code generation, providing\ndevelopers with a more powerful tool to address the evolving demands in\nsoftware development in practice."
                },
                "authors": [
                    {
                        "name": "Dianshu Liao"
                    },
                    {
                        "name": "Shidong Pan"
                    },
                    {
                        "name": "Xiaoyu Sun"
                    },
                    {
                        "name": "Xiaoxue Ren"
                    },
                    {
                        "name": "Qing Huang"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Huan Jin"
                    },
                    {
                        "name": "Qinying Li"
                    }
                ],
                "author_detail": {
                    "name": "Qinying Li"
                },
                "author": "Qinying Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05772v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05772v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.20009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.20009v2",
                "updated": "2024-10-28T05:57:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    57,
                    3,
                    0,
                    302,
                    0
                ],
                "published": "2024-03-29T06:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    29,
                    6,
                    48,
                    30,
                    4,
                    89,
                    0
                ],
                "title": "On Large Language Models' Hallucination with Regard to Known Facts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Large Language Models' Hallucination with Regard to Known Facts"
                },
                "summary": "Large language models are successful in answering factoid questions but are\nalso prone to hallucination. We investigate the phenomenon of LLMs possessing\ncorrect answer knowledge yet still hallucinating from the perspective of\ninference dynamics, an area not previously covered in studies on\nhallucinations. We are able to conduct this analysis via two key ideas. First,\nwe identify the factual questions that query the same triplet knowledge but\nresult in different answers. The difference between the model behaviors on the\ncorrect and incorrect outputs hence suggests the patterns when hallucinations\nhappen. Second, to measure the pattern, we utilize mappings from the residual\nstreams to vocabulary space. We reveal the different dynamics of the output\ntoken probabilities along the depths of layers between the correct and\nhallucinated cases. In hallucinated cases, the output token's information\nrarely demonstrates abrupt increases and consistent superiority in the later\nstages of the model. Leveraging the dynamic curve as a feature, we build a\nclassifier capable of accurately detecting hallucinatory predictions with an\n88\\% success rate. Our study shed light on understanding the reasons for LLMs'\nhallucinations on their known facts, and more importantly, on accurately\npredicting when they are hallucinating.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are successful in answering factoid questions but are\nalso prone to hallucination. We investigate the phenomenon of LLMs possessing\ncorrect answer knowledge yet still hallucinating from the perspective of\ninference dynamics, an area not previously covered in studies on\nhallucinations. We are able to conduct this analysis via two key ideas. First,\nwe identify the factual questions that query the same triplet knowledge but\nresult in different answers. The difference between the model behaviors on the\ncorrect and incorrect outputs hence suggests the patterns when hallucinations\nhappen. Second, to measure the pattern, we utilize mappings from the residual\nstreams to vocabulary space. We reveal the different dynamics of the output\ntoken probabilities along the depths of layers between the correct and\nhallucinated cases. In hallucinated cases, the output token's information\nrarely demonstrates abrupt increases and consistent superiority in the later\nstages of the model. Leveraging the dynamic curve as a feature, we build a\nclassifier capable of accurately detecting hallucinatory predictions with an\n88\\% success rate. Our study shed light on understanding the reasons for LLMs'\nhallucinations on their known facts, and more importantly, on accurately\npredicting when they are hallucinating."
                },
                "authors": [
                    {
                        "name": "Che Jiang"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Xiangyu Hong"
                    },
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Yang Cheng"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Bowen Zhou"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "arxiv_comment": "Accepted by NAACL 2024 MainConference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.20009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.20009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20763v1",
                "updated": "2024-10-28T05:56:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    56,
                    51,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T05:56:51Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    56,
                    51,
                    0,
                    302,
                    0
                ],
                "title": "Evaluating LLMs for Targeted Concept Simplification forDomain-Specific\n  Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs for Targeted Concept Simplification forDomain-Specific\n  Texts"
                },
                "summary": "One useful application of NLP models is to support people in reading complex\ntext from unfamiliar domains (e.g., scientific articles). Simplifying the\nentire text makes it understandable but sometimes removes important details. On\nthe contrary, helping adult readers understand difficult concepts in context\ncan enhance their vocabulary and knowledge. In a preliminary human study, we\nfirst identify that lack of context and unfamiliarity with difficult concepts\nis a major reason for adult readers' difficulty with domain-specific text. We\nthen introduce \"targeted concept simplification,\" a simplification task for\nrewriting text to help readers comprehend text containing unfamiliar concepts.\nWe also introduce WikiDomains, a new dataset of 22k definitions from 13\nacademic domains paired with a difficult concept within each definition. We\nbenchmark the performance of open-source and commercial LLMs and a simple\ndictionary baseline on this task across human judgments of ease of\nunderstanding and meaning preservation. Interestingly, our human judges\npreferred explanations about the difficult concept more than simplification of\nthe concept phrase. Further, no single model achieved superior performance\nacross all quality dimensions, and automated metrics also show low correlations\nwith human evaluations of concept simplification ($\\sim0.2$), opening up rich\navenues for research on personalized human reading comprehension support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One useful application of NLP models is to support people in reading complex\ntext from unfamiliar domains (e.g., scientific articles). Simplifying the\nentire text makes it understandable but sometimes removes important details. On\nthe contrary, helping adult readers understand difficult concepts in context\ncan enhance their vocabulary and knowledge. In a preliminary human study, we\nfirst identify that lack of context and unfamiliarity with difficult concepts\nis a major reason for adult readers' difficulty with domain-specific text. We\nthen introduce \"targeted concept simplification,\" a simplification task for\nrewriting text to help readers comprehend text containing unfamiliar concepts.\nWe also introduce WikiDomains, a new dataset of 22k definitions from 13\nacademic domains paired with a difficult concept within each definition. We\nbenchmark the performance of open-source and commercial LLMs and a simple\ndictionary baseline on this task across human judgments of ease of\nunderstanding and meaning preservation. Interestingly, our human judges\npreferred explanations about the difficult concept more than simplification of\nthe concept phrase. Further, no single model achieved superior performance\nacross all quality dimensions, and automated metrics also show low correlations\nwith human evaluations of concept simplification ($\\sim0.2$), opening up rich\navenues for research on personalized human reading comprehension support."
                },
                "authors": [
                    {
                        "name": "Sumit Asthana"
                    },
                    {
                        "name": "Hannah Rashkin"
                    },
                    {
                        "name": "Elizabeth Clark"
                    },
                    {
                        "name": "Fantine Huot"
                    },
                    {
                        "name": "Mirella Lapata"
                    }
                ],
                "author_detail": {
                    "name": "Mirella Lapata"
                },
                "author": "Mirella Lapata",
                "arxiv_comment": "to appear in proceedings of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10644v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10644v2",
                "updated": "2024-10-28T05:51:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    51,
                    46,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-16T18:20:38Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    20,
                    38,
                    0,
                    260,
                    0
                ],
                "title": "Improving Multi-candidate Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Multi-candidate Speculative Decoding"
                },
                "summary": "Speculative Decoding (SD) is a technique to accelerate the inference of Large\nLanguage Models (LLMs) by using a lower complexity draft model to propose\ncandidate tokens verified by a larger target model. To further improve\nefficiency, Multi-Candidate Speculative Decoding (MCSD) improves upon this by\nsampling multiple candidate tokens from the draft model at each step and\nverifying them in parallel, thus increasing the chances of accepting a token\nand reducing generation time. Existing MCSD methods rely on the draft model to\ninitialize the multi-candidate sequences and use static length and tree\nattention structure for draft generation. However, such an approach suffers\nfrom the draft and target model's output distribution differences, especially\nin a dynamic generation context. In this work, we introduce a new version of\nMCSD that includes a target model initialized multi-candidate generation, a\ndynamic sliced topology-aware causal mask for dynamic length adjustment, and\ndecision models to optimize early stopping. We experimented with our method on\nLlama 2-7B and its variants and observed a maximum 27.5% speedup compared to\nour MCSD baseline across three benchmarks with Llama 2-7B as the target model\nand JackFram 68M as the draft model. Additionally, we evaluate the effects of\nusing the target model initialized multi-candidate process with different draft\nmodels on output quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative Decoding (SD) is a technique to accelerate the inference of Large\nLanguage Models (LLMs) by using a lower complexity draft model to propose\ncandidate tokens verified by a larger target model. To further improve\nefficiency, Multi-Candidate Speculative Decoding (MCSD) improves upon this by\nsampling multiple candidate tokens from the draft model at each step and\nverifying them in parallel, thus increasing the chances of accepting a token\nand reducing generation time. Existing MCSD methods rely on the draft model to\ninitialize the multi-candidate sequences and use static length and tree\nattention structure for draft generation. However, such an approach suffers\nfrom the draft and target model's output distribution differences, especially\nin a dynamic generation context. In this work, we introduce a new version of\nMCSD that includes a target model initialized multi-candidate generation, a\ndynamic sliced topology-aware causal mask for dynamic length adjustment, and\ndecision models to optimize early stopping. We experimented with our method on\nLlama 2-7B and its variants and observed a maximum 27.5% speedup compared to\nour MCSD baseline across three benchmarks with Llama 2-7B as the target model\nand JackFram 68M as the draft model. Additionally, we evaluate the effects of\nusing the target model initialized multi-candidate process with different draft\nmodels on output quality."
                },
                "authors": [
                    {
                        "name": "Xiaofan Lu"
                    },
                    {
                        "name": "Yixiao Zeng"
                    },
                    {
                        "name": "Feiyang Ma"
                    },
                    {
                        "name": "Zixu Yu"
                    },
                    {
                        "name": "Marco Levorato"
                    }
                ],
                "author_detail": {
                    "name": "Marco Levorato"
                },
                "author": "Marco Levorato",
                "arxiv_comment": "Accepted by NeurIPS ENLSP 2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10644v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10644v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20755v1",
                "updated": "2024-10-28T05:42:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    42,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T05:42:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    42,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Provisioning for Solar-Powered Base Stations Driven by Conditional LSTM\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provisioning for Solar-Powered Base Stations Driven by Conditional LSTM\n  Networks"
                },
                "summary": "Solar-powered base stations are a promising approach to sustainable\ntelecommunications infrastructure. However, the successful deployment of\nsolar-powered base stations requires precise prediction of the energy harvested\nby photovoltaic (PV) panels vs. anticipated energy expenditure in order to\nachieve affordable yet reliable deployment and operation. This paper introduces\nan innovative approach to predict energy harvesting by utilizing a novel\nconditional Long Short-Term Memory (Cond-LSTM) neural network architecture.\nCompared with LSTM and Transformer models, the Cond-LSTM model reduced the\nnormalized root mean square error (nRMSE) by 69.6% and 42.7%, respectively. We\nalso demonstrate the generalizability of our model across different scenarios.\nThe proposed approach would not only facilitate an accurate cost-optimal\nPV-battery configuration that meets the outage probability requirements, but\nalso help with site design in regions that lack historical solar energy data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solar-powered base stations are a promising approach to sustainable\ntelecommunications infrastructure. However, the successful deployment of\nsolar-powered base stations requires precise prediction of the energy harvested\nby photovoltaic (PV) panels vs. anticipated energy expenditure in order to\nachieve affordable yet reliable deployment and operation. This paper introduces\nan innovative approach to predict energy harvesting by utilizing a novel\nconditional Long Short-Term Memory (Cond-LSTM) neural network architecture.\nCompared with LSTM and Transformer models, the Cond-LSTM model reduced the\nnormalized root mean square error (nRMSE) by 69.6% and 42.7%, respectively. We\nalso demonstrate the generalizability of our model across different scenarios.\nThe proposed approach would not only facilitate an accurate cost-optimal\nPV-battery configuration that meets the outage probability requirements, but\nalso help with site design in regions that lack historical solar energy data."
                },
                "authors": [
                    {
                        "name": "Yawen Guo"
                    },
                    {
                        "name": "Sonia Naderi"
                    },
                    {
                        "name": "Colleen Josephson"
                    }
                ],
                "author_detail": {
                    "name": "Colleen Josephson"
                },
                "author": "Colleen Josephson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02469v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02469v3",
                "updated": "2024-10-28T05:39:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    39,
                    23,
                    0,
                    302,
                    0
                ],
                "published": "2023-10-03T22:37:01Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    22,
                    37,
                    1,
                    1,
                    276,
                    0
                ],
                "title": "PrivacyMind: Large Language Models Can Be Contextual Privacy Protection\n  Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrivacyMind: Large Language Models Can Be Contextual Privacy Protection\n  Learners"
                },
                "summary": "The proliferation of Large Language Models (LLMs) has driven considerable\ninterest in fine-tuning them with domain-specific data to create specialized\nlanguage models. Nevertheless, such domain-specific fine-tuning data often\ncontains contextually sensitive personally identifiable information (PII).\nDirect fine-tuning of LLMs on this data without privacy protection poses a risk\nof data leakage of sensitive PII during inference time. To address this\nchallenge, we introduce Contextual Privacy Protection Language Models\n(PrivacyMind), a novel paradigm for fine-tuning LLMs that effectively injects\ndomain-specific knowledge while safeguarding inference-time data privacy. Our\nwork offers a theoretical analysis for model design and benchmarks various\ntechniques such as corpus curation, penalty-based unlikelihood in training\nloss, instruction-based tuning, etc. Extensive experiments across diverse\ndatasets and scenarios demonstrate the effectiveness of our approaches. In\nparticular, instruction tuning with both positive and negative examples stands\nout as a promising method, effectively protecting private data while enhancing\nthe model's knowledge. Our work underscores the potential for Large Language\nModels as robust contextual privacy protection learners. The complete code and\ndata for the work can be found at https://github.com/Yijia-Xiao/PrivacyMind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of Large Language Models (LLMs) has driven considerable\ninterest in fine-tuning them with domain-specific data to create specialized\nlanguage models. Nevertheless, such domain-specific fine-tuning data often\ncontains contextually sensitive personally identifiable information (PII).\nDirect fine-tuning of LLMs on this data without privacy protection poses a risk\nof data leakage of sensitive PII during inference time. To address this\nchallenge, we introduce Contextual Privacy Protection Language Models\n(PrivacyMind), a novel paradigm for fine-tuning LLMs that effectively injects\ndomain-specific knowledge while safeguarding inference-time data privacy. Our\nwork offers a theoretical analysis for model design and benchmarks various\ntechniques such as corpus curation, penalty-based unlikelihood in training\nloss, instruction-based tuning, etc. Extensive experiments across diverse\ndatasets and scenarios demonstrate the effectiveness of our approaches. In\nparticular, instruction tuning with both positive and negative examples stands\nout as a promising method, effectively protecting private data while enhancing\nthe model's knowledge. Our work underscores the potential for Large Language\nModels as robust contextual privacy protection learners. The complete code and\ndata for the work can be found at https://github.com/Yijia-Xiao/PrivacyMind."
                },
                "authors": [
                    {
                        "name": "Yijia Xiao"
                    },
                    {
                        "name": "Yiqiao Jin"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Xianjun Yang"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Wenchao Yu"
                    },
                    {
                        "name": "Xujiang Zhao"
                    },
                    {
                        "name": "Yanchi Liu"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Haifeng Chen"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Cheng"
                },
                "author": "Wei Cheng",
                "arxiv_comment": "Accepted at EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02469v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02469v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05695v2",
                "updated": "2024-10-28T05:38:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    38,
                    46,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-08T05:26:28Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    5,
                    26,
                    28,
                    1,
                    282,
                    0
                ],
                "title": "Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to\n  Quantify and Optimize Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to\n  Quantify and Optimize Chain-of-Thought"
                },
                "summary": "Chain-of-Thought (CoT) reasoning has emerged as a promising approach for\nenhancing the performance of large language models (LLMs) on complex reasoning\ntasks. Recently, a series of studies attempt to explain the mechanisms\nunderlying CoT, aiming to deepen the understanding of its efficacy.\nNevertheless, the existing research faces two major challenges: (1) a lack of\nquantitative metrics to assess CoT capabilities and (2) a dearth of guidance on\noptimizing CoT performance. Motivated by this, in this work, we introduce a\nnovel reasoning boundary framework (RBF) to address these challenges. To solve\nthe lack of quantification, we first define a reasoning boundary (RB) to\nquantify the upper-bound of CoT and establish a combination law for RB,\nenabling a practical quantitative approach applicable to various real-world CoT\ntasks. To address the lack of optimization, we propose three categories of RBs.\nWe further optimize these categories with combination laws focused on RB\npromotion and reasoning path optimization for CoT improvement. Through\nextensive experiments on 27 models and 5 tasks, the study validates the\nexistence and rationality of the proposed framework. Furthermore, it explains\nthe effectiveness of 10 CoT strategies and guides optimization from two\nperspectives. We hope this work can provide a comprehensive understanding of\nthe boundaries and optimization strategies for reasoning in LLMs. Our code and\ndata are available at https://github.com/LightChen233/reasoning-boundary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning has emerged as a promising approach for\nenhancing the performance of large language models (LLMs) on complex reasoning\ntasks. Recently, a series of studies attempt to explain the mechanisms\nunderlying CoT, aiming to deepen the understanding of its efficacy.\nNevertheless, the existing research faces two major challenges: (1) a lack of\nquantitative metrics to assess CoT capabilities and (2) a dearth of guidance on\noptimizing CoT performance. Motivated by this, in this work, we introduce a\nnovel reasoning boundary framework (RBF) to address these challenges. To solve\nthe lack of quantification, we first define a reasoning boundary (RB) to\nquantify the upper-bound of CoT and establish a combination law for RB,\nenabling a practical quantitative approach applicable to various real-world CoT\ntasks. To address the lack of optimization, we propose three categories of RBs.\nWe further optimize these categories with combination laws focused on RB\npromotion and reasoning path optimization for CoT improvement. Through\nextensive experiments on 27 models and 5 tasks, the study validates the\nexistence and rationality of the proposed framework. Furthermore, it explains\nthe effectiveness of 10 CoT strategies and guides optimization from two\nperspectives. We hope this work can provide a comprehensive understanding of\nthe boundaries and optimization strategies for reasoning in LLMs. Our code and\ndata are available at https://github.com/LightChen233/reasoning-boundary."
                },
                "authors": [
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Jinxuan Zhou"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Accepted at NeurIPS 2024 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.17122v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.17122v4",
                "updated": "2024-10-28T05:38:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    38,
                    29,
                    0,
                    302,
                    0
                ],
                "published": "2023-12-28T16:59:06Z",
                "published_parsed": [
                    2023,
                    12,
                    28,
                    16,
                    59,
                    6,
                    3,
                    362,
                    0
                ],
                "title": "LLM4Causal: Democratized Causal Tools for Everyone via Large Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Causal: Democratized Causal Tools for Everyone via Large Language\n  Model"
                },
                "summary": "Large Language Models (LLMs) have shown their success in language\nunderstanding and reasoning on general topics. However, their capability to\nperform inference based on user-specified structured data and knowledge in\ncorpus-rare concepts, such as causal decision-making is still limited. In this\nwork, we explore the possibility of fine-tuning an open-sourced LLM into\nLLM4Causal, which can identify the causal task, execute a corresponding\nfunction, and interpret its numerical results based on users' queries and the\nprovided dataset. Meanwhile, we propose a data generation process for more\ncontrollable GPT prompting and present two instruction-tuning datasets: (1)\nCausal-Retrieval-Bench for causal problem identification and input parameter\nextraction for causal function calling and (2) Causal-Interpret-Bench for\nin-context causal interpretation. By conducting end-to-end evaluations and two\nablation studies, we showed that LLM4Causal can deliver end-to-end solutions\nfor causal problems and provide easy-to-understand answers, which significantly\noutperforms the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown their success in language\nunderstanding and reasoning on general topics. However, their capability to\nperform inference based on user-specified structured data and knowledge in\ncorpus-rare concepts, such as causal decision-making is still limited. In this\nwork, we explore the possibility of fine-tuning an open-sourced LLM into\nLLM4Causal, which can identify the causal task, execute a corresponding\nfunction, and interpret its numerical results based on users' queries and the\nprovided dataset. Meanwhile, we propose a data generation process for more\ncontrollable GPT prompting and present two instruction-tuning datasets: (1)\nCausal-Retrieval-Bench for causal problem identification and input parameter\nextraction for causal function calling and (2) Causal-Interpret-Bench for\nin-context causal interpretation. By conducting end-to-end evaluations and two\nablation studies, we showed that LLM4Causal can deliver end-to-end solutions\nfor causal problems and provide easy-to-understand answers, which significantly\noutperforms the baselines."
                },
                "authors": [
                    {
                        "name": "Haitao Jiang"
                    },
                    {
                        "name": "Lin Ge"
                    },
                    {
                        "name": "Yuhe Gao"
                    },
                    {
                        "name": "Jianian Wang"
                    },
                    {
                        "name": "Rui Song"
                    }
                ],
                "author_detail": {
                    "name": "Rui Song"
                },
                "author": "Rui Song",
                "arxiv_comment": "Accepted by COLM2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.17122v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.17122v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20749v1",
                "updated": "2024-10-28T05:28:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    28,
                    51,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T05:28:51Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    28,
                    51,
                    0,
                    302,
                    0
                ],
                "title": "Matryoshka: Learning to Drive Black-Box LLMs with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matryoshka: Learning to Drive Black-Box LLMs with LLMs"
                },
                "summary": "Despite the impressive generative abilities of black-box large language\nmodels (LLMs), their inherent opacity hinders further advancements in\ncapabilities such as reasoning, planning, and personalization. Existing works\naim to enhance LLM capabilities via domain-specific adaptation or in-context\nlearning, which require additional training on accessible model parameters, an\ninfeasible option for black-box LLMs. To address this challenge, we introduce\nMatryoshika, a lightweight white-box LLM controller that guides a large-scale\nblack-box LLM generator by decomposing complex tasks into a series of\nintermediate outputs. Specifically, we consider the black-box LLM as an\nenvironment, with Matryoshika serving as a policy to provide intermediate\nguidance through prompts for driving the black-box LLM. Matryoshika is trained\nto pivot the outputs of the black-box LLM aligning with preferences during\niterative interaction, which enables controllable multi-turn generation and\nself-improvement in optimizing intermediate guidance. Empirical evaluations on\nthree diverse tasks demonstrate that Matryoshika effectively enhances the\ncapabilities of black-box LLMs in complex, long-horizon tasks, including\nreasoning, planning, and personalization. By leveraging this pioneering\ncontroller-generator framework to mitigate dependence on model parameters,\nMatryoshika provides a transparent and practical solution for improving\nblack-box LLMs through controllable multi-turn generation using white-box LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the impressive generative abilities of black-box large language\nmodels (LLMs), their inherent opacity hinders further advancements in\ncapabilities such as reasoning, planning, and personalization. Existing works\naim to enhance LLM capabilities via domain-specific adaptation or in-context\nlearning, which require additional training on accessible model parameters, an\ninfeasible option for black-box LLMs. To address this challenge, we introduce\nMatryoshika, a lightweight white-box LLM controller that guides a large-scale\nblack-box LLM generator by decomposing complex tasks into a series of\nintermediate outputs. Specifically, we consider the black-box LLM as an\nenvironment, with Matryoshika serving as a policy to provide intermediate\nguidance through prompts for driving the black-box LLM. Matryoshika is trained\nto pivot the outputs of the black-box LLM aligning with preferences during\niterative interaction, which enables controllable multi-turn generation and\nself-improvement in optimizing intermediate guidance. Empirical evaluations on\nthree diverse tasks demonstrate that Matryoshika effectively enhances the\ncapabilities of black-box LLMs in complex, long-horizon tasks, including\nreasoning, planning, and personalization. By leveraging this pioneering\ncontroller-generator framework to mitigate dependence on model parameters,\nMatryoshika provides a transparent and practical solution for improving\nblack-box LLMs through controllable multi-turn generation using white-box LLMs."
                },
                "authors": [
                    {
                        "name": "Changhao Li"
                    },
                    {
                        "name": "Yuchen Zhuang"
                    },
                    {
                        "name": "Rushi Qiang"
                    },
                    {
                        "name": "Haotian Sun"
                    },
                    {
                        "name": "Hanjun Dai"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Bo Dai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Dai"
                },
                "author": "Bo Dai",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20745v1",
                "updated": "2024-10-28T05:25:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    25,
                    47,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T05:25:47Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    25,
                    47,
                    0,
                    302,
                    0
                ],
                "title": "Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large\n  Language Models"
                },
                "summary": "Online shopping is a complex multi-task, few-shot learning problem with a\nwide and evolving range of entities, relations, and tasks. However, existing\nmodels and benchmarks are commonly tailored to specific tasks, falling short of\ncapturing the full complexity of online shopping. Large Language Models (LLMs),\nwith their multi-task and few-shot learning abilities, have the potential to\nprofoundly transform online shopping by alleviating task-specific engineering\nefforts and by providing users with interactive conversations. Despite the\npotential, LLMs face unique challenges in online shopping, such as\ndomain-specific concepts, implicit knowledge, and heterogeneous user behaviors.\nMotivated by the potential and challenges, we propose Shopping MMLU, a diverse\nmulti-task online shopping benchmark derived from real-world Amazon data.\nShopping MMLU consists of 57 tasks covering 4 major shopping skills: concept\nunderstanding, knowledge reasoning, user behavior alignment, and\nmulti-linguality, and can thus comprehensively evaluate the abilities of LLMs\nas general shop assistants. With Shopping MMLU, we benchmark over 20 existing\nLLMs and uncover valuable insights about practices and prospects of building\nversatile LLM-based shop assistants. Shopping MMLU can be publicly accessed at\nhttps://github.com/KL4805/ShoppingMMLU. In addition, with Shopping MMLU, we\nhost a competition in KDD Cup 2024 with over 500 participating teams. The\nwinning solutions and the associated workshop can be accessed at our website\nhttps://amazon-kddcup24.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online shopping is a complex multi-task, few-shot learning problem with a\nwide and evolving range of entities, relations, and tasks. However, existing\nmodels and benchmarks are commonly tailored to specific tasks, falling short of\ncapturing the full complexity of online shopping. Large Language Models (LLMs),\nwith their multi-task and few-shot learning abilities, have the potential to\nprofoundly transform online shopping by alleviating task-specific engineering\nefforts and by providing users with interactive conversations. Despite the\npotential, LLMs face unique challenges in online shopping, such as\ndomain-specific concepts, implicit knowledge, and heterogeneous user behaviors.\nMotivated by the potential and challenges, we propose Shopping MMLU, a diverse\nmulti-task online shopping benchmark derived from real-world Amazon data.\nShopping MMLU consists of 57 tasks covering 4 major shopping skills: concept\nunderstanding, knowledge reasoning, user behavior alignment, and\nmulti-linguality, and can thus comprehensively evaluate the abilities of LLMs\nas general shop assistants. With Shopping MMLU, we benchmark over 20 existing\nLLMs and uncover valuable insights about practices and prospects of building\nversatile LLM-based shop assistants. Shopping MMLU can be publicly accessed at\nhttps://github.com/KL4805/ShoppingMMLU. In addition, with Shopping MMLU, we\nhost a competition in KDD Cup 2024 with over 500 participating teams. The\nwinning solutions and the associated workshop can be accessed at our website\nhttps://amazon-kddcup24.github.io/."
                },
                "authors": [
                    {
                        "name": "Yilun Jin"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Chenwei Zhang"
                    },
                    {
                        "name": "Tianyu Cao"
                    },
                    {
                        "name": "Yifan Gao"
                    },
                    {
                        "name": "Pratik Jayarao"
                    },
                    {
                        "name": "Mao Li"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Ritesh Sarkhel"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Haodong Wang"
                    },
                    {
                        "name": "Zhengyang Wang"
                    },
                    {
                        "name": "Wenju Xu"
                    },
                    {
                        "name": "Jingfeng Yang"
                    },
                    {
                        "name": "Qingyu Yin"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Priyanka Nigam"
                    },
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Qiang Yang"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Bing Yin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Yin"
                },
                "author": "Bing Yin",
                "arxiv_comment": "NeurIPS 2024 Datasets and Benchmarks Track Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]